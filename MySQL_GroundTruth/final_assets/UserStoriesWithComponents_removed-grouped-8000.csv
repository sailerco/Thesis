ID,Description,Type,Component_Names
271,"""Currently, if a module in a stream throws an exception, the stream flow is broken, but the stream's creator is not notified or warned about the issue, unless a direct monitoring of the logs is performed.  This could work when an administrator is taking care of the whole life cycle of any stream, but it doesn't work when Spring XD is being used by external services with no human intervention.  It would be necessary some sort of notification mechanism so when any issue raises in the execution of a stream, its creator can react and decide on how to handle it.  For instance, I have an application that uses Spring XD to load data from different sources into HDFS. In my application, you simply define a type of source and the destiny in HDFS, without the user knowing that Spring XD is under the hoods. If the user defines, for example, a Twitter source, then I define a """"twitterstream"""" as a source for a new stream with a sink in HDFS. However, if anything goes wrong, I have no way to let my user know about the issue: an authentication issue, the dreaded 420 response from the Twitter API, etc. If I could somehow be notified about the issue, then I could provide that information to my user so she could fix the issue or at least understand what is going on.  I've seen that the REST API will support an endpoint to check the current status of a stream, but it is not yet available and it would require my application to be polling constantly the stream, with all the disadvantages compared to a pushing mechanism.""",Story,"Stream Module,Runtime"
342,"""h2. Narrative As an XD developer, I need to be able to create a composed job module as XML from the DSL an store it in the Module File repository.  While the user uses the composed job as if it is a normal job including seeing only the DSL.  In the background the JobFactory will deploy the composed job module.   * When the user destroys the job the module will be deleted from the file module repository. * When the user creates the job a module will be created in the file Module repository. h2. Back story For the composed job story, we need to create a """"real"""" job module to be expressed in XML, so that we can take advantage of the job execution tasklet in XD-3556, so that each job can be executed as a step in the composed job.""",Story,Batch
385,"""h2. Narrative As a user I need to be able to query the current state of a task that has been launched.  h2. Back story Given the fact that tasks are intended to go away, we need to record the state of them as well as their end result in a repository for a user to be able to query.  This repository will be the system of record when reporting the state of an executing/executed task.""",Story,Batch
411,"""h2. Narrative As a user, I need to be able to deploy a task (boot jar) via the CLI.  h2.  Back story Since the concept of jobs as an explicit primitive within Spring XD is going away in spring-cloud-data, the shell needs to be updated to reflect that.""",Story,CLI
526,"""h2.  Narrarive As a developer, I need to be able to test modules without pushing them to a remote maven repository.  I should be able to do {{$ mvn install}} in my module project locally (which will install the artifact into my local repository) and have it resolvable by spring-cloud-streams.""",Improvement,Stream Module
533,"""h2. Narrative As a developer, I need to be able to run batch jobs that use the centrally configured job repository to store job state.  h2. Back story The XD containers each used a {{BatchConfigurer}} implementation ({{RuntimeBatchConfigurer}}) to add a consistent configuration for the job repository.  This functionality needs to be replicated in some way in just a regular Spring Boot application.""",Story,Batch
534,"""h2. Narrative As a developer, I need to be able to create a partitioned batch job that uses Diego Tasks for partition slaves.  h2. Back story A new partition handler should be created that uses the Receptor API to launch tasks for each of the slaves (configurable via grid size).""",Story,Batch
536,"""h2. Narrative As a developer, when running a batch job on Diego/Lattice as a task, I want to be able to receive events based on the job lifecycle.  h2. Back story XD 1.x provides exposure to all of the main listeners Spring Batch supports via streams so they can be listened to.  This needs to be supported in the new spring-cloud-streams model.  *Note:* We may want this to depend on XD-2841?""",Story,Batch
537,"""h2. Narrative As a developer, I'd like to be able to run a boot jar as a task on CF and obtain the result reliably.  h2. Back story Currently Lattice/Diego's tasks implementation provides the ability to run things as short lived tasks.  However, obtaining the result of said task can be an issue.  There are two ways to do so:  # Poll for the result. # Register a callback URL to be called once the task completes.  Since a task is only available for a short time after its completion before it is deleted, polling can run the risk of missing the result completely.  When you consider the fact that the provided GUIDs that identify tasks can be re-used polling becomes a precarious option.  Registering a callback URL would be a better option, however there are no good guarantees that the message will be delivered.  The service will try to execute the callback until it's successful or the task is cleaned up.  """"Successful"""" is defined in this case as anything other than a 502 or a 503 return code.  In order for Spring XD to be able to support Diego tasks, a more durable option for maintaining the result of tasks will need to be developed.  *Note:* The outcome of this spike may be feature requests for the CF/Diego team.""",Story,Batch
538,"""h2. Narrative As a developer, I'd like to be able to configure common dependencies for the entire environment.  An example could be that I use MySql for my databases.  I want to be able to configure the MySql driver once and have all modules use it.  h2. Back story Spring Batch uses a database to store job state (the job repository).  This is a shared resource across all jobs (both custom developed and OOTB).  In order to support OOTB jobs, we'll need to have a way for users to provide the db driver to each module.  Ideally this would be possible without requiring that each of our OOTB modules be repackaged. """,Story,Packaging
616,"""As a user, I'd like to have the option to specify system properties that will be passed in to the Sqoop job which runs in it's own Java process. This is needed for defining memory usage and also for defining some options for various connector implementations.""",Story,Batch
667,"""As a developer, I want to be able to override Kafka bus defaults for module consumers and producers, so that I can finely tune performance and behaviour.   Such properties should include - autoCommitEnabled,queueSize,maxWait,fetchSize for consumers - batchSize,batchTimeout for producers""",Story,Runtime
685,"""h2. Narrative As a developer, when using jdbchdfs's incremental imports, I need to be notified that something went wrong in a previous run of the jdbchdfs job so that I can take the appropriate actions based on the data previously imported.  h2.  Back story As the incremental import currently works, if the job fails half way through, there is no check on the next run to see if the last run failed or not and how to address partially imported data.""",Story,Batch
768,"""Polled message sources return only one message per poll by default.  When polling, say, a file directory with many files, files will be emitted once per {{fixedDelay}}.  As a user I need to configure a limit for the number of messages that will be emitted per poll.""",Story,Stream Module
809,"""As a user I would like the ability to undeploy or suspend a module without losing the deployment properties.  Currently when temporarily suspending a module an undeploy and redeploy is executed.  During the redeploy the deployment properties need to be added again.  Instead, it would be nice if the properties are persisted so they automatically included with the deployment.""",Improvement,Configuration
883,"""As a user I want to be able to provide my own RowMapper<Tuple> implementation to enrich the jdbc data.   My use case requires me to add timestamp field and a delete flag field to records before they get written to HDFS. To do it, I have to implement a ItemReaderFactory and perhaps extend NameColumnJdbcItemReader. This is to override the afterPropertySet method to change the default implementation.  Otherwise I have to write my own Processor that can add these fields to Tuples, and since tuples are immutable I would have to recreate the tuples with additional fields in the processor. For large load this could be big overhead.  I would love to know any other technique to implement such a use case.""",Story,Ingest
890,"""As a user, I need to use XD Sqoop module to support the merge command. Currently, the SqoopRunner createFinalArguments method forces the requirement for connect, username and password options which are not valid for the merge option. A check of the module type to not force these options being assigned to sqoop arg list would be preferred""",Technical task,Batch
945,"""As a user, I'd like to have an option to have the hdfs sink not use a temporary inUseSuffix like .tmp. Instead we should write using the filename specified directly. This could be useful if we use """"Syncable"""" writes and the sink fails while the file is open. Without this new option the user would have to explicitly rename the file.""",Story,"Stream Module,Hadoop"
946,"""As a user, I'd like to have an option to have the hdfs sink use """"Syncable"""" writes to provide better resiliency in the case of sink/container failures. I'm willing to accept the performance penalty if I choose this option. """,Story,"Stream Module,Hadoop"
1034,"""As a developer, I'd like to add a mongodb source using an xml and a property file supporting mixing in of parameters so that I can use this module to ingest data from Mongo.""",Improvement,Stream Module
1296,"""As a user, I'd like to refer to the documentation so that I can connect to Sqoop as recommended and create job definition based on the exposed _metadata_ options. """,Technical task,Batch
1637,"""As a user, I want to know how to enable and configure LDAP as an authentication provider for the administration server, so that I can set up my security configuration accordingly.""",Technical task,Documentation
1712,"""As a user, I'd like to have the option to write into _Kafka_ sink so that I can publish mass data into Kafka broker.""",Story,Stream Module
1732,"""h3.  Narrative As a developer, I need to be able to configure a partitioned job's grid size so that the correct number of partitions are created (the current code is hard coded to 1 for the grid size).  h3.  Acceptance Criteria # Expose the gridSize attribute of the {{MessageChannelPartitionHandler}} as an option.  h3.  Assumptions # Existing OOTB jobs should not be impacted by this given they don't use the grid size. """,Bug,Batch
1906,"""As a minimum we need some common polling strategy on the client side to detect status changes of job + streams etc. (E.g. during deployment of streams/jobs)  Ideally, I would like to have this addressed on the server-side as well. It would be nice if we could propagate events between, containers and admin-server that would inform about any changes in the system. We could then use those to notify connected UI clients.""",Story,"Runtime,UI,REST"
1925,"""h2. Narrative As a developer, I need to be able to process the importing of files in parallel via the hdfsjdbc batch job.  h2. Acceptance Criteria # Be able to provide a list of files to the job and have them be read in parallel based on the number of slaves deployed. # Use {{MultiResourcePartitioner}} to create on partition per incoming file.""",Improvement,Batch
1930,"""h2. Narrative As a developer, I need to be able to process the importing of files in parallel via the filejdbc batch job.  h2. Acceptance Criteria # Be able to provide a list of files to the job and have them be read in parallel based on the number of slaves deployed. # Use {{MultiResourcePartitioner}} to create on partition per incoming file.""",Improvement,Batch
2675,"""As a Spring XD user I need to listen on a JMS Topic and ingest the messages, so I can process the messages.  Currently the module only allows for Queues""",Improvement,Analytics
2707,"""There is a need for a persistent store for messages so that they can survive the crash of a container node.  The redis implementation in SI is useful since users may already be using redis for other features such as analytics.  This module would sit alongside the current in-memory based aggregator.  It brings in redis as a dependency and there should be configuration options exposed so that one can use a different version of redis as compared to the one that maybe used for analytics.    Due to the need to have redis on the CP, and the large number of different options that each message store implementation provides and the different 3rd party library dependencies, I would like to avoid going down the path of using a profile here as it would seem to go beyond what we had discussed for profile support.  We can revisit as the module contribution story based on boot unfolds.""",Improvement,Stream Module
3360,"""As a user, I'd like to have the option to delete the queues/topics so that we can include an _optional_ attribute as part of the stream destroy command to also clean-up the associated queues/topics.  *Notes:* * Spring-AMQP {{RabbitAdmin}} now has a {{getQueueProperties()}} method which returns the number of consumers so it may be possible to use it for this purpose. * Consider the possibility of _producers_ and/or _queues_ still containing data * Consider the scenario even after the topics/queues are cleaned-up, what to do with fanout exchange?  *Some Further Thoughts* * Consider using the upcoming Spring AMQP REST API {{RabbitManagementTemplate}} if the timing is not right, we could temporarily invoke the rabbit REST API directly. * Should be optional; perhaps via {{stream destroy foo --clean}} * Should this be done by the admin? Or, via a new plugin handling module undeployments - in the rabbit case, undeploying a consumer would check for us being the last consumer and remove the queue/binding/exchange, since we undeploy left->right, everything can be cleaned up on the consumer side. * Third option would be new methods on the bus {{cleanConsumer}} etc invoked by the {{StreamPlugin}} * Down side of doing it on the admin is that he wouldn't necessarily know which rabbit cluster a stream was deployed to - so it probably has to happen on the container - even so, we'd need the admin url(s) for the cluster.""",Story,Runtime
3520,"""h2. Narrative As an XD developer, I need to be able to use a batch job to stream data as a source.  h2.  Acceptance Criteria # Implement the ability for a job to be defined as a source in the DSL # Add the configurations for the batch infrastructure transparently to the user # Add the ability to specify if the job is stateful (picks up where it left off if it stops or restarts at the beginning).""",Story,Ingest
3538,"""h2. Narrative As a user of XD, I want to be able to use a job as a source.  To do so, we need the output of a job to be written to a message channel  h2.  Acceptance Criteria # Create a new ItemWriter in the Spring Batch project to write to a Spring Integration message channel.""",Story,Ingest
4292,"""I can't seem to find any information on this so I am logging a bug but I have a problem where I have a single app that needs to connect to different Cassandra clusters.  Basically in the end I need two CassandraOperations that go against different clusters.  I was trying to do this by declaring two CassandraClusterFactoryBean's and making them unique but this seems to be a problem as the CassandraClusterFactoryBean wants to be a singleton and cannot seem to support multiple clusters.  I spent some time trying to make this work but I can't seem to get the right java config.  As an alternative I was thinking of creating the Cassandra sessions manually then in Java spring config create the two different clusters.  I am not sure if what I am describing makes sense, please let me know if you need more examples.  I am hopeful I am just configuring this wrong so if this is the case I can do a pull request and update the documentation.""",Improvement,Infrastructure
4681,"""As a user of Conda and REST, I would like the ability to use REST for my Conda repository needs.  This ticket implements GET, PUT and POST endpoints.""",Improvement,REST
4696,"""I'm trying to upgrade from Nexus 2.14.18 to Nexus 3.26.1; I only want to transfer repository contents, however: our old server has a lot of bad config and unnecessary user accounts that I don't want on the new server.    However, during the upgrade process, even if I check only """"Repository configuration and content"""" and do not check """"Server configuration"""", the PREPARE phase still lists steps such as:   * """"Upgrade configuration: security.target-privileges""""   * """"Upgrade configuration: security.users""""   * """"Upgrade configuration: security.roles"""", and   * """"Upgrade configuration: security.user-role-mappings""""    I don't want those to transfer, that's why I unchecked the """"Server configuration"""" box in the first place. Obviously, I can just let the upgrade complete and then delete the extra users and roles after-the-fact, but, this still seems like a bug.         !image-2020-08-20-12-34-33-579.png!    !image-2020-08-20-12-34-55-760.png!""",Improvement,Migration
4722,"""As an administrator of Nexus Repository Manager 3, if I find my database out of sync with my blob store or in need of repair and I use NuGet, I would like the `Repair - Reconcile component database from blob store` to include the NuGet format.""",Improvement,"Documentation,Scheduled Tasks"
4816,"""As an administrator of NuGet, I'd like to be able to use Release type Cleanup policies against my NuGet repositories.""",Improvement,Cleanup
4857,"""As a user of REST, I'd like the ability to administrate my RubyGems repositories.  This ticket will implement PUT and POST endpoints.""",Improvement,REST
4858,"""As a user of REST, I would like the capability to administrate my Raw repositories.  This ticket implements PUT and POST endpoints.""",Improvement,REST
4859,"""As a user of Cocoapods and REST, I would like the ability to use REST for my Cocoapods repository needs.  This ticket implements GET, PUT and POST endpoints.""",Improvement,REST
4936,"""As an admin I would like to be able to enable or disable anonymous access using the REST API for a fully automated configuration of Nexus.""",Improvement,"Security,REST"
4937,"""As a user of content selectors, I'd like to be able to include tag == <tag> in my CSEL queries.    This ticket is to gauge interest and understand use cases.  Please vote if this interests your company.""",Improvement,Documentation
4957,"""As an administrator, I would like to be able to keep notes about repositories, so that I can relay important information to myself/others.         *Details*    The +Capabilities+ section currently allows you to keep notes about specific capabilities:    !image-2020-04-16-09-25-28-093.png|width=327,height=181!      This feature would also be useful for managing repositories/house-keeping — e.g. """"added at the request of $TEAM."""", """"remote credentials owned by $PERSON""""""",Improvement,UI
4958,"""As an administrator, I would like to be able to provide descriptions for repositories, so that users have more information of what they're about.    *Details*    Presently, the only information exposed to users is the repository's name, format, and type. Administrators should also be able to provide descriptions for repositories, which show in the +Browse+ and +Browse>Repository+ section, to provide users important context and information.""",Improvement,UI
4960,"""As a developer, I would like to be able to view the remote URL of proxy repositories, so that I can know exactly what they are for.         *Details*    Presently, it is not possible for non-administrators to view the remote url of proxy repositories. This provides a poor user experience, and—especially with a large number of repositories—necessitates documenting what each one does in a separate place.    Proxy repositories should show the remote URL on their page so that users can make informed decisions about what they're using. Additionally, proxy repositories should be searchable by their remote URL so that users can easily find suitable internal repositories.         _Note_: It's possible to grant users access to this information with the {{nx-repository-admin-*-*-read}} privilege, however, this is far from an ideal solution.""",Improvement,UI
4994,"""As an engineer who helps clients deploy Nexus using Infrastructure-as-Code, I would benefit from a way to be able to """"initialize"""" the admin password to a specific value. This method should work with Kubernetes as well.    Acceptance Criteria:  Given a defined password    When the Nexus application is start (or restarted as well perhaps?)    Then I would expect that the running Nexus instance would use that Admin password    In my thinking, this could be implemented using environment variables or a properties file... """,Improvement,Security
5039,"""As a user of Maven and REST, I would like the ability to create and update a group repository via the API.""",Story,"REST,Maven"
5207,"""NXRM 3 has an automation focused and supported REST API. NXRM 2 does not.    A complaint from NXRM 2 users is the REST API there did not allow """"partial updates"""". """"Partial updates"""" means the REST API caller provide enough info to identify the resource to update, and then supply only the config of that resource they want to change, rather than the entire pre-existing payload. NXRM 2 API was designed for the UI ONLY which in theory had all the original information to submit back to update.     In NXRM 3, we would like to do better.    Examples of """"partial updates"""" using a REST API would be:    - add a new group repo member, without needing to supply the entire list of members  - change a hosted repo deployment policy to READ ONLY  - change a proxy repo Blocked status  - remove a privilege from a Role  - add a role to another role    Variations which are more subtle would be:    - delete a role from a user, and not erring if that role does not exist - since the goal is make sure the role is not assigned to that user  - add a privilege to a role and not erring if that priv is already added to the role, since the intent of the action is already met    Lack of the """"partial updates"""" feature increases the likelihood of unforced errors because the caller needs to pull all data first, modify one small part of it and submit back to the server and hope that no other user modified the record in between    h4. Expected    Implement partial update capability to all REST API endpoints which would have the most positive impact to users.  """,Improvement,REST
5280,"""I'm not able to manage a particular local user in nexus oss 3.14. Options to delete user/change password are greyed out    We noticed this problem when application using this particular user id was not able to retrieve artifacts after Aug 1st. With different user id(same permissions), application is able to retrieve all artifacts. Only this user options are greyed out, rest of the ids are manageable.    I want to delete this user and recreate it - please help me troubleshoot the issue    I'm logged in as admin and this user is not part of ldap realm""",Bug,Security
5298,"""I noticed that if I had a large layer deleted from my database to simulate a condition where I would need to run the """"Repair - Reconcile component database from blob store"""" task, that it would error with an out of heap error (below) despite me having the recommended heap for my machine/hardware.    {code}  2019-08-01 18:30:43,816-0400 INFO  [qtp341555250-206] admin org.sonatype.nexus.quartz.internal.task.QuartzTaskInfo - Task 'dockermac' [blobstore.rebuildComponentDB] state change WAITING -> RUNNING  2019-08-01 18:30:43,840-0400 INFO  [quartz-3-thread-3] *SYSTEM org.sonatype.nexus.blobstore.restore.RestoreMetadataTask - Task log: /Users/<USER>Documents/Work/nexus/NX3/nexus-internal/target/sonatype-work/nexus3/log/tasks/blobstore.rebuildComponentDB-20190801183043832.log  2019-08-01 18:30:48,304-0400 INFO  [quartz-3-thread-3] *SYSTEM com.sonatype.nexus.blobstore.restore.internal.DockerRestoreBlobStrategy - Restoring a v2 generated asset/component: v2/-/blobs/sha256:7fa8ea8ddbeedf89f7e47e9f8f0d7fdc7e238d1faf3d2b33b785a801bbcd5f23  2019-08-01 18:31:03,433-0400 ERROR [quartz-3-thread-3] *SYSTEM org.quartz.core.JobRunShell - Job nexus.70f68763-e7bc-4315-9313-cdb2502d9da7 threw an unhandled Exception:   java.lang.OutOfMemoryError: Java heap space   at java.util.Arrays.copyOfRange(Arrays.java:3664)   at java.lang.String.<init>(String.java:207)   at java.lang.StringBuilder.toString(StringBuilder.java:407)   at com.google.common.io.CharStreams.toString(CharStreams.java:164)   at com.google.common.io.CharStreams$toString.call(Unknown Source)   at org.sonatype.nexus.repository.docker.internal.V2ManifestUtilImpl$_manifestDigest_closure1.doCall(V2ManifestUtilImpl.groovy:155)   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:98)   at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)   at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:264)   at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1034)   at groovy.lang.Closure.call(Closure.java:418)   at groovy.lang.Closure.call(Closure.java:434)   at org.codehaus.groovy.runtime.IOGroovyMethods.withCloseable(IOGroovyMethods.java:1630)   at org.codehaus.groovy.runtime.NioGroovyMethods.withCloseable(NioGroovyMethods.java:1759)   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at org.codehaus.groovy.runtime.metaclass.ReflectionMetaMethod.invoke(ReflectionMetaMethod.java:54)   at org.codehaus.groovy.runtime.metaclass.NewInstanceMetaMethod.invoke(NewInstanceMetaMethod.java:56)   at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoMetaMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:274)   at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:56)   at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:128)   at org.sonatype.nexus.repository.docker.internal.V2ManifestUtilImpl.manifestDigest(V2ManifestUtilImpl.groovy:154)   at org.sonatype.nexus.repository.docker.internal.DockerRestoreFacetImpl.getV2Digest(DockerRestoreFacetImpl.java:281)   at org.sonatype.nexus.repository.docker.internal.DockerRestoreFacetImpl.getDigest(DockerRestoreFacetImpl.java:271)   at org.sonatype.nexus.repository.docker.internal.DockerRestoreFacetImpl.doSaveAsset(DockerRestoreFacetImpl.java:235)  2019-08-01 18:31:03,435-0400 ERROR [quartz-3-thread-3] *SYSTEM org.quartz.core.ErrorLogger - Job (nexus.70f68763-e7bc-4315-9313-cdb2502d9da7 threw an exception.  org.quartz.SchedulerException: Job threw an unhandled exception.   at org.quartz.core.JobRunShell.run(JobRunShell.java:213)   at org.sonatype.nexus.quartz.internal.QuartzThreadPool.lambda$0(QuartzThreadPool.java:143)   at org.sonatype.nexus.thread.internal.MDCAwareRunnable.run(MDCAwareRunnable.java:40)   at org.apache.shiro.subject.support.SubjectRunnable.doRun(SubjectRunnable.java:120)   at org.apache.shiro.subject.support.SubjectRunnable.run(SubjectRunnable.java:108)   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)   at java.util.concurrent.FutureTask.run(FutureTask.java:266)   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)   at java.lang.Thread.run(Thread.java:748)  Caused by: java.lang.OutOfMemoryError: Java heap space   at java.util.Arrays.copyOfRange(Arrays.java:3664)   at java.lang.String.<init>(String.java:207)   at java.lang.StringBuilder.toString(StringBuilder.java:407)   at com.google.common.io.CharStreams.toString(CharStreams.java:164)   at com.google.common.io.CharStreams$toString.call(Unknown Source)   at org.sonatype.nexus.repository.docker.internal.V2ManifestUtilImpl$_manifestDigest_closure1.doCall(V2ManifestUtilImpl.groovy:155)   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:98)   at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)   at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:264)   at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1034)   at groovy.lang.Closure.call(Closure.java:418)   at groovy.lang.Closure.call(Closure.java:434)   at org.codehaus.groovy.runtime.IOGroovyMethods.withCloseable(IOGroovyMethods.java:1630)   at org.codehaus.groovy.runtime.NioGroovyMethods.withCloseable(NioGroovyMethods.java:1759)   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at org.codehaus.groovy.runtime.metaclass.ReflectionMetaMethod.invoke(ReflectionMetaMethod.java:54)   at org.codehaus.groovy.runtime.metaclass.NewInstanceMetaMethod.invoke(NewInstanceMetaMethod.java:56)   at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoMetaMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:274)   at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:56)   at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:128)   at org.sonatype.nexus.repository.docker.internal.V2ManifestUtilImpl.manifestDigest(V2ManifestUtilImpl.groovy:154)   at org.sonatype.nexus.repository.docker.internal.DockerRestoreFacetImpl.getV2Digest(DockerRestoreFacetImpl.java:281)   at org.sonatype.nexus.repository.docker.internal.DockerRestoreFacetImpl.getDigest(DockerRestoreFacetImpl.java:271)   at org.sonatype.nexus.repository.docker.internal.DockerRestoreFacetImpl.doSaveAsset(DockerRestoreFacetImpl.java:235)  2019-08-01 18:31:03,439-0400 INFO  [quartz-3-thread-3] *SYSTEM org.sonatype.nexus.quartz.internal.task.QuartzTaskInfo - Task 'dockermac' [blobstore.rebuildComponentDB] state change RUNNING -> WAITING (FAILED)  {code}    The test image I used was couchbase which currently has a ~400MB layer.  I originally noticed testing a 1.6GB foreign layer but it is reproducable without that.    This may be an improvement (optimization) but I'm starting with bug.""",Bug,"Docker,Scheduled Tasks,Backup"
5301,"""The summary says it all. Either as a user or an anonymous user allowed to list the repositories, I need to be able to see what host:port serves a given hosted docker registry, but I am unable to find this information as I am not allowed to view the registry detailed configuration.""",Improvement,"UI,Docker"
5368,"""As a Professional user or administrator of NXRM3, I'd like to use the Staging function with my PyPI repositories.    This is a ticket to track interest and use cases.""",Improvement,"Documentation,Staging"
5606,"""As a user of Cleanup, I'd like the ability to create a cleanup policy allowing me to retain the last # of versions of each component (to be used optionally amongst the other criteria).""",Improvement,Cleanup
5852,"""When I browse my (hosted) Docker repository and try to extend a """"Tags"""" branch, an {{IllegalArgumentException}} is thrown (displayed as a warning """"Comparison method violates its general contract!"""" in the UI):    Entry in the server log:  {code}  2018-07-30 11:33:26,378+0000 ERROR [pool-22-thread-6]  <USER>org.sonatype.nexus.extdirect.internal.ExtDirectServlet - Failed to invoke action method: coreui_Browse.read, java-method: org.sonatype.nexus.coreui.BrowseComponent.read  java.lang.IllegalArgumentException: Comparison method violates its general contract!   at java.util.TimSort.mergeHi(TimSort.java:899)   at java.util.TimSort.mergeAt(TimSort.java:516)   at java.util.TimSort.mergeForceCollapse(TimSort.java:457)   at java.util.TimSort.sort(TimSort.java:254)   at java.util.Arrays.sort(Arrays.java:1512)   at java.util.ArrayList.sort(ArrayList.java:1462)   at org.sonatype.nexus.repository.storage.BrowseNodeStoreImpl.getByPath(BrowseNodeStoreImpl.java:222)   at org.sonatype.nexus.common.stateguard.MethodInvocationAction.run(MethodInvocationAction.java:39)   at org.sonatype.nexus.common.stateguard.StateGuard$GuardImpl.run(StateGuard.java:270)   at org.sonatype.nexus.common.stateguard.GuardedInterceptor.invoke(GuardedInterceptor.java:53)   at org.sonatype.nexus.repository.storage.BrowseNodeStore$getByPath.call(Unknown Source)   at org.sonatype.nexus.coreui.BrowseComponent.read(BrowseComponent.groovy:81)   at com.palominolabs.metrics.guice.ExceptionMeteredInterceptor.invoke(ExceptionMeteredInterceptor.java:49)   at com.palominolabs.metrics.guice.TimedInterceptor.invoke(TimedInterceptor.java:47)   at sun.reflect.GeneratedMethodAccessor256.invoke(Unknown Source)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at com.softwarementors.extjs.djn.router.dispatcher.DispatcherBase.invokeJavaMethod(DispatcherBase.java:142)   at com.softwarementors.extjs.djn.router.dispatcher.DispatcherBase.invokeMethod(DispatcherBase.java:133)   at org.sonatype.nexus.extdirect.internal.ExtDirectServlet$3.invokeMethod(ExtDirectServlet.java:236)   at com.softwarementors.extjs.djn.router.dispatcher.DispatcherBase.dispatch(DispatcherBase.java:63)   at com.softwarementors.extjs.djn.router.processor.standard.StandardRequestProcessorBase.dispatchStandardMethod(StandardRequestProcessorBase.java:73)   at com.softwarementors.extjs.djn.router.processor.standard.json.JsonRequestProcessor.processIndividualRequest(JsonRequestProcessor.java:502)   at com.softwarementors.extjs.djn.router.processor.standard.json.DefaultJsonRequestProcessorThread.processRequest(DefaultJsonRequestProcessorThread.java:72)   at com.softwarementors.extjs.djn.servlet.ssm.SsmJsonRequestProcessorThread.processRequest(SsmJsonRequestProcessorThread.java:43)   at org.sonatype.nexus.extdirect.internal.ExtDirectJsonRequestProcessorThread.access$1(ExtDirectJsonRequestProcessorThread.java:1)   at org.sonatype.nexus.extdirect.internal.ExtDirectJsonRequestProcessorThread$1.call(ExtDirectJsonRequestProcessorThread.java:61)   at org.sonatype.nexus.extdirect.internal.ExtDirectJsonRequestProcessorThread$1.call(ExtDirectJsonRequestProcessorThread.java:1)   at com.google.inject.servlet.ServletScopes$4.call(ServletScopes.java:450)   at org.sonatype.nexus.extdirect.internal.ExtDirectJsonRequestProcessorThread.processRequest(ExtDirectJsonRequestProcessorThread.java:75)   at com.softwarementors.extjs.djn.router.processor.standard.json.DefaultJsonRequestProcessorThread.call(DefaultJsonRequestProcessorThread.java:56)   at com.softwarementors.extjs.djn.router.processor.standard.json.DefaultJsonRequestProcessorThread.call(DefaultJsonRequestProcessorThread.java:30)   at java.util.concurrent.FutureTask.run(FutureTask.java:266)   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)   at java.lang.Thread.run(Thread.java:748)  {code}    **EDIT**  Update 1: This doesn't happen on all images – I've attached a list of tags for the image showing the problem.  Update 2: It seems this happens whenever I deleted a tag of that image from the registry (via Docker Registry v2 API).""",Bug,"Docker,Browse Storage"
6010,"""As a user of NXRM2 who would like to utilize NXRM3, I would like the ability to use the  upgrade tool to move my NXRM2 yum repositories to NXRM3.""",Improvement,Migration
6162,"""As a user, when I browse to an asset via a group I'd like to be able to click on something to allow me to view the file outside of the group so that I can delete the asset.""",Story,Browse Storage
6163,"""As a user, I want a message to warn me when the browse tree is being rebuilt so that I know why the tree contains partial results.    problem to solve:   * user is aware that tree is in partially loaded state.   * User is aware when tree is completely built    possible:   * suggest that user goes to search if the tree is taking a long time to build""",Improvement,Browse Storage
6296,"""As an administrator of a NXRM docker respoitory with invalid assets, I would like a non-manual way to clean them to continue to save disk space and to avoid WARNs in my logs on execution of the """"Purge unused docker manifests and images"""" cleanup task.    *Background*    In NEXUS-12962, the solution to the ticket prevented the scheduled task from erroring as well as stopping the cleanup process however it still leaves invalid assets around as it could not be determined if they could be cleaned up.  If this number grows, it becomes increasingly frustrating to clean these out manually and for each one that is present a WARN fires in the nexus.log currently, so if there were (example) 20000, you'd have 20000 WARNs.    Hopefully there's a clean way to identify these """"invalid"""" assets are valid or to perform a cleanup.  This is a ticket to track that potential as well as guage public interest.""",Story,Docker
6655,"""Nexus 2 had a UI timeout default of 60 seconds. This was often adjusted to prevent UI rendering problems when the server needed more time to do work and return information.    Nexus 3 does not expose such a setting and we have no published information as to what the default is.    I'm guessing it is 60 seconds based on some forced testing with Charles proxy.    However, there appears no published way to change the UI timeout value in Nexus 3. The UI: Settings capability has a ping timeout, but that is the check to see if the server is alive, which is not the same thing. Server can be alive, but a particular type of request may be slow.  h3. Expected   - document the UI timeout default value   - document what to do if the user consistently sees a UI timeout ( help article ) in Nexus 3 ( ie. help us fix the real problem )   - provide some configurable method to adjust the timeout value in cases where that is the only reasonable solution to avoid problems using the UI   - Do not migrate the UI timeout from NXRM2 to NXRM3. This value is recommended to be changed only in specific performance issues that may or may not exist in NXRM3. Whatever the default UI timeout is in previous 3.x versions, that should be maintained.""",Bug,"UI,Documentation"
6840,"""There are commonly customized configuration options that are currently stored in the installation folder and a user needs to remember to copy these over every time.    As an admin, I want to be able to store the commonly customized property files inside the data folder so that I don't have to remember to reapply them or copy them every time I upgrade.    This should include things like the jetty config, port, webapp context, logging, jvm options, ha (hazelcast, orient), ehcache.xml etc.    Acceptance Criteria    * Create a single property file with commonly-changed properties  ** this lives in the data folder, which is read and pushed into other configuration via templates/property placeholders, so that (e.g.) jetty configuration files can still change structure version-to-version  ** a similarly structured default properties file provides values for properties not set by the user (e.g. in cases where Nexus 3.(n+1) adds a new user-configurable property)  ** When NX3 first boots, if the property override file does not exist, NX creates it  *** (No overrides are set in this NX-created file, but comments explain the available configuration properties)   * Subdivide the /etc directory into sub-directories to separate routinely changed configuration from rarely/never-changed configuration files   * Documentation exists to explain this, including the different manual steps needed to upgrade from pre-3.0 to 3.1   * Circulate the proposed layout with the NX team and Support prior to implementation""",Story,Configuration
6849,"""I would like to ask if it was possible to add another staging rule, that would work very similarly to the standard uniqueness check (that prevents closing/promoting staging profile if the artifact is already in the system), however it would not fail if the artifact was already present in the system but was binary equal.     This would enable possibility of deployment of multiple projects, that share dependency on identical common artifacts (it is not always possible to make separate repository with the shared content).  """,Improvement,Staging
6957,"""I noticed on repository group edit (was testing PyPI but also checked Maven), when you save, you briefly see the field validation for the Group - Member Repositories section, even though it has proper members.  I noticed no adverse affects, it is just eye catching and strange.    If this seems familiar, it was mentioned in NEXUS-7977 in a comment.  The description (tasks) still seems to be fixed, so I am filing distinctly rather than reopening that.  I am actually somewhat skeptical because of that fix that this issue is limited to Group Repositories however I could not quickly find another place where that field type was used.  If I find one, I'll try and remember to check against it.    See attached, let me know if unclear.  I did not check older NX3 or NX2 at this time.  I am somewhat curious if this may have regressed (and been fixed with NEXUS-7977) vs was a partial fix.  If I get time and check I'll update here.  Debug was off during this test.""",Bug,"UI,Repository"
6974,"""I'm trying to use Nexus as a Image repository for docker.    While it works fine with pure Docker Daemons (tested on v. 1.10.3), trying to use with Openshift returns me an error of invalid or missing key.    Doing some researches (and even talking with Openshift Developers), we've noticed that the manifest file generated from a docker push to Nexus is different from a docker push to a private Docker Registry v2 (aka Docker Distribution).    I've attached both the manifests (loaded into Nexus and loaded into Docker Registry), so you can see that the manifests are different.    My config on Nexus is a Docker Hosted Repository, with v1 compability disabled""",Improvement,Docker
7009,"""Found testing 2.13.0-01. I've got a simple sample project configured to deploy into Nexus using the nexus-staging-maven-plugin. This project doesn't generate any javadoc.jar.    I then configure Nexus with a staging profile that intercepts all deployments. When I add a 'unique GAV' as a close-time rule set, my second deployment fails (expected - this confirms that the profile is being used, and I know my rule set is being used).    When I add a 'javadoc' rule, however, the deployment passes.  There are no javadoc jars in the staging repository.""",Bug,Staging
7052,"""Just noticed that assigning a role (and a user) nx-users privileges (first tried nx-users-all then all of them) does not seem to grant any access in the UI.  I expected (and believed it used to based around NEXUS-8778 documentation) that it'd grant permission to the Security-Users administration items.  If I'm right (and the affects version of NEXUS-8778 is right) this changed (regressed) sometime between m4 and now.  I did not check older NX3 at this time.  NX2 seems to behave this way, though the permission structure is slightly different.    Only workaround I know is to be admin but there may be another as admin just seems to have nx-admin which should be everything.  I'm leaving major until it's found however.""",Bug,Security
7173,"""Noticed that in Gnarly  the """"Verify user mapping"""" results from LDAP show results however, there's no modal title, columns or close button and the results are half off the screen and unscrollable to.  You can however close the modal with the escape key as a workaround (though there's no workaround I see to seeing the full results or sorting).    This behavior is specific to Gnarly as it works correctly in m6.  I've included screens of both.  [~<USER> noted this was this way when he started work on NEXUS-9370.    There were a couple LDAP fixes in Gnarly however none I saw/believe specific to this button/modal.  Debug was off during this test.  I cleared my browser cache before each attempt to be sure nothing odd was caching.""",Bug,"UI,LDAP"
7331,"""I noticed that if I went to Custom Search from any of the other child searches (Docker, Maven, NuGet, Raw) I got a continual loading mask that did not go away.  If you go from Search parent or outside stuff (such as Welcome or Browse) it works.  This occurs with Debug on and off and nothing in the log/js console shows (though I mull if it's really continual that's why).  This occurs with contents in the repos or with a fresh (content free) install.  None of the other child searches seem affected in this way.    I've attached a screen since it's about the same info as a vid, but if you want a vid let me know.    I did not check older NX3 or NX2 at this time.""",Bug,UI
7490,"""As a User I would be able to copy the link for the page I am looking at and share that link with someone else.    [~<USER> noticed that if you copy a link for an element you are looking at details for, when you hit the bookmark it takes you to the base Browse UI page (selecting a repo).  I verified this and the fact it does not occur for search, although likely will be an issue for any of our Drilldown views which require multiple levels of context during normal navigation.    This is new to NX3 and NX2 is not browsable in this exact way (but is bookmarkable for the non-tree/path version).""",Bug,UI
7519,"""I went to create an LDAP connection, filling in the first page then clicked next.  I immediately realized, I had forgotten to test """"Verify Connection"""" so clicked """"back"""" (Create LDAP Connection) in the breadcrumb and I was asked if I wanted to discard changes.  I hadn't input anything on the screen (User and group) so feel this is being overchecked.    Workaround is to discard.    I did not check older versions of NX3 or NX2 at this time.  Debug was off.""",Bug,"UI,LDAP"
7520,"""While changing a users password through Security>Users, I entered the new password twice and hit enter.  Nothing happened.  I was able to continue using the mouse and clicking Change Password.  The authentication modal just before this allowed enter, so I'm assuming this is a bug.""",Bug,UI
7523,"""When trying to configure Oracle Maven Repository, I am receiving the following error message under routing tab, Discovery section :    Status: Unsuccessful.  Message: No scraper was able to scrape remote (or remote prevents scraping).  Last run:  Fri Jun 05 2015 15:24:22 GMT-0600 (Mountain Standard Time)    I've followed the Oracle documentation and I still get the message.  i've attached to log if that helps.""",Task,"Repository,Maven"
7547,"""I see no evidence currently that on NX3 LDAP configuration using Verify Connection the """"retry after"""" and """"max failed attempts"""" (latter I believe new for NX3) is working.  I noticed this testing NEXUS-8655 after verifying the wait seconds before timeout field was working as I expected.  This appears to """"work"""" in NX2, but without """"max failed attempts"""" it basically tries to connect forever (which is probably why the third field was added).  I did not check older NX3 at this time.    On discussion with [~<USER>, he asked if I would close NEXUS-8655 if the first field was working and open a distinct ticket on the second and third fields.  This is that ticket.    Repro steps:  1) Setup a valid LDAP (first page) but make the port an invalid port.  2) Click verify connection.  The connection will try for the number of seconds listed in the first variable then nothing.  I attempted low settings and retries to get this to fire faster (such as 10/2 and 1/5).""",Bug,LDAP
7600,"""While testing the revamped ?describe UI, I noticed that the page no longer has a version at the top.  Instead it says $nexusVersion.  Assuming a variable went awry somewhere?  Since (largely) an internal tool this might be the most trivial thing I've ever filed but I think worth correcting (if not intentional).    Did not check older NX3 at this time, but the screen from [~<USER>'s PR for NEXUS-7711 shows it working, so I'm assuming recent regression, likely because of NEXUS-7711.""",Bug,UI
7693,"""While testing NEXUS-8486, I noticed that the default mavan group shows Repository Policy MIXED.  At first I thought this was the same bug, however, on reflection and NX2 check and JasonD check, I realized that Group repos in NX2 do not have a Repository Policy, so I'm guessing this field should be hidden/not shown/not exist for (Maven) Groups.    If I am wrong, then wanted to note that MIXED does not show as an option in the dropdown nor can you switch back to it after changing so I suspect that's another bug that I'd file.    This is new to (CMA) repositories so did not check older versions of NX3.""",Bug,"UI,Repository,Maven"
7706,"""While testing NEXUS-8446, I noticed after creating a task in IE11 and clicking to edit, on the settings page, I saw a horizontal scrollbar that did not scroll.  IE was expanded and on VM window expand it went away.  I was able to reproduce this twice more in about 10 attempts, so it's intermittant but does happen.  Attached screen.    I was unable to repro in Chrome MacOSX at all, quickly.  I'm super curious if this would happen outside of VM and if it does, if there's a way to get rid of it, since my fix is essentially changing the VM window which people whose OS is Windows couldn't do.  Unfortunately I have no easy way of testing that.    I did not check older versions of NX3 at this time.""",Bug,UI
7708,"""As a new user, I want to have a clear status indicator for staging repositories, so that I can not search google for hours trying to find answers because I'm lazy and don't read the documentation.  Can this be changed from open/closed to """"needs validation""""/""""validated"""" statuses and the """"close"""" button changed to """"Validate"""" """,Story,Staging
7718,"""During UX review, I noticed when opening Support>System Information the loading mask appeared at the top of the page rather than in the middle.  I'm guessing this is because if in the middle, it'd literally be in the middle which would essentially be unseen 99% of the time because the system information is so long.    I asked <USER>and he replied:  """"This is actually the feature loading mask, not the form loading mask. Subtle difference. If you click through some of the other menu items, you’ll notice this mask flashing briefly.    I’d file this anyway, since it should mask the entire content window (including breadcrumb), not just the buttons and body. There might also be a way to center the loading indicator, since I agree it’s weird to have it anchored to the top.""""    Filing as an improvement, though I suppose it could be a bug too.""",Improvement,UI
7826,"""While reviewing NEXUS-8244, I created a template and then went to edit it.  I put in """"aaa"""" randomly near the top and saved.  On leave and return, my save did not appear to take.  After some review (including vs NX2), I realized that malformed saving is checked against, but the pretty balking that NX2 gives you is not performed in NX3.  In fact no warning at all (UI, console, js console) is given and even the save loader appears.  The only hint is if you discard changes (not possible on create) the failed adjustment discards.  I'd be surprised if this was intentional so I'm filing at least for review.  I did not see any tickets about this.  My guess is that this section may not totally be done however in that case I hope this is used as a reminder to preserve this.    I back checked m3 and this is the same there - so do not believe this is related to NEXUS-8244.""",Bug,UI
7857,"""While testing NEXUS-7758, I went to create a task and filtered, but realized I missed something so switched to repositories.  After finding the info I needed, I returned to Tasks but my create button was greyed out/unselectable.  I was able to repro this reliably by filtering on a second level (Tasks) returning to a base level (not even filtering) and then returning.  Steps below.    I went to see if this was an issue in m3 but as far as I can tell there are no second level filters in m3, this is a new concept to Dizzam.  This also does not affect NX2 (same reason but more basic).    Borderline minor as the workaround is to restart/refresh browser.    I do not believe this is related to where I found it but linking in case wrong and for completeness.    Steps:  1) Login to NX3 (Dizzam) as admin  2) Click Tasks (admin section, under System)  3) Click Create  4) Filter by something (like """"whatever"""")  5) Click Repositories (left nav item under Repository)  6) Click Tasks again.  BUG: Create is disabled/greyed out.""",Bug,"UI,Scheduled Tasks"
7942,"""There is no """"Backups to Keep"""" option for the new npm backup task. For a 600MB database, a 80MB backup zip file can get created. I would expect Nexus admins to typically schedule this backup task daily.    A backups to keep option would help prevent filling up disk with unneeded backups. The Nexus Configuration Backup also has a Backups to Keep option.""",Improvement,NPM
7971,"""Two feeds in NX3 are named """"New File"""" and are distinct items.  While this text is cleaner than NX2 (long descriptions) there is a distinction in NX2 beyond just the path.    Requesting this be differentiated somehow.    Filing as a bug since this was different (""""not a bug"""") in NX2.  I've attached NX3 and NX2 screens.  Will also reach out to <USER>Manfred to see if they have any suggestions.  I am too verbose to make suggestions for this one=)""",Bug,UI
8069,"""As a user (admin), I would like the capability to view all text available in the UI.    ---    I noticed with the addition of the Security warning (fixed in 2.11.1-01) the full text on the Welcome page does not display.  However, I could not scroll down to view the full text using mouse wheel, trackpad or select """"pulling"""" all text.  I assume this is not intentional since it includes our pitch for people to get Nexus Pro.    This does not appear to affect NX3 however there is not enough text there to test by default.  All browsers on MacOSX were affected; some worse than others but none showed the full text we have on the screen.    This may be limited (easy) reproducability while we have the security alert there but still a bug.""",Bug,UI
8084,"""As a Nexus administrator I want a supported procedure to rebuild the npm metadata from existing npm repository storage, particularly from hosted repositories.    This is useful for rebuilding metadata from rsynced npm storage in a failover instance and as a database corruption recovery tool.    Notes:  - Look into OrientDB backup, export as potential options if we can't rebuild    """,Improvement,NPM
8087,"""As a user, it'd be nice to have a persistent message when I come back to a session and am logged out.    ---    Two times today, I've been idling on NX3 and on return to the screen have had a pause then realized I am logged out presumably due to idling.  One benefit of NEXUS-7755 was that there was a history including that you were logged out.  Right now, you can potentially just click anonymously before realizing what happened.  Suggesting a message on the screen relaying you have been logged out that persists until the user clicks it off (or otherwise notes it as being seen).  If NEXUS-7755 or like history is restored, then this could likely be OBE.    Acceptance Criteria:  - When a user is logged out due to timeout, show the user a dialog that indicates to them they have been logged out""",Improvement,UI
8090,"""While discussing NEXUS-7811, Jason noticed the screen I took had a second frame repeating the milestone resources available for admin on the welcome page.  I was able to reproduce this but not consistently by logging in and out of the system as admin (deployment and anonymous do not have these resources at this time).    Since I was able to repro several times despite not every time I am filing.  If I see further tidbits about this I will add.    I am making this minor because once this frame shows up, it remains even on logout, exposing these resources to a non-admin.  I also have noticed less frequently while testing this that the resources sometimes stay on the page on logout (but on the non-second framed screen).  I mulled filing a second ticket but was unable to repro this behavior in FF or Chrome, so I'm holding off for now, semi-hopeful this ticket fix may cleanup that as well.    I did not specifically check NX2 for this at this time, however, I've never seen this there before.""",Bug,UI
8448,"""Filing feature request on behalf of customer:    {quote}  We would like to delegate administration of Staging Profiles on a per-target basis to our release engineers. Could such an ability please be added to the Nexus roadmap?    As an example, if I have a target 'com.firm.departmentOne', I'd like to be able to say that everyone with the 'departmentOne Bosses' role could define Staging Profiles and Groups for that target. Ideally the Staging Profiles and Repository Groups would be generated from a template so we could, for example, impose a naming standard on them.  {quote}    I'm interpreting this is as supporting a workflow where access is defined as a declaration of what artifact paths/identifiers are deployable and retrievable, then roles controlling Nexus management are created in Nexus based off of these root privileges.      Given there is method to control which components(artifacts) are allowed CRUD  Then restrict configuration management achored off of component access""",Improvement,Configuration
8470,"""Test setup (this is a bit complex because I was reproducing a customer issue):    # Started with Nexus 2.7.2-03 because this is what customer was using  # Set up a smart proxy server publishing through a group  # Set up a subscriber that proxied the group, with smart proxy enabled (no preemptive fetch)  # Pre-populated the subscriber's cache with a few thousand items  # Slowed down the walker (the mechanism I used was a 20ms sleep in DefaultLinkPersister.isLinkContent because I was reproducing the customer issue)  # Bumped some walker levels to info in code to make logging clearer.  # Bumped com.sonatype.nexus.plugins.smartproxy.event.internal.handler.CacheExpiredHandler to debug in log configuration    Then I did the following:    # Added a repository to the group repo on the publisher (this kicked off a cache walk on subscriber that lasted about 4 minutes)  # Attempted to retrieve an artifact through the subscriber proxy that hadn't been deployed yet to populate NFC  # Aeployed the artifact to a repository in the publisher that is in it's group repository  # Attempted to retrieve the artifact through the subscribe proxy again    The last step failed until the expire cache running on the subscriber from step 2 was completed.    I've attached a wrapper.log that has this additional logging, and also has thread dumps taken during 3 occasions where the NFC cache block was blocked.    One other oddity... the first time running the above test after a clean start of subscriber doesn't seem to have this problem, but subsequent tests did.  Not sure if there is a timing issue here, or something else going on.        """,Bug,Maven
8651,"""Hi,    We are not able to access Nexus url suddenly and ive noted that it was a JVM issue. I'm not sure, at sudden why nexus doesnt pickup the java from /usr/bin/java. I've provided absoulte path in """"$Nexus_HOME/bin/jsw/conf/wrapper.conf"""" and started the nexus and it works fine.     Just to know, why it was not picking up from """"/usr/bin/java"""" suddenly. Did anyone faced the same issue?""",Bug,Configuration
8827,"""I'm converting my plug-in to use the new siesta/jersey based framework. One of the resources has a POST method that consumes """"multipart/form-data"""". Declaring a parameter like this:  {code:none}  @FormDataParam(""""value"""") FormDataBodyPart value  {code}  results in that the resource fails to be discovered (it doesn't show up in the log under """"org.sonatype.nexus.plugins.siesta.SiestaModule - Resources:"""" during boot). If I change the FormDataBodyPart into a String, then the resource shows up and can be called. Using a String results in a value that contains the full raw content of the part so it's not really an option.    I'm currently using 2.7.0.m4""",Bug,REST
9021,"""The """"repository"""" drop list in the """"add/repository target privilege"""" panel presents a sorted list of regular repositories, followed by a sorted list of group repositories.    If you have a large number of repositories this makes it really difficult to find what you are looking for, because you can no longer triangulate the approximate location by alphabetical position.  Go to oss.sonatype.org for an example of this.    We should just present this as a case insensitive sort of all repositories.    I'm putting this in 2.7 under the assumption that this is a trivial change, please let me know if isn't.""",Bug,"Security,UI"
9155,"""Log into OSSRH as admin.  Bring up the user list.  Then do a thread dump.    You'll see nearly 300 threads like this:    {noformat}  """"Thread-303"""" daemon prio=10 tid=0x00007ff800196000 nid=0x232e runnable [0x00007ff7bde5d000]     java.lang.Thread.State: RUNNABLE          at java.net.SocketInputStream.socketRead0(Native Method)          at java.net.SocketInputStream.read(SocketInputStream.java:150)          at java.net.SocketInputStream.read(SocketInputStream.java:121)          at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)          at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)          at java.io.BufferedInputStream.read(BufferedInputStream.java:334)          - locked <0x00000000f41d4f00> (a java.io.BufferedInputStream)          at com.sun.jndi.ldap.Connection.run(Connection.java:849)          at java.lang.Thread.run(Thread.java:722)  {noformat}    These threads persist for a very long time, and it appears that only a full GC cleans them up.    I've been playing around with this on a local instance using OSSRH configuration, and I've found that setting the """"com.sun.jndi.ldap.connect.pool.maxsize"""" system property keeps the number of threads from growing without limit.  However, once the limit is reached all subsequent user queries fail.  This indicates to me that we may have a leak, the threads in the pool are not available for reuse.    """,Bug,LDAP
9157,"""I was upgrading from 1.8.0.1 by doing a new install on a separate linux host and used 2.3.1 and copied over some of but not all of the files.      When I tried to browse there was a new field called Quality and it came back stating issues with the maven repo...  The components in the repository were inspected but their indentity could not be confirmed.  This may have... and showed a zero 0  Under quality...    the only file was an archtype-catalog.xml and no files.  Looking at the release notes I didn't see anything obvious.  Browsing the storage...  But when I browse index I see the directory.  Was there a setting I needed to do or adjust or ?    I stopped nexus, deleted the directory then restarted and repaired the index to no avail.    Thank you for any help.""",Bug,Repository
9576,"""As a user, I would like to delete multiple versions of an artifact without having to get into the Browse Storage tab of the UI, then right click --> delete each version directory individually.  Also, I do not want to delete all versions of the artifact by right clicking the artifact directory and choosing delete.      We have custom artifacts for internal use only that have many versions, no one is using the old versions any longer (nor should they), but it's difficult to get people to clean up the old stuff because doing this through the UI is quite tedious.  Deleting multiple versions via a REST service would be acceptable too; at least we could script it.""",Improvement,UI
9819,"""As a user, I sometimes find it necessary to exclude files from proxy repository file content validation. But I want to leave content validation on for most files.    Specific Example:    For XML files, we validate based on the presence of the XML header.   Not all XML files have this header, and switching to an alternative method of validation would likely be too expensive.    Note:  This is not a request to disable validation for certain file types, only for specific files which are known to be OK yet don't pass validation.""",Improvement,Repository
10010,"""With a setup like this:  - Proxy Repo """"<USER>""  - Group """"Public"""" which contains X  - Group """"FID"""" which contains Public    We set up an Inclusive route (with value .*/.*atlassian.*/.*), put just """"<USER>"" in the Ordered Route Repos list, and then Repository Group as """"All Repository Groups"""".  However this meant nobody was able to pull down <USER>artifacts.  By turning logging up to debug and picking through the output we saw the routing logic was saying the rules resulted in 0 target repos to look in.    I guess the logic being applied is to get the list of constituents to the starting group (FID) and then straight away intersect with the ordered repo list, rather than resolve any contained groups down into their constituent repos first.    I appreciate this could be considered """"by design"""", but if so then I'd say it's a bit unintuitive, so should at least be clarified in the docs.  """,Bug,Repository
10030,"""I've try to run Nexus 1.9 and 1.9.2.4  on my computer ,but It can't be run. It reports a ERROR like :    jvm 1    | 2011-12-29 22:59:40 WARN  [er_start_runner] - org.sonatype.security.configuration.source.FileSecurityConfigurationSource - No configuration file in place, copying the default one and continuing with it.  as a result I find it can't generate nexus.xml and then stop working.  the runtime environment is ORACLE jdk1.7.0_01  and windows 7 sp1 windows 32bit.  I've tried run it through nexus with jetty and tomcat 7.0 .Both of them can't work.  But it can work well on my another computer which is windows 7 32bit and ORACLE JDK 1.6.  Hope you guys can fix this bug.  thanks!""",Bug,Build
10125,"""In order to mitigate invalid content to be downloaded into Nexus when checksum validation is disabled on a repo  As a consumer of artifacts distributed by Nexus  I want to have Nexus perform shallow content validation on additional common file types before allowing them to be served to consumers    This is a story to expand content validation to include the following file types:    tar  tar.gz  swc  swf  sha1  md5      bzip2 was to be left out for now ( see NEXUS-4632 )          """,Improvement,Repository
10252,"""I've been trying to create a broken task in order to test another issue.      I've tried all manner of file permission problems, broken index files, etc.  Tasks never show up as broken, seemingly no matter what exception I cause to get thrown.  I even had an NPE show up as a successful run.    See log.""",Bug,Scheduled Tasks
10258,"""Hello,    I am having an issue with LDAP configuration of our Nexus OSS instance and don't see a documented way to get out of it.  I was under the impression that my setup was fairly common, but perhaps this is not so...    Our (OpenLDAP) directory uses static, nested groups.  The users are all in one tree under ou=Users and all the groups are under ou=Groups (there is no nesting of group levels).  Each project has a set of groupOfUniqueNames under ou=Groups such as:    acme-users  acme-developers  acme-administrators    The groups follow the standard method of nesting static groups and use a DN to point to the nested group.   Example:    In cn=acme-users,ou=Groups,dc=my,dc=site,dc=com:   uniqueMember: uid=user1, ou=Users, dc=my, dc=site, dc=com   uniqueMember: uid=user2, ou=Users, dc=my, dc=site, dc=com   uniqueMember: uid=user3, ou=Users, dc=my, dc=site, dc=com   uniqueMember: cn=acme-developers, ou=Groups, dc=my, dc=site, dc=com    In cn=acme-developers,ou=Groups,dc=my,dc=site,dc=com:   uniqueMember: uid=developer1, ou=Users, dc=my, dc=site, dc=com   uniqueMember: uid=developer2, ou=Users, dc=my, dc=site, dc=com   uniqueMember: cn=acme-administrators, ou=Groups, dc=my, dc=site, dc=com    In cn=acme-administrators,ou=Groups,dc=my,dc=site,dc=com:   uniqueMember: uid=admin1, ou=Users, dc=my, dc=site, dc=com    In our other LDAP-enabled applications, this works perfectly well.  There are options to configure nested groups, and if you were to query the contents of acme-users, you would see the user records for     user1, user2, user3, developer1, developer2, admin1    However, in Nexus OSS (at least), there doesn't seem to be any recognition that nested memberships are allowed.  that same admin1 user only shows up in the directory in which he actually exists:    admin1   ....     Roles: acme-administrators    Looking at the DefaultLdapGroupDAO, it appears to me that Nexus is probably not currently capable of handling this user setup. This is making permissions much more difficult to organize than they should be, as some of our project administrators (tech leads) are also developers.    I realize that converting the userbase to use dynamic groups is a possible workaround, but that isn't a method I want to explore right now; it hides the problem rather than fixing it.  Is there any possible filter magic I can use to make this work properly, or does nested group support simply need to be added to the LDAP implementation?  """,Improvement,Security
10413,"""# Start a fresh Nexus  # Login as Admin, select Maven Central Proxy repo in repo ui  # Select Download indexes and save  # Open Scheduled Tasks UI  # Stop the running index task by using Stop button    Shutdown Nexus. This stops the download index in the background.    Start Nexus    # Login as Admin and open scheduled tasks  # Create a new Download Indexes task with Manual schedule and save  # Run the new Download index task by selecting it and clicking run  # Cancel the running task.    Now, even after the Download index task completes in background and done all its work, the status in the UI remains """"CANCELLED"""" and never returns to submitted. Clicking refresh button does not help. If I look in conf/nexus.xml, task state is recorded as SUBMITTED - I would expect this state to match the UI.  """,Sub-task,Scheduled Tasks
10705,"""I'm responsible for keeping Nexus working and for uploading artifacts which are not available in any other repositories.  Some members dont know me, so they have no way to write an EMail to me.  It would be nice, if there was a link to contact system administrator, which is configured in nexus.xml     <smtpConfiguration>      <hostname>MYSMTPSERVER</hostname>      <port>25</port>      <systemEmailAddress>NEXUS_MAIL_BOX</systemEmailAddress>    </smtpConfiguration>    wfg    Anton BORS""",Improvement,UI
11073,"""There is a lot of confusion regarding some of the scheduled tasks (particularly indexing).    As a user, I would like to see detailed descriptions of tasks appear in the Nexus UI when I am adding new tasks.  As I change the task type, the description field would be updated accordingly.""",Improvement,UI
11119,"""As a user, I would like my anonymous users to be able to see the repositories contained in my group repository.    Perhaps extend the summary tab to groups, along with a specific permission to allow seeing it?    """,Improvement,UI
11253,"""I'm configuring ldap on my nexus instance, and it seems to me that the """"user subtree"""" feature can't work with static groups, unless there was another variable for groupMemberFormat I could use.    In my case it could simply be {{member=${dn}}}, since my users are in different subtrees ({{ou=employees,ou=users}} for some, {{ou=community,ou=users}} for others, etc). I've tried a few different variations for Group Member Format, such as {{uid=${username},ou=*,ou=users,dc=mycomp,dc=com}}, to no avail. (I'm also trying to write a component-matching filter, but can't get that to work even outside of Nexus for now; if that'd work, it would require some hackery to write the filter, since the = operator is hardcoded in {{org.sonatype.security.ldap.dao.DefaultLdapGroupDAO#getGroupMembershipFromGroups}})    I'm not sure which way to go to provide a patch, since as far as I can tell, the user's DN is not passed through to {{LdapGroupDAO}}, only her username.""",Improvement,Security
11543,"""As a user, I support multiple products with multiple versions of Maven involved.  Given my rather large series of repositories being mirrored (12), I need an easy way to create a group like """"Public"""" that is an instance Maven1 copy.    The current method involves creating virtual repositories for each maven2 repo, then creating a new group.    In my vision, a user would elect to create a new group, from an existing group, and select the new group format (i.e. maven1).  Underneath the covers, Nexus can do whatever it needs to, without my knowing.  The key is that the new """"virtual"""" repositories should *not* be created (or at least not exposed in the UI), because this will confuse users.""",Improvement,Repository
12227,"""Found this while verifying NEXUS-1767, and I'm filing it as a separate bug because it doesn't cause a loss of functionality and  I'm not sure if we will be able to fix it.    If you drag content from a list box in the latest 1.4 Nexus builds the drag content looks huge. This is a regression from 1.3.6.""",Bug,UI
12282,"""The Nexus browser UI does not set the text (foreground) colour for most selectors/classes.  For people like me who use an inverted colour scheme (white foreground text colour), this leads to white-on-white text  in (e.g.) the Welcome and Repository tabs.    The sidebar is fine, it has an exlicit foreground colour.    I am using greasemonkey to add this CSS snippet, which makes the UI usable:  {code}  html, body, div, dl, dt, dd, ul, ol, li, h1, h2, h3, h4, h5, h6, pre, form, fieldset, input, p, blockquote, th, td {   color: black ;   }  {code}  This isn't an uncommon problem.. the JIRA text field I'm typing into now has an explicit foreground colour (black) but no explicit background colour, so I'm typing blind here.. have to highlight everything to check my spelling :)""",Bug,UI
12302,"""We're using the standard 3rd-party hosted repository for our commercial / non-mavenized artifacts.   Some days ago I've setup a task of type """"Evict Unused Proxied Items From Repository Caches"""" to prevent that our disks will run full.  The Job was setup to run on the Public Group (since we have quite some number of additional proxied repositories) which also has all hosted repositories in it.  The Job was setup to purge all stuff older than 30 days.    The Job ran and also purged old hosted artifacts from the 3rd party repository.   This created quite some additional work for some of us.   I was not able to find a notice of which artifacts got purged in the Feeds or Logs.     My expected result would be:   - Hosted artifacts never get purged - at least with this kind of Job   - On purge an email is send or a feed entry is made that lists all purged artifacts.""",Bug,Scheduled Tasks
13149,"""I add a proxy repository to a Maven 1 repository.    I create a virtual repository to represent that proxied M1 repo as a M2 repo.       Repository ID = virtual_m1_to_m2       Repository Name = virtual_m1_to_m2       Repository Type = virtual       Format = Maven 2       Source Nexus Repository ID = My Maven 1 Proxied Repo       Synchronize on Startup = True    I try to add the virtual repository to the public group, but I cannot.  The < button does nothing, and when I try to drag it over it has the red X on it.    I need to add a proxied Maven 1 repo to the public group, but I cannot do so since the virtual repository will not add.    I also tried restarting Nexus, but that didn't help.  Maybe because the proxied 1 Maven repo hasn't cached anything on the Nexus server yet it isn't working?""",Bug,Repository
13728,"""I can launch Nexus successfully using the start script:       /usr/local/nexus/bin/jsw/solaris-sparc-64/nexus start    However neither the stop or status scripts work because /usr/ucb/ps is not installed.    The following transcript shows what happens when issuing the status command. The /usr/bin/ps -ef command shows that nexus is running.    ====================  -bash-3.00$ ./statusNexus  /usr/local/nexus/bin/jsw/solaris-sparc-64/nexus: /usr/ucb/ps: not found  Removed stale pid file: /usr/local/nexus-webapp-1.0.0/bin/jsw/solaris-sparc-64/./nexus-webapp.pid  Sonatype Nexus Repository Manager is not running.  -bash-3.00$ ps -ef | grep nexus      patp 21336 20589   0 08:12:48 pts/3       0:00 grep nexus      root 14385     1   0   Sep 08 ?           1:26 /usr/local/nexus-webapp-1.0.0/bin/jsw/solaris-sparc-64/./wr  apper /usr/local/nex  -bash-3.00$  ====================    Is it possible to adjust these scripts so that when /usr/ucb/ps is not installed on solaris, the script will use an alternative ps? I would do this myself except for the fact that every time an update to Nexus is installed, the script would have to be adjusted since it's run from the current nexus installation.    As a workaround to stop nexus, I end up just using a kill with a /usr/bin/ps -ef to find the nexus process.    """,Bug,Build
13730,"""  ScheduledServiceListResource has a few 'date' fields    - lastRunTime  - nextRunTime    These are strings, i would expect them to be date objects""",Bug,Scheduled Tasks
13760,"""Using Chrome browser => As anonymous user, select Browse Repositories => Select a repo that the anon user does not have access to view => The Authentication Required call out appears.  If you allow this call out to time out, an error -1 appears on the screen.    I would have expected that the screen refreshed and the message of Access Denied appear in the Repository Information grid.""",Bug,Security
13776,"""I would like to see more consistency with the log messages.  As an example, take the following:  INFO   | jvm 1    | 2008/08/30 22:19:17 | 2008-08-30 22:19:17,897 INFO [org.sonatype.nexus.feeds.FeedRecorder:default] - Clearing caches for repository ... not yet finished.  INFO   | jvm 1    | 2008/08/30 22:19:17 | 2008-08-30 22:19:17,898 INFO [org.sonatype.nexus.Nexus:default] - Clearing caches in repository snapshots from path /  INFO   | jvm 1    | 2008/08/30 22:19:17 | 2008-08-30 22:19:17,911 INFO [org.sonatype.nexus.feeds.FeedRecorder:default] - Clearing caches for repository ... finished succesfully on Sat Aug 30 22:19:17 CDT 2008    I would like to direct your attention to the word 'caches', it is spelled the same way all three times.  Whereas with 'Rebuild Attributes' does not follow this convention:  INFO   | jvm 1    | 2008/08/30 22:24:39 | 2008-08-30 22:24:39,896 INFO [org.sonatype.nexus.feeds.FeedRecorder:default] - Rebuilding Attributes of repository ... not yet finished.  INFO   | jvm 1    | 2008/08/30 22:24:39 | 2008-08-30 22:24:39,897 INFO [org.sonatype.nexus.proxy.repository.Repository:maven2] - Recreating attributes on repository snapshots  INFO   | jvm 1    | 2008/08/30 22:24:39 | 2008-08-30 22:24:39,906 INFO [org.sonatype.nexus.feeds.FeedRecorder:default] - Rebuilding Attributes of repository ... finished succesfully on Sat Aug 30 22:24:39 CDT 2008    As you can tell the word attributes is spelled two different ways in regards to casing.  I was performing a """"tail -f wrapper.log | grep attributes"""" one day and finally gave up waiting (I was scared something had blown up).  Until I realized the casing was off.    So then I decided to grep on the success message (as I was actually performing many tasks).  I gave up on this as well as the word 'succesfully' is spelled incorrectly.    So-    As a user, I would like more consistent messages in the log as well correctly spelled words.    Thanks""",Improvement,UI
14673,"""We have at any given time ~100 frameworks connected to our Mesos Master with agents spread across anywhere from 6,000 to 11,000 EC2 instances. We've been encounter a crash (which I'll document separately) and when that happens, the new Mesos Master will sometimes (but not always) struggle to catch up, and eventually crash again. Usually the third or fourth crash will end with a stable master (not ideal, but at least we can get to stable).    Looking over the logs, I'm seeing hundreds of attempts to contact dead agents each second (and presumably many contacts with healthy agents that don't throw an error):  {noformat}  W1003 21:39:39.299998  8618 process.cpp:1917] Failed to send 'mesos.internal.UpdateFrameworkMessage' to '100.82.103.99:5051', connect: Failed to connect to 100.82.103.99:5051: Connection refused W1003 21:39:39.300143  8618 process.cpp:1917] Failed to send 'mesos.internal.UpdateFrameworkMessage' to '100.85.122.190:5051', connect: Failed to connect to 100.85.122.190:5051: Connection refused W1003 21:39:39.300285  8618 process.cpp:1917] Failed to send 'mesos.internal.UpdateFrameworkMessage' to '100.85.84.187:5051', connect: Failed to connect to 100.85.84.187:5051: Connection refused W1003 21:39:39.302122  8618 process.cpp:1917] Failed to send 'mesos.internal.UpdateFrameworkMessage' to '100.82.163.228:5051', connect: Failed to connect to 100.82.163.228:5051: Connection refused{noformat}  I gave [~<USER> a perf trace of the master on Slack at this point, and it looks like the master at is spending a significant amount of time doing framework update broadcasting. I'll attach the perf dump to the ticket, as well as the log of what the master did while it was alive.    It sounds like currently, every framework update (100 total frameworks in our case) results in a broadcast to all 6000-11000 agents (depending on how busy the cluster is). Also, since our health checks rely on the UI currently, we usually end up killing the master because it fails a health check for long periods of time while overwhelmed by doing these broadcasts.    Could optimizations to be made to either throttle these broadcasts or to only target nodes which need those framework updates?""",Improvement,master
15387,"""I wish there was a way to create role name aliases.    Currently if I decide on a role name once within my org it's very hard to ever change it later. This is visible in DC/OS where the role name slave_public was chosen at a time when Agents were still called Slaves. Without the ability to alias this role there's virtually zero chance of us ever updating that name to public_agent. I would imagine that other organisations run into similar issues once a role name is being used widely.""",Wish,"master,HTML"
15654,"""I'm trying to make a private dedicated slave to one docker container of elasticsearch. This slave also has a dedicated disk that I want to make it available to the container with """"mount"""" type persistent volume.    I use static reserved resources which the agent is able to load and are available.    cat /etc/default/mesos-slave  MASTER=`cat /etc/mesos/zk`  MESOS_RESOURCES='[\{""""name"""":""""ports"""",""""ranges"""":{""""range"""":[{""""begin"""":30000,""""end"""":32000}]},""""type"""":""""RANGES"""",""""role"""":""""*""""},\{""""name"""":""""cpus"""",""""type"""":""""SCALAR"""",""""scalar"""":{""""value"""":4},""""role"""":""""*""""},\{""""name"""":""""cpus"""",""""reservations"""":[{""""role"""":""""eh"""",""""type"""":""""STATIC""""}],""""scalar"""":\{""""value"""":4},""""type"""":""""SCALAR""""},\{""""name"""":""""mem"""",""""reservations"""":[{""""role"""":""""eh"""",""""type"""":""""STATIC""""}],""""scalar"""":\{""""value"""":15023},""""type"""":""""SCALAR""""},\{""""name"""":""""mem"""",""""type"""":""""SCALAR"""",""""scalar"""":{""""value"""":15023},""""role"""":""""*""""},\{""""disk"""":{""""source"""":{""""mount"""":{""""root"""":""""/data""""},""""type"""":""""MOUNT""""}},""""name"""":""""disk"""",""""reservations"""":[\{""""role"""":""""eh"""",""""type"""":""""STATIC""""}],""""scalar"""":\{""""value"""":99000},""""type"""":""""SCALAR""""},\{""""name"""":""""ports"""",""""ranges"""":{""""range"""":[{""""begin"""":9100,""""end"""":9400}]},""""reservations"""":[\{""""role"""":""""eh"""",""""type"""":""""STATIC""""}],""""type"""":""""RANGES""""}]'  MESOS_ROLES=""""eh""""  MESOS_DEFAULT_ROLE=""""eh""""         When trying to create the app with """"acceptedResourceRoles"""": [""""*"""",""""eh""""]: """"A resident app must have `acceptedResourceRoles = [""""*""""]` """".    Why is the slave not private ? I'm not using any """"slave_public"""" parameter set.    If I use """"acceptedResourceRoles"""": [""""*""""]: the offer is declined (even with logging at level3) it doesn't say why it's doing that: """"Processing DECLINE call for offers: [ 4c1cdf04-d664-4c23-9762-da8df8031716-O280 ] for framework d1047c54-d05d-44fb-92d4-be7b462565c7-0000 (marathon) at scheduler-186208c1-08c7-452e-922a-cfbdeedd4ca4@10.101.0.211:34634""""         Is there any way on how to manage this kind of deployment ?     """,Task,allocation
15689,"""As an operator, I'm periodically checking slave's monitor/statistics endpoint to get the memory/cpu usage/cpu throttle for each running task. However, if there is a short-term memory usage peak (lets say seconds), I might miss it (the memory might have been allocated and also released in between my 2 collect-metrics intervals). Since the max used memory is logged in the `/sys/fs/cgroup/memory/docker/CID/memory.max_usage_in_bytes`, it would be great, if this info would have been exposed in the api as well..""",Improvement,"containerization,HTML,docker"
16147,"""Here is my case: I have a  machine with an IP 10.115.13.142, whose hostname is n115-013-142.byted.org. However, it is a cloud machine in a foreign region. And I can only visit it with IP, but not hostname.      !image-2018-02-01-16-59-24-093.png!    As a result,  I can't see any frameworks/executors from the Webui since the /state and /monitor/statistics APIs are all called with {color:#333333}hostnames{color}.    !image-2018-02-01-17-13-59-420.png!    I {color:#ff0000}replaced hostnames with IPs in controllers.js{color} and it worked fine.    !image-2018-02-01-17-08-54-897.png!    So I'm wondering what's the advantage of using hostnames over IPs. """,Bug,webui
16251,"""/proc/mounts is read many, many times from src/(linux/fs|linux/cgroups|slave/slave).cpp.    When using overlayfs, the /proc/mounts contents can become quite large.   As an example, one of our Q/A single node running ~150 tasks,  have a 361 lines/ 201299 chars  /proc/mounts file.    This 200kB file is read on this node about 25 to 150 times per second. This is a (huge) waste of cpu and I/O time.    Most of these calls are related to cgroups.    Please consider these proposals :    1/ Is /proc/mounts mandatory for cgroups ?   We already have cgroup subsystems list from /proc/cgroups.  The only compelling information from /proc/mounts seems to be the root mount point,   /sys/fs/cgroup/, which could be obtained by a unique read on agent start.    2/ use /proc/self/mountstats    {noformat}  wc /proc/self/mounts /proc/self/mountstats  361 2166 201299 /proc/self/mounts  361 2888 50200 /proc/self/mountstats  {noformat}    {noformat}  grep cgroup /proc/self/mounts  cgroup /sys/fs/cgroup tmpfs rw,relatime,mode=755 0 0  cgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0  cgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0  cgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0  cgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0  cgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0  cgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0  cgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0  cgroup /sys/fs/cgroup/net_cls cgroup rw,relatime,net_cls 0 0  cgroup /sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0  cgroup /sys/fs/cgroup/net_prio cgroup rw,relatime,net_prio 0 0  cgroup /sys/fs/cgroup/pids cgroup rw,relatime,pids 0 0  {noformat}    {noformat}  grep cgroup /proc/self/mountstats  device cgroup mounted on /sys/fs/cgroup with fstype tmpfs  device cgroup mounted on /sys/fs/cgroup/cpuset with fstype cgroup  device cgroup mounted on /sys/fs/cgroup/cpu with fstype cgroup  device cgroup mounted on /sys/fs/cgroup/cpuacct with fstype cgroup  device cgroup mounted on /sys/fs/cgroup/blkio with fstype cgroup  device cgroup mounted on /sys/fs/cgroup/memory with fstype cgroup  device cgroup mounted on /sys/fs/cgroup/devices with fstype cgroup  device cgroup mounted on /sys/fs/cgroup/freezer with fstype cgroup  device cgroup mounted on /sys/fs/cgroup/net_cls with fstype cgroup  device cgroup mounted on /sys/fs/cgroup/perf_event with fstype cgroup  device cgroup mounted on /sys/fs/cgroup/net_prio with fstype cgroup  device cgroup mounted on /sys/fs/cgroup/pids with fstype cgroup  {noformat}    This file contains all the required information, and is 4x smaller    3/ microcaching  Caching cgroups data for just 1 second would be a huge perfomance improvement, but i'm not aware of the possible side effects.            """,Improvement,containerization
16421,"""As a framework author I'd like information about the gpu that was assigned to a task.    `nvidia-smi` for example provides the following information GPU UUID, boardId minor number etc. It would useful to expose this information when a task is assigned to a GPU instance.    This will make it possible to monitor resource usage for a task on GPU which is not possible when""",Improvement,containerization
16937,"""Hi,  The way you guess generated WHL filename relies on a Python wheel bug. Despites the code contains ABI dependent code (the way Python has been built), the generated WHL have """"-none-"""" as ABI restriction. The only way to make it sure to detect the generated filename (proper or buggy ones, depending on wheel version) is to mimic wheel behavior.  Old version:  {noformat} <USER>HI-LIN-3RPRRY1:~$ dpkg -l | grep python-wheel | awk '{ print $3 }' 0.24.0-1 <USER>HI-LIN-3RPRRY1:~$ python -c 'from wheel import pep425tags; print(""""-"""" + """"-"""".join(pep425tags.get_supported()[0]))' -cp27-none-linux_x86_64 <USER>HI-LIN-3RPRRY1:~$ {noformat}  New versions: {noformat} <EMAIL>:~$ dpkg -l | grep python-wheel | awk '{ print $3 }' 0.29.0-2 <EMAIL>:~$ python -c 'from wheel import pep425tags; print(""""-"""" + """"-"""".join(pep425tags.get_supported()[0]))' -cp27-cp27mu-linux_x86_64 <EMAIL>:~$ {noformat}  The patch below allows me to built Mesos with different external wheel version. Be careful, I'm not sure how it behaves when building with bundled Python wheel... It has to be checked...  {noformat} Description: Use pep425tags to compute arch dependent WHL filenames  ABI may not be none depending on the Python version.  .  pep425tags.py extracted from pip master branch, because the one included  in provided pip may not contain everything.  .  glibc stuff has been commented because I don't need manylinux support  and it depends on newer pip.  .  mesos (1.2.0-2) unstable; urgency=medium  .    * Add lsb-release build dependency.    * Use LSB to generate unique binary packages version and pass it to      dh_gencontrol (fix repo mirroring with files pooling systems).    * Add Fix_build_with_GCC_6 patch to fix FTBFS on Stretch.    * Build with bundled because Stretch libboost-dev is way too recent.    * Build with CPPFLAGS += -Wno-deprecated-declarations otherwise      bundled libboost FTBFS.    * Remove python-protbuf (<< 3) dependency, it seems to work fine      (patch).    * Remove unecessary Python google-common>=0.0.1 (patch).    * Add missing dh-python b-dep. Author: Adam Cécile <<EMAIL>>  --- Origin: other Bug: TODO FIXME Forwarded: yes Last-Update: 2017-06-21  Index: mesos-1.2.0/configure.ac =================================================================== --- mesos-1.2.0.orig/configure.ac +++ mesos-1.2.0/configure.ac @@ -2091,10 +2091,8 @@ There are two possible workarounds for t       print """"-py"""" + sys.version[[0:3]]'`     PYTHON_WHL_POSTFIX=`$PYTHON -c \ -    'import sys; \ -     from distutils.util import get_platform; \ -     print """"-cp"""" + sys.version[[0:3]].replace(""""."""", """""""") + """"-none-"""" \ -       + get_platform().replace(""""."""", """"_"""").replace(""""-"""", """"_"""")'` +    'from wheel import pep425tags; \ +     print(""""-"""" + """"-"""".join(pep425tags.get_supported()[[0]]))'`     PYTHON_WHL_PUREPY_POSTFIX=`$PYTHON -c \      'import sys; \ {noformat}  Best regards, Adam.""",Bug,build
18006,"""*Description*: The MachineID lists on the /maintenance/status page are formatted differently. Specifically, with one machine `down` and one machine `draining`, the page is formatted as shown below. Note that the `draining_machines` list has a label (""""id"""") on the machine ID. This formatting extends to multiple items in each respective list.  {code:xml} {   """"down_machines"""": [     {       """"hostname"""": """"agent2"""",       """"ip"""": """"172.17.0.2""""     }   ],   """"draining_machines"""": [     {       """"id"""": {         """"hostname"""": """"agent3"""",         """"ip"""": """"172.17.0.3""""       }     }   ] } {code}  *Steps to reproduce*: # Add two agents to the maintenance schedule # Move one agent to the down state # View the status page at <masterIP:port>/maintenance/status  *Proposed solution*: Unify the format. Either way is fine - personally I would prefer them to both be in the current form of `down_machines` with no label on the MachineIDs - but regardless, they should be the same.""",Bug,HTML
18081,"""The Mesos {{/teardown}} endpoint is: - Removing the framework on the mesos-master. As a result, the framework is in state {{removed}} - Shuts down all executors and tasks running on the Mesos agents  However, I'd also expect that a message from the mesos-master is sent to the framework (Scheduler API) so that the framework processes can initiate a shutdown as well. This is not the case. As a result, it is necessary to manually {{suspend}} the framework, e.g. by using the DC/OS UI.  A possible solution would be to provide an additional callback {{teardown}} at the scheduler API that will notify the framework that the mesos-master has initiated a teardown. Mesos-master should only mark the framework as removed if the framework has been successfully terminated, e.g. the framework could send a message to mesos-master indicating that the termination was successful / has been started.  This change will also affect the {{dcos service shutdown}} command which uses the {{/teardown}} endpoint. From a DC/OS CLI perspective, I'd expect that the {{dcos service shutdown service-id}} command shuts down all components of the framework, not only the executors and tasks.  Also, for consistency reasons I'd expect that this shutdown action can also be taken by using the DC/OS UI. So far on DC/OS, you can only {{Suspend}} a service / framework which will stop the framework instances, but will not remove the framework from mesos-master and terminate it's executors. As far as I am aware there is no documentation that explains in detail the difference between the {{shutdown}} command in the DC/OS CLI and the {{Suspend}} button on the DC/OS UI. A user should carefully understand what these actions are doing with the system, especially if they are not consistent. Again, I'd recommend adding a new button to the DC/OS UI that uses the {{/teardown}} endpoint.  Tested on DC/OS with the frameworks conductr and elasticsearch.""",Improvement,"HTML,cli"
18631,"""Use case:  We have local daemons at every machine in the cluster. They often need to know what tasks is running at the local host and perform operations.  Right now they can only poll `/state` endpoint or operator API. Let them able to subscribe task changes on the agent seems a good improvement.  The only way to be actively notified otherwise is to implement part of all logic as a hook, but doing this directly in agent is better and easier for everyone.  For MVP, something similar to current master API's `TASK_ADDED` and `TASK_STATUS` seems a pretty good starter.""",Improvement,HTML
19537,""" Calls to 'nvidia-smi'  fail inside a container even if access to a GPU has been granted. Moreover, access to /dev/nvidiactl is actually required for a container to do anything useful with a GPU even if it has access to it.      We should grant/revoke access to /dev/nvidiactl and /dev/nvidia-uvm as GPUs are added and removed from a container in the Nvidia GPU isolator.""",Bug,containerization
20516,"""One of the major problems this logger module aims to solve is overflowing executor/task log files.  Log files are simply written to disk, and are not managed other than via occasional garbage collection by the agent process (and this only deals with terminated executors).  We should add a {{ContainerLogger}} module that truncates logs as it reaches a configurable maximum size.  Additionally, we should determine if the web UI's {{pailer}} needs to be changed to deal with logs that are not append-only.  This will be a non-default module which will also serve as an example for how to implement the module.""",Improvement,Modules
20592,"""Basically, in using Firefox, I noticed that over time, my Firefox would get to become unusable when I had Marathon and Mesos WebUIs up and running.  At first, I thought it was a function of my home (Mac) computer and Firefox. But when I started a PoC for work, and my Windows install of Firefox had the same issues, I started doing more investigation.   First of all, both at home and at work, my Firefox is only dedicated to """"cluster"""" related tasks. Thus, I don't have other tabs that are not Cluster UIs.    My typical setup is to have MapR UI, Mesos UI, Marathon UI, Chronos UI, Myriad UI, and Yarn UI all up and running.    After about 3-4 hours, my browser would get really slow, and non-responsive. I'd kill all and start again.  Rinse repeat.    So I did some analysis, and basically found a plugin for Firefox that shows on each tab the amount of memory being used. I found that both Marathon and Mesos UI were the culprits, and things got really bad after just 2-3 hours.  With those setup, on Windows, I have the following memory usage:  MapR UI: 7.7mb Myriad UI: 9.8mb Yarn: 2.7 mb Chronos 10.2 mb Marathon: 163 mb Mesos UI: 463 mb  Both Marathon and Mesos continually climb, slowly, up some, down a few, up some more, but obviously generally trending up.  I guess I wanted to toss it out here to see if it's something in my settings, or something that others are seeing.  It's a problem for me from a usability standpoint, and I am guessing that it's one of those things that while not a priority for a project, should be looked at. """,Bug,webui
21650,"""As an operator, I might not want frameworks using the experimental dynamic reservations/persistent volumes APIs in 0.23, since there are no ACLs or operator endpoints for me to manage them. That means that a rogue framework could start reserving resources and creating volumes with all resources provided, and I would have no way to clean them up. Is it possible to disable these features from the master (flags, etc.)?""",Improvement,master
22334,"""MESOS-1886 changed default behavior.  Prior to commit 8682569d being merged, docker pull was always executed before a """"docker run"""".  If I'm reading the commit correctly, docker pull is now only executed when DockerInfo.force_docker_pull is specified as true in the protobuf.   We can't change behavior like this without a deprecation cycle.  Make force_docker_pull to default to true before any releases with this change go out, i.e. 0.22.0""",Bug,docker
22752,"""h4. Motivation Render a well structured and mesos-alike naming and folder tree. This shall be similar to what we know from the awesome! containerizer structure.  h4. Folder Structure For the flat-file Authenticator Module but also for authentication as a whole, I would like to propose the following structural changes:  - move {{authenticator.hpp}} out of {{sasl/}} - create a new {{authentication/}} -- put the module interface here and name it {{authenticator.hpp}} - create a new {{authentication/cram_md5}} -- put the default/module implementation here and name it {{authenticator.hpp}}  -- mind that I am proposing {{cram_md5}} instead of {{sasl}} due to the fact that cyrus-sasl2 offers many more mechanisms but our current default (CRAM-MD5) and further dividing the tree into sasl and non-sasl based authentications may be too verbosive/bloated.  h4. Module Naming Following the module naming scheme, I would like to propose {{org_apache_mesos_authenticator_crammd5}} to be used for our first/default Authenticator. Mind that in this case I did not reflect the folder tree into this reverse domain alike description as I find it more intuitive to have the module before the kind - objections?  h4. Library Naming Given that we will also add an authenticatee to this library, a proper name would be  {{libauthentication_crammd5.so/dylib}} """,Improvement,Modules
22754,"""h4. Motivation Design an interface covering authenticator modules while staying minimal invasive in regards to changes on the existing flat-file Authenticator implementation.  h4. Status Quo  The current sasl based, """"flat-file"""" Authenticator ( {{sasl/authenticator.hpp}} )  is spawning a (libprocess) process in its constructor and demands the authentication client process UPID as a parameter - not a perfect fit for the current Module API as that one does only allow default constructors.   h4. Design Instead of wrapping the flat-file Authenticator with yet another class that satisfies a proper interface design, we might just as well slightly change the current design into a feasible interface, no?   I would like to propose this:  {noformat} class Authenticator { public:   Authenticator() {}   virtual ~Authenticator() {}    virtual Option<Error> initialize(const process::UPID& clientPid, const Option<Credentials>& credentials) = 0;   virtual process::Future<Option<std::string> > authenticate(void) = 0; }; {noformat}  As a result, the flat-file authenticator would spawn its process within that proposed initialize method and not within the constructor already.""",Improvement,Modules
22854,"""Currently Mesos supports the ability to reserve resources (for a given role) on a per-slave basis, as introduced in MESOS-505. This allows you to almost statically partition off a set of resources on a set of machines, to guarantee certain types of frameworks get some resources.  This is very useful, though it is also very useful to be able to control these reservations through the master (instead of per-slave) for when I don't care which nodes I get on, as long as I get X cpu and Y RAM, or Z sets of (X,Y).  I'm not sure what structure this could take, but apparently it has already been discussed. Would this be a CLI flag? Could there be a (authenticated) web interface to control these reservations?  Follow up epic: MESOS-6514.""",Epic,"master,allocation"
23071,"""I recently had an issue where a slave had a process who's parent was init that was bound to a port in the range that mesos thought was a free resource.  I'm not sure if this is due to a bug in mesos (it lost track of this process during an upgrade?) or if there was a bad user who started a process on the host manually outside of mesos.  The process is over a month old and I have no history in mesos to ask it if/when it launched the task :(  If a rogue process binds to a port that mesos-slave has offered to the master as an available resource there should be some sort of reckoning.  Mesos could:     * kill the rogue process    * rescind the offer for that port    * have an api that can be plugged into a monitoring system to alert humans of this inconsistency""",Improvement,"containerization,allocation"
23133,"""I've noticed with mesos 0.18 that task sandbox browse requests 404 when using work dir outside of /tmp and I've been unable to determine why. I've verified that the work dir outside of /tmp has at least 755 permissions for each directory created by mesos (as well as the root work directory) and there are no related log entries on the slaves indicating any issue. Changing the work dir to /tmp/mesos on the same cluster causes these requests to succeed.""",Bug,webui
24384,"""Exposing that will make it easier to check and manage a cluster of machines. It might make sense to export all of the resources on the command line, as well as all of the attributes. We currently get something like:     """"resources"""": {         """"cpus"""": 8,          """"mem"""": 4096     },   Even though on the command line we have:  sbin/mesos-slave --port=5051 --resources=cpus:8;mem:4096;ports:[31000-32000];disk:400000 --attributes=rack:r1;host:dc-r1-somehost;dedicated:foo --master=file:///usr/local/mesos/conf/zk.config --log_dir=/var/log/mesos --work_dir=/var/tmp/mesos  I'd love to see this (and if there are other things """"defaulted"""") to something like:      """"resources"""": {         """"cpus"""": 8,          """"mem"""": 4096,         """"disk"""": 400000,         """"ports"""": """"[31000-32000]""""     },      """"attributes"""": {         """"rack"""": """"r1"""",         """"host"""": """"dc-r1-somehost"""",         """"dedicated"""": """"foo""""     }""",Improvement,webui
24461,"""I've attached a patch for cdh3u3 which uses the patch for 0.20.205 as a bases plus adds protobuff jar and a mesos-0.9.0.jar compiled from RC3.  I tested it on my local machine with wordcount to validate the tasks run on mesos.""",Improvement,build
27592,"""{quote}As a developer,  I want to be able to mark an option as being the option to use for cancelling an OptionDialog in Alloy XML,  So that I can use OptionDialog in view without dependency on controller.  {quote}    Currently, I can do this:    {code:xml}         <OptionDialog id=""""dialog"""" title=""""Delete File?"""" cancel=""""2"""" >              <Options>                  <Option>Confirm</Option>                  <Option platform=""""ios"""">Help</Option>                  <Option>Cancel</Option>              </Options>            </OptionDialog>    {code}    However, if an option is not displayed due to conditional code, then the index number will no longer be valid.    Proposed new attribute:    {code:xml}         <OptionDialog id=""""dialog"""" title=""""Delete File?"""">              <Options>                  <Option>Confirm</Option>                  <Option platform=""""ios"""">Help</Option>                  <Option cancel=""""true"""">Cancel</Option>              </Options>            </OptionDialog>    {code}  """,Improvement,XML
27673,"""This is a new attempt after ALOY-968 was closed with little discussion.    The following view snippet:    {code:xml}  <Label focusable=""""false"""" borderWidth=""""5"""" borderColor=""""#000"""">Hello, world!</Label>  {code}    Will compile to:    {code:javascript}      $.__views.__alloyId0 = Ti.UI.createLabel({          width: Ti.UI.SIZE,          height: Ti.UI.SIZE,          color: """"#000"""",          text: """"Hello, world!"""",          focusable: """"false"""",          borderWidth: """"5"""",          borderColor: """"#000"""",          id: """"__alloyId0""""      });  {code}    For some properties/values (mostly Number) this is not a problem, but {{focusable: """"false""""}} will evaluate to {{true}}.    Of course these could and in most cases _should_ be set in TSS which supports Boolean and Number types. However, in particular boolean flags that impact the markup/layout (show, hide, animate..) it can make sense to set them in the XML view.    Simply casting all {{/^[0-9]+$/}} values to Number and all {{/^(true|false)$/}} to Boolean might break apps and will make it impossible to keep them as strings if that is in fact what you want.    A workaround for Boolean values would be to set {{Alloy.Globals.TRUE = true;}} in {{alloy.js}} and then use this as value in the XML. Maybe we can introduce {{Ti.TRUE}} and {{Ti.FALSE}} as an official """"workaround""""?    I must say I'm not keen on either two.. so open for other ideas.""",Improvement,XML
27954,"""Alloy currently has a feature of specifying a module attribute against tags, allowing a custom JS module to be used to create the elements, enabling developers to return modified or different elements.    e.g:    <Label module=""""ui'>Foo</Label>    It's a very powerful feature and means we can create our custom tags for elements that are OS specific e.g. TitleControl and have that return a TitleControl for iOS (no change) or a Ti.UI.View for Android, with custom title text etc.    It would be useful to be able to add this namespace / module element ONCE so that you don't have to add it to every / multiple tags in a view.    So, adding the ability to specify it in the Alloy tag would mean for every tag instance, the custom module is checked first.    <Alloy module=""""ui.js""""> for example?""",Improvement,XML
28034,"""As a developer, I'd like to be able to enter shortcut strings when declaring TextFields in XML or styling them in TSS. For example:    {code}  <TextField id=""""txt"""" keyboardType=""""DECIMAL_PAD"""" returnKeyType=""""DONE""""/>  {code}    {code}  """"#txt"""": {     keyboardType: """"DECIMAL_PAD"""",    returnKeyType: """"DONE""""  }  {code}    This is done for SystemButtonType constants (see code in parsers/Ti.UI.Button.js)    All keyboardType constants begin with Ti.UI.KEYBOARD_    All returnKeyType constants begin with Ti.UI.RETURNKEY_      """,Improvement,XML
28257,"""By default, the first root-level element of a view will get the view's name as ID. If you assign the same ID to a child element, this will result in conflicts.    I would suggest a simple check to be made by the compiler, which I will add as a PR soon.""",Improvement,XML
28394,"""h2. update (2/25/2013)    The new syntax will be, allowing for the existing syntax as well as allowing for a prepared statement to be defined with an object.    {code:javascript}  // plain query  collection.fetch({      query: 'select * from some_table where column1 = """"somevalue""""'  });    // prepared statement  collection.fetch({      query: {          statement: 'select * from some_table where column1 = ?',          params: [values]      }  });  {code}    h2. original    Currently there is no support for prepared statements.  The only way to make an SQL statement with parameters is to have code insert the parameters.  This is a security risk, a bad practice, and reflects poorly on Alloy models as a whole.    Currently I have to:     collection.fetch({query: """"select * from some_table where column1 = '"""" + value + """"'"""");    Ideally I would like to:    collection.fetch({query: """"select * from some_table where column1 = ?"""" + params: [value]});    Assuming this uses the Ti.Database.execute() in the background, this should be a small change as the execute() method already supports this.""",Improvement,Runtime
28439,"""When running the models/journal test app, I'm getting a series of this warning in the console:    {code}  [DEBUG] [WARN] Called remove for [object container] on [object scroll], but [object container] isn't a child or has already been removed.  [DEBUG] [WARN] Called remove for [object container] on [object scroll], but [object container] isn't a child or has already been removed.  [DEBUG] [WARN] Called remove for [object container] on [object scroll], but [object container] isn't a child or has already been removed.  {code}    and in the app the entries are not being cleared after the fetch(), causing lots of duplicates to appear in the scrollview. We need to figure out a reliable way to empty all views from a proxy's view hierarchy.""",Bug,Runtime
28482,"""In ALOY-370, a fix was put in place so that developers would be able to define functions for markup event handlers with either function declarations or expressions.     {code:javascript}  // function declaration  function doClick() {}    // function expression  var doClick = function() {}  {code}    Expressions did not work before ALOY-370 as the event handlers were created before the function assignment, causing the event handler callback function to be undefined. To get around this, the reference to the event handler function was deferred inside of an anonymous caller. So:    {code:javascript}  $.someProxy.on('click', myFunction);    // became    $.someProxy.on('click', function() {      myFunction.apply(this, Array.prototype.slice.apply(arguments));  });  {code}    In this way, it didn't matter how the event handler function was defined or where, it would be in scope and defined by the time the event handler was actually fired. The problem is that now when you attempt to remove the event handler from the above code, it doesn't work because the functions won't match. The developer will be passing 'myFunction', but the eventing system has the anonymous function as its callback.    {code:javascript}  // won't work since we wrapped myFunction in an anonymous function  $.someProxy.off('click', myFunction);  {code}    So the problem is how do we maintain the ability to reference both function declarations and expressions, but still be able to have developers simply remove these functions without having to worry about an anonymous wrapper?    h3. possible options    # We go back to not supporting function expressions. This might not be so bad as all alloy examples use function declarations, and it remained like that up til version 0.3.2 before 1 person mentioned it. A small bit of documentation in the guides would make this easy to convey.  # We move the event handler declaration code after the controller code. This seems like a logical, simple solution, but there is a distinct drawback. A developer will not be able to manually trigger these events if they want from the controller during its initial execution. For example:  {code:javascript}  // this would still work  $.win.on('open', function() {      $.someProxy.trigger('click');  });    // but this would not, but it also wouldn't throw an exception. It just wouldn't fire  // any event since the event handler would not have been defined yet.  $.someProxy.trigger('click');  {code}  This may not be a very common use case with UI components, but it is with <Model> and <Collection> elements, which use the same method for tying markup eventing to code. For example, devs often call model.fetch() in their controllers, which wouldn't fire any event handlers if we push the event handler code to the end.  # Use AST manipulation to either move the function expressions to the top of the code, or convert them to function expressions. I always prefer to make AST manipulation for this sort of thing a last resort. This would likely also be shot to hell if the developer employs any kind of inheritance in their controller, as the function I'm looking for may reside in another file.  # Something I haven't thought of yet.""",Bug,Runtime
28825,"""h2. Description    Right now, the markup syntax for {{Ti.UI.iPad.SplitWindow}} assumes that it has 2 children, and that each is a {{Ti.UI.Window}}. It also assumes that the first window is the {{masterView}} and that the second is the {{detailView}}. While this convention over configuration may fit for this component, I wanted to throw out a few alternatives to see what others think.    h2. Alternatives    h4. Add custom attribute to child windows  {code:xml}  <SplitWindow ns=""""Ti.UI.iPad"""">      <Window split=""""master""""/>      <Window split=""""detail""""/>  </SplitWindow>  {code}    h4. Enforce IDs of master and detail (I don't like this one, but it's a possibility)  {code:xml}  <SplitWindow ns=""""Ti.UI.iPad"""">      <Window id=""""master""""/>      <Window id=""""detail""""/>  </SplitWindow>  {code}    h4. Check class for master and detail  {code:xml}  <SplitWindow ns=""""Ti.UI.iPad"""">      <Window class=""""master otherclass""""/>      <Window class=""""detail""""/>  </SplitWindow>  {code}    h4. Use an entirely new markup  {code:xml}  <SplitWindow ns=""""Ti.UI.iPad"""">      <Master>          <!-- Implicitly creates a window, so you can put any markup in here ->      </Master>      <Detail></Detail>  </SplitWindow>  {code}    h2. Thoughts?    I kinda like the entirely new markup, cause its clear what's going and you don't need to explicitly list the Window. Though perhaps it is overkill for a component that has a pretty specific configuration. Anyone else?""",Improvement,XML
29121,"""Hi,    I've found a strange behavior while using the variables view in <USER>Studio 3 (build: 3.6.0.201407100658) as well as RubyMine 6.3.3 (Build #RM-135.1104) during debugging.    What I have done:  I have a small script for demo  #################################  #!/usr/bin/env ruby    def print_array_    puts 'now in print_array'  end    def print_hash_    puts 'now in print_hash'  end    def print_string_    puts 'now in print_string'  end    sample_ary = [1,2]  sample_str = 'sample string'  sample_hsh = {'ary' => [1,1],                'str' => 'sample'}    puts 'Breakpoint 1'            puts 'Breakpoint 2'            puts 'Breakpoint 3'            #################################    The last tree lines were break points in the debugger.      This script works well with the method names given in the script and the content of variables was displayed in the variables view when the first breakpoint was reached (see files *working_fine.png).    After renaming the method's names by removing the last underscore, the behavior during debugging becomes strange. Using the name print_array (without trailing underscore) and trying to see the content of the sample_ary at the first breakpoint, aptana and rubymine skipped the breakpoint and the variable view changed to the screen seen in the files *print_array.png. The behavior was similar when print_hash_ was renamed to print_hash and I tried to see the content of sample_hsh.    When renaming print_string_ to print_string RubyMine gave the same result. In Apatana the picture was a little different (see <USER>- print_string.png).    I'm unable to determine the exact reason for this behavior. Is it ruby-1.9.3-p547, ruby-debug-ide 0.4.22, ruby-debug-base19x 0.11.30.pre15, <USER>Studio or RubyMine? Maybe print_array is a reserved name for the use of ruby-debug-ide, which uses the name for an internal method.    Please keep me informed about your opinion on this topic.    Best regards  Bertram      """,Bug,Debugging
29300,"""Traceback:      !SESSION 2013-12-18 23:34:41.139 -----------------------------------------------  eclipse.buildId=unknown  java.version=1.7.0_45  java.vendor=Oracle Corporation  BootLoader constants: OS=linux, ARCH=x86_64, WS=gtk, NL=en_US  Command-line arguments:  -os linux -ws gtk -arch x86_64    !ENTRY org.eclipse.ui 2 0 2013-12-18 23:34:45.651  !MESSAGE Warnings while parsing the images from the 'org.eclipse.ui.commandImages' extension point.  !SUBENTRY 1 org.eclipse.ui 2 0 2013-12-18 23:34:45.651  !MESSAGE Cannot bind to an undefined command: plug-in='com.appcelerator.titanium.android.ui', id='com.appcelerator.titanium.mobile.command.run_on_device.android'  !SUBENTRY 1 org.eclipse.ui 2 0 2013-12-18 23:34:45.651  !MESSAGE Cannot bind to an undefined command: plug-in='com.appcelerator.titanium.android.ui', id='com.appcelerator.titanium.mobile.command.debug_on_device.android'    !ENTRY com.aptana.scripting 4 0 2013-12-18 23:34:47.344  !MESSAGE (Build 3.2.0.201312171855) [ERROR]  Failed to load bundle /home/<USER>Downloads/Compressed/Titanium_Studio/plugins/com.appcelerator.titanium.core_3.1.2.1387086213/bundles/titanium_mobile.ruble  !STACK 0  org.yaml.snakeyaml.error.YAMLException: Cannot create property=children for JavaBean=bundle """"titanium_mobile"""" {    bundle_precedence: APPLICATION    path: /home/<USER>Downloads/Compressed/Titanium_Studio/plugins/com.appcelerator.titanium.core_3.1.2.1387086213/bundles/titanium_mobile.ruble/bundle.rb    author: <USER>Aylott/Appcelerator    copyright: null    description: null    repository: null  }  ; Cannot create property=icon for JavaBean=project_sample """"Geocoder"""" {    id: null    category:   com.appcelerator.titanium.mobile.samples.category    name: Geocoder    location: null    description: A sample Master/Detail app that uses native maps to plot locations. With it you can forward geocode addresses and add them as annotations to the map.  }  ; No single argument constructor found for interface java.util.Map   at org.yaml.snakeyaml.constructor.Constructor$ConstructMapping.constructJavaBean2ndStep(Constructor.java:290)   at org.yaml.snakeyaml.constructor.Constructor$ConstructMapping.construct2ndStep(Constructor.java:191)   at com.aptana.scripting.model.BundleCacher$BundleElementsConstructor$ConstructBundleElement.construct(BundleCacher.java:816)   at org.yaml.snakeyaml.constructor.BaseConstructor.constructObject(BaseConstructor.java:181)   at org.yaml.snakeyaml.constructor.BaseConstructor.constructDocument(BaseConstructor.java:140)   at org.yaml.snakeyaml.constructor.BaseConstructor.getSingleData(BaseConstructor.java:126)   at org.yaml.snakeyaml.Yaml.loadFromReader(Yaml.java:296)   at org.yaml.snakeyaml.Yaml.load(Yaml.java:290)   at com.aptana.scripting.model.BundleCacher.load(BundleCacher.java:328)   at com.aptana.scripting.model.BundleCacher.load(BundleCacher.java:286)   at com.aptana.scripting.model.BundleManager$BundleLoadJob.run(BundleManager.java:110)   at org.eclipse.core.internal.jobs.Worker.run(Worker.java:53)  Caused by: org.yaml.snakeyaml.error.YAMLException: Cannot create property=icon for JavaBean=project_sample """"Geocoder"""" {    id: null    category:   com.appcelerator.titanium.mobile.samples.category    name: Geocoder    location: null    description: A sample Master/Detail app that uses native maps to plot locations. With it you can forward geocode addresses and add them as annotations to the map.  }  ; No single argument constructor found for interface java.util.Map   at org.yaml.snakeyaml.constructor.Constructor$ConstructMapping.constructJavaBean2ndStep(Constructor.java:290)   at org.yaml.snakeyaml.constructor.Constructor$ConstructMapping.construct2ndStep(Constructor.java:191)   at com.aptana.scripting.model.BundleCacher$BundleElementsConstructor$ConstructProjectSampleElement.construct(BundleCacher.java:921)   at org.yaml.snakeyaml.constructor.BaseConstructor.constructObject(BaseConstructor.java:181)   at org.yaml.snakeyaml.constructor.BaseConstructor.constructSequenceStep2(BaseConstructor.java:275)   at org.yaml.snakeyaml.constructor.BaseConstructor.constructSequence(BaseConstructor.java:246)   at org.yaml.snakeyaml.constructor.Constructor$ConstructSequence.construct(Constructor.java:502)   at org.yaml.snakeyaml.constructor.BaseConstructor.constructObject(BaseConstructor.java:181)   at org.yaml.snakeyaml.constructor.Constructor$ConstructMapping.constructJavaBean2ndStep(Constructor.java:287)   ... 11 more  Caused by: org.yaml.snakeyaml.error.YAMLException: No single argument constructor found for interface java.util.Map   at org.yaml.snakeyaml.constructor.Constructor$ConstructScalar.construct(Constructor.java:372)   at org.yaml.snakeyaml.constructor.BaseConstructor.constructObject(BaseConstructor.java:181)   at org.yaml.snakeyaml.constructor.Constructor$ConstructMapping.constructJavaBean2ndStep(Constructor.java:287)   ... 19 more    !ENTRY org.eclipse.e4.ui.css.core 4 0 2013-12-18 23:34:50.189  !MESSAGE CSS property 'maximize-visible' has been deprecated: renamed as swt-maximize-visible    !ENTRY org.eclipse.e4.ui.css.core 4 0 2013-12-18 23:34:50.190  !MESSAGE CSS property 'minimize-visible' has been deprecated: renamed as swt-minimize-visible""",Story,Editor
29365,"""JSDT and JDT allow for a very convenient selection mechanism called expanding selection.   While editing a Java file this is available under the Edit > Expand Selection To menu.  Sublime Text 2 also has an expand selection mechanism that works nearly identically.    In a quick summary it can start with selecting the word your cursor is on, then another up leads to statement, then another up goes to block, or function, then another up is parent block or function.  For functions its smart enough to do block, then params, and then entire function definition.  Needless to say this is extremely powerful for moving and understanding code - especially when outlines dont show the full story.    The goal would be for all reasonable editors to have this feature.  JavaScript should be an initial implementation since its nice and bracey, but ultimately any editor would benefit from it.  Frankly this is the only reason I even would consider downgrading to JSDT again.  Its extremely helpful in diagnosing syntax errors, or bracing errors, or refactoring large blocks such as in Ext js development.    Without it, I'm left to little recourse except install some other overlay editor like Vrapper (please help me escape vim) or downgrading to JSDT or using another editor entirely.    <USER>has come a long way since I used it off and on a long time ago, keep up the great work!  """,Improvement,Editor
29556,"""I've recently moved from <USER>2 to <USER>3 when upgrading my eclipse version. I have noticed that the Synchronize function checks the whole project even though i right click on a subfolder of the project and click Synchronize from there.  In <USER>2 if i started the Synchronization from a subfolder it only checked from that point on, and it was a very useful feature. I know i can do a similar thing by using the transfer files feature and selecting the 2 resources (local and remote) i want to synchronize, but it's more time consuming than the old method.    I don't know if this affects previous or later versions of <USER> but i'd like to have this feature working again""",Improvement,Publishing
29622,"""When passing an anonymous function as an argument, <USER>gets anry and yells at you.  Pretty trivial fix (I think), just thought I would let you guys now.    Thanks!""",Bug,PHP
29641,"""As a developer,  I created a plugin project on Eclipse indigo 3.7 with the standard tamplate “Plugin with a view” by the wizard.  I export the plugin project to jar file by the standard export wizard.   Then I want test it on <USER>studio 3 ( Windows platform) so I put it in dropins folder. After lunched <USER>studio 3 I expected to find the view of the plugin on Windows -> Show view -> Other-> Simple category but I can’t find Simple category.  I performed the same test with the jar file in plugins folder, with the same wrong result.  With the following configuration I can find plugin’s view:  - Eclipse Indigo 3.7 ( on Windows platform)  - Eclipse Indigo 3.7 with aptana plugin (on Windows platform)  - Eclipse indigo 3.7 (on Ubuntu 12.04)  - Eclipse Indigo 3.7 with aptana plugin (on Ubuntu 12.04)  - <USER>studio 3 ( on Ubuntu 12.04)  Only with <USER>studio 3 (on Windows) I can’t find the plugin.    """,Bug,Performance
29654,"""For the story: I wanted to find a WEB IDE which that I can manage my web projects (essentially PHP/SQL/HTML/CSS/JAVASCRIPT), then I choose <USER>Studio (based on eClipse)!    The feature I would want in <USER>will be the AutoSwitcher of Charset when opening files.    ** As an example, you should look to the free netBeans web Editor (which it auto-analyze the charset of ANY files opened in the editor and then switch to the correct charset (ISO-8859-1/UTF-8) automatically).    In <USER>Studio (it's sad), I don't understand why a such of feature isn't available.    All we can do in <USER>is choose GLOBALLY (for the project) or FOR EACH FILE (before opening) the charset to use..    The problem is, I use sometimes files which I don't know (before opening) the charset of the file which will be opened. And then, <USER>mess all the code replacing NON-UTF-8 character by incorrect characters.    **(for example: a black filled losange with a white question-<USER>inside)..    It will be certainly more easy for ANY of US to have a native feature which can detect the correct charset to use.    And since netBeans can do that, and it's also free, I don't understand why in <USER>it will be impossible to have a such of feature !    Thanks for reading and answers.      Happy holidays!    """,Improvement,"CSS,Editor"
29662,"""To whom it may concern,    I'll preface this by saying that I'm new to Rails and Ruby, but I started using <USER>Studio 3.3.1.201212171919 (current version) for a Rails project.    I have a test case for a controller and I wanted to set a breakpoint in the test case. I set a breakpoint and then ran the debugger. The debugger correctly stops at the breakpoint in the test case, but instead of opening my test case file accounts_controller_test.rb and highlighting the breakpiont line, it opens a test runner file setup_and_teardown.rb and highlights the breakpoint line in that file.    I believe this has to do with the fact that in the case of a test """"description """" do [block] end statement, the code is being passed in as a block, which must confuse the <USER>debugger. When I have the test     class AccountsControllerTest < ActionController::TestCase   ....    test """"should get new"""" do      # I set a breakpoint to the below line, line 63 in my file. The <USER>debugger stops at it, but then opens setup_and_teardown.rb and highlights line 63 in that file instead.      sign_in users(:john)      get :new      assert_response :success    end  end    If I use a regular method e.g. def test_require_login ....  end instead of test """"require login"""" do [block] end, then I don't have this problem, i.e. the <USER>debugger stops at the breakpoint and correctly opens my source file accounts_controller_test.rb and stops at the breakpoint.    I have included Screen shots to make it clearer.     I think <USER>Studio is a great product from my initial use of it.""",Bug,Debugging
29791,"""Hi there, I am <USER>Studio newbie and I am not sure if this bug was already submitted, but I would like to find out if Refresh Running Browser(s) works in Windows. I did some research and all I could find was the information that a specific key shortcut assigned for this command was removed from Mac.  I created a project with html and css files in <USER> I made some change to HTML file and then I opened IE9, FF15 and Chrome22 through <USER>(using a white arrow in green circle icon on the menu)and then I selected Command->HTML->Refresh Running Browser(s). Unfortunately none of the browsers were refreshed. I appreciate your prompt response on my inquiry. Thank you.""",Bug,HTML
30013,"""I searched for this case but could not find it. To reproduce:    # Assure PyDev > Editor > Hover > Show docstrings? is checked  # Open a python module that is big enough to have a scroll bar  # Hover over a variable or import statement to pop up the docstring tooltip  # Scroll with the middle mouse wheel    The hover tooltip remains on screen and the only way to get rid of it is to highlight/select the text under the tooltip, or scroll farther so the tooltip goes off screen.     Since the delay is pretty short and I don't worry about where I leave the cursor when navigating a script I run in to this quite often. If this is an esoteric issue one workaround I could see would be to be able to assign a hotkey for tooltips, such as Alt. For example when I press Alt and hover over a variable I want to see the docstrings for, they appear. then I would be much less likely to use the middle mouse button since the action was intentional.""",Bug,PyDev
30155,"""This problem is apparent at least when running the GTK version of Eclipse under X11 forwarding using the Hummingbird Exceed as an X server, but it could be applicable to other situations where there is a latency in GUI drawing as well.  The problem is that the ProgressMonitorDialog sends updates of task names directly to screen, causing a resource starvation due to X11 traffic. This is most apparent when restoring module info in the Pydev interpreter preferences panel.  The problem is actually even worse because I have received reports that this actually causes the resolution process itself to take a very long time to complete (I've heard reports in the order of 15-20 minutes as opposed to maybe 30 seconds locally). I believe that this problem is only apparent in the ProgressMonitorDialog, not in e.g. background jobs and the Progress Monitor.  I have a solution for this, and the results look promising. The proposed solution is to create a subclass of ProgressMonitorDialog that catches task name changes and holds back updates that occur too often, while still displaying the last known update (after a given time interval, here 300 ms).  I am attaching the source for my overridden version of ProgressMonitorDialog so you can have a look at it. If there is interest I could make it into a patch.  Please let me know if this could be looked into, either using a solution like the one I proposed here, or at least by significantly increasing the modulus in name changes (now e.g. """"if(i % 17 == 0)\{""""). As a side note, name changes seems to be *really* expensive in the context of a ProgressMonitorDialog --- even when running locally. This could be because (among other things?) they use synchronous event loop calls (see my test code if you're interrested).  Anyway, I guess this means you really sped up module loading :)""",Bug,PyDev
30192,"""For the statement:     InterfaceProvider.getTerminalPrinter().print(msg) Pydev reports the error:     Encountered """"print"""" at line 47, column 44. Was expecting:     <NAME>  This is Python 2 (not 3) series, so print is a keyword, not a function. """"print(x)"""" is indeed invalid. However, I'm running Jython, and InterfaceProvider.getTerminalPrinter() returns a Java class that has a method call print. So the print(x) is actually ok.  Is there a way I can persuade Pydev to not report a syntax error on that line? This is the latest Pydev (1.5.3)  """,Bug,PyDev
30201,"""Hello,  I just started to use pydev, and it looks great!  However, I've noticed one repeating warning, which I think is superfluous. Pydev warns about an """"unused variable"""" even when that variable is created as a part of tuple unpacking. For example:  x, y = get_xy() do_something_with(x)  for x, y in coords:     do_something_with(x)  In those cases, it's not a mistake to define y, even though it's unused - it's the most readable way to get x from the tuple. I could replace 'y' with '_', but then, if I find out that I need to use the second element, I'll have problems finding out what's that second element is.  Could this be fixed?  Thanks, Noam""",Bug,PyDev
30207,"""System: Windows Eclipse 3.5.1 PyDev: 1.5.0, 1.5.1 Eclipse install location: c:\eclipse Project Location: c:\eclipseWorkspaces\Fable\xxx Log: Attached -> Not attached.  It got rejected.  After updating some projects via SVN, Eclipse hung building the workspace at 18%.  The Progress dialog had these items.  Building Workspace   Pydev: Analyzing 1 of 6 (ImagePlot.py) PyDev: Restoring projects python nature  Building Workspace is the first process and is running. """"PyDev: Restoring projects python nature"""" is the second process, which is waiting.  Pressing Cancel on the second process just gives a """"Cancel requested"""" message, as does pressing cancel on """"Building Workspace"""".  Before this SVN update I was getting no PyDev errors on any projects, including the ones that were updated (though I'm not sure I trust it).  I tried restarting Eclipse, but it would not let me shut down owing to the building workspace. I killed the Java process and restarted.  This situation continued after several restarts.  It gets to the same place and has to be killed.  1. It hangs. 2. It won't shut down, leaving the progress dialog with these two processes. 3. It must be killed.  I was able to change the preferences to uncheck doing analyze.  I verified that it was, in fact, still unchecked after a restart.  I was able to upgrade to 1.5.1.1258496115.  The original was 1.5.0.  The situation continued.  I unistalled PyDev and the problem went away.  I cannot provide more hanging information since I no longer have PyDev.  I have a deadline to meet this week, and I will not be available to try reinstalling PyDev until in December.  The behavior with Analyze has been flakey since I installed 1.5.0.  I don't understand what it is doing, and there appears to be no such information available.  I have gone from > 1000 errors to none with only minor changes.  The cause and effect of changes is not clear to me.  It is not clear how to make it reanalyse, though Project | Clean appears to work.  Before uninstalling PyDev, I had one project that never showed errors on Windows, but gave 1404 errors on Linux, while the ones that had trouble in Windows, showed no problems on Linux.  It is a mystery to me.  I also had problems with the errors showing in the Navigator and on Tabs, but not appearing in the Problems view.  Error Uploaded file must be no larger than 256k. """,Bug,PyDev
30223,"""Eclispe has recently been recommended to me as a good IDE, so I have downloaded Eclipse 3.5.1 and got the latest Pydev plugin 1.5.2.xxx and tried a simple """"hello world"""" script. The script ran, but also generated the following error:  Traceback (most recent call last):   File """"/opt/nick/system_addons/eclipse/plugins/org.python.pydev_1.5.2.1260362205/PySrc/pydev_sitecustomize/sitecustomize.py"""", line 142, in ?     sys.path.extend(paths_removed) AttributeError: 'NoneType' object has no attribute 'path' Hello World  I have tested sys.path and sys.path.extend and both work fine so I have no idea what this error is complaining about, but since I'm running Pydev and Eclipse straight out of the box, I think this qualifies as a bug. I'm running python 2.4.3 on linux (Centos 5).""",Bug,PyDev
30267,"""I'm getting all my django imports as unresolved even though django runs correctly. For example """"from django.shortcuts import render_to_response"""" without the quotes shows up as an error in Eclipse as an unresolved import despite the fact it works. I'm new to Pydev and Django, but I'm using the latest version of Django (1.5.6) and I'd really like to get this working. Thanks! """,Bug,PyDev
30293,"""Windows Server 2003 Eclipse 3.4.2 pydev 1.4.5.2721 (logged against 1.4.4 because 1.4.5 isn't in the tracker's group list) Python 3.1a1 (r31a1:70244, Mar  8 2009, 18:15:03) [MSC v.1500 32 bit (Intel)] on win32  import pickle print(dir(pickle)) pickle.|<-(code completion -- see screenshot) pickle.load(""""test.pck"""")  The print statement outputs (correctly):  ['APPEND', 'APPENDS', 'BINBYTES', 'BINFLOAT', 'BINGET', 'BININT', 'BININT1', 'BININT2', 'BINPERSID', 'BINPUT', 'BINSTRING', 'BINUNICODE', 'BUILD', 'BuiltinFunctionType', 'DEFAULT_PROTOCOL', 'DICT', 'DUP', 'EMPTY_DICT', 'EMPTY_LIST', 'EMPTY_TUPLE', 'EXT1', 'EXT2', 'EXT4', 'FALSE', 'FLOAT', 'FunctionType', 'GET', 'GLOBAL', 'HIGHEST_PROTOCOL', 'INST', 'INT', 'LIST', 'LONG', 'LONG1', 'LONG4', 'LONG_BINGET', 'LONG_BINPUT', 'MARK', 'NEWFALSE', 'NEWOBJ', 'NEWTRUE', 'NONE', 'OBJ', 'PERSID', 'POP', 'POP_MARK', 'PROTO', 'PUT', 'PickleError', 'Pickler', 'PicklingError', 'PyStringMap', 'REDUCE', 'SETITEM', 'SETITEMS', 'SHORT_BINBYTES', 'SHORT_BINSTRING', 'STOP', 'STRING', 'TRUE', 'TUPLE', 'TUPLE1', 'TUPLE2', 'TUPLE3', 'UNICODE', 'Unpickler', 'UnpicklingError', '_EmptyClass', '_Pickler', '_Stop', '_Unpickler', '__all__', '__builtins__', '__doc__', '__file__', '__name__', '__package__', '__version__', '_binascii', '_extension_cache', '_extension_registry', '_inverted_registry', '_keep_alive', '_test', '_tuplesize2code', 'bytes_types', 'classmap', 'codecs', 'compatible_formats', 'decode_long', 'dispatch_table', 'dump', 'dumps', 'encode_long', 'format_version', 'io', 'load', 'loads', 'marshal', 'mloads', 're', 'struct', 'sys', 'whichmodule']  However, completion lists only __file__ and __name__.  Without pydev-extensions installed the pickle.load statement is accepted without complaint.    With pydev-extensions installed, it is flagged as an error:  Undefined variable from import: load  function: <function load at 0x00B94108>   In both cases the code executes correctly, it's just completion and syntax checking that fail.  This seems to affect only the pickle module.  I tried the above code with a dozen stdlib modules at random and the only one to show any problems was pickle.""",Bug,PyDev
30306,"""System: Windows XP Eclipse version: 3.3 and 3.4.2 Pydev version: 1.4.4 Eclipse install location: C:\eclipse3.3 and C:\eclipse-j2ee.3.4.2 Project location where the bug appears: C:\myworkspace\PPComWebTester Jython: 2.2.1  Hi, I'm on 1.4.4 of PyDev and extensions.    On eclipse 3.3 I can access my external jars no problem, but the internal jars cannot be accessed.     But on 3.4 it's the other way around. The internal java jars are fine but the external jars are causing errors.    In eclipse 3.4.2:    I have added both grinder.jar and jdom.jar as external libraries to the project. Any access to the methods available from these libraries are given as an error from Pydev.    On further inspection it seems that the issues appears for accesses to public members of static objects.    In eclipse 3.3:    The external libraries work fine. But any access to the methods available from the JRE are given as an error in Pydev.    Previous to upgrading (can't remember the old version of Pydev), I was using eclipse 3.3, and everything was fine. I did not change the project in any way. It simply started to produce errors once I upgraded. """,Bug,PyDev
30315,"""Hello,  You're going to hate me because I report annoying bugs :-). I was waiting to see it it was going to be fixed when my last bug ([2057092] """"Cursor jumps to method definition when an error is detected"""", thanks by the way!) was fixed, but it has actually gotten worst. My CPU usage went up to 100% every once in a while, it was usually after a long period of usage without restarting Eclipse (several days). I would have to quit Eclipse and restart it to make the problem go away. It seamed to be triggered when the code-completion was used and maybe in other use cases, but I don' t remember. Now though it happens several time a day and I can' t find what triggers it. Once it was triggered just by selecting a couple of lines of code with the mouse, and the CPU usage stayed at 100% for several seconds after I stopped selecting the text. I tried to reproduce the problem right away, but it didn' t trigger it this time. I suspect it may be a background task that runs periodically, like the syntax verification maybe? Here's my info :  - System Linux, Ubuntu 8.04 - Eclipse version : 3.2.2 - PyDev version 1.4.4.2636 - Eclipse install location : /usr/lib/eclipse/ - Project location where the bug appears : ~/Documents/Code/Python/Arimaz - PyDev error log :  eclipse.buildId=M20070212-1330 java.version=1.6.0_07 java.vendor=Sun Microsystems Inc. BootLoader constants: OS=linux, ARCH=x86, WS=gtk, NL=en_US Command-line arguments:  -os linux -ws gtk -arch x86 This is a continuation of log file /home/<USER>Documents/Code/Python/.metadata/.bak_0.log Created Time: 2009-02-26 11:32:21.720  Error Fri Feb 27 09:45:44 CET 2009 Failed to execute runnable (java.lang.NoSuchMethodError: org.eclipse.ui.editors.text.EditorsUI.getTooltipAffordanceString()Ljava/lang/String;)  org.eclipse.swt.SWTException: Failed to execute runnable (java.lang.NoSuchMethodError: org.eclipse.ui.editors.text.EditorsUI.getTooltipAffordanceString()Ljava/lang/String;)  at org.eclipse.swt.<USER>error(<USER>java:3374)  at org.eclipse.swt.<USER>error(<USER>java:3297)  at org.eclipse.swt.widgets.Synchronizer.runAsyncMessages(Synchronizer.java:126)  at org.eclipse.swt.widgets.Display.runAsyncMessages(Display.java:3157)  at org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:2859)  at org.eclipse.ui.internal.Workbench.runEventLoop(Workbench.java:1930)  at org.eclipse.ui.internal.Workbench.runUI(Workbench.java:1894)  at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:422)  at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:149)  at org.eclipse.ui.internal.ide.IDEApplication.run(IDEApplication.java:95)  at org.eclipse.core.internal.runtime.PlatformActivator$1.run(PlatformActivator.java:78)  at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:92)  at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:68)  at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:400)  at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:177)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  at java.lang.reflect.Method.invoke(Method.java:597)  at org.eclipse.core.launcher.Main.invokeFramework(Main.java:336)  at org.eclipse.core.launcher.Main.basicRun(Main.java:280)  at org.eclipse.core.launcher.Main.run(Main.java:977)  at org.eclipse.core.launcher.Main.main(Main.java:952) Caused by: java.lang.NoSuchMethodError: org.eclipse.ui.editors.text.EditorsUI.getTooltipAffordanceString()Ljava/lang/String;  at org.python.pydev.editor.hover.PyTextHover$1.createInformationControl(PyTextHover.java:208)  at org.eclipse.jface.text.AbstractInformationControlManager.getInformationControl(AbstractInformationControlManager.java:620)  at org.eclipse.jface.text.AbstractInformationControlManager.internalShowInformationControl(AbstractInformationControlManager.java:851)  at org.eclipse.jface.text.AbstractInformationControlManager.presentInformation(AbstractInformationControlManager.java:837)  at org.eclipse.jface.text.AbstractHoverInformationControlManager.presentInformation(AbstractHoverInformationControlManager.java:502)  at org.eclipse.jface.text.TextViewerHoverManager.doPresentInformation(TextViewerHoverManager.java:231)  at org.eclipse.jface.text.TextViewerHoverManager$5.run(TextViewerHoverManager.java:221)  at org.eclipse.swt.widgets.RunnableLock.run(RunnableLock.java:35)  at org.eclipse.swt.widgets.Synchronizer.runAsyncMessages(Synchronizer.java:123)  ... 20 more   and also :  eclipse.buildId=M20070212-1330 java.version=1.6.0_07 java.vendor=Sun Microsystems Inc. BootLoader constants: OS=linux, ARCH=x86, WS=gtk, NL=en_US Command-line arguments:  -os linux -ws gtk -arch x86  Error Fri Feb 27 09:45:44 CET 2009 Unhandled event loop exception   Does anyone also have this?  Thank you, Gabriel  """,Bug,PyDev
30393,"""I am running Pydev Extensions under Eclipse 3.4 on Linux.  I have recently started using the pyglet project as a dependency to my own. I've added a pyglet checkout from subversion as a project, and my own project depends on it.  Every several minutes Eclipse will display the following dialog. """""""""""" Problem Occurred. 'Periodic Workspace Save.' has encountered a problem.   Could not write metadata for '/pyglet'. encoded string too long: 136618 bytes. """"""""""""  The pyglet support seems to be working well, aside from this error. Is there a way to disable this dialog? Or even better, fix the metadata for the pyglet project? """,Bug,PyDev
30455,"""I have recently been using the grinder framework. Which uses Jython. Pydev is very useful in writing scripts for use with grinder. There is however a bug in PyDev I believe.  If you have a PyDev project and have grinder.jar as an external jar for the project you cannot import static member variables without PyDev giving an """"Unresolved Import"""" error. The """"from"""" part of the import is fine and offers code completion, after the import does not.  from net.grinder.script.Grinder import grinder  So I decided to try this on a system library:  from javax.swing.JFrame import EXIT_ON_CLOSE  And it worked!  Following this frame of thought I moved grinder.jar to the System PYTHONPATH but no joy. As well as this the line:  from javax.swing.JFrame import EXIT_ON_CLOSE  now also reports an error as the same as the grinder import. Even I put grinder.jar back into the project python path and remove it from the system pyhton path it no longer works.  I'm using PyDev and Extensions 1.3.13.  You can find The Grinder at grinder.sourceforge.net""",Bug,PyDev
30542,"""Refactoring local variables at one time worked for me but this is the 1st time I've tried it while evaluating PyDev Extensions.  This time it correctly showed the results in preview but when actually performing the refactoring, it messed up the variable.  I was changing 'mapProcess' to 'dictProcess'.  It seems to be adding an extra character to the variable name, incrementing the number of extra characters for each instance of the variable.  The 'return' statement was also messed up as well.  I'm running PyDev/PyDev Extensions 1.3.8 on Europa 3.3 on Suse 10.1 with JDK 1.6.02  <USER> #-- Before refactoring def createProcessMap(self):     mapProcess = \{}     curs = pg.Cursor( self.cxn, query)     for row in curs:         procName = row['proc_name'].toString()                  if mapProcess.has_key(procName):             pass         else:             mapProcess[procName] = Process(procName)                      return mapProcess   #-- After refactoring def createProcessMap(self):     dictProcesss = \{}     curs = pg.Cursor(self.cxn, query)     for row in curs:         procName = row['proc_name'].toString()                    ifdictProcessss.has_key(procName):             pass         else:           dictProcessess[procName] = Process(procName)                      retudictProcesscess""",Bug,PyDev
30735,"""  The interactive console can be invoked with Ctrl-Enter in an edit pane,   but any line of text that it is asked to process results in a SyntaxError:  invalid syntax pointing to the end of the line.  example:  Python 2.4.1 (#2, Mar 31 2005, 00:05:10)  [GCC 3.3 20030304 (Apple Computer, Inc. build 1666)] on darwin Type """"help"""", """"copyright"""", """"credits"""" or """"license"""" for more information. >>> >>> import StringIO   File """"<stdin>"""", line 1     import StringIO                    ^ SyntaxError: invalid syntax  In a monospace font you'll see the little caret indicator is positioned past  the 'O'.  This is the same behavior as any line it gets fed via the edit pane.   I'm using Eclipse SDK 3.1.2, Pydev 1.0.8, Mac OS X 10.4.6. """,Bug,PyDev
30749,"""With pydev extensions, when I type:  x = None<PRESS ENTER>  No newline is inserted, since it thinks I want to  select None as a keyword completion.  The keyword completion should vanish if I type a  complete keyword, since I really don't get any benefit  from the extra keypress, now do I?   """,Bug,PyDev
30888,"""I'm developing for Zope (Python) and there seems to be a problem with the highly erratic """"open resource"""" or """"F3"""" navigation in Eclipse.  Consdier the following small code fragment:  from OFS.Folder import Folder from Products.CMFCore.utils import getToolByName from Globals import InitializeClass, DTMLFile  class MDClient(Folder, ActionProviderBase):     manage_overview = DTMLFile( 'explainMDTransformTool', _wwwdir )      def manage_afterAdd(self, item, container, uid):         getToolByName(self, 'md_uidhandler').setUid(item, uid)         Folder.manage_afterAdd(self, item, container)  I have setup PYTHONPATH in both preferences and project properties to include OFS and Products directories (/usr/local/zope-2.8.1/lib/python and /usr/local/zope-instance respectively) and Globals.py (again /usr/local/zope-2.8.1/lib/python).  Also I have linked the Products directory for example into my project as a folder.  There are many problems with the above simple example:  1) Using F3 on DTMLFile goes nowhere (Globals.py is a file that itself imports DTMLFile from it's full location - it is not followed).  2) Using F3 on the Folder's manage_afterAdd goes to MDClient.manage_afterAdd instead of Folder.manage_afterAdd  3) Using F3 on getToolByName goes nowhere and generates an error with a blank reason.  Apart from the PYTHONPATH, it doesn't even find utils.py in the Eclipse Navigator's Project/Products/CMFCore folder.  As you can see, there are many errors with this facility.  In Java, I used it extensively to jump between classes, but in Python it doesn't work most of the time.  <USER> (BTW - thanks for such a promising project!) """,Bug,PyDev
30965,"""This isn't really a bug, but it's not really a feature request either.  In any case, here is a description of my use case.  I am creating some plugins based on pydev, and one of things I needed to create was a New Project Wizard.  My New Project Wizard will basically create a pydev project, but add an additional Nature to it.  The best way for me to accomplish this is to extend the PythonProjectWizard.  However, there were some changes I needed to make to the wizard to get it working as I needed it.  I have included these changes in an attached patch file for your review.""",Bug,PyDev
30990,"""Hello,    I'm using Eclipse 3.01, PyDev 0.9.0, and Python 2.3.4 on  my Windows XP Pro SP1 machine.    I was able to configure Eclipse to open an interactive  Python 2.3.4 session in the Console Window and here's  a minor problem I noticed when I enter the following  lines :    >>> i=1  >>> i    The output would normally be     1    However, in my case, the result comes with the >>>  characters like this :    >>> 1    Is there anything I missed to configure ?   The same problem even when I make Eclipse use Python  2.4's python.exe.   Hope someone can help. """,Bug,PyDev
31089,"""This patch adds a new extension point to pydev which will allow plugins to contribute custom PYTHONPATH entries to a python project.  Whenever the project python path is requested, contributions will be asked to provide any additional path entries.  Use Case:  I want to ship a Python library/sdk as an Eclipse/Pydev plugin.  If my Eclipse Python library plugin is installed, I want the SDK to automatically available to pydev projects, just as though it were installed as a python system lib.  This change allows the plugin to contribute the SDK, which makes the SDK code known to the AST manager *and* adds the SDK directory to the runtime PYTHONPATH.""",Bug,PyDev
31111,"""pydev ext seems to sometimes have trouble with:  from SimPy.Simulation import activate, Process, hold  marking it as an error of Unresolved import: activate  even though I can hit ctrl-enter (love that, BTW) and  successfully do:  import SimPy dir(SimPy) ['__SimPyVersion__', '__builtins__', '__doc__', '__fil e__', '__loader__', '__name__', '__path__'] SimPy.__file__ 'C:\\Python24\\Lib\\site-packages\\simpy-1.7.1- py2.4.egg\\SimPy\\__init__.pyc' from SimPy.Simulation import activate, Process, hold SimPy.Simulation.__file__ 'C:\\Python24\\Lib\\site-packages\\simpy-1.7.1- py2.4.egg\\SimPy\\Simulation.pyc'  and it all works fine in the interpreter.  I'm  wondering if the problem is that the *.py* files in  question are still wrapped up in the egg file and are  not actually in any dir?  Seems to be a new quirk of  the installer. """,Bug,PyDev
31118,"""When I double-click on an identifier to select it, it doesn't always select what I would anticipate.  I understand that the editor is probably just doing the default action here, but it would be quite nice for it to select based on the standard rules for python identifiers.  For example, double-clicking on the """"job_code"""" part of the following line:      self.job_code  Does not select all of """"job_code"""" like I'd expect, but stops at the underscore.  I guess it's using underscores as a delimiter.  When I double the """"person"""" part of the following line:      person.name  It selects the whole thing;  I would expect it to use the period as a delimiter.""",Bug,PyDev
31166,"""I have a single project with source folders.   The source folders are the default 'src' folder and one I added for common routines called 'common'.  Both are the PYTHONPATH for the project.  In my src folder I have a module called test_std_functions.py which does a 'from std_functions import *' at the top.   It appears that imports are resolved OK in the editor when writing code (i get an warning about using wild imports, but other than that it is OK).  Code assist work fine for the imported module. When I run my test_std_functions.py module as a python run I get the error shown below between the two lines.  If I then copy the module called std_functions.py from the common to the src folder (so they are both in the same folder now), and run test_std_functions.py as a python run, it works fine.  I get the expected output.  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Traceback (most recent call last):   File """"C:\blah\blah\blah\pydevIssueReportingWorkspace\imports_testing\src\test_std_functions.py"""", line 6, in <module>     from std_functions import * ImportError: No module named std_functions --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Here is the source for the module I am running: ''' Created on May 27, 2010  @author: <USER> All of the calls below are to std_functions.  ''' from std_functions import *  startup()  logDetailed(""""Hello from """" + jobSettings['SCRIPT_NAME'])  shutdown()  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  I'm using python 3.0, Pydev 1.5.7 on windows xp sp3 with (Eclipse) Sprintsource tool suite IDE latest version and JDK 1.6.  I can provide more details if you would like them (source code, zipped workspace, etc.)  Thanks - <USER>",Bug,PyDev
31169,"""I've got two Django projects, one on the trunk, one on a branch.  Both have identical structures, yet for some reason Pydev cannot figure out how to order them alphabetically.  In the branch project, 'local_apps' is at the top of the list, and in the trunk project, 'base' is at the top of the list.  There is no difference between them, except that 'local_apps' icon in branch has a different icon for that directory.  Also, for some reason, these two projects float to the top of the projects list when they are opened.  Cannot see what causes that behavior.  None of the other projects do that.""",Bug,PyDev
31226,"""I'm trying to work with the VPython visual module. Normally, the code below should work in any other python IDE:  from visual import * ball = sphere(color = color.red)  sphere and color are defined in visual and its superclasses. But Pydev keeps flagging color and sphere as undefined variables. I have seen many people with similar errors resulting from import packages. I tried adding the ...\site-packages\visual to the libraries of my python 3.1 interpreter, and restarting the IDE, but it didn't work. Could the problem be because I have two python interpreters installed 2.7 and 3.1, and autoconfig only picks up 2.7 automatically, I have to add 3.1 manually?  What's the temporary fix? And I hope this'll be fixed properly soon, I was so discouraged as a beginning python programmer that I wanted to ditch Eclipse and go back to good ole (V)Idle, which is simple as hell, but works without a hundred configurations or '#@UndefinedVariable'. And no, I'm not gonna add that to every single line that contains a reference to visual subclasses and what not!  Btw, I'm using the latest Eclipse and PyDev (  PyDev for Eclipse 1.6.5.2011020317 org.python.pydev.feature.feature.group)""",Bug,PyDev
31253,"""Hi there,  So I have been bitten by utf-8 chars causing havoc at the terminal window.  As a matter of course for scripts which are designed to run at the command line and have the possibility of showing utf-8 I simply do define my encoding like this.      # We really want UTF-8 at a terminal..     reload(sys)     sys.setdefaultencoding('utf-8')     log.info(""""Encoding forced to %s"""" % sys.getdefaultencoding())  The problem is that this is showing up as an undefined variable from import PyDev Problem.  I want to know how to suppress this error.  I've attempted to place both setdefaultencoding and sys.setdefaultencoding in the undefined globals section (which seems wrong but..) but that didn't work.  Is this a problem or just end user training??  Thanks.  BTW: PyDev on 3.6 with Studio 3 is great.  Really like the new app explorer""",Bug,PyDev
31348,"""As told in the mail list ('Pydev vs Python Interpreter'), there is some strange problem with the way that strings are used in PyDev, it should mimic exactly what the console interpreter would do in the default system shell, and that's not happening. In the attachment, i put two codes for testing purpose (one single line), the codes are commented with their intents.  But basically the problem is:  print unicode(""""Ã£"""") # Should not work unicode decode as ascii by default  print u""""Ã£"""" # Should work, the console output should be capable of showing unicode strings  I'm using pydev 1.5.7, python 2.4.5 and Ubuntu 8.10""",Bug,PyDev
31364,"""I have several branches of Twisted open as projects in my workspace, as well as a system-installed version on my PYTHONPATH.  If I open the PyDev globals browser and type, for example 'setContent', I see a list that looks like this:      setContent - twisted.python.filepath     setContent - twisted.python.filepath     setContent - twisted.python.filepath     setContent - twisted.python.filepath     --- Workspace Matches ---      setContent - twisted.python.filepath     setContent - twisted.python.filepath     setContent - twisted.python.filepath  I can close the projects I'm not actively working on, but that still gives me an extra copy for the system installation, and I have no way of determining which one is going to open when I hit """"enter"""".  It would be nice to contextually always prefer the entry in the same editor I'm using, or in a project linked from that project, but at the very least it would be good to allow me to tell which path entry each choice is a part of.""",Bug,PyDev
31416,"""Ways to reproduce: 1. Create python module moo.py, as: cow = 1 co 2. press Ctrl-Space to activate code completion for word co. 3. First suggestion is 'co' (dynamic import from traceback package), while the name cow (my local variable) is found farther down the list at 7th place.   What I expect to see: Names in my current scope at the top of the list, possibly with priority to local variable names. (I realize you list exact matches (from anywhere) at the top, but that's just no good; see below.)  Why I expect it: Most programmers use names that already exist in the current scope way more frequently than they import new names (duh!), and for optimal usability, the most frequently used options should be the most easily accessible ones, rather hidden among other, less frequently used options.   Why current behavior is bad: My variable is harder to directly see in this place, and my spinal chord never gets completely convinced this is really """"my"""" cow and not some dynamic import that will get added to the top of the file, and which I'll have to remember to check for, and also remember that I never actually intended to use and so possibly delete at some later point in time, so I often spend seconds looking at this list, making sure this choice will really do what I want and with no side effects, and losing my train of thought instead of writing code.  This right here is why I post this as a bug rather than a feature request. This is important enough that I find myself favoring the emacs-like Alt-/ raw text completion despite all the code completion sweetness built into pydev. Its a lot more (and more awkward) keypresses; multiple Shift-Alt-7 on my keyboard, but at least it always does what I expect it to do, quickly and consistently.  Additional guesswork: Given that code tends to be organized in functional blocks even within scopes, a likely guess would be that the most recently used names are also the ones that are most likely to be used again, so perhaps it would be useful to rank matching names in the current scope by distance from the cursor (lines/chars some other metric?). Or maybe an alphabetized list is more efficient because of consistency, possibly with priority to local variables. Dunno. Guess that's something one would have to test.""",Bug,PyDev
31498,"""I would like to see the ability to set environment variables on a per project basis (in the properties for a project).  The main reason I need for this is that I use SWIG to access C/C++ libraries from Python and the resulting '.so' files (that can be imported as Python modules) link to project specific libraries. Therefor LD_LIBRARY_PATH need to differ depending on which project I am running. If it was possible to set it on a per project basis I would not need to restart Eclipse every time I change project (which I do several times a day).  Note that setting LD_LIBRARY_PATH in the 'Run' menu is actually not possible. It has an arbitrary length restriction on environment variables values that is much shorter than an average LD_LIBRARY_PATH. Setting it in 'Run', if it worked, would not be usable either since I would have to do it for every possible way to run my scripts (I often select a directory and use 'Run as Python unittest). """,Bug,PyDev
31501,"""I think I found a bug on how the Pydev environment configuration works. Since some time I've added the following to my *interpreter* environment configuration: - PATH=%PATH%;D:\gtk+-bundle_2.18.7-20100213_win32\bin  Then I created 3 run configurations for each gtk project I'm working on: - the first with a default empty *project* environment page - the second adding LANGUAGE=nl to the *project* environment page - the third adding LANGUAGE=fr to the *project* environment page  Only the first run configuration picks up the PATH environment variable set in the *interpreter* configuration. The second and third do not (resulting in glib/gobject/gtk dll's not found errors).  Is this the intended behavior? I expected the environment configuration to be cumulative, like the """"project references"""" configuration [1], but this seems not to be the case.  Just to be complete, i'm running Pydev 1.5.5.2010030420 on Eclipse 3.5.2 on Windows XP (my Linux box has the same problem).  Thanks, <USER> [1] If project-x has a reference to project-y and project-y has a    reference to project-z, then when doing a """"Python Run"""" for a    file from project-x, pythonpath will include the configuration    for project-x and project-y and project-z.""",Bug,PyDev
31505,"""I'm working on a (very) large python project with a large team of developers. This project has a single """"src"""" directory that contains:  1. the project's python source files. 2. symbolic links to the source files of external dependent projects.  I'm attempting to use PyDev to develop this project. I've created a PyDev project and added the """"src"""" directory to the project's PYTHONPATH (using Project>Properties).  However, I've discovered a problem. It seems that on my system, PyDev doesn't follow the symbolic links with """"src"""", and hence the symbolically-linked files and directories are not included in the source code analysis. This prevents the """"go to definition"""" feature from working, and creates false errors in the source code analysis.  This makes it difficult to use PyDev with our project. Would it be possible to fix PyDev to follow symbolic links?  Cheers  <USER>",Bug,PyDev
31544,"""This bug is *not* related to the usual issues people have with insertions of tabs vs. spaces.  The issue is that the Tab key behavior is not consistent with other Eclipse editors.  After code is inserted on your behalf, it's customary for the Tab key to cause the cursor to skip over the automatically inserted text so you can continue editing.  For instance, if I type a single quote, then the end quote is added and the cursor is put in between the two.  The """"normal"""" use case for this is to fill in the string and then hit Tab to skip over the end quote (or whatever was auto-inserted).  In PyDev, under these circumstances when the cursor is adjacent to auto-inserted end-text, the Tab key simply inserts spaces/tabs as usual.  Since this feature does not work the same as in other editors, I'm wondering how the PyDev authors intend it to be used?    When I type 'hello'                            ^----- cursor is now just before the auto-inserted end-quote  am I supposed to use the arrow-keys and/or mouse to skip over the auto-inserted quote?  That kind of defeats the purpose -- it would be easier simply to type the quote.  Mostly this is just disconcerting because after many years of using the C++ IDE in Eclipse, I'm used to hitting Tab in these circumstances...  I can retrain my brain easily enough, but thought that maybe PyDev should be consistent with the other IDEs?  Let me know what you think and/or if there's an alternate way to use this as you intended. """,Bug,PyDev
31576,"""The following import is being marked as an error even though the code runs OK:  from .data.myusers import MyUserEntity  PyDev 2.2.1.2011071313 Eclipse 3.7.0 Windows 7 64-bit  It seems like I'm getting the same problem as in bug ID 3199805, here is the additional information you asked for there, the structure:  src/     __init__.py     data/         __init__.py         myusers.py     service/         __init__.py         users.py  The import line above is in users.py  """,Bug,PyDev
31605,"""Apparently there is a problem with Pydev and namespace packages. I'm using Paste here as an example, but it seems to happen with others too, including WebCore (which contains the """"web"""" namespace package). Steps to reproduce:  1. Use pip to install the paste, pastedeploy and pastescript distributions 2. Reinitialize interpreter information so that the new packages get cached (pip installs directly to dist-packages/site-packages) 3. Open up an interactive interpreter from Pydev and run """"import paste"""" to verify that it works 4. Create a new Pydev project 5. Create a new module 6. Add the line """"import paste"""" 7. Save  Expected result: no errors Actual result: error marker on that line saying """"Unresolved import: paste"""" Used Pydev version: 2.0""",Bug,PyDev
31657,"""Currently, every folder under a local pythonpath is seen as a module by the package explorer.  Doesn't it make more sense that the explorer only treats folders containing an __init__.py file as a module?    It's more of an aesthetic thing I guess, but I'd find the visual separation useful.""",Improvement,PyDev
31679,"""I'm embedding Python into a Verilog simulator as a DPI  application which is very cool and fun.  When debugging embedded python using Pydev Extentions (of which we purchased 10 licenses :)), I can type stuff in the console but can't see the output.  (Regular, non embedded console works excellent, can see the output from my expressions evaluations).  It would be great (if possible) to give the user an option to somehow route the output from  the console, in addition to where it's currently going (STDOUT/ERR?) to an external file that the user specifies (tee-style).  This will give me back my beloved console which I love and need :)  Thanks in advance,  Gal. """,Improvement,PyDev
31727,"""In version 1.2.3 the word next to the cursor got a yellow background, together with every word written the same in the file.   I think this was a very usefull feature, because one could see where a variable was used in a function. Therefore I want this feature back.  Keep up the good work,  Eike.""",Improvement,PyDev
31749,"""Ctrl-Shift-T (Python Class Browser) is a great addition to pydev.  However, I would really love to see it expanded to work even if an editor is not currently open.  I presume this has something to do with the pythonpath of the project containing the editor or something (since the class browser will NOT show classes declared in other projects).  Maybe if the browser is opened globally, it could simply use the pythonpath from all of the open pydev projects.  I often use Ctrl-Shift-T in Java as a starting point for something.  In PyDev, I find myself opening some random .py file so that I can get the class browser to open up to find the thing I REALLY need.  (note: the largest amount of pain for me right now is in code navigating - the addition of the class browser is amazing...this would just make it better for me) """,Improvement,PyDev
31753,"""On many occasions I've had the problem where I ahve  renamed or deleted a .py file and the .pyc file is  left behind.  In PyDev, these files do not show up in  any of the navigators.  Also, selecting Project >  Clean doesn't seem to delete them.  Ideally, PyDev should show .pyc files which have no  matching .py file in the explorer(s) either in the  tree or as a warning message on the folder.  Thanks  """,Improvement,PyDev
31762,"""When you've got a number of different constant text values, you sometimes want to line up the contents. Many IDEs implement a tab-stop feature so that transforming  CONSTANT1 = """"this"""" CONSTANTWITHLONGERNAME = """"that"""" CONSTANT142 = """"the other""""  to  CONSTANT1 =                """"this"""" CONSTANTWITHLONGERNAME =   """"that"""" CONSTANT142 =              """"the other""""  is easier.  Basically, this feature request is that when the insertpoint is AFTER some text some text, e.g. (at the pipe character)      CONSTANT1 = |""""this""""  then pressing TAB won't just insert fourspaces, but instead will insert enough spaces so that the insertpoint is put onto the next """"tab-stop"""" - tabstop being some mulitple of 4 characters.  (I would still reserve SHIFT-TAB for dedenting the whole line, as currently)  PS - I understand from reading the style PEPs that BDFL Guido doesn't like this style of alignment. He doesn't need to read my code though... ;-> sometimes it's just more readable like this!  FYI I think there are some other issues with alignment and tabstop type issues when editing existing text (typically pressing TAB/SHIFT-TAB/RETURN within a textblock, particularly inside a parenthesized statement). I will try to write this up as another feature request.""",Improvement,PyDev
31779,"""This request is for supporting PyObjC programming in pydev. PyObjC  apps are OSX applications. One can developer PyObjC programs in pydev  by specifying the compiled application's executable as a python  interprter.  So i'd like to specify an interpteter like: $\{project_loc}/Build/Foo.app/Contents/MacOS/Foo  [This is required as all dialog, string, etc resources are looked up relative  to the launched app. Because the launched app is the python interpreter,  the interpreter should be the gui app's executable. PyObjC has a patch  that enables the executable to act as a pyton interpreter. ]  This required 2 changes: 1. support variable expansion in interpreter PythonRunnerConfig.java  2. allow project specific interprteters MainTab.java part of the patch""",Improvement,PyDev
31803,"""I am on Windows XP, using Eclipse 3.0.1 with pydev 0.9.3.  I am testing Eclipse and pydev on Chandler, which has a fairly nonstandard setup. In Chandler we have our own Python (almost stock 2.4), and need to set PYTHONPATH as well.  As far as I can tell everything else works right, but pylint does not seem to pick up the PYTHONPATH environment I set by going right click project name > properties > pydev > PYTHONPATH. If I set PYTHONPATH in Windows and relaunch Eclipse, PyLint can also find the imports from the PYTHONPATH.  However, since I have several Python's on my system (cygwin, standard windows, and one for each copy of my chandler development environment) I can't set the PYTHONPATH in Windows as an effective workaround. I need to be able to set PYTHONPATH for each project.""",Improvement,PyDev
31841,"""I've been playing a bit with JythonConsole, from Jyleo, which was the only good swing console I could find for jython.    Since pydev's console is not very good, perhaps you could check this one out, and eventually even integrate it. It used to ship as part of jyleo (open source), but today the author did a separate release at :     It would be a very cool feature to have integrated into pydev, if we could select a jvm from the running process list and open a console for it.     It's so cool it even has auto-complete!   """,Improvement,PyDev
31863,"""One of the difficulties I have had in coming back to pydev periodically after some time (I alternate between java/python/etc development) is remembering how to do things - especially remembering shortcut key assignments.  Obviously I can look these up in the online documentation, but what I'm used to doing with eclipse (and other tools) is to look for the command in some menu or sub menu and then (re)learn the shortcut key assignment (if I'm likely to need it).  It strikes me this would be a valuable addition to pydev (particularly pydev extensions), and make it much more """"learnable"""" as well as clarify the documentation (which is peppered with references to (for example) ctrl-1, rather than the command to which that key is assigned (code completion I think).  This sounds like the sort of thing one (even I maybe) could do in eclipse as an extra plugin, but think it would be better if it was an integral part of pydev/pydev extensions... what are your thoughts?""",Improvement,PyDev
31875,"""I'm using <USER>3 (beta), I've read that I cannot install PyDev as independent plugin (because <USER>ships with integrated PyDev), why? Couldn't I just disable the integrated PyDev and install new one as independent plugin? Is there some sort of code sharing that disallows this, can't it be separated?  Root core is the fact that the <USER>Studio 3 beta is lagging behind, integrated PyDev in <USER>is at 1.6.3, while PyDev is at 1.6.5. A bit annoying.   I would like to install PyDev independently, and *still* use <USER>3 (as a plugin or installed) so I could get updates more frequently for PyDev too, with <USER>2 this was possible.""",Improvement,PyDev
31876,"""Lets say i have a breakpoint set in a loop.  I want to see how a variable X changes with each iteration.  So i open the """"Variables"""" tab and then i hit F8 to continue to the next iteration,  what happens?  It switches to the console?!?!  I clicked on VARIABLES,  why on earth would  i want to see the console?   As a consequence i have to do a lot of clicking,  very  very annoying. """,Improvement,PyDev
31881,"""The vast majority of Python projects (from what I've seen on PyPI) host their packages directly at the project root. Thus it would be logical for Pydev to default to setting the project root as a source directory and not creating an src directory.""",Improvement,PyDev
31882,"""I'm missing the """"inline variable"""" option when standing on a variable definition and pressing Ctrl+1  This will naturally replace all the occurrences of a variable with the value assigned to it in definition. Very useful when taking existing code and deciding to extract a method from it: - First change the literals you want to introduce as parameters to a variable (Using Ctrl+Alt+L) - make sure the variable definitions are as a block all together before the code block you want to extract to a method. - Then <USER>the code you want to extract to a method, without the variables definition. - Finally use the (missing) inline variable option on all defined variables to remove the definition line and pass the value as a parameter to the extracted method instead.""",Improvement,PyDev
31899,"""Given that Pydev already has a jython.jar packaged with it, I'd like to be able to use that as my jython interpreter and not install another one on every computer where I use it.  My main use for a jython interpreter is to associate it with the project I use to store my own Pydev scripts, so I'd like it to match anyway.  I can do this right now by manually selecting the jython.jar packaged along with pydev, but every time I update pydev it breaks.""",Improvement,PyDev
31905,"""I would like to see the Editor caption be formatted as """"packagename.filename"""".  E.g. """"mydjangoapp.views"""", """"mydjangoapp.models"""".  I like the improvement that from icon I can differentiate the difference of views, models, tests. But that is not very general way, I have several """"forms.py"""" for instance in my django apps. Besides the change in the icon to my eye is too invisible.  If the editor caption were """"packagename.filename"""" it could in some edge cases be too long, but I don't see that as a big problem. I have many jquery.something.js open, and they are good on the eye.""",Improvement,PyDev
31954,"""I would like to see an improved interactive console similar to those in MatLab and the eclipse-based IDL workspace. Basically I would like to a shell or interactive console window that is always open, and which can access the variables from a script that has just been run. This type of feature is available in the MacPython IDLE, DrPython, SPE and eric4. In my opinion not having a similar interactive console in pydev is its biggest downside, and if it gained one it would become my editor of choice.   I also enjoy some of the features of the ipython improved interactive console, mainly its matplotlib mode. If I start ipython with ipython -pylab, then the script will continue running after making a plot with matplotlib. It is basically an automatic way to make plots run as a separate process. Incorporating something like this into pydev would also be a great feature. """,Improvement,PyDev
31959,"""It would be great to have variables for the interpreter location and possibly lib/site-packages and scripts/ directly.  It looks like there are a lot of variables that I can use such in a launch as: $\{workspace_loc} but what I really need is the path to the interpreter. The use case for this is that I'm trying to run a .py file that is in the PythonDir/Scripts folder.    I can set it up as an explicit link to C:\Python25\Scripts\my-script.py for now, but I'd love to be able to do something like:    $\{python_loc}\Scripts\my-script.py    so that it works with any python interpreter location and on other platforms.  """,Improvement,PyDev
32037,"""Auto build should be implemented in real-time like the Navigator tab, and not require a save each time you want build reliant functionality enabled.  See Stani's Python Editor while using #TODO: for this as an example.   pyro9219 -at- gmail -dot- com""",Improvement,PyDev
32189,"""I'm lazy and do not want to click 8 times at different things in different windows to add/remove source directories in my project. Also, the proper place where one should do this has always been the teensiest bit hard to find, and (pardon my senility) getting harder to find in every release due to getting padded in among more and more options.   It would be neat to have an option to do this in the package explorer context menu, for example:  Package explorer menu > Pydev > Set (Unset) as source directory.   This option could apply not only to dirs, but also to containing dirs for clicked files.   Great work by the way Fabio! Pydev just keeps getting better and better with every release.""",Improvement,PyDev
32202,"""Launch debugger but halt at first line of execution (as if there was a breakpoint at the first line).  I think this should be handled by a new way of starting the debugger (ie an additional launch button). Possibly it could be behavior set with a preference, but based on my own wishes, I'd prefer immediate access to both the current behavior and this new behavior.""",Improvement,PyDev
32207,"""add the interpreters configuration as a relative path,  so if i install it on eclipse and i change the directory of the python location i want that it remember it relative location from workspace.  thank a lot <USER>",Improvement,PyDev
32229,"""I've noticed that if I shut down and restart Studio, it appears to be reindexing a number of files, even though they haven't changed. This is a non-trivial list of files, enough to add some additional indexing. As an example:    Before (ran 2x):  !MESSAGE (Build 0.0.0.qualifier) [INFO] com.aptana.core/debug/builder Finished unified build of 'JavaScript Style'. Took 1,103.952 ms.  !MESSAGE (Build 0.0.0.qualifier) [INFO] com.aptana.core/debug/builder Finished unified build of 'JavaScript Style'. Took 1,081.843 ms.    After commenting out the indexing of files with encoding changes:  !MESSAGE (Build 0.0.0.qualifier) [INFO] com.aptana.core/debug/builder Finished unified build of 'JavaScript Style'. Took 239.482 ms.    Similarly, before:  !MESSAGE (Build 0.0.0.qualifier) [INFO] com.aptana.core/debug/builder Finished unified build of 'debug_android201'. Took 2,152.666 ms.  !MESSAGE (Build 0.0.0.qualifier) [INFO] com.aptana.core/debug/builder Finished unified build of 'debug_android201'. Took 1,731.863 ms.    Afterwards:  !MESSAGE (Build 0.0.0.qualifier) [INFO] com.aptana.core/debug/builder Finished unified build of 'debug_android201'. Took 191.405 ms.    List of files with encoding changes:    Resource changed encoding:new.aptana.com/Basil Notes.txt  Resource changed encoding:new.aptana.com/doc/README_FOR_APP  Resource changed encoding:new.aptana.com/features/contribute.feature  Resource changed encoding:new.aptana.com/features/downloads.feature  Resource changed encoding:new.aptana.com/features/home.feature  Resource changed encoding:new.aptana.com/features/news.feature  Resource changed encoding:new.aptana.com/features/studio.feature  Resource changed encoding:new.aptana.com/features/support.feature  Resource changed encoding:new.aptana.com/log/development.log  Resource changed encoding:new.aptana.com/log/newrelic_agent.log  Resource changed encoding:new.aptana.com/public/download.txt  Resource changed encoding:new.aptana.com/public/images/products/aptana/README  Resource changed encoding:new.aptana.com/public/robots.txt  Resource changed encoding:new.aptana.com/script/about  Resource changed encoding:new.aptana.com/tmp/restart.txt  Resource changed encoding:new.aptana.com/vendor/gems/compass-0.10.2/features/command_line.feature  Resource changed encoding:new.aptana.com/vendor/gems/compass-0.10.2/features/extensions.feature  Resource changed encoding:new.aptana.com/vendor/gems/crack-0.1.7/LICENSE  Resource changed encoding:new.aptana.com/vendor/gems/devise-1.0.7/TODO  Resource changed encoding:new.aptana.com/vendor/gems/devise-1.0.7/generators/devise_install/templates/README  Resource changed encoding:new.aptana.com/vendor/gems/haml-3.0.9/test/sass/data/hsl-rgb.txt  Resource changed encoding:new.aptana.com/vendor/gems/haml-3.0.9/vendor/fssm/LICENSE  Resource changed encoding:new.aptana.com/vendor/gems/haml-3.0.9/vendor/fssm/spec/root/duck/quack.txt  Resource changed encoding:new.aptana.com/vendor/gems/haml-3.0.9/vendor/fssm/spec/root/moo/cow.txt  Resource changed encoding:new.aptana.com/vendor/gems/newrelic_rpm-2.11.3/CHANGELOG  Resource changed encoding:new.aptana.com/vendor/gems/newrelic_rpm-2.11.3/LICENSE  Resource changed encoding:new.aptana.com/vendor/gems/rack-1.1.0/README  Resource changed encoding:new.aptana.com/vendor/gems/warden-0.10.5/LICENSE  Resource changed encoding:new.aptana.com/vendor/plugins/redirect_routing/README  Resource changed encoding:new.aptana.com/vendor/rails/actionmailer/CHANGELOG  Resource changed encoding:new.aptana.com/vendor/rails/actionmailer/README  Resource changed encoding:new.aptana.com/vendor/rails/actionmailer/lib/action_mailer/vendor/tmail-1.2.7/tmail/vendor/rchardet-1.3/README  Resource changed encoding:new.aptana.com/vendor/rails/actionpack/CHANGELOG  Resource changed encoding:new.aptana.com/vendor/rails/actionpack/README  Resource changed encoding:new.aptana.com/vendor/rails/actionpack/test/fixtures/multipart/hello.txt  Resource changed encoding:new.aptana.com/vendor/rails/activerecord/CHANGELOG  Resource changed encoding:new.aptana.com/vendor/rails/activerecord/README  Resource changed encoding:new.aptana.com/vendor/rails/activerecord/test/assets/example.log  Resource changed encoding:new.aptana.com/vendor/rails/activeresource/CHANGELOG  Resource changed encoding:new.aptana.com/vendor/rails/activeresource/README  Resource changed encoding:new.aptana.com/vendor/rails/activeresource/test/debug.log  Resource changed encoding:new.aptana.com/vendor/rails/activesupport/CHANGELOG  Resource changed encoding:new.aptana.com/vendor/rails/activesupport/README  Resource changed encoding:new.aptana.com/vendor/rails/railties/CHANGELOG  Resource changed encoding:new.aptana.com/vendor/rails/railties/README  Resource changed encoding:new.aptana.com/vendor/rails/railties/bin/about  Resource changed encoding:new.aptana.com/vendor/rails/railties/configs/empty.log  Resource changed encoding:new.aptana.com/vendor/rails/railties/doc/README_FOR_APP  Resource changed encoding:new.aptana.com/vendor/rails/railties/guides/images/icons/README  Resource changed encoding:new.aptana.com/vendor/rails/railties/html/robots.txt  Resource changed encoding:new.aptana.com/vendor/rails/railties/lib/rails_generator/generators/components/plugin/templates/README  ----  Resource changed encoding:debug_android201/CHANGELOG.txt  Resource changed encoding:debug_android201/LICENSE  Resource changed encoding:debug_android201/LICENSE.txt  Resource changed encoding:debug_android201/README  Resource changed encoding:debug_android201/build.log    It seems that these files should not be reporting encoding changes.""",Bug,Indexing
32377,"""  My notebook computer, also running <USER>3, now has the same issue as my desktop.  I have now verified this issue on two different computers, both Windows 7 x64.      -----    I followed all of instructions in email, attached my theme, reset defaults on annotations and set defaults on theme and as you can see from the attached stillbuggy.png the issue remains.  In order to fix this I need to go to preferences > aptana studio > themes and press """"reset defaults"""" *without changing anything*    Every time I start I need to go do that reset defaults.  I don't think it's the theme: something buggy happens when <USER>initializes that changes something in my settings.  Having to go through settings to undo it is lame, and it has to be a change that happened with the latest upgrade.  Please get someone to fix it... :)    -----    After latest upgrade, highlighted text color (PyDev editor) is opaque and changing theme colors does not work reliably.  All themes show behavior like the attached screenshot: the highlighted text foreground and background are the same color.  Changing selected foreground colors does not work as expected: selecting a dark-colored selection background results in a light-colored background.    Selecting the theme override for pydevOccurrenceIndication seems to fix the issue for the current session, but when I shut down <USER>and restart the highlight is broken again, even though the little 'B' icon next to pydevOccurrenceIndication is still selected.  I need to click it off and back on again to fix the highlight, which again only works until I restart.    I've been using <USER>without issues for at least 6 months prior to this issue, and have not touched color settings in months.  The latest update seems to have broken theme handling.    MOTE UPDATES: See screen2.png attached.  If you look closely, I selected a very dark gray(almost back) color for the line highlight and <USER>is using a bright gray.  Any color I choose for the line highlight is ignored, and <USER>chooses a much brighter color.  <USER>is just ignoring my setting for selected text altogether.      MORE UPDATES: I have had two instances today where I started <USER> went in and changed pydevOccurrenceIndication so that I could see highlighted text (the broken same-background-and-foreground bug is 100% repeatable).  After switching to another window and switching back the highlight was broken again, requiring me to change the pydevOccurrenceIndication setting again.      This is profoundly irritating.  I hope whatever change broke color handling was awesome enough to cause this much pain.""",Bug,Editor
32382,"""I attempted to create a project on a mapped network drive, <USER>complained that it couldn't write to the .project file in the root. Sure enough there was already one in there, so I got rid of it. <USER>continues to complain about the file though, even though it doesn't exist. I've remounted the drives, refreshed <USER>s file tree, rebooted both <USER>and the machine.  """,Bug,Project
32423,"""The terminal output occasionally goes blank on several lines. It still accepts commands fine, and has no other blatant issues other than the screen's being completely blank. Or perhaps the text color simply matches the background window. Either way, after sending multiple commands the display returns, but with empty lines between the commands. A better way to describe it is """"flickering""""—the screen sometimes will visibly flicker off and on, or alternate between visible and invisible.    In the example I took a screenshot of, I was issuing a SASS watch command when the screen blanked out. The blank space in terminal 1 was where the text was. I was able to see the text as I was typing, then the output disappeared after executing. The behavior happens at some point on just about every type of terminal operation I've performed.""",Bug,Terminal
32432,"""This is a clone of bug APSTUD 4661 as I made the following comment on it, but I guess because that bug was deemed fixed, the comment went unnoticed. I've included the previous bug description after the new additional problem description.     The previous bug was not correctly fixed and the following describes the remaining, rather peculiar, issue.     I've upgraded to <USER>Studio 3, build: 3.1.2.201205041324 and have tested the following on Mageia 1.x and Ubuntu 12.04.    If I turn off the Refresh checkbox, save and view it again it remains off. If I close down <USER>and restart it, the checkbox remains off. If I close <USER> log out, log back in again, and restart <USER>and view it, it remains off. The same is true if I don't close <USER>before logging out.    However, if I reboot my machine, log in and then restart <USER> it is now back on again. In other words, the setting doesn't seem to survive a reboot.    I would guess this hints that the value is somehow being kept in memory and linux is reassigning that portion of memory to <USER>during the restarts of <USER>and even over a log out and log in again. Don't know if that is possible, but I can't imagine why it's misbehaving only on a reboot otherwise.    Steps to reproduce:  a) log in to either Mageia 1.x, or Ubuntu 12.04 or possibly other variants of linux or other OSes(haven't tested on those)  b) start up <USER>and turn OFF the Refresh button  c) REBOOT your machine  d) log in again and start up <USER> and view the value. All things going well it's back on again.    Who'd have thought one checkbox could be so mysterious!!      The original bug description was ...      I've just upgraded to the latest aptana release (3.1.1.201204131931) and immediately hitting cpu usage issues with the CPU jumping to 90%ish every few seconds corresponding to a refresh of the workspace every few seconds. So I guess on my system <USER>the option 'Refresh using Native hooks or polling' is polling rather than hooks as it happens regardless of whether or not I'm doing anything.    I've traced it down to the 'Refresh using Native Hooks or Polling' which is under the Workspace menu. If I turn it off, the system quietens down and all is fine.     However, on a restart, this 'option' has reselected itself meaning (I guess) that the option is not reading it's setting correctly from the options saved (or perhaps isn't getting saved to the file?)    So the bug I'm reporting here is the preference can't be set beyond a restart and, of course, like all preferences it should be.    A second bug I guess is quite why the refresh of the workspace uses so much CPU - my system has 8000 files on a fairly old machine so I'm guessing it's hitting limits on my machine (results will of course vary on other machines). I've commented on that before (in the forums) and it was all fine in version 3 until now. I'll repeat my guess that there is a very inefficient method involved in the refresh of the workspace that still hasn't been resolved.     Anyway - hopefully to replicate ...    Type Workspace in to the preferences and select Workspaces at the bottom to get the """"Refresh ... """" option. Unselect it. Restart the studio, go back to that preference and you'll (hopefully) see it selected again.    I've attached two images - the first when I deselected the option. The second when I restarted showing the option has been selected again.""",Bug,Performance
32465,"""Periodically, after a number of successful FTP transfers to any host server, the FTP connection will fail with the following Error message:     Failed to upload file  Establishing FTP connection failed: Read timed out  Read timed out    Every attempt at this will fail for the next 20 minutes or so. This happens on every server at some point, and when this happens to <USER>I can connect using FileZilla or Dreamweaver and successfully FTP the same file.    The problem will fix itself automatically in about 20 minutes, possibly when the FTP connection has automatically timed out from the server. Restarting <USER>does not fix the problem. After time I can resume FTP normally but much time is lost waiting for these FTP errors to subside. This problem happens at least two or three times every day throughout my day. It happens even more so when I'm handling a lot of remote files. Restarting my computer seems to work, but this happens so frequently I can't afford to restart my computer every time this happens.""",Bug,Publishing
32498,"""When working with any existing App or large Framework it's _incredibly_ useful to be able to easily browse/explore all available methods and properties that the current class has access to and to be able to navigate quickly through the hierarchy.     Working with something like Magento or anything based on ZF/Cake/Kohana etc means that you almost never create classes from scratch but are always extending someone else's class.    While inherited methods/properties are visible in autocomplete they do not show in the Outline view - so finding what's available in a large or an unknown project is a nightmare.     In contrast <USER>1.5 showed all inherited methods/properties in the Outline view - see attached screen shot for an example of using this in a Magento project.    I still use <USER>1.5 as my everyday IDE because the PHP Outline view does such a fantastic job of displaying and filtering through deep object hierarchies, I've tried (and purchased!) every other PHP IDE I can find to get a modern release that can beat it, and nothing can, it really is a killer feature.    Netbeans and PhpStorm both to a good job of letting you search for class usage etc but neither aid in exploring the hierarchy as well as the <USER>Outline view used to - please bring it back so I can upgrade to Studio 3!    Given that autocomplete already parses the hierarchy and Outline view already shows indented methods with a working filter system, it must be possible to render the hierarchy in Outline view.""",Improvement,PHP
32531,"""After updating to <USER>Studio 3, build: 3.1.1.201204131931, I immediately started experiencing the issue where """"Building workspace"""" would hang forever upon each file save.  A particular file I was editing was a PHP file (.php) and/or a PHP template file (.tpl).  Unfortunately, before """"Building workspace"""" operation is completed, I am not able to save the file.  Since the mentioned operation never completes, the files in the Editor will never get saved.  I've checked """"[x] Save automatically before build"""" in Workspace preferences, and it lets me save the file first time, but consequent saves fail since the """"Building workspace"""" operation occurs right after file save and is stuck indefinitely.    Attached is the screens shot demonstrating the issue.""",Bug,"Indexing,Project,PHP,Validation"
32602,"""I've just upgraded to the latest aptana release (3.1.1.201204131931) and immediately hitting cpu usage issues with the CPU jumping to 90%ish every few seconds corresponding to a refresh of the workspace every few seconds. So I guess on my system <USER>the option 'Refresh using Native hooks or polling' is polling rather than hooks as it happens regardless of whether or not I'm doing anything.    I've traced it down to the 'Refresh using Native Hooks or Polling' which is under the Workspace menu. If I turn it off, the system quietens down and all is fine.     However, on a restart, this 'option' has reselected itself meaning (I guess) that the option is not reading it's setting correctly from the options saved (or perhaps isn't getting saved to the file?)    So the bug I'm reporting here is the preference can't be set beyond a restart and, of course, like all preferences it should be.    A second bug I guess is quite why the refresh of the workspace uses so much CPU - my system has 8000 files on a fairly old machine so I'm guessing it's hitting limits on my machine (results will of course vary on other machines). I've commented on that before (in the forums) and it was all fine in version 3 until now. I'll repeat my guess that there is a very inefficient method involved in the refresh of the workspace that still hasn't been resolved.     Anyway - hopefully to replicate ...    Type Workspace in to the preferences and select Workspaces at the bottom to get the """"Refresh ... """" option. Unselect it. Restart the studio, go back to that preference and you'll (hopefully) see it selected again.    I've attached two images - the first when I deselected the option. The second when I restarted showing the option has been selected again.""",Bug,Performance
32617,"""During testing, I noticed that we have a number of error markers on the closure-library project. Digging deeper, I'm seeing non-standard syntax for Mozilla. As an example:    {code}  @-moz-document url-prefix() {    .g-section {      overflow: hidden;    }  }  {code}    The CSS parser fails at url-prefix() and error recovery seems to have trouble recovering for a while after that.    Another failure occurs with an equal sign within a function:    {code}  .goog-custom-button-disabled {    background-image: none !important;    opacity: 0.3;    -moz-opacity: 0.3;    filter: alpha(opacity=30);  }  {code}    And another occurs at the second colon in the filter property:    {code}  #t3 {      filter:progid:DXImageTransform.Microsoft.DropShadow(color='#e7e7e7',          offX='2',offY='2');      box-shadow: 2px 2px 0 #e7e7e7;      -moz-box-shadow: 2px 2px 0 #e7e7e7;      -webkit-box-shadow: 2px 2px 0 #e7e7e7;  }  {code}""",Bug,CSS
32631,"""I've been trying to switch over to <USER>from Vim for a couple of months now, the one thing that really grinds my gears about it is that in Vim I was able to do code folding based on tabs (ex. lines 1 and 5 has 1 tab then some text, lines 2-4 have 2 or more tabs and then some text, I would like to easily fold lines 2-4 together)  Currently you have, what I like to call, smart-folding which is based on the language, this feature could be generic enough to work across all editors.      This seems like it would be very easy to do, but I do not know the source code well enough. I think <USER>is a great product, but this is one of the biggest, if not the biggest reason I have not been using it.""",Improvement,Editor
32655,"""I get this syntax error, however I'm pretty sure there are no errors on this line. For some reasons aptana doesn't line the word 'protected' as an object property. This is a javascript code. (See attached image)""",Bug,Editor
32708,"""I want to create php project for the existing non-empty folder. In previous <USER>versions, warning was shown """"WARNING: The selected location already contains pre-existing files. ..."""" but the finish button was active and I was able to create the project.    After 3.1 update the finish button is disabled. However, ruby project creation works fine - php doesn't.""",Bug,Project
32887,"""Hi,  it would be very helpful if aptana would show me witch methods from an abstract class i need to implement. This is my abstract class:  {code:php}  abstract class Foobar  {      public function foo();         public function bar();  }  {code}    If i am creating a new class called {code:php}MyFoobar extends Foobar{code} and i an typing {code:php}public function ...{code} and there it would be helpful if i had the needed methods from the abstract class as auto completion so that i know witch methods i need and what they do.    Thanks and greetings  <USER>",Improvement,"Editor,PHP"
32897,"""Our current implementation is overly complex and makes it quite difficult to make maintenance changes. Given the simplicity of the JSCA format, we should be able to break processing down into chunks. Initially, I would suggest consuming a single type and a single alias at a time. This will avoid memory issues and should simplify processing.""",Improvement,Indexing
33304,"""Hi friends,  i would be very nice if <USER>would support multiple (scripting) languages in one file. In the current aptana version a *.php file supports syntax highliting and auto completion for php source. In *.html files we have the support for HTML. But very often you have template files where the markup is mixed. I addet you an example as attachment.    In this example i have a .html file and the full html support, but the php code is not realy good readable and i have no support for php functions, ... . So it would be very cool if aptana would see that in the html file the <?php starts php code and switch to the php syntax mode for this part. The same think is possible with inline css code.    What do you think?  thanks and greetings  <USER>",Story,Editor
33405,"""Hello. I would like to see the improved editor functionality.    In old Apatana versions a litle green triangle was displayed near the inherited method. And I was able to jump to the parent method by clicking that icon (It was on the left sidebar. Just right to the line numbers).    Can this be done in the new <USER> Thank you.    p.s. I use php editor.""",Story,Editor
33559,"""This is a """"clone"""" of issue APSTUD-3061, because even though it was closed and marked as """"fixed"""", the issue remains. I have tried to comment on the the APSTUD-3061 issue, but I am not sure if people are following the comments. I have been trying to track down the occurrences of this problem and I think that I was able to narrow it down:  - paste works always  - cut/copy does not work when <USER>s processor load is above around 20%     - copy and cut gets grayed out in the """"Edit"""" menu     - I have been recently running <USER>side by side with the activity monitor so that I can see when I can actually copy, because otherwise it is extremely frustrating  - selecting text in editor with SHIFT + arrows seems to make the load jump up  - actual copy-paste makes the load jump up (up to around 65%)  - moving around with arrows makes the load spike up  - I have been seeing loads up to 80% - 90% which seems weird on a quad core machine (Core i5 iMac)    I would be happy to provide any logs or do any other testing in order to get this issue resolved, because it is highly frustrating when the copy/cut functions are not available most of the time.""",Bug,Editor
33780,"""I'm not sure if this is at all possible, but would it be feasible to include HTML syntax within PHP quoted strings?  It's horrifying to have read HTML code within a php string with no syntax highlighting. It's equally horrifying to have to jump out of php, code out some html, then jump right back in.    You wouldn't have to need to offer it by default, but as an option which can be toggled on and off through some sort of configuration panel. I've managed to alter Textmate's syntax highlighting to provide html syntax within php quoted string's, it was simply a matter of including the HTML syntax text.html.basic in php's string-single-quoted and string-double-quoted selectors as shown below:    {code}  string-single-quoted = {      name = 'string.quoted.single.php';      contentName = 'meta.string-contents.quoted.single.php';      begin = """"'"""";      end = """"'"""";      beginCaptures = { 0 = { name = 'punctuation.definition.string.begin.php'; }; };      endCaptures = { 0 = { name = 'punctuation.definition.string.end.php'; }; };      patterns = (          {   include = 'text.html.basic'; },          {   name = 'constant.character.escape.php';              match = '\\[\\'']';          },      );  };  {code}    It was a bit buggy and sub-par, but I'm sure you guy's can refine it for aptana.    Thanks in advance.     P.S: Awesome IDE by the way.""",Story,"Editor,HTML,PHP"
34008,"""Steps:  0. Setup  a. Ruby 1.9  b. Cucumber 1.0.2  c. Create a Ruby project and add features/support/env.rb  1. Create new Ruby Application debug configuration  a. Program = /usr/local/bin/cucumber (Mac) or c:\Ruby\bin\cucumber (Windows)  2. Set breakpoint somewhere on top of env.rb    For example, env.rb:  ----------------  require 'logger'  require 'tmpdir'  require 'rspec/expectations'    log = Logger.new(STDOUT)  ----------------  The breakpoint is set on line #5.    3. Launch debug configuration    Expected: Debugger stops at line env.rb:5  Actual: Debugger stops at rb_language.rb:5    rb_language.rb:  -----------------  require 'cucumber/core_ext/instance_exec'  require 'cucumber/rb_support/rb_dsl'  require 'cucumber/rb_support/rb_world'  require 'cucumber/rb_support/rb_step_definition'  require 'cucumber/rb_support/rb_hook'  -----------------    The thread is showing (Breakpoint at .../env.rb) correctly, but the stack below does not include env.rb as it usually does. Subsequent breakpoints, for example in a step definition also cause debugger to stop at a wrong context, for example, debugger stops at:    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/core_ext/instance_exec.rb:15    Interestingly, it seems to get *line numbers* correctly, that is it hits correct line number, but in a wring file (!).  Here is the example of a stack trace in the Debug window:    Ruby Thread - 1 (Breakpoint at /Users/DK/Projects/RAP/WorkingCopy/features/step_definitions/RAP_steps.rb:15)    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/core_ext/instance_exec.rb:15    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/core_ext/instance_exec.rb:48    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/core_ext/instance_exec.rb:36    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/rb_support/rb_step_definition.rb:62    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/step_match.rb:26    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/step_invocation.rb:59    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/step_invocation.rb:38    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/tree_walker.rb:99    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/tree_walker.rb:98    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/step_collection.rb:15    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/tree_walker.rb:93    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/tree_walker.rb:92    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/scenario.rb:41    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/background.rb:51    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/background.rb:38    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/tree_walker.rb:57    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/tree_walker.rb:56    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/feature.rb:41    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/tree_walker.rb:20    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/tree_walker.rb:19    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/features.rb:29    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/features.rb:17    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/features.rb:28    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/tree_walker.rb:14    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/ast/tree_walker.rb:13    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/runtime.rb:45    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/cli/main.rb:43    /usr/local/lib/ruby/gems/1.9.1/gems/cucumber-1.0.2/lib/cucumber/cli/main.rb:20    /usr/local/bin/cucumber:14    /usr/local/bin/cucumber:19     I'm marking this as regression, because Cucumber debugging in examples above works just fine for me in my old installation of Eclipse 3.6.2, <USER>RDT 1.4.0, Ruby 1.8. Here is how stack trace looks there when it stops in the correct context:    Ruby Thread - 1 (Breakpoint at RAP_steps.rb:15)    /Users/DK/Projects/RAP/WorkingCopy/features/step_definitions/RAP_steps.rb:15    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/core_ext/instance_exec.rb:48    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/core_ext/instance_exec.rb:36    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/rb_support/rb_step_definition.rb:62    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/step_match.rb:26    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/step_invocation.rb:63    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/step_invocation.rb:42    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/tree_walker.rb:99    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/tree_walker.rb:98    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/step_collection.rb:15    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/step_collection.rb:14    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/tree_walker.rb:93    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/tree_walker.rb:92    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/background.rb:41    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/background.rb:51    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/background.rb:38    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/tree_walker.rb:57    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/tree_walker.rb:56    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/feature.rb:41    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/tree_walker.rb:20    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/tree_walker.rb:19    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/features.rb:29    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/features.rb:17    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/features.rb:28    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/tree_walker.rb:14    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/ast/tree_walker.rb:13    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/runtime.rb:45    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/cli/main.rb:43    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/lib/cucumber/cli/main.rb:20    /Library/Ruby/Gems/1.8/gems/cucumber-1.0.0/bin/cucumber:14    /usr/local/bin/cucumber:19    In RDT I was able to switch Ruby interpreters, and when I switch to Ruby 1.9 there I'm having the same problem. However, in <USER>Studio I can't, and it is a real mystery what Ruby and what Fast Debugger <USER>Studio is using. For example, I could not find this version of Fast Debugger that I see in the Eclipse console even after scanning entire hard drive:    Fast Debugger (ruby-debug-ide 0.4.9) listens on :53168  """,Bug,Debugging
34017,"""Installed <USER>Studio 3 with my admin account on my work laptop. Launched it as admin and configured PyDev environment. Logged out and back in as my regular user account (required to work in this account by company policy) and tried to launch <USER>Studio and get:    """"Could not launch the product because the specified workspace cannot be created. The specified workspace directory is either invalid or read-only.""""    It is normal practice for us to have to use our admin accounts to install software because of restrictions on the regular user account, and then switch to the regular account to actually run the software.    I suspect that my regular user account is trying to use the admin's workspace? Should it not create a new workspace for my regular user account on first launch?  There was no option during the installation to """"install for all users"""" that I noticed?    I suspect a workaround might be to uninstall it from my admin account and attempt to install it from my regular account. Because of the restriction on my regular account I'm not sure I'll be able to.""",Bug,Publishing
34493,"""I am a first time user, and on both of my x64 7 machines I receive a near constant host of lost data when trying to sync to a server. Originally, it would sync, save, and I could continue, and I am hopeful that this implies the issues are as a result of system changes rather than actual application issues, but I am doubtful as it began within a week of starting with <USER>3, it affects two separate but nearly identical computers, and I have made no major changes.Using SFTP, when connecting to the server the application becomes immediately unresponsive, occasionally (and I mean once out of every ten saves) it saves the data, but any major changes are guaranteed lost. This is a major issue, and I have lost close to 2000 lines of code cumulatively from this issue. I'm inclined to believe it happens when I submit more than * kb of data, where * is an arbitrary amount that overloads an address somewhere. (This fits, as in the beginning when I made minor changes, this never happened)""",Bug,"Debugging,Publishing"
34969,"""{html}<div>  <p>Hi,</p>  <p>I'd like to see a feature that most IDEs ignores - workspace  lock. It would be nice if there was an option/button that would  lock the workspace, so I can't change postion and size of workspace  elements. It's not a killer-feature and I might be the only one who  would like to see it tho :)</p>  </blockquote></div>{html}""",Story,Usability
35599,"""{html}<div><p>Environment:<br>  - Windows Vista Home - Java 6 Update 23 (build 1.6.0_23-b05) -  Eclipse for PHP Developers Helios Service Release 1 (build  20100917-0705) - <USER>Web Development Tools (version  2.0.5.1278614541-7D-7O7iRJci-jVhz-KFyoijH)</p>  <p>I'm using 1and1 for my hosting and their FTP policies cause the  FTP server to disconnect despite a regular transfer of files. I am  unsure as to the minimum file size for a file to be counted as a  valid transfer, but I am uploading a large number of small files;  when uploading regular/large sized files I do not experience the  disconnect. I am disconnected with the following message:</p>  <p>421 No transfer timeout (600 seconds): closing control  connection</p>  <p>My issue is that after the disconnect the FTP connection is not  re-established and I must manually begin the sync/upload process  again. This is becoming very time consuming.</p>  <p>Additionally, when using the SmartSync, this failure to  reconnect means that on large syncs (where it takes more than 10  minutes to prepare the sync) I am unable to see a list of files  that require syncing. This prevents me from being able to sync the  entire site and means I have to do it one subdirectory at a  time.</p></div>{html}""",Bug,Publishing
35821,"""From a wish-list thread a long time ago, but it seems like a simple and easy option. If I open a HTML file and it references some CSS and JS files, open those automatically too. Dreamweaver does this (CS4), and people reported missing it a lot. I would check to see if the behavior is on or off by default (assuming there should be a preference for this, or possible we just implement it as a command in the HTML ruble?)""",Improvement,HTML
36247,"""Right now we have all our HTML metadata stuffed in one giant file. If a user asks for code completions they get anything in there. Some users may want that list limited by the doctype declared in the document, or an explicit setting per-project. As an example, I'd like to work with HTML5 and only receive the values that are valid in 5 and aren't deprecated/removed. There is currently no way to limit it in this way - and thus I may not be aware that <plaintext>, <basefont>, <big>, <font>, <tt>, <s>, <strike>, <u>, <center>, <acronym>, <applet>, <dir> are all deprecated/removed.""",Story,HTML
36767,"""Please open the attached script and then try adding some text  after the commented code. The php editor slows to a snails pace,  with long delays between keystrokes, and the CPU works hard, with  the fan kicking in. This seems to occur on any script with large  amounts of commented-out code. In such cases, I need to use another  editor, as aptana is virtually unusable in this situation.""",Bug,PHP
36923,"""I would like to see a basic Visual HTML Editor added into <USER>    I see that there has been a lot of debate about the usefulness of a WYSIWYG or Visual HTML editor in an IDE with the power of  <USER> Here are my two cents:    I work with designers who have little to no HTML skill. I also work with Engineering teams that want to make sure the HTML pages  created by those designers are put up on a test server before being pushed to the production servers, and IT departments that want  everything backed up/versioned using something like SVN.    <USER>give us almost everything we need to make these disparate groups happy. The last missing piece is the Visual Editor.    The editor does not even have to be capable of more than CSS changes. While it would be awesome to have the same kind of split  view that Dreamweaver & NVu do, what we need most is for the Design team to be able to tweak pages an engineer has built by  changing things like the hover behavior of links or the background color of an area.    Could they figure out the HTML? Maybe.    Is forcing them to try to use HTML to make minor changes time and resource efficient? No.    So, count this as a vote for some kind of WYSIWYG or Visual editor.  """,Story,HTML
41149,"""The NCLOC metric doesn't seem to count groovy code as a NCLOC. I'm attaching a screenshot from our report that shows a groovy class but doesn't seem to consider any code in it as actual code. """,Suggestion,Groovy
41434,"""Follow up of the CLOV-1256    As a I developer I would like to have an ability to use Clover's test optimization feature for tests written in the Spock framework in order to speed up my test cycle.     """,Suggestion,"Instrumentation,Groovy,Database"
41569,"""During implementation of CLOV-1162 I've found that anonymous inline classes does not have their own ClassInfo object crated. Instead of this, methods of inline class are added to the enclosing class.    For instance, a following code:    {code:java}  public class AggregatedMetrics {      /**       * Inner class, case with a method having an inline anonymous class       * statements = 3 (sum of direct methods' statements)       * aggregatedStatements = 5 (sum of direct methods' aggregated statements)       */      class C {          /**           * Method: statements = 2; aggregatedStatements = 4 (method's statements + inline class aggregated statements)           */          Iterator methodThree() {              int d = 4;              /**               * Inline class: statements = aggregatedStatements = 2 (sum of its methods)               */              return new InstrumentationSessionImplTestSample2() {                  /** statements = aggregatedStatements = 1 */                  public boolean hasNext() {                      return false;                  }                    /** statements = aggregatedStatements = 1 */                  public Object next() {                      return null;                  }                    /** statements = aggregatedStatements = 0 */                  public void remove() {                    }              };          }            /** statements = aggregatedStatements = 1 */          void methodFour() {              int e = 5;          }      }  }  {code}    is stored in clover database in this way:    {code:xml}  <class name=""""AggregatedMetrics.C"""" qualifiedName=""""AggregatedMetrics.C"""">   <metrics  statements=""""5"""" aggregatedStatements=""""5""""/>   <method name=""""hasNext() : boolean"""">    <metrics  statements=""""1"""" aggregatedStatements=""""1""""/>   </method>   <method name=""""next() : Object"""">    <metrics  statements=""""1"""" aggregatedStatements=""""1""""/>   </method>   <method name=""""remove() : void"""">    <metrics  statements=""""0"""" aggregatedStatements=""""0""""/>   </method>   <method name=""""methodThree() : Iterator"""">    <metrics  statements=""""2"""" aggregatedStatements=""""2""""/>   </method>   <method name=""""methodFour() : void"""">    <metrics  statements=""""1"""" aggregatedStatements=""""1""""/>   </method>  </class>  {code}    It means that on reports the AggregatedMetrics.C class is presented as having 5 methods, while it has 2 actually.     It's not a bug. There was a design decision to not present anonymous classes in a report.     Possible improvement:   * create ClassInfo objects for anonymous classes   * add option for reporting whether to show anonymous classes (like InstrumentationSessionImplTestSample2$1) or not  """,Suggestion,Database
42222,"""CPARTY natter on the topic:  {quote}  <USER> multi-processing == multi-threaded test execution?  <USER> not so sure. I assumed so.  <USER> would be good to know. we should have something in place (thread-local slice coverage) so that we at least support it even if not very performant which we can more easily do now with my per-test coverage refactoring   <USER> yes! was thinking that. looking good btw!  <USER> if we did have to support this, we'd probably be better off changing instrumented code so that the thread-local slice coverage was fetched once per per method call (of the instrumented app) and stored as a local to minimise thread-local slowness  <USER> TestNG already supports multi-threaded execution of tests, which is how bamboo run their tests i believe  <USER> there may even be some hotspot mojo benefit to storing as a local ordinarily.  <USER> i'll raise a JIRA with my thoughts and we can think about doing it when there's a need  <USER> ah, so you mean only store per-test coverage at the method level?  <USER> and not for each line?  <USER> no  <USER> i mean, a call to recorder.inc(0) will, if we have thread-local slice coverage, mean a call to a thread local for every element - performance FAIL  <USER> if we at least cache the thread-local as a local variable of the instrumented method we avoid a lot of these thread-local calls  <USER> so # thread-local calls == # method calls in test run  <USER> wouldn't lead to spurious results?  <USER> not # thread-local calls == # elements in test run  <USER> how so?  <USER> if two tests enter the same method at the same time and both see the same thread local?  <USER> impossible   <USER> two tests, same method => two threads => two thread locals  <USER> 1 cache == 1 bug   <USER> sorry, to back up: we would have 1 x slice-coverage per thread  <USER> no caches here  <USER> not a cache in the conventional sense  <USER> k, gotcha. so many thread locals would be cached at the start of each method  <USER> there would only ever be one local variable defined per instrumented method which would, as local variables do, live on the stack associated with the executing thread  <USER> k  <USER> i see  <USER> it would be a tradeoff  <USER> currently we have 1...N slice recorders in play but normally only 1 - and they record coverage across all threads of the app  <USER> with my suggestion, we would have 1 x slice recorder per thread but they would only record coverage by their associated thread  <USER> so if they spawned a thread the coverage of that spawned thread would be unattributed to a test (unless we used thread groups or something - not clear how this might work)  {quote}""",Suggestion,Instrumentation
42572,"""Currently Crowd only supports reading Azure AD.     As an admin, I would like to be able to manage all users in one place - in Crowd.   I would like to add/remove/edit users in Crowd and sync those changes to Azure AD.""",Suggestion,Directories
42573,"""h3. Summary  As a system admin  I would like to be able to set a different alias for users with the same usernames in different directories  So that the users can be able to login on each application with the alias, allowing the same username to exist across directories, beloging to different people    h3. Problem Definition  Customer has multiple directories configured in Crowd. The usernames are flattened, so that a {{jsmith}} may exist on more than one of the Directories, belloging to different actual users.  When user {{jsmith}} tries to login, only the one in the first Directory, in the order set in Crowd, will be allowed to login.  Customer tried to add aliases to each of them, but aliases are associated with the user and the application, and not with the directory:   !Screen Shot 2020-09-07 at 19.03.16.png|thumbnail!     h3. Suggested Solution  Development could consider including alias associated with a combination of username+directory, not only  username. So that we can have {{jsmithteamA}} and {{jsmithteamB}} (for instance) each of them associated with a different directory and user.    h3. Workaround   Currently, the user has to be renamed, or customers should use another attribute for username which is not duplicated across directories, such as e-mail for example.  If the remote Directory allows alias to be added to users, and it is set for all users, that alias attribute could be used as username in Crowd too.""",Suggestion,Directories
42608,"""h3. Summary  +As a+ System Admin  +I want to+ have database credentials on local nodes  +So that I can+ have a better handling of the different instances on my environment, such as Production versus Staging versus Business Continuity, etc    h3. Problem Definition    Customer has different environments with Crowd Data Center clusters for each scenario, such as Production, Business Continuity, Staging, Development, QA, etc.  To refresh these environments data from production storage (shared folder) is copied to those environments, so is also copied the {{crowd.cfg.xml}} file, which contain the database credentials. Those need to be updated on each refresh, to point to the correct database instance    In a situation, for example, when Production cluster is totally down and they want to activate Business Continuity one, they need to make sure that the database connection information and credentials are the correct ones. Whether, if that was local in each node, they could have that correctly setup during server build.    h3. Suggested Solution  Have Crowd handle the {{crowd.cfg.xml}} file the same way Confluence does with {{confluence.cfg.xml}} or Jira with the {{dbconfig.xml}} one. Or let it be configurable.     h3. Workaround   Customer currently have scripts on crontab to handle that.  """,Suggestion,Database
42668,"""as an administrator I need to be able to rename a group in Crowd""",Suggestion,Authentication
42702,"""h4. Summary:    As a Crowd administrator, I would like to use OKTA SSO (SAML) for my Atlassian applications, however, my idea is to have only Crowd integration with OKTA and acting as a bridge app in between OKTA and Jira/Confluence/Bitbucket. This means that if an unauthenticated user logs to Confluence, Confluence sends an authentication request to Crowd and Crowd talks to OKTA to get the SAML Authentication done.  h4. Scenario:   * User tries to access Confluence/Jira.   * They reach Crowd Logon page.   * They insert their user and password which is authenticated against the information in OKTA.   * OKTA sends the confirmation to Crowd.   * Crowd redirects to the application being requested.       h4. How does SAML Works?    Each application needs to be SAML Versed in order for them to be able to integrate with Okta. The way we establish this is by either using the Connectors provided by Okta (available for Jira and Confluence only) or through third party plugins provided by vendors such as Mini Orange(e.g).    Once that's done, we integrate those applications with Okta with the aid of those plugins as well. At this point, the applications will know that login requests should go to Okta to be processed and will also know how to read a SAML response accordingly.  h4. Considerations:    The scenario explained above seems ambiguous, since both Crowd and Okta in their scenario seem to have the same or very similar proposal which is to act as the IdP. However, even though this is similar, it can be important for some organizations or companies.""",Suggestion,SSO
42874,"""I want Crowd to be able to connect to the SAML IDP as an example Centrify, Okta or OneLogin to provide delegated sign-on for Atlassian products so that my other Atlassian products like Jira, Confluence or Bitbucket can utilize it and be connected to the SAML IDP through Crowd.  """,Suggestion,"Authentication,User Management"
42897,"""{panel:title=Atlassian Update - 27 March 2020|borderStyle=solid|borderColor=#3c78b5|titleBGColor=#3c78b5|bgColor=#e7f4fa}  Hello everyone,    We’re happy to say that we getting very close to release this feature. You can expect it in the next release of Crowd Data Center.    Thanks again for your patience and support!    We’ll let you know once the release is available. Stay tuned!    Best regards,    <USER>  Senior Product Manager, Crowd   [<EMAIL>|mailto:<EMAIL>]  {panel}      As a Crowd administrator, I would like to manage local group memberships for users synced from Azure Active directory. Ideally, *Manage groups locally* option should be available for Azure AD.""",Suggestion,Directories
42923,"""Some configuration of client applications might caused increased traffic to Crowd and degraded performance.    As an admin, I would like to be able to see such misconfigured application in Crowd in order to take action and improve the performance of my setup""",Suggestion,Embedded
43123,"""I wonder if this possible to store users off the directory?    This could be useful when user should exist in several directories, attached to different Atlassian applications    For example: I want Joe and Mary to be performed at Jira user dirctory as well as at Confluence, but I want just Joe to have access to Bitbuket Server    Sure, i can create duplicate user records at each directory, but how about passwords, name, last name and so on?    I suppose best way is to store users separately, having just user membership across directory""",Suggestion,Directories
43159,"""As an administrator of an environment of Atlassian Systems using Crowd as an centralised   user base imported to crowd from LDAP directory I want to be able to change group names   in the LDAP directory without losing their mapping to users and groups in Crowd.    (!) Due to the Problem that Groups and their Mapping to other Groups and Users are handling by Group Name instead of an ID the Mapping for LDAP Groups are broken as well.    Acceptance Criteria:  - group names in LADP directory can be changed at will  - crowd reflects the changes in group name after sync  - modified group keeps their mapping to users and groups  - (internal:  cwd_group.id does not change)""",Suggestion,Directories
43230,"""I need access to the directory on the user principal to know which directory they authenticated with (using this as a grouping mechanism), essentially allowing each company/customer to have their own directory.      The soap API returns the directoryId, and so I've resorted to that to get some handle on the directory, but ultimately it would be nice if the REST API could return the directory name or id on the principal.""",Bug,REST
43372,"""As a user of {{/search}} resource, I want to add {{expand=attributes}} when I search users and obtain users with attributes.""",Suggestion,REST
43505,"""I would like very much to see extended LDAP attributes supported in future Crowd versions.    Currently, Crowd (and all the tied-in Atlassian applications like JIRA, Confluence and Bamboo) have to rely on user to enter their attributes in the database by themselves. But why bother if we (usually) already have them in our LDAP directories?    The usual attributes we're using at our company are:  * telephoneNumber  * mobile (That's for the cell phone, obviously)  * jpegPhoto  * title (Job Title)  * manager (that's for the person's immediate supervisor)    I'm sure there will be other people that'd like very much for Crowd/JIRA/Confluence to support this, as entering that manually was always an unneeded and cumbersome task.  Anyway, thank you for your time Atlassian, you're doing a great work!""",Suggestion,Directories
43588,"""As a REST client, if I want to invalidate the current password of a user so the user cannot use it anymore and is forced to reset it, then I have to generate a random password string, which may or may not match the current password policy.    I'd prefer to have a REST API to invalidate a user's password that is guaranteed to succeed, not matter which password policy has been set.""",Suggestion,REST
43644,"""As an admin who has connected Crowd (or an Embedded Crowd product) to an LDAP server, I want to know if the LDAP queries are performing well, or they are being very slow.""",Suggestion,Directories
43645,"""As a <USER> I want to install the plugins outside of the Crowd home directory.""",Suggestion,Plugins
43875,"""From JRA-27353    Steps to reproduce:    1. install JIRA  2. setup LDAP directory - create test data (see below)  3. add LDAP directory in JIRA configuration with option """"Enable Nested Groups"""" enabled  4. synchronize data  5. change name of """"sub_group"""" to """"Sub_group"""" and update uniqueMember parameter of parent_group to match changed name  6. synchronize data again - here crowd should fail on inserting data into database (CWD_MEMBERSHIP table)    I was able to reproduce it with HSQL and PostgreSQL.    Test data - two groups with relation:        parent_group, uniqueMember= {sub_group}      sub_group    LDIF export of my test data (I've configured LDAP to use root entity dc=atlassian,dc=pl):  parent_group.ldif    version: 1    dn: cn=parent_group,dc=atlassian,dc=pl  objectClass: groupOfUniqueNames  cn: parent_group  uniqueMember: cn=sub_group,dc=atlassian,dc=pl  description: Parent group    sub_group.ldif    version: 1    dn: cn=sub_group,dc=atlassian,dc=pl  objectClass: groupOfUniqueNames  cn: sub_group  uniqueMember:   description: Child group    Also to see effect of exception which you should get in step 6 you may add user and set membership using uniqueMember attribute - this user won't be added to any group because of synchronization fail (user should be created, groups also, but no user membership in group will be created).    This can also be reproduced without the use of nested groups.""",Bug,Directories
43882,"""As a customer I want to be able to map AD/LADP groups to crowd with all members (including people that are members of a sub-groups) but without creating the huge list of nested sub-groups.     -- I would call this flattening of the group membership    By using LDAP/AD filtering we can limit which groups are to be mapped inside Crowd but we also want to put all members inside this groups without having to create tons of sub-groups.    Lets say we have """"Support"""" that contains """"Americas Support"""", """"EMEA Support"""", some users and some other groups. """"EMEA Support"""" can also have groups with each country or city and so on..    Obviously, as a customer we want to be sure that the """"Support"""" group inside Crowd does contain all people from all sub-groups.     I would even go so far to propose this to be the default behavior.     When you send and email to a mailing list, this will reach all people even if they are not direct-members. Still, on crowd this happens only when you import all the nested memberships, and this sucks because it does SPAM the list of groups (a lot!)    As an implementation idea, you could import subgroups as """"disabled"""" groups unless someone is manually activate them.    Also, another concern for me is, the lack of flattening support could prevent us from using Crowd with systems that are not supporting the nested group behavior.""",Suggestion,Directories
44143,"""Time to get everyone using the same directory configuration. This will also help Crowd know when they break things in the admin plugin! Or even need to add features as API's and features in Crowd change and evolve.""",Suggestion,"Embedded,User Console"
44375,"""As an application, I want to create a token for user that is valid for a short period of time. For instance, to temporarily grant the user access to a restricted part of the application.""",Suggestion,"SSO,REST"
44559,"""Actually I have some Atlassian apps such as JIRA, Confluence etc. All apps were using JIRA as a user management base.   Now I've got CROWD and I've successfully moved all users from JIRA to the CROWD internal dicertory. And this is fine.  But also I have Microsoft AD, and I also have connected it as a second CROWD directory.  Now I have 2 user directoties in CROWD - internal and AD. And most of users in both directories are the same persons.  And here I've got a problem. I have a lot of issues in JIRA and pages in Confluence which were created by users from JIRA directory. And now I want to make accounts from AD and JIRA to be merged for most of the users to keep history/backlog of all the issues/pages.  So for example issue KEY-123 was created by user <EMAIL> (from JIRA/CROWD directory). But this user has an account in my AD - <USER> And my goal is to let user <USER>become a pair to user <EMAIL> and login to JIRA/Confluence under AD account and see issue KEY-123 as my own.    Please put in on your scheduler.""",Suggestion,User Management
45287,"""For various reasons, I have Crowd set up to do anonymous bind. That's fine, until Firefox does an autofill (Firefox's auto-fill seems to have become more aggressive since the field-rename in CWD-599). I can remove the ldap.userdn but not the ldap.password — a blank password apparently means """"don't modify"""". To fix this, I have to poke the database (good thing I'm not using HSQLDB).    Moreover, in addition to losing the password (CWD-1763), the """"test connection"""" button also uses the password in the field instead of the one in the database. This means the """"Test Connection"""" button will fail where it would otherwise succeed.    A simple (if slightly horrible) solution would be to treat {{\*\*\*\*\*\*\*\*}} as a string meaning """"whatever is in the database"""". This means it's impossible to set the password to eight asterisks, but that's something we can live with. It also means the """"test connection"""" button can know when it has to pull the password out of the database.    Not quite sure which component this goes in; it's largely a UI problem.""",Bug,"Directories,User Console"
45356,"""Here's how to replicate the problem I'm experiencing:    Setup a fresh 2.0.3 install using a internal directory.    Problem #1    * go to administration and disable """"Authorisation Caching"""" (for good measure)  * add a new user to the directory  * go to groups > crowd-administrators > direct members and add the new user to the group  * log into the crowd console using the new user account    -> this results in the """"self service"""" console, not the admin console as expected  * restart crowd, then log into the crowd console using the new user account again    -> after the restart you get the admin console    Problem #2    * remove the user from the crowd-administrators group  * go to applications > crowd > directories and change """"Allow all to authenticate"""" to True  * log into the crowd console using the new user account    -> this should result in self-service console, instead the user has admin privs  * restart crowd, then log back into the crowd console    -> now the admin privs are gone and you'll see the self-service console     """,Bug,Caching
45778,"""LDAP queries in general should not be """"contains"""" or """"starts with"""" or """"like"""". I'm getting whacked by my administrator because the crowd queries are needlessly complex and are all """"contains"""" - i.e. *<USER>. The system should really default to equals (i.e. exact match) and provide the ability to customize how they are done.  Also, there is no need at all to force an object class search. As an example - in Active Directory, all you should have to do is search for sAMAccountName={input user} with nothing else needed. Unfortunately,  I will have to default to the internal LDAP mechanisms in your other products until this can be worked through - great concept though!      """,Bug,"Directories,Directories"
46125,"""I've been testing Crowd 1.3, and I really like the new Delegated Directory feature.  One thing I found non-obvious about it was that users have to be manually created or imported in the delegated directory in order to be visible, even if they actually exist in the underlying connector-based directory.  It would be nice if there was a way to have the Delegated Directory have a fallback when a user isn't found in the internal directory, which is to check in the connector-based directory and if the user exists in there, to create it in the internal directory.    One important advantage of such a fallback is that users wouldn't have to be created in two separate places - Crowd would automatically know about a user as soon as it was created in LDAP.  (With the Delegated Directory the way it currently is, the user might exist in LDAP but not be visible in Crowd.)""",Suggestion,Directories
46298,"""This is more of a Christmastime-induced wish than a requirement. The Crowd API ({{SecurityServerClient}}) uses Arrays pretty much exclusively. When programatically dealing with Crowd objects, it's difficult to do using Arrays.    For example, if I want to add an attribute (or many attributes) to a SOAPEntity, I have to pack the attributes into an Array and worry about sizing the Array, etc. It would be wonderful if SOAPEntity stored Attributes as a Collection so that adding a new attribute would be as simple as {{entity.getAttributes.add(attribute)}}.    I know this would lead to a large amount of changes to the Crowd API. Feel free to WONTFIX this request if that's too much to ask. Just thought I'd throw it out there.""",Suggestion,SOAP
46299,"""SOAPEntity currently only has a {{getAttributes()}} method which returns an array of SOAPAttributes. A convenience method to return a specified Attribute would be a real benefit. Something like this:    {{public String[] getAttribute(String name)}} _This could throw an AttributeDoesNotExist Exception or something like that._    More from the forum post:  {quote}  While I'm on the subject, there are a few convenience methods in SecurityServerClient regarding attributes, but one seems to be missing: getAttributeFromPrincipal(String principal, String attribute) which returns a single SOAPAttribute or maybe just the String value of that attribute. As it stands, I'm probably going to subclass SSC and add this myself since we have a demand for it.    Alternatively (and probably a better solution) would be to have a getAttribute(String attributeName) method on a SOAPPrincipal (really all SOAPEntitys). Having to iterate through the array returned by getAttributes() is just too cumbersome. I'm not sure if I want to go subclassing SOAPEntity to add this method though, since the casting would be a pain.  {quote}""",Suggestion,SOAP
46696,"""The soap interface is not appropriate for scripting of tasks; it requires the generation of proxy-objects which must be instansiated and populated for even simple requests.  It would be nice to have a simplified interface (SOAP, XML-RPC or both) that accepts base-types as arguments.  I'd like to be able to do something like this (Python code, based on the confluence xmlrpc interface):    {noformat}      srv = xmlrpclib.ServerProxy(crowdurl)      token = srv.crowd1.login(user, pass)        srv.crowd1.addUser(token, uname, passwd)      srv.crowd1.addUserAttrib(token, uname, """"email"""", email)       srv.crowd1.addUserToGroup(token, uname, group)  {noformat}  """,Suggestion,SOAP
47004,"""h3. Issue Summary  Fisheye treating the *.doc* and *.xslx* MIME type as binary. As for the *.xslx*, this was added in the {{FISHEYE_INST/raw-mime.types}} file but the changes were not applied.    h3. Environment  * Fisheye Crucible version 4.6.1  * File type Excel - .xslx and Document - .doc    h3. Steps to Reproduce  h5. Document file type  # Add a file called _sample.doc_ in the SVN repository.  # View the _sample.doc_  in the Fisheye UI's repository _Source_.     h5. Excel file type  # Stop the Fisheye.  # Edit the {{FISHEYE_INST/raw-mime.types}} to have the following:  {code:java}  text/xml   xml xsl xlsx  {code}  # Restart the Fisheye  # Add a file called _sample.doc_ in the SVN repository.  # View the _sample.doc_  in the Fisheye UI's repository _Source_.     h3. Expected Results  View the file content    h3. Actual Results  Fisheye showing the file as binary. The file is only available to download.    h3. Workaround  Currently there is no known workaround for this behavior. A workaround will be added here when available""",Bug,Indexing
48049,"""Currently the installer for Windows created a FISHEYE_INST, but it doesn't check for old data.    The suggestion would be to allow the user to specify the location of the old instance, in order to allow the installer to take its data and move it manually to the new FISHEYE_INST created.    I'm not suggesting to detect if FISHEYE_INST is set or not because the customer might have FishEye running as a service, and in this case there is no environment variable to define FISHEYE_INST, as it is defined in the wrapper.conf.""",Suggestion,Server
48430,"""As an administrator it would be very useful if I could add a Stash _Project_  to Fisheye, and all repositories in that project would be added as Fisheye repos. I'd also like an option to make it a one-time add or automatically add/remove repos as the Stash project changes. This feature would cut down on my administrative time in FishEye and allow users to help themselves. It would also put more of our source into FishEye, making FishEye more valuable to us. """,Suggestion,Integration
48887,"""As we take on administrating fisheye our developers have a exploding number of repositories that they add/change weekly. Currently i have added 280, and they add a 10-12 a week. What i would like to see is a tab on the Repositories Admin page that lets me at a glance see Errors in repositories. Or filter by State.         NOTE: As an administrator of Fisheye and other Atlassian products I have found inconsistancies among searching and categorizing products/repos/etc but that is a seperate issue or future enhancements.  """,Suggestion,"Server,Repository"
48939,"""I would like to request a feature to allow you add a category to the repository list. Like Jira, we have a lot of Repositories that will be maintained in Fisheye. We can limit who can see what repositories through the permissions, however if a user has access to multiple business unit repositories I would like to see a Category or Group to help lessen the clutter.""",Suggestion,Repository
49240,"""I see the LOC numbers come up in fish eye and can't help comparing my productivity against the others.  I'm quite familiar with the issues of   LOC, but it's still interesting. There are a few issues that make it  of much more questionable value in our group however. The metric counts too much and blows the numbers out of the water for some of us.     - adding a way to limit LOC to just .java files would help eliminate noisy and very larget data files would help keep checkins of that type of file from making it look like someone is on performance enhancing drugs checking in 100,000 lines over a weekend. I might implement this as a filter that could by directory or file type. You never know when someone is doing something similar, but doing it in java files.    - In a way that may be more valuable to people that don't check in large amounts of test or resource data, having the LOC counter not count the creation of a branch would help as well. If you happen to be the guy who created a branch, you get """"points"""" for all 100k LOC or so. It's a trivial operation that throws metrics way off.    """,Suggestion,Integration
49505,"""I copied an existing file in a Git repository and made a minor change to the target. The review shows it as a completely new file, but I would expect it to be shown as a copy.    Could this be down to git only looking for copies of files that have changed in the same commit? {{--find-copies-harder}} would be great in reviews that copy an existing file without changing the original.  """,Suggestion,Indexing
49530,"""Feedback from customer   {quote}  I've never been clear on exactly what goes into the external database. It seems like only Crucible data goes in there, whereas the FishEye cache is all in local disk files. If this is the case, is there any advantage to using an external database in a FishEye-only environment?    Furthermore, if it's true that the external database is only helpful for Crucible users, I would strongly urge you to make that much, much clearer in your documentation and to eliminate the scary bar at the bottom of the admin page admonishing you not to use HSQLDB.    The """"robustness"""" issue vis à vis HSQLDB isn't a big deal for us: in the worst case, we'd just need to wipe out the cache directories and re-index from scratch. Our code base is small enough that this would only be a minor inconvenience. Obviously this would not be true for much larger code bases, but this, too, should be documented as a reason to chose an external database over HSQLDB.    On the other hand, if using an external database improves FishEye performance in some way, I'm more than happy to invest the time and effort into moving over to that.    Regardless of what the answer is, I would again strongly urge you to update the documentation stating exactly what role the external database plays in a FishEye-only setup. Every time I upgrade FishEye I have to think about this situation again, and it would be nice to have a definitive answer about whether I should care about using an external database.{quote}    Clarify use of external database for Fisheye only deployments;  # Repo indexes are stored on disk,  # user data is still stored in hsql DB  # does it have significant performance improvements for medium/small/large deployments  """,Suggestion,Documentation
50063,"""Don't think you can do this as of now, but if so please point me in the right direction, anyways here goes.  Whenever Code Metrics are shown in Fisheye they include all file types, as well as all directories.  h4. Directory Filters  Im not sure if the """"Subversion Repository Structure"""" configuration can help me out on the directory part, but in my SVN repositories, I have the following structure for each project:  * \{projectName}/trunk/3rd - Third Party assemblies and related files * \{projectName}/trunk/\{sub-dir-1} * \{projectName}/trunk/\{sub-dir-2}  And so on, so currently when I view the """"LOC"""" diagrams, it inclues the """"3rd party folder"""" that I have, and when the """"documentation xml"""" files is included (.Net projects), that sometimes really sum up in addition to License files etc.  So it would be desirable to exclude that from the metrics, yet I still wan't it in the """"repository"""" browser since it is an important folder in terms of downloading tar balls and looking at changes.  h4. File Filters  Adding the ability to exclude files from the diagrams, there is as an example many files in .NET project structures that is not interesting when looking at the LOC, e.g. csproj, sln, designer.cs etc... since this is not developer written files, further in some cases I would like to just be able to view the .cs files.  It would be extra neat if this was a dynamic feature on the diagram pages, where one could enter an advanced filter  - Regex: """".\*\.(cs|xml|config)"""" (unless ^ and/or $ is included, automatically add ^.\* and .\*$ to the start and end respectively).  Or in simple cases choose a file type from a drop down populated with maybe all or most common types.  Just as above these files are interesting when browsing the directories, but not much when viewing Metrics.""",Suggestion,Repository
50167,"""Just as a background - we have essentially stopped using FishEye due to the UI changes between 1.5 -> 2.1.4 - after heavily used FishEye for every day before, this is really a bit painful. So let's start with the major issue:  The list of commits that has been done now uses way to much space vertically, making it impossible to see more than a few commits.  I've attached a screenshot outlining all the dead space in a typical layout - I'd really expect to be able to see at least 25-30 commits in that space, rather than 7 (!) which is all that fits now. With the previous UI, it was possible to see a lot more at a glance, avoiding continuous scrolling.  I don't advocate reverting to the old UI, but some way of displaying more data 'on the same line' and to compact the display significantly is really needed - the content here are the commits, 'content is king' - let the 'king' rule ;-)  I've been using FishEye to get a good sense of 'what has recently been changed and by whom', this requires to be ablet to scan a fair number of recent commits without using the scrollbar...  I'm attaching a second (very ugly, sorry) screenshot with one possible suggestion on how compact the layout significantly by putting things on one line and moving the 'modified X files' to the rightmost side instead.  """,Suggestion,User interface
50390,"""We've just upgraded from FishEye 1.5 to 2.1.4 - looking forward to the newly improved GUI - at least, that is what we thought.    I'm not sure what happened here, but it is extremely difficult to navigate or get an overview of the information now - the output format takes up a lot more room than in the previous version and it feels like we've lost control unfortunately.    It seems that Jira/Confluence (which has a lot more of functionality) is significantly better structured and much easier to use than FishEye is now in the new version.    I really think the UI needs to be rethought significantly from scratch - I could try to come with individual cases for improvement, but I don't think that is a fruitful way forward - if you'd like a sounding board, I'd be happy to do it.    Just some short comments, not complete:    - One wants to follow commits and statistics for just a few releases (two, max three for us), one one or perhaps two screens (similar to Jira dashboards)  - Using FishEye as the """"heart beat"""" of development by looking at the commits     I really love the functionality and we are very happy customers for several of your other products - it just seems that FishEye really took a step backward in usability here - we know you can do a lot better! We've had multiple users complaining about it (not that it is different, we all hoped for that - just that it is unfortunately not even as good as the previous version)    """,Suggestion,User interface
50404,"""During my initial scan, I'm seeing a bunch of errors in the logs. There are multiple cases like the below, for different files. In each case, the quoted path passed to """"cleartool desc"""" has a trailing space before the end quote, which results in the desc failing. When I ran the same command myself, I got the same error - but when I removed that trailing space, the cleartool desc worked just fine. As I said, this doesn't seem to be universal, just for some files. Here's the text from Fisheye's error log (filename and branch name scrubbed for confidentiality reasons):    2010-02-02 01:48:45,796 WARN  [InitialPinger1] fisheye.app com.atlassian.fisheye.clearcase.ClearCaseContext-handleProcessError - Error executing command (desc -fmt %On  """"vob/foo/somepath/src/com/foo@@/main/foobranch/13/Bar.java """" ) :    cleartool: Error: Unable to access """"vob/foo/somepath/src/com/foo@@/main/foobranch/13/Bar.java """": No such file or directory.          at com.atlassian.fisheye.clearcase.CleartoolInteractiveProcess.exec(CleartoolInteractiveProcess.java:307)          at com.atlassian.fisheye.clearcase.CleartoolInteractiveProcess.createProcess(CleartoolInteractiveProcess.java:133)          at com.atlassian.fisheye.clearcase.ClearCaseContext.runInteractiveProcess(ClearCaseContext.java:240)          at com.atlassian.fisheye.clearcase.ClearCaseContext.executeInteractiveCommand(ClearCaseContext.java:228)          at com.atlassian.fisheye.clearcase.ClearCaseContext.executeCommand(ClearCaseContext.java:149)          at com.atlassian.fisheye.clearcase.ClearCaseContext.executeCommand(ClearCaseContext.java:137)          at com.atlassian.fisheye.clearcase.ClearCaseContext.executeCommand(ClearCaseContext.java:123)          at com.atlassian.fisheye.clearcase.ClearCaseContext.getAttribute(ClearCaseContext.java:395)          at com.atlassian.fisheye.clearcase.ClearCaseContext.getObjectId(ClearCaseContext.java:372)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner.processClearCaseCommit(ClearCaseRepositoryScanner.java:180)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner.access$000(ClearCaseRepositoryScanner.java:66)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner$1.processCommit(ClearCaseRepositoryScanner.java:118)          at com.atlassian.fisheye.clearcase.client.ClearCaseChangeParser.handleCommit(ClearCaseChangeParser.java:131)          at com.atlassian.fisheye.clearcase.client.ClearCaseChangeParser.processVersionChange(ClearCaseChangeParser.java:112)          at com.atlassian.fisheye.clearcase.client.ClearCaseChangeParser.processVersionDeletion(ClearCaseChangeParser.java:97)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner$2.processDiffInfo(ClearCaseRepositoryScanner.java:409)          at com.atlassian.fisheye.clearcase.client.ClearCaseDirectoryDiffParser.processSection(ClearCaseDirectoryDiffParser.java:100)          at com.atlassian.fisheye.clearcase.client.ClearCaseDirectoryDiffParser.processLine(ClearCaseDirectoryDiffParser.java:79)          at com.atlassian.fisheye.plugins.scm.utils.process.LineOutputHandler.process(LineOutputHandler.java:40)          at com.atlassian.fisheye.plugins.scm.utils.process.PluggableProcessHandler.processOutput(PluggableProcessHandler.java:29)          at com.atlassian.fisheye.clearcase.CleartoolInteractiveProcess.createProcess(CleartoolInteractiveProcess.java:134)          at com.atlassian.fisheye.clearcase.ClearCaseContext.runInteractiveProcess(ClearCaseContext.java:240)          at com.atlassian.fisheye.clearcase.ClearCaseContext.executeInteractiveCommand(ClearCaseContext.java:228)          at com.atlassian.fisheye.clearcase.ClearCaseContext.executeCommand(ClearCaseContext.java:149)          at com.atlassian.fisheye.clearcase.ClearCaseContext.executeCommand(ClearCaseContext.java:137)          at com.atlassian.fisheye.clearcase.ClearCaseContext.executeCommand(ClearCaseContext.java:123)          at com.atlassian.fisheye.clearcase.ClearCaseContext.findDifferencesWithPreviousVersion(ClearCaseContext.java:354)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner.getDiffInfo(ClearCaseRepositoryScanner.java:420)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner.processClearCaseCommit(ClearCaseRepositoryScanner.java:205)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner.access$000(ClearCaseRepositoryScanner.java:66)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner$1.processCommit(ClearCaseRepositoryScanner.java:118)          at com.atlassian.fisheye.clearcase.client.ClearCaseChangeParser.handleCommit(ClearCaseChangeParser.java:131)          at com.atlassian.fisheye.clearcase.client.ClearCaseChangeParser.processVersionChange(ClearCaseChangeParser.java:112)          at com.atlassian.fisheye.clearcase.client.ClearCaseChangeParser.processChange(ClearCaseChangeParser.java:79)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner.getChangeSetCommits(ClearCaseRepositoryScanner.java:671)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner.processChangeSet(ClearCaseRepositoryScanner.java:621)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner.processBranch(ClearCaseRepositoryScanner.java:588)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner.processVobs(ClearCaseRepositoryScanner.java:552)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner.processRevisions(ClearCaseRepositoryScanner.java:521)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner.doSlurp(ClearCaseRepositoryScanner.java:469)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryEngine.slurpTx(ClearCaseRepositoryEngine.java:171)          at com.atlassian.fisheye.clearcase.ClearCaseRepositoryEngine.doSlurp(ClearCaseRepositoryEngine.java:140)          at com.cenqua.fisheye.rep.RepositoryEngine.slurp(RepositoryEngine.java:319)          at com.cenqua.fisheye.rep.ping.OneOffPingRequest.doRequest(OneOffPingRequest.java:30)          at com.cenqua.fisheye.rep.ping.PingRequest.process(PingRequest.java:90)          at com.cenqua.fisheye.rep.RepositoryHandle.processPingRequests(RepositoryHandle.java:120)          at com.cenqua.fisheye.rep.RepositoryHandle.queuePingRequest(RepositoryHandle.java:110)          at com.cenqua.fisheye.rep.ping.PingRequest.run(PingRequest.java:57)          at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)          at java.lang.Thread.run(Thread.java:619)  2010-02-02 01:48:45,798 ERROR [InitialPinger1] fisheye.app com.atlassian.fisheye.clearcase.ClearCaseRepositoryScanner-processClearCaseCommit - Invalid version id : vob/foo/somepath/src/com/foo@@/main/foobranch/13/Bar.java   """,Bug,Indexing
51003,"""As I tried to view diff between 2 revisions of a file (contains few long lines), the Display Preferences > Soft Wrapping is not working. After I tried to select *Soft Wrapping*, it reverted back to value *No Wrapping*. As a result, I need to scroll horizontally to view the diff, especially when I pick *Side-by-Side Diffs*.    The soft wrap is not working well in version 1.6.6 either. It works fine for certain files, but sometimes fail. """,Bug,User interface
51099,"""> Do we indicate if the backup has been anonymized? Or is that something separate?    It gets printed as a warning to the console when you create the archive:    """"Warning: this backup has been anonymised and should NOT be used as a backup. It does NOT contain your data.""""    But I agree that we should also add that to the archive meta data here.  In fact: I'll take a look to see if its possible/feasible to include the entire log/output in the archive (or otherwise at least all error/warning messages). """,Suggestion,Server
51119,"""Anyone who has re-indexed since 1.6 has noticed a very large increase in disk space requirements, as a consequence of FishEye's more comprehensive content searching capabilities.  Customers have been annoyed by the sudden, unexplained increase in disk space requirements, and the lack of guidance we provide on disk space dimensioning in general.  (See the parent issue)    I've been able to apply some tweaks that will reduce index sizes somewhat for all customers (~12%).  Customers will only see this benefit once they re-index after upgrading.    In addition, I've added a new per-repository setting """"enable-hit-highlighting"""", which is enabled by default.  Customers may choose to reduce their index sizes further (~30%) by disabling this feature.  The customer must re-index the relevant repositories to realise the disk space saving.  Once they have done so, they will no longer see contextual """"hit highlighting"""" in their search results.  This only affects the presentation of search results; it does not affect the set of results returned for any search query.    Unfortunately, it is impossible to provide customers with guidelines for accurately predicting their disk space requirements, as there are just too many variables:  Repository implementation, file sizes, content type, diff sizes, comment sizes, etc.  The best we can do is provide them with some guidance based on our own experience, and advise that their mileage may vary, and that they should monitor their FishEye instance's use of disk space.    I'm planning to make other indexing-related changes before 2.0.  Once they're done, I'll collect some indicative disk usage data for you to include as guidelines.""",Suggestion,Documentation
51736,"""Steps to reproduce     I'd like to have one e-mail account for 2 records but when I try to modify it as admin I've get the following error:  {noformat}  Email address must be unique, currently used by <ANOTHER ACCOUNT>  {noformat}    Incidental if you press cancel, then you get a NullPointerException (we should fix this).     However if the user itself goes and updates email address using Profile this error does not appear and the e-mail is successfully duplicated.     The customer (in FSH-1299) would like to allow duplicate email addresses and asks that the restriction on admins should be removed (esp since the users themselves can do it). """,Bug,Server
51853,"""In order to pick up retroactive changes to our P4 repository, sometimes we completely reindex our one of our p4 repositories.    When we do this, all RSS feeds (at least for those using Thunderbird as an RSS reader) """"reset"""" back to their original state, and we end up with entries in our readers for things that we've already seen.    The reason why we care about this is that because people go in and """"fix"""" changelist descriptions and JIRA linkage after initial submission, we'd like to start automatically reindexing our major repositories every day, but that would cause so much RSS chaos that we'd not do it.    I've had reports that those using Opera aren't affected by this, so it might be some Thunderbird/Firefox/RSS issue.  """,Suggestion,Indexing
52064,"""I would like to be able to force a directory as hidden that has already been indexed such that users cannot view them at all. The files should not be visible when searching either.    This is a solution to get around the fact that adding an exclude forces you to re-index, which in our case takes about 2-3 weeks to actually complete and therefore makes it worthless unless you know the specific location before someone checks it in (which is rare in my experience).        """,Suggestion,User interface
52140,"""Hello,    There is a bug in that when I am logged in to FishEye/Crucible as a user which is authenticated via Crowd it does not let me access the /admin/ pages. I can put in my password and see the menu, but as soon as I click on a link in the menu it asks me to enter the FishEye admin password again.    I get around this by logging out of the Crowd user. But ideally I would be able to authenticate against the admin console as a user in the fisheye-admin group *(in crowd or any other LDAP dir attached).    Thanks,  <USER>",Bug,Server
52160,"""Our software development is based on Subversion, FishEye and Crucible on Linux platform.  We are a 30 member team of developers.    When the project comes close to QA hand off (or to a major release),  the program managers need to """"throttle"""" the branch. This means:    1) a special """"release"""" branch is pulled off the mainline    2) the program manager identifies a set of """"must-fix"""" JIRA defects for the         release.    3) Subversion is configured with a pre-commit hook which makes sure that         only those code commits are permitted which identify a defect from the           must-fix defect list in the code commit comment.     4) As soon as a valid defect is identified, the commit is permitted as usual           and the defect is taken off the allowed list.     5) When the must fix defect list becomes empty, no more commits will be allowed         by the pre-commit hook.    Would FishEye provide such a capability? If FishEye is a """"read only"""" tool, could you  please point me to such a pre-commit hook for subversion which I can use? This  requirement is not unique to us and I assume some other team must have  invented this capability.    I would immensely appreciate any suggestions/pointers.    Regards,    Rk  (408) 596-1377""",Bug,Indexing
52171,"""It would be nice if there was a way for administrators to view and administer the watch lists.  I would like to be able to pull up who is watching what and also be able to delete certain watches...like right now watches appear to stay configured for users even after they are deleted (FE-280) so it would be useful to be able to go in and delete those watches.""",Suggestion,Server
52252,"""In the individual 'file history' view, logical view mode, a revision is shown when a branch is created that includes that file.  This is different than in the CVS mode, where branches are only included once a change is made on that branch.  FishEye should offer an option (with a globally settable default) to hide these 'branch creation' revisions, both from listing as a revision and from appearing on the 'History' graph for that file.  (I'd prefer not even having the option, and just always doing this... but I could see how some people might want it listed.)    Listing the branch creation revisions shows a ton of clutter on the individual file history pages and makes using them inconvenient.  (It also makes the history graphs completely unusable for projects that branch a lot.)""",Suggestion,User interface
52271,"""h2.Background:    We have a continuous integration build process that generates 20-30 build tags a day, sometimes more (when the developers are extra busy or the builds are failing). Performance got real bad. I've fixed it -- this isn't a support request, but I've learned some things I think you need to do something about.    We started off 5 months ago with a few hundred tags. This grew to thousands of tags. (Meanwhile the repository also grew, to probably around 15,000 files, with about 10,000 being tagged for builds).    Performance went from slightly disappointing to delays in minutes to bring up the root page, and not that much better for things deeper in the tree. I've understood the problem for a while but haven't had time to work on it.    Some of the tags, particularly tags brought over from CVS, didn't quite have the right logical path prefix, but I don't think that's the main cause here. In any event, fixing that wasn't much of an option, given the need for a full reindex for any changes, and the indexing was taking days back in July. Now, I suspect it would never complete, and only fall further and further behind as new tags are added. (It already sometimes ran a couple days late in delivering emails).    FishEye's data directory was also up to 1.5 GB.    h2.Actions:    So over Thanksgiving (a long 4-day holiday here in the US, counting the weekend) I've addressed the issue.    My first attempt was to delete tags for obsolete builds, upgrade from 1.3.1 to 1.3.8, and then carefully ensure that all the tags and branches had the right logical path. This meant a rather large number of regular expression entries to accommodate tags that tagged different portions of the repository.    Note also that sometimes the mismatch was due to only tagging a portion of the repository, while other times it was due to reorganization, and a logical path which might have been correct at one point may no longer point to the same file. I'm not quite sure how that's handled...    After letting that index for a day, I decided that wasn't an adequate solution. For one thing, it was still tagging all these obsolete builds as it found them in the history.  And performance was still quite poor (though better than it was before I started, but I suspect we'd have been back to where we were before if I'd let it finish indexing the repository).    So I decided more extreme measures were needed. I added 'trunk', 'branches', and 'tags/keep' to the includes. 'tags/keep' did not yet exist, so basically I disabled tagging entirely.    h2.Results:    Indexing took about 3 hours! And even while indexing, performance was downright snappy! The root page for the repository, always the slowest, was taking about 200 ms -- over a VPN routed through a virtual machine. Bet it's even snappier in the office!    Sometimes it would take more like 1000 ms for the line count/timeline graph to appear, when I turned those on (I'd had them off because they slowed things down too much before). But even that 1000 ms wasn't a problem, since the rest of the page appeared instantly!    And the Fisheye cached data requirements dropped by a factor of 5.    h2.Recommendations:    # The  performance increase is so startling, I believe there must be algorithmic improvements that could be made in handling large numbers of tags.  # I think there are a number of areas which deserve documentation  ## As I've noted elsewhere, better documentation of how include/exclude interact with the symbolic tagging entries.  ## Documenting just what FishEye does when the logical path doesn't match the current or contemporary path of an item.  ## I think you should recommend, that when you have large numbers of automated tags, you should segregate them into those that you want indexed, versus those that should be regarded as transient.  ##* This will also make the tags more useful as landmarks in the history.  ##* Including the Subversion revision number in the tag name also means you don't need to rely on FishEye for this (our build numbers are based on the Subversion revision, a practice I highly recommend!)  ## I think you should recommend, when dealing with tagging problems, that you pick a new directory for your tags, and use include/exclude to not include the old tags. This will speed up indexing and make it much quicker to iterate if you need to.  # I don't know what happens when a tag is deleted. I think it should be removed from FishEye as well, but it was too slow for me to observe whether that happens. But the behavior in this case *should be documented!*  # If tags are not removed when deleted from the repository, or if reindexing time becomes too long, I'd recommend renaming the tags directory, so old tags are not seen.  # I am integrating our BuildTracker application with Subversion to move the build tags to the tags/keep directory only when builds are tagged with a non-transitory status of KEEP, TEST, RELEASE, etc. I've also made it delete tags for builds that are deleted. This should keep the number of tags down around 50 or so. I suggest you recommend similar management practices for automatic tags, so Fisheye works on the tags that really add value.  # The display of huge numbers of tags on infrequently-changing files could be improved.    h2.Conclusions:    I think that throwing huge numbers of tags with large numbers of files at FishEye is an inherently bad idea, both from a performance perspective, and usability of the result (HUGE numbers of tags on files that infrequently change). Tags and branches in Subversion are a significant indexing event. We have to expect that at *some* point (but perhaps much later than we experienced it!),  FishEye will run into a limit. I think better guidance in the documentation would go a long way toward helping people use it effectively.    But by all means, try to improve the performance -- better to have fewer people run into the problems of suboptimal strategies!    In any event, I couldn't be more pleased with the new performance. But I'm sure we've lost users due to the past slowness, I'll have to campaign to get people to give FishEye another try.""",Bug,Indexing
52509,"""I would like to see User Defined Time Tracking Categories introduced into the Jira Time tracking feature and by extension visibility of this in Roadmaps / Advanced Roadmaps (Portfolio).    This way a ticket for a change request can track time spent towards the different types of activity:  R&D, BA, DEV, QA, UAT, DEPLOY, etc....    Right now as a workaround we have to create additional tickets by type of activity to get around the time tracking limitation.""",Suggestion,Roadmap
52571,"""Like that we can let others edit filters if we want to do that. I have normally 1 person I want to allow to edit filters I have created. The following would be nice to be able to have:    1)  Set in user profile an always set user for editing, like we have for sharing filters where we can set who can automatically see newly created filters.    2)  Mass edit the permissions for editing.    3)  When filter is copied the editing permissions is also copied.     For releases I create about 25 to 30 filters each time and right now I don't go and add someone to edit only because it is time consuming to edit permissions on every filter.  Normally I copy from another so the copy from one to the other would be nice but if you know you always want a certain person or group to be able to edit it would be nice to be able to have that as a default in your user settings.    Mass edit would be nice so if you want to put someone on to edit ones you created a while back but they may need to edit you could do that easily.""",Suggestion,Development Panel
52578,"""As a architecture role user I have visibility for long list of project. I would like to go to """"projects"""" view, click star-icon on header and get the list sorted so that my favourite one's are on top of the list.    Currently all other columns can be used for sorting except star and type. Currently type sorting not seen that useful.""",Suggestion,Roadmap
52685,"""As a recipient of a Jira issue notification email, I would like to reply to the email to add a comment to the Jira issue.  This should be have like typical service desk applications where ou can have an open Jira ticket and any of the recipients of the     So for example, I see someone comment on a ticket that I""""m a watcher on. I want to add some info to contribute to the discussion.  I should be able to reply to the email notification and have that comment added to the ticket.     """,Suggestion,next-gen
52736,"""The next-gen project type includes the following hierarchy of issue types:    - Epic  - Story/Bug/Task/ other custom issue types  - Sub-task    The Roadmaps feature shows 2 levels of this hierarchy: Epics and Story/Bug/Task/ other custom issue types.    As a Jira Software user, I want to be able to add other hierarchy levels, like Initiatives and Themes (above Epic) and display it in Roadmaps""",Suggestion,Roadmap
54295,"""Currently, the release reports in Jira Software (Version Report and Release Burndown) display the information based on the stories assigned to the releases.    As a Jira Software user, I want to be able to have reports showing the release data based on Epics' progress.  """,Suggestion,Classic - Reports
54312,"""As a Jira administrator, I want to have the option to assign a task to more than one developer.    This could be very useful when more than one developer is working on a bug fixing.""",Suggestion,Workflow Triggers
54330,"""{quote}As a Jira-board-using team member    I want the ability to order the issues on my board with a board sort order on the filter    So that I can see the most important delivery work without impacting the product owner's backlog rank order  {quote}       Acceptance criteria:   * Projects have a backlog, and zero-to-many boards (with reports): the backlog and boards should have their own sort order for issues   * The backlog sort order will be set using the new Backlog Settings feature   * The board sort order will be set using the Board Settings.  This order will also change the order of data supplied to the board's reports.   * For Scrum boards, the open and active sprint will be the basis of the board filter, and the order by expression sets the issue order on the board   * For Kanban boards, there is no longer a need for a board sub-filter as the backlog has its own filter now   * When swim lanes are used for a board, the board sort order is the final ordering clause after any ordering determined by the swimlane selection   * The default backlog sort order is Rank ASC   * The default board sort order is Rank ASC         Business case:    Ideally, teams should work on the highest priority issues and limit their work in progress to one item at a time (single piece flow). In the real world: expedite situations occur, teams have knowledge silos, and delivery decision-making is different from backlog refinement decision-making.  Having independent sort order criteria for backlogs and boards helps teams meet many of these needs, and frees the product owner/champion to organize and select work based upon their own decisions.     """,Suggestion,Classic - Reports
54334,"""{quote}As a Jira administrator    I want to set a sort order attribute for Status    So that teams can better manage their work items  {quote}       Acceptance criteria   * Status has an attribute of _status sort order_ (similar to status category) which determines the display and reporting order of status values   * _Status sort order_ impacts the drop-down lists of status values, such as on Issue View and in-line editing   * _Status sort order_ impacts reporting/filtering, allowing selection of the value as ASC or DESC, for better control of the output   * _Status sort order_ can be selected on dashboard gadgets as a sort value   * _Status sort order_ can be set using drag-n-drop on a list of the possible status values within the scope of the view: board, project, or instance   * Optional: _Status sort order_ attribute can also be set numerically: 1, 2, 3, etc.  When used, no two statuses can have the same Status sort order value.   * Status values can be created at a board and project level, so _status sort order_ should configurable at that scope as well   * _Status sort order_ at a project level *can be shadowed* by values set at higher scopes, such as instance.  This supports multi-project board and reporting functions.         Business justification:    Having the ability to sort items by status order enables easier configuration and better reporting. When editing an issue, people tend to think in the order of their team's workflow.  The current design appears to show status values in alphanumeric order, which may be very different from workflow order.  When reporting on items with filters, ordering by status order would lead to a list much easier to understand and take action upon.          """,Suggestion,"Gadget,Classic - Reports,Workflow Triggers"
54339,"""h3. Issue Summary    After added the time tracking field in the issue type story (used as a test) on the next-gen project, when I create a story, it does not show the option to configure the field. The filed is showing when I open the issue created but is not showing when I'm creating an issue.  h3. Steps to Reproduce   # Create a next-gen project.   # Go to *project settings >> issue type*.   # Add the *Time tracking* field to the issue type that you want.   # *Save* it.   # Create an issue using the issue type that you configured the *time tracking* field.    h3. Expected Results    When you are creating an issue, the time tracking field is showing as an option to configure.  h3. Actual Results    When you are creating an issue, the time tracking field is *not* showing as an option to configure. However, the field is showing when I view the issue created.  h3. Workaround    There is no workaround.""",Bug,next-gen
54355,"""{quote}As a Jira administrator    I want centralized logging of all changes performed by an administrator    So that I can self-service diagnose errors, repeat actions that worked, and provide education to new administrators  {quote}  AC:   * All actions within Jira by an administrator are logged   * Logged actions are centralized and viewable by scope.  For example, a system admin can view a complete log of changes by admins for the entire instance, from top level to project and board level.  A project admin can only see their scope of changes for their project.   * Major system actions by Atlassian are added to system logs, showing Atlassian as the role.  These would include, and not be limited to: release notifications, sunset of in-use features, changes to licensing limits for instance, changes by in-use marketplace vendor functions, and warnings for approaching limits for license (e.g. automation, storage, active users)   * Logged actions are searchable and filterable by: changing person, role, date/time, scope, project, and type (e.g. change role, add project, create workflow, delete board, etc.)   * Action logged can be exported/reported in order to create reference and training information for customer admins   * Action logs can be displayed within Confluence pages   * Action logging is immutable, and cannot be edited, with the exception of document retention policies (noted below)   * And relevant document retention/expiry windows for logs are clearly communicated by Atlassian to admins         Business case:     Jira administration, from an end-user perspective, is a black-box.  There is very limited visibility to what has happened or when changes were made.  This puts an unnecessary burden upon customers, preventing them from self-service correction and improvement, and an unnecessary burden upon Atlassian support to field requests.""",Suggestion,"Classic - Reports,Workflow Triggers,Documentation"
54357,"""Currently, if a user mistypes the estimation of an issue, there is no option to remove the entry from the history of the issue what makes the reports show a spike in the estimation.    As a product owner, I want to be able to fix estimates which break Burndown and other reports to make it appear properly.""",Suggestion,Classic - Reports
54368,"""Roadmap view is useful to see the association of child-issues (stories, tasks) to Epics, and to see the association of Epics with time periods (calendar months, weeks, or days).    However, in KANBAN process, the main unit of time for planning purpose is the SPRINT.    Also, Epics may span multiple sprints, and the current Roadmap does not visually show to which sprint (or sprints) a child issue has been assigned.    As a user, in the Roadmap view, I want to be able to select SPRINTS (in addition to month, week or day) as unit of time, and then to be able to assign the child issues (stories and tasks) to those sprints within the roadmap view.    Ideally, I'd like the association of child issues to sprints to be shown on the roadmap view in the form on rectangles, like a GANTT chart (as is currently already displayed for the Epics)    Ideally, I'll like those child-issue rectangles to have a progress bar, with colors corresponding to issue statuses (in the same way that progress bars are already shown for Epics).  That way, I could see at a glance how and Epic is progressing and can visually see when a child issue that was supposed to be completed in a single sprint ended up needing to be assigned to multiple sprints.    These changes would make the Roadmap view correspond to the way I plan for projects:   * First I figure out the Epics I need to pursue to accomplish the business objectives   * Second I figure out the stories and task required to achieve the Epic   * Third I figure out how Epics lay out over time, based on my customers' priorities and the logical sequence of development and dependencies   * Fourth, I figure out how the stories and tasks can be assigned to sprints (at least for the next few sprints)   * Then, after each sprint, I adjust the plan based on what I learned and what was actually completed during prior sprint.""",Suggestion,"next-gen,Roadmap"
54413,"""Based on the comments and feedback in the community portal and in the JSWCLOUD-17392 issue, there's a lot of frustration about people who have put their faith in Next Gen projects and have been consistently let down. Maybe if you had called it """"Project Streamline"""" or """"Jira Lite"""", you wouldn't be on the receiving end of all the animosity by your community.    As a first step, I'd highly recommend switching to Next Gen and start dogfooding the half-baked product your most devoted users have eagerly adopted.     aside: In who's mind is a single issue with 211 subtasks a reasonable method of running a project.""",Suggestion,next-gen
54441,"""As a team lead I need to have an overview over billable hours as well as registered hours""",Suggestion,next-gen
54472,"""Hi,    As a Jira user, I would like to assign multiple people to certain tasks based on business requirements.       Jira: Jira Next Gen.    Currently, this is a HIGH requirement for my company. """,Suggestion,next-gen
54496,"""I work at a company with a large service team and a small dev team, I would love to be able to communicate the road map to the rest of the organization without them needing Jira Accounts (we currently can't afford that extra burn).    *As a* Product Owner *In Order To* keep the organization abreast of what we are developing *I want*  a <USER>semipublic version of the roadmap that I can share out on slack, or put on wallboards around the office.    AC:    - A non-guessable link to a read-only version of the roadmap that would allow minimal browsing of issues. Just subject lines for epics/stories/subtasks.""",Suggestion,"next-gen,Roadmap"
54508,"""As a devops designer, I would like to use Jira's version release date to propagate to other release management systems the precise date and TIME a release should occur. Saying I would like a release on June 3rd isn't enough - I might not want it to start at midnight of June 3rd, I might want it to start at 8am EST. I will know this is done when I see a time value selector included in the date selector for fix version release dates. I do not need time zone selection - Jira already has that in its main System settings of course!""",Suggestion,"REST,Release Hub"
54512,"""As a PM, I would like to change the color of the labels of the issues in the next-gen backlog.    We are working with different customers and some features are profitable to different customers, so we want to easily identify different customers choosing one color per label.    Thank you in advance for considering.""",Suggestion,next-gen
54563,"""h3. Issue Summary    On the Jira project we allow users to add duplicate repositories as an Item. See screen shot attached as an example  h3. Steps to Reproduce   # Go to your Jira project   # Click on """"Add Item""""   # Repository > Add    # Enter the repository link and Connect it    You are able to repeat this process on the same project multiple times for the same repository and that repository item will be added to the project same number of times  h3. Expected Results    I would expect that once the repository was added, next time the user tries to add it we should say that this Item has already been added and not allow the user to add it again.     And removing that Item is not very intuitive,  the Icon looks like a Sync/Refresh instead of Settings or Options you have for it and hovering over it doesn't prompt you on what you expect to happen if you press on it.  I think this behavior can be improved by updating the Icon, Added a prompt on when you hover over it ...  h3. Actual Results    Able to add duplicate Items on the Project View and its not intuitive how to remove it  h3. Workaround    Workaround is to remove a duplicate once it was added by clicking on the Refresh/sync  Option > then Edit Repository > Delete""",Bug,next-gen
54591,"""As a user, I'd like to see the Sprint Burndown report that is available in Next-Gen projects in Classic Projects.    The report contains full information and allows ordering issues based on the column which Classic Reports don't allow.    It would be great having the same report for Classic Projects.""",Suggestion,Classic - Reports
54606,"""In Jira 8.3, Issue Author is automatically, and irreversibly a Watcher on an issue.    As a developer, I find issues in software that I need to report, but I don't necessarily want to know every time a change to that issue happens.  This is especially true when .      Often, I've fulfilled my responsibility by *reporting the issue*.  If $MGMT or $PRODUCT_OWNER doesn't think the issue is worthwhile, that's fine.         DoD:   * Creating a new Issue does not automatically make me a watcher.   * I can configure whether I want to be a watcher automatically for issues I create.       ----  I think the """"Component/s"""" field is wrong, but I couldn't figure out which one was pertinent.""",Suggestion,Workflow Triggers
54667,"""As an Atlassian Community member    I want the community board to clearly indicate posters who are employees of Marketplace vendors    So I am better informed about the answers and advice provided         Acceptance criteria   * Community members are required to self-identify when they are employed by a marketplace vendor in their profile   * An icon and label are displayed next to all postings by community members to indicate when they are employed by a marketplace vendor     """,Suggestion,Documentation
54694,"""As a Jira Cloud user    I want filter query language (JQL) to include missing, common SQL features    So my company has access to our valuable data without the need to buy marketplace add-ons for features missing from Jira.         Acceptance criteria:   * SQL features of JQL match the most recent ISO SQL standard, where possible and performant   * Regular releases update JQL as the ISO SQL standard evolves   * Features are part of native Jira Cloud, not provided at additional cost         Business case:    Jira Cloud can help teams with basic, work management.  For real-world production of enterprise products, teams need access to all of the valuable information which is available in the Jira issues and their logs.  While native reporting capabilities are limited, enhanced JQL would allow teams to create filters using current reporting to make their valuable data visible.  This enhancement would make Jira's products a more viable alternative to competitor work management tools.     """,Suggestion,Classic - Reports
54703,"""Currently, the Control Chart report dates and times are based on the timezone of the device used to access the report, while other parts of Jira (the issues' history, for example) use the timezone defined in Jira general settings.    If the device uses a different timezone than defined in Jira, it's hard to compare the report results with JQL searches.    Considering this, as a Jira Software user, I want the Control Chart to use the timezone configuration defined in Jira.""",Suggestion,Classic - Reports
54776,"""As a product owner and manager, i would like to have see the context menu for each cards in the Next gen project Board and customize the context menu as well.    As of now i am able to see only """"Add Flag"""" and """"Add Label"""" and no provision to include other options.    For example: I would like to have another option as Change Assignee where i can assign or change an assignee.""",Sub-task,next-gen
54780,"""As a user, I'd like to be able to decrease the story point estimate as it is possible with time estimation to report for what has been done inside a sprint based on the Story Estimation in cases that the issue was carried over to the next sprint.  With that, we can have more accurate data of what has been accomplished inside one sprint and what has been carried over to the new sprint for teams that don't use time tracking.""",Suggestion,Classic - Reports
54813,"""I never understood why dashboards can only be populated with a non-interactive wallboard.     As a PO/<USER> I want my virtual dashboard to be as rich and interactive as a physical board.""",Suggestion,Gadget
54825,"""As a user/admin, I would like to add issue collectors to Next-Gen projects just like classic projects.""",Suggestion,next-gen
54839,"""As a product owner, I would like to allow users that submit a request to up-vote existing requests, so that I can gain an understanding of what are the most popular feature requests.      """,Sub-task,next-gen
54855,"""As a PM, I want the ability to quickly pull up the % complete of an epic via JQL so that I can populate many of my reports/dashboards automatically, without the need to calculate this one value manually.           Ideally % complete would be the number of (incomplete issues / total number of groomed issues)""",Suggestion,Classic - Reports
54920,"""As a user, I'd like to be able to select custom fields of type Date Picker in the Two Dimensional Filter Statistics gadget, X and Y-axis.    """,Suggestion,Gadget
54942,"""As a Jira Software user, I would like to have the option to bulk edit issues while selecting multiple issues in a next-gen board backlog.    """,Sub-task,next-gen
55030,"""As a Jira Software user, I want to be able to manually link to pull requests or commits for the development panel on a given issue.    This would allow users to retroactively relate older or malformed commits and pull requests to an issue without having to rebase or force-push changes to the codebase. Minor, and easily avoidable by enforcing the format of the commit message and pull request title.""",Suggestion,Development Panel
55031,"""As a user, I'd like to be able to select when moving an issue to a Next-Gen project if it will be displayed at the board or at the backlog.    Currently, any moved issue will be automatically placed at the backlog, even that the status is not the first status of the workflow.""",Sub-task,next-gen
55043,"""As a project manager, I want Jira to prompt my team to input worklog time when they transition an issue to certain states so that my team does a better job time tracking and I can keep my project on budget.  h2. Acceptance Criteria   * I can make a rule that prompts the user for standard Jira worklog input (time/duration work log)   * The rule can be applied to 0 or All state changes   ** e.g. A dev moves an issue from in-progress to Awaiting-QA   ** e.g. A QA moves the same issue from In-Test to Verified     """,Sub-task,next-gen
55058,"""As a user, I'd like to have the ability to export a customer friendly roadmap on Next-Gen projects such that customers are abstracted from task size, start date and that the end date is only shown with Quarter granularity.    The current roadmap is perfect for an internal roadmap, but provides too much data for customers.    Proposal:       ||Available||Q2||Q3||Q4||Q1 2020||  |Feature 1  Feature 2     |Feature 3  Feature 4     |Feature 5  Feature 6  Feature 7|Feature 8  Feature 9     |Feature 10  Feature 11  Feature 12|    The available column would show if the Status filter includes """"Done"""" features.     """,Sub-task,"next-gen,Roadmap"
55065,"""As a Jira project administrator, I want to be able to map new issues to existing epics when using bulk import, so that I can save time and effort when working with a large number of issues at once.""",Suggestion,next-gen
55086,"""As a Jira administrator, I would like to have a Sprint Report where I could see a global view of all existing projects and sprints in Jira.    This report should include filters for:  # Project.  # Assignee.  # Planned, Active and Completed sprints.    This report should show:  # The sprint start date, end date, completion date.  # The issue key, the issue status, the issue assignee.      """,Suggestion,Classic - Reports
55148,"""h3. Problem Definition  As a user, I want to be able to share a project's roadmap/project without consuming licenses.    h3. Suggested Solution  A user should be able to share a link to somebody else that provides view access to the project without having to explicitly consume a license of its intance    Also, allow users to embed the Roadmap view in external pages/websites    h3. Workaround  Customers can already export a roadmap to an image and thus share a non-interactive version of the roadmap with stakeholders. The image export is available in the roadmap's _Share_ button, see image below.   !exportRoadmapAsImage.png|thumbnail! """,Sub-task,Roadmap
55155,"""{panel:title=Atlassian Status as of 25 August 2020|borderStyle=solid|borderColor=#3C78B5| titleBGColor=#3C78B5| bgColor=#E7F4FA}    Unfortunately, in light of more urgent priorities, our team is not working on this feature anymore. We know this will disappoint many of you and is not the answer many of you were looking for. We’ll let you know when we are ready to begin work on it.    {panel}    h3. Problem Definition    Currently, it's not possible to define a default issue type in next-gen projects. The selected issue type will be the last issue type used by the user. Considering different users may use different issues types, there will not be a """"default issue type"""" defined to all users    h3. Suggested Solution    As a next-gen user, I want to be able to define a default issue type in the project's settings to make sure all users will have the same issue type selected.""",Sub-task,next-gen
55157,"""As the next-gen projects are described as a """"buisness solution"""", I would love to have a feature that already exists in Asana system.  In short:         email notification regarding my observed tasks, would have field buttons, that are redirecting to the edit view for that specific field.  example attached:         !image-2019-04-11-13-56-01-894.png!     """,Suggestion,next-gen
55179,"""As a user: When *creating* an issue, I want to have the ability to add the Epic link (from the  dropdown). See screenshot attached.    As an Admin - In project settings/Issue types:  I want to be able to add the Epic Link to an issue type (e.g. story, Task, Bug).         I searched but didn‘t find a similar task yet. If it‘s a duplicate, feel free to close.      """,Suggestion,next-gen
55223,"""As a user, I want to be able to be able to see past sprints information in the Sprint field like it was in Classic Projects.    Example:  *Next-Gen:*  !Image 2019-03-11 at 10.16.10 AM.png|thumbnail!   *Classic*   !Image 2019-03-11 at 10.16.51 AM.png|thumbnail! """,Sub-task,next-gen
55235,"""As a Product Manager    I want to add Milestones to my Roadmap    So that I can visually communicate significant dates on my roadmap to stakeholders         *Acceptance Criteria*   # User can create milestone(s) against a roadmap.   # User can give the milestone a name and description(and specify a date)   # Milestones should appear as lines on the chart (similar to the way current date works).   # The user can drag the milestone to different dates.   # The user should see the name of a milestone is on the roadmap view.   # User can add and remove milestones from the view using the filters above the main chart.   # A user can select a milestone to edit or delete it.""",Sub-task,"next-gen,Roadmap"
55236,"""As a user, I'd like to be able to delete connected pages from my projects. Right now, if a page is connected to a Next-Gen project, it's not possible to simply remove the page. Instead, the only way to remove the page is to replace it with another one.""",Sub-task,next-gen
55290,"""In Classic projects, when you resize your browser window the columns auto-adjusts their width, however, in Next-Gen projects, it does not happen.    As a user, I'd like to resize my browser window and have the Next-Gen board columns width adjusted as it adjusts on Classic Projects.""",Sub-task,next-gen
55297,"""As a Jira Software user, I want to be able to configure swimlanes on next-gen boards, as it is possible to configure in Jira Software classic projects.    This feature helps to distinguish tasks of different categories, such as workstreams, users, application areas, etc.""",Sub-task,next-gen
55321,"""_*As a*_ Jira Cloud administrator     _*I want*_ the ability to query Jira to show tickets whose sub-tasks do not have time logged against them    _*So that*_ I can easily identify all tickets which are completed, but do not have any time logged against them and notify the appropriate teams and/or users who need to go back and log their time         My company has recently begun mandatory time tracking for ALL new development work we do. This means all of our Jira tickets which are completed (not closed as illegitimate, etc.) should have time logged against them. If I want to verify that our developers are logging time appropriately, I can easily do an advanced search for tickets where """"timespent is EMPTY"""". However, this query still shows stories which have sub-tasks where there is time logged against the sub-tasks but not logged against the story itself.           Maybe I am mistaken, but I have not found a way to quickly identify stories which have no time logged against them AND have no time logged against their sub-tasks. I hope this is understandable. Please let me know if I can provide anymore info/context on this.""",Suggestion,Classic - Reports
55359,"""At this moment, integrating Jira with Bitbucket enables a _Development panel_ in the Jira issue view screen. This panel contains information about pull requests, branches, commits etc.    As a developer, I would like to see some information about the pipelines in that development panel of a Jira issue""",Suggestion,Development Panel
55369,"""As an admin, I've created custom fields which I expected to be able to report off of.   By using the basic search, users are unable to search for issues using a custom field.    As a workaround, users must change to advanced search and use the JQL to search for those issues.    It would be great if we could use the basic search for that.""",Sub-task,next-gen
55380,"""As a user working with a Next-gen project I would like the ability to add existing custom fields to the a project.          My company uses Tempo for our time tracking and we have invested some time in integrating tempo accounts with our CRM to keep an up to date list of possible client's to track time against. We would like to use the new next-gen projects but we are unable to add existing custom fields so although the tempo plug in works to log work we can't associate that work with a specific client.         This would also be useful for other fields which are common to many of our development teams. For instance we have dropdowns for other business units that need to review our work that should be consistent across projects. Maintaining these separately in the current next-gen projects is onerous.""",Sub-task,next-gen
55434,"""I would like the ability to copy a Next-Gen project in the same way as a Classic project.""",Suggestion,next-gen
55440,"""As a user, I'd like to have a global release report.    Would be great to have an easy way to see all the releases that are going out at a macro level. I would like to see """"here is what is being released by us today"""" from all projects and it is not possible to do that without going into each and every project.""",Suggestion,Classic - Reports
55504,"""As a user, I'd like to have the ability to export roadmap on Next-Gen projects to PDF, as an image, or allow to display Roadmap information on different apps  """,Sub-task,Roadmap
55526,"""As a user, I'd like to have the ability to create an issue inside the sprint at the Next-Gen project.  Currently, when creating a new issue through + sign on blue left toolbar there is no option to add the issue to a sprint, it will automatically move to the backlog and then I need to drag and drop the card inside the sprint.    It would be great to have the feature to assign the issue into an existing sprint for the Next-Gen project.""",Sub-task,next-gen
55527,"""I've been looking through the forums and have found similar questions, but not quite the exact thing I'm looking for. So I apologize in advance if this question has already been answered and I missed the post (I hate creating duplicates).    I've been playing around with the built-in reports in JIRA, in particular the Workload Pie Chart report. I wanted to see if I could get a report of how many hours an individual in a project has logged across all the tasks he has logged work in. What I found was disappointing.    I made a single test ticket in a brand new fix version (so it was the only ticket available in my query) and assigned it to myself (person A), and logged 5 hours on the task. I then assigned the ticket to a co-worker (person B), and he logged 10 hours on the task. When I ran the workload report by assignee and on time spent, the only person that appeared in the report was person B with a total log time of 15 hours (his hours plus my hours). What I was hoping I would get was person A having 5 hours and person B having 10 hours.    The problem is, person B was the last assignee for the task. And the task currently had 15 hours logged on it. So, the report says that person B has 15 hours logged. The report does not go into the task and extract the hours per individual -- just the total hours on the task and the last person assigned.    So, I think you know my question. How can I generate a time logged report per person regardless of if the task they were involved in was also worked on by other individuals?  """,Suggestion,Classic - Reports
55552,"""Currently, it is only possible to rename a next-gen board when there is no active sprint visible on it.      !Screen Recording 2018-11-09 at 07.44 PM.gif!     As a Jira software user, I want to be able to see and rename a next-gen board when a sprint is active on it.    h4. Workaround    The board can be renamed when there is no active sprint on the board""",Sub-task,next-gen
55563,"""As a Jira Software user, I want to be able to reorder the issue's children on the issue's view on next-gen project type.    Currently, it is only possible to move children issues (subtasks or stories) of a given issue in the classic projects.    It would be very helpful to be able to do the same on both classic and next-gen project types.  """,Sub-task,next-gen
55858,"""I need an easy way to see if my parent issues are in progress or not at a glance.      Many of us use sub tasks (or sub bugs) to break down tasks on a parent story or bug.   If one of the sub tasks (or sub bugs) is in progress I want greenhopper to change the status of the parent item to in progress.  It makes no sense for a sub-issue to be in progress (or resolved) and show the parent item as just """"open"""".  If work is being done on one of the sub-issues, of course the parent item is in progress.  As a nice to have,  if all of the sub-issues are closed, then the parent should at least be """"resolved"""".    Right now there seems to be no way to even indicate a story or bug with sub-issues is in progress.  The only way to do it is look at their sub-issues which is laborious.""",Suggestion,Workflow Triggers
55946,"""h3. Summary    When using the burndown chart, the line can be obscured by the legend. As a scrum master, I would like the burndown legend to be moved off the chart so I can see the burndown (or should I just say """"burn"""" - not so much """"down"""") line no matter how poorly the team is doing.    Example:   !Screen Shot 2017-04-04 at 9.44.50 AM.png|thumbnail!     See attached a screen shot of three charts laid out side-by-side on a three-column dashboard. You can see how the legend is covering the red burndown line on two of the three charts.    h3. Steps to Reproduce    # Start a sprint with some issues assigned  # Log work/change scope in the sprint such that the line goes to the top right corner of the chart    h3. Expected Results    The line should not be obscured by the legend.    h3. Actual Results    The line is obscured as per the summary.    h3. Suggestions   The legend could be an on-off toggle since most people who are looking know what they are looking at.   The legend could display in that unused white space below.   Legend could be a """"tool tip"""" (not good for wallboards)    * Would love to see the left column on the main dashboard page (that copies the same list of dashboards in the pulldown) go away, to give more horizontal space on the dashboard page, and this could have an effect on whether the line is covered.  * Would love the option to hide non-working days (like there used to be, and is on the board page).  * Often used the old option to hide the guideline as well. (Less confusing for the higher-ups.)""",Bug,"Gadget,Classic - Reports"
56097,"""As a Product manager running multilpe teams with a single release platform I want to be able to create releases and assign to multiple project boards so I can manage a release, you can do this in the server based version but I can't find out how to do it in the cloud based version.""",Suggestion,Release Hub
56275,"""As a Jira Software user, I want to be able to choose which column in a SCRUM board will """"burn"""" data in the Burndown Chart and Burnup chart report""",Suggestion,Classic - Reports
56304,"""{quote}{color:red}*Work In Progress*{color}{quote}    h1. User Story:  X, Y, Z.  -----    h2. Current Implementation Issues:  x, y, z.  -----    h2. Acceptance Criteria:  x, y, z.  -----    As it stands now, the Velocity Chart shows all active Sprints, which typically represent pods. As of now, I only have 2 simultaneous Sprint pods and that means each time I look at the Velocity Chart, I'm only getting 1/2 of its full value. If I were to add a third pod, it would be basically useless without external data tracking. As it stands, I already do this as a precautionary effort.""",Suggestion,Classic - Reports
56305,"""{quote}{color:#FF0000}*Work In Progress*{color}  {quote}  h1. User Story:    As a <USER> I want to be able to clearly see the difference in initial commitment and the total commitment at the end of a Sprint, so that I can visualize scope creep.       ----  h2. Current Implementation Issues:   # I can only see a Sprint's commitment value after the Sprint has been Completed.   # If I want to see scope creep visually it is not possible through any reports to see it as a separate chunk.   ## Think of Stacked Bar Graphs as an example.    ----  h2. Acceptance Criteria:        # Each bar should be split into two shades of their respective color representing values:   # Commitment at Sprint Start and Sprint Close + Completed within the Sprint   In the numerical table under the chart, each column could simply have 2 values, one as is and the second in parentheses directly behind it:   ** Commitment at Sprint Start + total Commitment at Sprint End   ** Completed within the Sprints time being open + Completed without the Sprints time being open   *** The later would include: Completed while having the Sprint value of X but not having any other value in openSprints() as well if it were completed while being in closedSprints() and having Sprint value of X while X is in futureSprints().""",Suggestion,Classic - Reports
56830,"""As a developer, I want to write a program that has access to the connections from a JIRA issue to the related branches, pull requests, commits, and builds so that I can automate more aspects of the deployment pipeline.""",Suggestion,REST
57227,"""Follow up from JRA-38604:  With the setup of Crucible enabled not Fisheye, Crucible users can not see commits in JIRA. They are not aware from JIRA that there is some code changes which may need review. They have to check it manually in Crucible - they are still shown there. In the other words - you screwed Crucible users.   I hope that this clarifies the situation - there never was any """"free functionality"""". When the user tried to reach Fisheye functionality (without licence), then there was an error. I wanted to remove the error. You might replace error with something like """"For this feature buy Fisheye"""" or you may disable hyperlink from JIRA to Crucible. Instead you removed the functionality at all!""",Suggestion,Development Panel
57244,"""*_As a_* person preparing release on a multi-repository project looking at a ticket's Pull Request list  *_I want_* be able to easily tell PRs that implement the fix (branch name or PR title contains the ticket) from other pull requests that merely mention the ticket  *_So that_* I don't accidentally merge a PR that implements a different ticket  *_As a_* developer collaborating on code in a multi-repository project  *_I want_* be able to mention any ticket in my [PR description, commit message, comment] without worrying about causing mistakes during release  *_So that_* I can share all the important information about tickets related to my change  In our team the project spans a great many repositories. Because of that there is almost always 4~5 Pull Requests that a ticket introduces. So, a ticket's PR dialog is used as the authoritative information to tell which pull requests should be merged to release a the ticket's feature. Moreover, merges are done by the QA team which is not involved in details of the development.  We could easily prevent a mistake of merging a wrong PR if the ones that are associated based on title texts (PR title/source branch) were made much more prominent than those associated on mentions in 'prose texts' (PR description, commit message, code review discussions, if ever implemented).  Some implementation options: * Have a separate main list for when ticket occurs in branch name or PR title, and a separate 'Mentioned in' list with all the rest    (This would be the preferred solution for teams like ours, because of the importance we attach to the main group.) * Add pictogram flags next to a PR that show the types of associations which were found (ideally pictograms for the branch name & PR title should be much more prominent than the others)""",Suggestion,Development Panel
58071,"""h3. Problem As a user I would like to create dashboard widget for showing upcoming releases (like JIRA Road Map widget) but with style like on project's Releases tab.  At this moment we have the Built-in Road Map widget but it is not as readable as the Releases tab.""",Suggestion,Release Hub
58255,"""I know you ask that I search before adding an issue. However, search is very liberal and not easily filtered and so my issues are rarely addressed by search returns. I am giving written feedback as a last ditch effort to get some help, and even finding this took upwards of 5 minutes (way too long for a feedback box).  My main issue is that documentation explains features from a development perspective, which doesn't serve me as I am a business user and would like features discussed more broadly so that I may relate them to my work.  Documentation is either very sparse or not tagged well because, like I said, searching hardly every addresses my needs. I am rarely able to use documentation to solve my problems, and the learning curve on this product is steep so I've spent several months just fumbling around trying to """"figure it out."""" This product is so full of features and functional that it (and those who created it) deserve better documentation to help customers actually use and work with it. As things are, I use it sparingly because I never really know where to find things on a board designed for developers when I am in a very different business field. The features are also named and labeled using industry terms like """"scrum,"""" which means nothing to me and requires research before I can know to look for something else, which is often also not labeled in a straight-forward manner.  At the moment, I am trying to learn the differences between various project types (for about the 5th time; thought I understood, but somehow I am constantly finding new info that wasn't available before...) as well as how to delete or back up data, none of which simple searches have been able to explain.  I may not be developer-style computer savvy but I am no Luddite either, and this should be way easier. One good start would be to include a text box in the page-specific feedback area (instead of the multiple choice of three vague options). You'll get better page-specific feedback that may allow your teams to address needs better than via a separate task or suggestion that is now removed from its context... Another good place to start would be to consider that development language does not always translate across industries, and when you claim to address regular business needs, your help language should reflect that.  I'm sorry if this sounds harsh. I like the product overall; I'm just very frustrated that help is so hard to come by. I can't even call anyone...""",Suggestion,Documentation
58347,"""As an engineering executive, I'd like an Epic Burndown Gadget so we can track progress across many epics via Dashboards or Confluence Report Pages.  *_Additional Comments:_* _Atlassian has really under invested in Gadgets and Dashboards.  For example, I've been waiting literally years for you to add the ability to have Story Points as the """"value"""" field for the two dimensional filter.  Similarly, when you redesigned Jira and added the Reports section, all reports should have been made available as Gadgets via Dashboards.  For any meaningful implementation, its inefficient to have to go to each project/board to view reports.  Dashboards/Gadgets are a very powerful capability.  I'd think improving this area would be low effort/high reward._""",Suggestion,Gadget
58372,"""In the Project Releases page there are two quick filters, Released and Unreleased.  !172266.png|thumbnail!  As a user I would like to add new quick filters to this page and/or change the quick filters query.  """,Suggestion,Release Hub
59335,"""As a scrum master, I would like the burndown legend to be moved off the chart so I can see the burndown (or should I just say """"burn"""" - not so much """"down"""") line no matter how poorly the team is doing.  I've attached a screen shot of three charts laid out side-by-side on a three-column dashboard. You can see how the legend is covering the red burndown line on two of the three charts.  Ideas:  The legend could be an on-off toggle, since most people who are looking know what they are looking at.  The legend could display in that unused white space below.  Legend could be a """"tool tip"""" (not good for wallboards)  * I would love to see the left column on the main dashboard page (that copies the same list of dashboards in the pulldown) go away, to give more horizontal space on the dashboard page, and this could have an effect on  whether the line is covered. * Would love the option to hide non-working days (like there used to be, and is on the boards page). * I always used the old option to hide the guideline as well. (Less confusing for the higher-ups.)""",Suggestion,Gadget
59422,"""As a team, we work on a mix of stories and bugs.    Some individual bugs can take a long time to resolve (multiple sprints).  Is it possible to determine the amount of time logged to a ticket *in a certain time period * (typically, the last sprint).  Current reports show total amount of time spent (e.g. Workload pie chart gadget), but I'd like to be able to also see e.g. """"time spent on bugs in the last sprint""""  Possible? """,Suggestion,Gadget
59613,"""As a <USER> I would like to be able to define what statuses are used to determine the 3 blocks on the Sprint Health Gadget using custom JQL.  Currently the gadget uses the first column for blue, last for green and all others for yellow.  I would like to be able to select each block, and use JQL to choose which of the status labels gets grouped in each of the blocks. See diagram for how I would like to group my labels. """,Suggestion,Gadget
60541,"""As a product owner, I want to use the epic gadgets in a confluence page summary or <USER>dashboard summary to communicate the epic's overall status.   This helps to fulfill a need of showing overall status of a product roadmap and portfolio. """,Suggestion,Gadget
64153,"""h3. Issue Summary    As a Jira administrator, I would like to set-up read only fields based on project permissions. As of now, to achieve this functionality, administrators have to rely on workflow customization or third-party apps to manage these checks.    Allow for fields to be edited in any of the screens based on project permissions or groups, also allowing to make them read only based on the same criteria.     """,Suggestion,Issue
64208,"""h2. Use case  As a project admin, I'd like to be able to maintain boards that belong to my project, i.e. which contain issues from my project.    h2. Actual result  As a project admin I'm able to view boards in my project, but I'm unable to enter the board configuration.   (!) If the board admin e.g. leaves the project, only a <USER>is able to maintain the board (e.g. add new board admins).    h2. Expected result  A project admin should be a board admin for any board shown within his / her project.    Feature Request based on PS-63150""",Suggestion,Board configuration
64218,"""As a Jira Software user, when creating an issue I would like to see related articles/issues as Jira <USER>shows.    This would avoid duplicate issue creation to report or request the same.""",Suggestion,Issue
64289,"""As an engineer, I have a Python script to archive unused projects in bulk using the Jira REST API. I would like the API to delete projects that have been archived. Trying to do this results in a 404 error, stating """"You cannot view this project."""" I am using an admin account that has global permissions and can manually archive projects via the Jira UI. This process would take too long with over 3000 projects that need to be deleted. """,Suggestion,"Data Center,REST"
64293,"""As a team that wants to limit our work in progress (WIP), and having broken down our """"in progress"""" column into several columns, I want to limit not just the number of items in any one of those columns but also the number of items in all of them put together.""",Suggestion,AgileBoard
64294,"""As a team member looking to improve my teams workflow and process with an eye to the theory of constraints, I would like to see at the top of every column on our board: how much time does an item usually spend in that column?""",Suggestion,AgileBoard
64299,"""As a blind-less user, I'd like to have a dark mode.    I'm currently use a browser plugin to invert color (High Contrast in Chrome), but the auto refresh of the board switch to his original colors which are too much lighten for me. I always have to hit refresh on my browser to have the inverted color back again.    I'd like to have a dark mode theme in my preference, so I can works without hurting my eyes.""",Suggestion,AgileBoard
64346,"""As a non-admin Jira user, I want to go to my profile and view project roles and get a great matrix that shows all the projects and all the roles and what role I have in each project    So that I can get a view of my project permissions and roles across all of my projects without having to go to my admin  h3. Workaround    The user can see what projects they have at least browse access to.    The user can ask the admin what they have access to.""",Suggestion,Data Center
64352,"""As a functional application manager I raise issues on behalf of my company. Sometimes the issue affects a single user, sometimes a small group of users, but sometimes an issue affects the entire user population (approximately 4000 users (and increasing)).    When I raise a bug or request I’m very often beeing pointed out to a (other) feature request and I’m recommanded to vote on that feature request. But when I vote it looks like I am the only user who wants the new feature / looks like I am the only user who is affected by the bug instead of me being the company representative.    As an functional application manager, and being the company representative, I would like to give an indication of the number of affected users when I vote on an issue.""",Suggestion,Issue
64365,"""As a User    I want to be able to groupe my issues in my board by more than one criteria    In order to be able to see a board with cascading Swimlanes          Exemples :    Swimlane 1 : Assignee          Swimlane 2 : Stories (to see the subtasks of the story.         Visual Exemple :         Robert Chase (8 Stories)           Story 1 : (1 Subtasks)                      ------------------                      |   Subtask 1       |                      ------------------          """,Suggestion,Board configuration
64399,"""I am having a hard time multi-selecting projects from the applicable context of the custom fields.     *Where?*  * Administration --> Issues --> Custom fields, choose a custom field, click on """"Configure""""   * From Applicable contexts for scheme click on """"Edit Configuration""""   * Under """"Choose applicable context"""" click on """"Apply to issues under selected projects""""  * When selecting projects, you will have to click and hold CTRL+click to select a project/s.    *Problems*  * The issue here is you run into a big problem when you dont remember to press Ctrl next time you add a project: All other project selections will be deleted.   * As well as this field has a huge problem handling of large numbers of projects, you don't remember what you selected then when checking you can unselect projects OR all project selections will be deleted.      *Suggestions*   I would suggest the following:    # Adding box to """"tick"""" select the project/s  # Adding scrolling bar into the field for easier navigations when having a large number of projects.  # Adding a search bar to help look for projects using their names to improve the handling of large numbers of projects.  # Adding another field that shows the selected/active projects list for an easier view of what projects you selected. """,Suggestion,Issue
64411,"""This is a suggestion for Automation rules for <USER> Sadly, there is no way to open a suggestion for that project since it does not have any components... :(    As an admin, looking at all the automation rules, I would like to be able to look at the audit logs and search for errors on it, or search for specific keywords.    Currently the only functionality is to go page by page and look at the list.    Alternatively, I would like to export the audit log (by dates) to excel.""",Suggestion,Workflow Triggers
64429,"""I've made this request on numerous occassions in the past, but I am sick and tired of wasting time, reviewing with addon developers to determine the addon file-names if they hyjinx my system.    To provide rapid servicing of my server, I could instead keep an active list ( I can maintain ) of addon files which can be removed from ${JIRA_HOME}/plugins directory to rapidly resolve issues if during an upgrade, or any other issue, the addon makes the server inoprable, as is the case with a number of authentication plugins.    Instead, I presently have to request support from the addon developers,  to identify their filenames.    FOR THE RECORD; While UPM is related to this request, and is the most cognizant actionable item, it is not immediately relateable, as any number of run-time scripts could be responsible for this, as it isn't presently provided.""",Suggestion,UPM (Universal Plugin Manager)
64505,"""Similar to the way a story can add sub-tasks from within the work item, I would like work items (issues to you, Atlassian) - (BTW, you might want to change the fact that your work items (PBIs) are called issues and issuetypes. Not all work items are issues like a defect. If you are going to become an ALM tool, you'll need to eventually get rid of the fact that Jira started off as a defect tracking tool.), to be able to have Defects (Bugs) created against a work item (issue) so that I don't have to link all the fields in order to track defects against a release, sprint, or product increment. I'm guessing you can automate this, and the question I have is, why would you make it so that one has to? It should be automatically established that if I need to enter a defect against a work item (story, feature, etc.), I can do so with efficiency and ease of use. Make it happen, Atlassian. Thanks.""",Suggestion,Issue
64514,"""h3. Description:   ||As A|<USER>using time base story pointing|  ||I Want|To be able to add time estimates to sub-tasks|  ||So That| - I can more accurately reflect the amount of time each sub-task has remaining   - I can estimate sub-tasks based on the assignee   - I don't have to work with parent level tasks when estimating stories|  ||I'll Be Happy When| - I can provide a time estimate on a sub-task issue type   - The parent issue type displays the combined sum of all sub-task estimates + parent issue estimate as a Total value   - I can report on time estimates down to the sub-task level if required|  h3. Example:    I have a story which is to make a decision. This issue has a sub-task for each team member to investigate, discuss and review final documentation. Each team member wishes to estimate their own time value. As a scrum master I don't want to have to manually calculate this and keep a record of this for later review.""",Suggestion,"Board configuration,Backlog"
64516,"""If you decide to display the labels associated with a card in the backlog of a project by:    {panel}  # Navigate to the project's backlog. Click on *Board* at the top right corner of the screen.  # Click on *Configure*.  # Select *Card layout* at the sidebar.  #  In the *Backlog* section, insert *Labels* into the *Field Name* box and click on *Add*.  {panel}    After that, the cards will double in size and as a Jira admin, I would like to avoid that.    h3. Backlog's appearance before applying changes.    !Screen Shot 2019-11-06 at 15.16.20.png|thumbnail!     h3. Backlog's appearance after applying changes.    !Screen Shot 2019-11-06 at 15.15.03.png|thumbnail!     """,Suggestion,Backlog
64523,"""As an Administrator, I would like to be able to set the IDs being used by Jira.         Allowing configuration to align with our needs.""",Suggestion,REST
64560,"""Please consider a different design for the days in column dots.  The fading makes it difficult to read for those who do not have additional accessibility support, I'd guess it's unusable for this who do.  Please find a way to just display this as a number, even if we have to opt into it as an additional field to display.         Also, do you perform accessibility reviews of this product and act on issues?""",Suggestion,Board configuration
64573,"""Currently, Jira doesn't have the ability to set the background of an issue to match that of the category. As an admin, I would like to have the ability to set the background color to match that of the current category selected for the issue.""",Suggestion,Issue
64577,"""As a Jira admin, I'd like to be able to prevent the users from sending out notifications for all the users when adding the <USER>users to a notification scheme.""",Suggestion,Issue
64581,"""*Problem:*  As a user, I would like the activity tab to describe the issue description clearly so that I don't confuse it with a comment when clicking on the read-more link.    *Steps to Reproduce:*  1. Create a task.  2. Add the description (10-20) lines  3. Add a comment (10-20) lines  4. Click at the """"Activity"""" tab  We will see:  a. Part of the comment added on 3. inside a blue box with the link << read more >>  b. The description of the new task created on 2. with the link << read more >>    5. Click at the <<read more>> on a: The tool will change the focus to the comment tab and show the full comment added on 3.     6. Click at the <<read more>> on b.    *Expected behavior (by design of the product):*  It will go to the top of the task, showing the description of the task, where you can read all the lines of the description.    *Customer expectation:*  The Customer was expecting to see the same behavior observed on 5. However, as it was a description, the tool just refreshed the page, positioning at the top.    This confusion could be avoided if the description was clearly identified, for instance, starting the window with a label like:  """"Description:""""  <text of the description> <read-more link>  """,Suggestion,Issue
64625,"""As a user    after I perform a bulk operation    I want to click a button that says """"OK"""" or no button at all    so I can quickly continue on with my work    ---    The """"acknowledge"""" button in Jira is confusing, and could be worded more simply to """"OK"""" or to not having a button at all.""",Suggestion,Backlog
64644,"""As a Jira user who works with both tools Jira and Confluence    I want to see content (parts) from any Confluence page in the Jira ticket description section    so that I don't need to follow the added Confluence link in the ticket to access the Confluence page.    (The scenario of following a Confluence page link in Jira and then looking for the right content section in the Confluence page is not very convenient...)""",Suggestion,Documentation
64696,"""As a administrator I would like to be able to configure this.""",Suggestion,Data Center
64710,"""As an admin I would like to have the ability to configure this for code blocks to disable it.         Currently if you have a code block like the following Slack will pull the data from Jira.       {code:java}  'This would be a slack code block for Ticket ABC-123'   {code}       Currently it will then display the Issue detail from Jira. If I was able to disable the slack code block unfurling it would be very useful.""",Suggestion,Issue
64722,"""Currently if a properties file has a password Jira doesn't encrypt it. As an admin I would like to have the option to encrypt the entries within the properties.    Currently there is no know workaround for this.""",Suggestion,Documentation
64732,"""Currently in Jira if you add a new user in the UI you have the option to send the user a notification. If the user is added by a directory sync the new user will not get a notification.    As an Admin I would like to have the option to enable by default the option to send a notification to the newly added user.""",Suggestion,Issue
64771,"""As a product manager I to be able to hide the Everything Else Swimlane on board.    Background: I've built a board to view a subset of the work in flight for our group so our Director is able to report to the GM on specific, but evolving buckets of work.  The board only displays Epics, and in our setup, Epics are children of Requirements.      I designed 3 swimlanes, one per concept, that contain 1:many requirement links as the determining factor if an Epic appears in the swimlane.     The problem is the Everything Else swimlane.  The only way I have found to eliminate the Everything Else swimlane is to duplicate the JQL for the swimlanes in the filter criteria.      The result is every time there is a change in scope I will need to update both the Query and the Swimlanes, duplicating work.    I want the ability to hide/remove the Everything Else swimlane, and all of its contents, from view via the Board Configuration process.    This way I can send the same query and only need to update the swimlane JQL.    As 'Everything Else' is out of scope of the purpose of the board, displaying it just adds noise and the possibility of confusion by users. """,Suggestion,Board configuration
64783,"""As a <USER> I want to be able to use story points as a unit of measure in the various gadgets, specifically the pie chart, as opposed to issue count.         My current use case as an example: I want to be able to see the percentage of work being done in a current sprint as a function of story points, not issue count.     Currently the Jira dashboard Pie Chart gadget accomplishes most of what I am looking for in this example. I enter in the query for the work in my current sprint and select """"Account"""" as the statistic type, but the chart currently shows the distribution of work by number of stories, and I would like it to show story points.          Acceptance Criteria:    -Chart shows pie slice percentages such that story points comprise the percent of the grand total (for example, “30% Project A” should mean that 30% of the current sprint's story points come from stories under Project A)    -Legend should display percentage of current sprint next to name of initiative so one does not have to hover over the pie chart to see the percentage    -Should be able to see total story points as opposed to total issues    -Should be able to select story points as a parameter in other gadgets beyond the pie chart, as that would be very useful too.""",Suggestion,Gadget
64789,"""As a User of Jira I want to have the ability to extend the Boards of Jira based on Labels attached to the Issue, without interfering with the underlying process.    The rows extend to a correspondig issue status.    E.g, an Issue is open. I add another Row next to this status named 'Awaiting Feedback' and add a Label 'waiting' to it.    All Issues, which are open and have the corresponding label will be shown in this row.    This gives users the flexibility to extend given processes without the need of administrative rights. Thus administrators will be able to govern their processes in a better way and give users the flexibility they demand""",Suggestion,Board configuration
64804,"""As a project manager, I want to be able to group my Epic's together by making them children of other larger Epics so that I and my supervisors can track the progress of related initiatives more efficiently.    It would be nice if Epics could have either Stories or other Epics as children.  This would enable managers at all levels to have a quick tree level view of the progress of initiatives with the amount of detail appropriate to their level.""",Suggestion,"Reports,Backlog"
64809,"""Hi,    Have two use cases where we need your suggestions/ work around,    Use Case 1 - While creating a user story issue type, I do not enter story points as a result the subject field is not available in the user story detail view. I would like to update it without being required to go to its detail view.    Is it possible for making this field available even it has no value in the first place?    Use Case 2 - While grooming user story/stories in the backlog view, I would like to have story points updated without being required to go to its quick view or detail view.    Currently story points field in a user story issue type is visible but not editable in the backlog view. It will be helpful especially during story grooming to let the user update that field from the backlog view itself.     Appreciate your early reply!    Thanks  <USER>    """,Suggestion,Backlog
64882,"""As a Kanban board -> Reports -> Control Chart user, I would like to have options to automatically exclude outliers outside of some levels of standard deviations, so that I can see what the Cycle Time numbers are without having to use exclusion labels and quick-filters.    It would be nice-to-have if it could support exclusion of multiple levels of deviation - e.g. exclude data points outside 2 deviations, but within one.     Not to get into how to do this, but perhaps check-boxes might work for quick ease of use, similar to the """"Include non-working days"""" box.    Another nice to have would be the standard deviation as calculated against all teh data points be included in the mean, median, row of numbers. I do not believe the exiting swath and hover number is correct.""",Suggestion,Reports
64914,"""Currently it is pretty common for us to create a Kanban Board that allows us to see all tickets that are with our team members and also with other teams.    It would be helpful to have the ability to create for the same Kanban Board different views for different profiles.    For instance: if I am the manager, I want to see what is with <USER> QA team or Business Team. So, my Kanban view has all columns for all statuses. I would like to be able to have a 'Development' view/profile, on which the columns that shows tickets with QA Team and with Business Team would not be displayed, showing the columns that are related to <USER>bigger in the screen. Similarly, we might be able to select the 'QA' view/profile, on which only the columns related to tasks with QA team would be displayed.    Currently, the workaround for that is to create different Kanbans, but this means that in case we are updating the Swimlanes, Quick Filters, Card Colors, Card Layout or any other information, we need to manually cascade these changes to the other (duplicated) Kanbans.          """,Suggestion,AgileBoard
64936,"""As a product owner, in the majority of cases I would like to filter out the tasks and just focus on the stories in the backlog. It seems like this is not supported at least in the next generation Jira.    (I am using Scrum project type).""",Suggestion,Backlog
64946,"""As Product Owner I use links between Jira User Stories in order to define dependencies, e.g. the dependency “has to be done before/after”. These dependencies can cover a sequence of several user stories.    In my product backlog I have a large amount of user stories, which are ordered by priority (rank). There are user stories without links user stories with links.    If I move a user story in my backlog, i.e. I do a re-prioritization, I want to have a simple way  to move the whole group of linked user stories to the new position in the backlog.    In the current version I have to move all stories one by one. E.g. the whole group of stories could be moved automatically when one story is moved. Alternatively, all linked stories could be selected if you select one story out of the group and then you can move them as a group.""",Suggestion,Backlog
65021,"""Today agile boards allow quite limited options to prioritize items: Top of backlog (or a swimlane), Bottom of a backlog (or swimlane).     For projects with big backlogs it's not always helpful - because for some features, as a Jira user and a Product Manager on my project,  I have a pain via drag-and-drop an item to the right place when I don't want it to be put to the very top/bottom. So, I would suggest more flexible prioritization options.      For example:    * Top and Bottom 10% of a backlog   * Top and Bottom 25% of a backlog   * Mid of backlog   * and so on...    So like imagining the backlog in the form of virtual 'percentiles' with the ability to quickly rank an issue into the appropriate percentile.     Thoughts for percentiles (green exist today, blue are to add): *{color:#00875a}Top{color} , {color:#4c9aff}*Top 10%, Top 25%, Mid, Bottom 25%, Bottom 10%*{color}, {color:#00875a}Bottom{color}*.     While specific 'percentiles' can be different, I hope the overall pain and the idea how to improve are more or less clear.          Thank you. """,Suggestion,AgileBoard
65059,"""If you have an issue in an active sprint and then Delete it (Say a placeholder issue), it's not """"Removed"""" from the sprints in the report, leaving the RTE incorrect if that issue had any estimates on it...    I know there is a workaround for not ending up in this situation, however what do you do if you happened to end up in this particular situation?...    Also I don't know if it has been fixed in a newer release or if it's a know issue, I have not been able to find anything that indicates that it was known.    (*Note:* _I wanted to report this as a BUG, but apparently I was missing some assign issue permissions_)""",Suggestion,Reports
65085,"""it is important for end users to add columns in Jira reports. Currently for example in sprint report it does not list the assignee. I would like to request support to add any fields as a column in reports""",Suggestion,Reports
65243,"""As a user, I'd like to be able to configure swimlanes based on reporter.    Currently the only available options are queries, stories, assignees, epics or projects.""",Suggestion,AgileBoard
65268,"""We use Jira for various compliance related needs, and we've found Jira to be a very powerful tool with its custom issue types, custom fields, and custom a workflows that use custom statuses.    As a Jira admin, I'd like to repeat our customizations to another Jira instance with a script, but at the moment, the Jira REST API doesn't seem to cover what we need.    Please extend the Jira REST API so that it can be used for automating such customizations:   * A REST API for creating a new status (or ensuring that a status with a certain name exists)   * A REST API for importing a workflow   * A REST API for creating a field configuration   * A REST API for creating a screen""",Suggestion,REST
65285,"""I would like to officially request support for running Jira Software Data Center version specifically, in Docker containers. There are some community versions of docker containers available, or we can build our own containers (though an official dockerfile provided by Atlassian would obviously be welcome), but we will have no official support from Atlassian in the event of something going wrong (unless I am missing something?).    So, this request isn't really for providing docker container versions of major versions of Jira, but rather the support of running this as an enterprise option.    Of course, in a world where upgrading is supposed to happen faster and faster, and become more and more automated, having the ability to make the upgrade process easier, and enabled as part of an automation pipeline, would obviously be incredibly useful.""",Suggestion,Data Center
65317,"""I have profile picture set and I'm logged in. In the right top corner I can see my avatar.    When I choose another user and open their profile page (path contains <server>/ViewProfile.jspa?name=<user_name>) I get a page of that person showing his/her information as well as avatar. At the same time avatar of that person starts to appear in the top right corner as if that user was logged in. If I mouse over it it still says """"User profile of <My name>"""". Very creepy.         BTW, I was not able to create a bug, just a suggestion.""",Suggestion,Development Panel
65325,"""As a Jira Administrator I would like to be able to restrict users from estimating story points on issues, but not remove the entire _Edit Issues_ permissions.""",Suggestion,Backlog
65389,"""{panel:title=User Story}  As a scrum team member, I want to be able to add more than one avatar icon to one card, to indicate that more people are working on this card.  {panel}  The request for having more than one assignee has been made a couple of years ago (see links). All of them got rejected, due to the policy, that Jira sicks with 1 assignee.    My request is not to break this, but to add a functionality to the agile board only, that for example, the username behind an avatar is part of a user picker (multi users) custom field. The assignee can stay as it is (first drag&drop of avatar will fill assignee, next goes into custom field).    During the past years, working as an agile coach, I have seen many times that teams, working with a physical agile board, mark issues with multiple avatars (magnetic white-board, post-its and individual magnetic avatars).    It absolutely make sense, because the whole agile movement is all about """"individuals and their interaction"""". +Reviews+, +knowledge transfer+ and +pair programming+ (XP) are best examples for marking an issue with more than one worker. Not only the solo work on one issue.    Implementing this feature will make Jira Software come very close to the physical boards.""",Suggestion,AgileBoard
65417,"""As a Jira Administrator / Jira System Administrator, I want that only those two roles can exclude boards. In the current scenario Board Administrators can delete boards too.""",Suggestion,AgileBoard
65436,"""h2. Steps to reproduce    # Go to a board (instance: Bulldog)  # Click an issue X, so the 'detail view' pops up on the right  # Write a comment, don't save it  # Right-click an issue Y on the board, e.g. to copy issue URL    h2. Expected result    Issue X is in detail view, comment is still in 'edit' mode    h2. Actual result    Issue Y is in detail mode, comment is lost :(    This is particularly annoying, as:    * Description or any other field in 'detail mode' is 'inline' editable and saved upon navigating away  * Any other comment or page where you navigate away from something you're editing has a 'Are you sure you want to navigate away? You'll lose your things'  * Right-click should not trigger another issue to get focused/become detail-moded    I've lost data, and this sucks big time.    NB this was on version 7.8.0-RELEASE-BDOG-2815, but had to select an affected version below (so chose 7.7.0 Server)""",Bug,AgileBoard
65503,"""As a Jira Software Administrator, I would like to be able to create and use a schema for Issue Detail View in Agile boards.    It would facilitate the standardization of board configuration.""",Suggestion,AgileBoard
65541,"""I would like to be able to configure Kanban and Scrum boards, so that I can define a custom text for each column, as also for the whole board itself, that gets displayed when hovering over the column header or board name. Basically a bit like when the description text is displayed, when hovering over a status.    This way I could implement a Definition of Done text for each column, or for a whole board and would be able to follow Kanban principle """"Make Process Policies Explicit"""".""",Suggestion,AgileBoard
65572,"""As Product Owner I would like to hide """"Sprints remaining"""" box from Release Burndown report, so that I and my Stakeholders can see the prediction about remaining sprints.     Currently when the prediction contain multiple Sprints, this box does overlap data. [^atlassian-<USER>release-burndown-overlap-example.png]    Suggestion: _How to read this chart_ section already has a _Hide this information_ feature, it would be great if exact the same feature would be available for the _Sprints remaining_ box. [^atlassian-<USER>release-burndown-feature-hide.png]     """,Suggestion,Reports
65702,"""*Existing behavior:*    Epic Burndown chart shows bars for each sprint and for """"original estimate at start of epic""""    Clicking the bar shows the sprint name, dates, and numeric values for the following:   * """"at start of sprint""""   * """"completed""""   * """"remaining""""   * """"added to epic""""    !eb3.PNG|thumbnail!    *Suggested Change:*    Ability to determine what issues are comprising the numeric values that show.   * Example: """"5 added to epic"""" - Way to determine what issues these 5 came from    *Use Case:*   As a user I am confused why the Epic Burndown chart appears the way it does. I believe the sprint's value to be incorrect. I need to better understand where the Epic Burndown is coming up with the values   * Currently I can see issues that were completed. These show in the table below the chart and are grouped into sprints.   * Issues added do not show. I cannot tell what issues were added and cannot tell why the chart shows """"5 added to epic""""    !eb1.PNG|thumbnail!!eb2.PNG|thumbnail!""",Suggestion,Reports
65791,"""As a user, I would like to have a possibility to configure how the gadget name (like Issue Statistics, Two Dimensional Filter Statistics, Filter Results, etc.) should be displayed on the Dashboard.  """,Suggestion,Gadget
66034,"""h1. Story    As a Product Manager, I want to be able to represent my end-to-end lifecycle in a single board, so that I can more effectively keep track of my projects.  h1. Expansion    Our team is currently going through a periodic review and optimisation of our end-to-end development processes. One of the things we've found is that, in order to represent the full lifecycle of a project (conception, development & test, user acceptance, launch and on-boarding activity), we're creating more and more Agile boards which each focus on a different """"stage"""" in the process.    Looking at the division of """"Stages"""" in Portfolio as an example, with Team or Project-focused planning, it would be _really_ useful to be able to setup and configure a single agile-board which represents those different stages.    As an example, I currently have 3 agile boards for 1 project:   # Ideation & Design   ## Kanban board which is used to prioritise and track pre-development work, especially important when I'm pulling-in ad-hoc role, such as Designers and Architects for short-term engagements   # Development & Test   ## Scrum board which is used by the development & test teams to develop features and functions following User Stories and addressing Bugs   # UAT & Launch    ## Kanban board which is used to track UAT and Launch activity following completion of development milestones / checkpoints using external constituents who aren't dedicated to the project or long-tail activity such as customer on-boarding following software release    I could envisage a """"meta-board"""" which, perhaps follows or mimics my Portfolio stage configuration, combines several Agile Boards into a single place/link/menu option, separating the different boards using tabs, and includes a good summary tab empowering project overview, health and statistics.""",Suggestion,AgileBoard
66320,"""As a JIRA Admin, I should be given default admin access to all boards and projects created in JIRA, so that, when a user leaves the company, I can fix and/or update the items created by that user so I'm not stuck with a corrupt board which impacts the integrity of my data.""",Suggestion,Board configuration
66401,"""Follow up from JRA-38604:    With the setup of Crucible enabled not Fisheye, Crucible users can not see commits in JIRA. They are not aware from JIRA that there is some code changes which may need review. They have to check it manually in Crucible - they are still shown there. In the other words - you screwed Crucible users.     I hope that this clarifies the situation - there never was any """"free functionality"""". When the user tried to reach Fisheye functionality (without licence), then there was an error. I wanted to remove the error. You might replace error with something like """"For this feature buy Fisheye"""" or you may disable hyperlink from JIRA to Crucible. Instead you removed the functionality at all!""",Suggestion,Development Panel
66418,"""*_As a_* person preparing release on a multi-repository project looking at a ticket's Pull Request list   *_I want_* be able to easily tell PRs that implement the fix (branch name or PR title contains the ticket) from other pull requests that merely mention the ticket   *_So that_* I don't accidentally merge a PR that implements a different ticket    *_As a_* developer collaborating on code in a multi-repository project   *_I want_* be able to mention any ticket in my [PR description, commit message, comment] without worrying about causing mistakes during release   *_So that_* I can share all the important information about tickets related to my change    In our team the project spans a great many repositories. Because of that there is almost always 4~5 Pull Requests that a ticket introduces. So, a ticket's PR dialog is used as the authoritative information to tell which pull requests should be merged to release a the ticket's feature. Moreover, merges are done by the QA team which is not involved in details of the development.    We could easily prevent a mistake of merging a wrong PR if the ones that are associated based on title texts (PR title/source branch) were made much more prominent than those associated on mentions in 'prose texts' (PR description, commit message, code review discussions, if ever implemented).    Some implementation options:  * Have a separate main list for when ticket occurs in branch name or PR title, and a separate 'Mentioned in' list with all the rest     (This would be the preferred solution for teams like ours, because of the importance we attach to the main group.)  * Add pictogram flags next to a PR that show the types of associations which were found (ideally pictograms for the branch name & PR title should be much more prominent than the others)""",Suggestion,Development Panel
66542,"""As a user of JIRA's software,   I would like a lock button (defaults to on) that prevents issues from being re-organised   So that I don't accidentally move issues around when scrolling on a touchscreen device.    Basically, JIRA is nigh on impossible to interact with using a """"pocketable"""" mobile device. (In my case, ZTE T83 with the Firefox web browser.) Not only does the UI need lots of screen real-estate (a problem even on many laptops with 1366×768 pixel screens), but in the backlog, the (small!) elements respond to dragging events almost instantly.    This is exacerbated by having minimal space to stick ones finger that isn't a task entry. The end result being when tapping on tasks or attempting to scroll, the task is often moved to a wildly different position. Add some lag, and you've got a recipe for making a right dog's breakfast of the carefully curated ordering of tasks.    The tasks should be *fixed* in their position, unless the user explicitly tells JIRA that they wish to alter positions, indicated by a padlock icon or similar, at which point the old behaviour should be restored allowing re-arrangement of tasks. The same UI element should also make it possible to re-enable the lock.    (Thanks to Nic Brough for pointing this JIRA instance out to me.)""",Suggestion,AgileBoard
67296,"""When an issue on backlog is in different project's Epic (not part of agile board filter), then the Epic Name is not visible on Backlog view.    However, on Active Sprints view, the Epic Name is visible.    As a user I'd like to see the Epic Name on issue's row the same way as it is displayed on Active Sprints view.    See screenshots for more information.""",Bug,AgileBoard
67341,"""We have sprints with lots of issues and all of them have multiple subtasks. I have the healthy habit of collapsing all issues that are completely done, to keep an uncluttered overview of the work still to do. JIRA will remember which issues are closed and which are not. So far, so good.    Now something changes in the order of the sprint, like moving an issue up in de order or adding a new issue at the top of the sprint. I know this is not supposed to happen in Scrum during a sprint, but of course it will from time to time. The collapsed / uncollapsed view is now messed up: some previously collapsed issues are now uncollapsed and vice-versa. Of course this is annoying, because I expect all collapsed issues to be done.    I'm a developer myself and did some research to help find & fix the problem. I found that the localStorage entry gh.boardSettings.x has a collapsedSwimLanes property which is an array of numbers. I assume these are issue indices referring to the n'th issue at the sprint board. This would explain why the view gets messed up when you add issues somewhere else than at the bottom of the sprint cause the indices of all 'lower' issues will change.    My suggested solution is to use issue numbers, like JOC-142 instead of indices as entries in the collapsedSwimLanes array. These are unique per JIRA instance and not depending on any order.    Thanks for considering this fix and making an already happy JIRA user even more happy! :)    P.S. I also see numbers higher than the amount of issues on my current sprint, so it seems these are not reset when a new sprint opens and also not during the sprint. When using issue numbers, maybe this should not happen anymore and the whole list should be refreshed every time a change occurs.""",Bug,AgileBoard
67403,"""h3. Problem  As a user I would like to create dashboard widget for showing upcoming releases (like JIRA Road Map widget) but with style like on project's Releases tab. In addition to showing information from releases tab, show information such as Environment, and Issue Keys.    At this moment we have the Built-in Road Map widget but it is not as readable as the Releases tab.""",Suggestion,Release Hub
67475,"""h3. Problem Definition  In Software projects, we have sidebar to navigate to Backlog, Active Sprints, Reports and Issues. Clicking on *Issues* will open custom filter and results on the right panel on the same page. The default custom filter is *Open Issues*, which only displays all issues which has no Resolution set. Today, we are only able to sort the result in the list based on one field, however, we can't add a column to display a selected field.    h3. Suggestion  As a user, I would like to be able to add columns to display when using the custom filter like *Open Issues*, *My Open Issues* and etc.   !Jira_Detail_View_Issue_List.png|thumbnail!     h3. Workaround  Click on the *View all issues and filters* link on the top right, it will direct us to the Issue Navigator page.""",Suggestion,Board configuration
67536,"""I've found a number of JIRAs about the Issue Detail View. I think some of the frustration I read in the comments on them is because people are experiencing what we are experiencing and the request to address this is getting confused with other issues with the Issue Detail View.     We don't use/need Issue Detail View. When we click on a card, we want to see the View Issue screen. As is today, clicking on the card or link on card, goes to Issue Detail View. Then we have to click on link there to get to View Issue screen. This is an extra step that we have to do hundreds of times a day.     The current board option """"Hide detail view / Show detail view"""" *+opens and closes+* the Issue Detail View side panel, but it doesn't *+turn on and off+* the Issue Detail View. Enable it to be turned off so people who don't use it don't have to hop through it to get to the Issue View they do use. The ability to get to the View Issue screen with ctrl+click in another tab doesn't help because it's still another step to the View Issue screen.     From clicking on issues from filter results, we're trained to expect issue keys to take us to the View Issue screen. We want that behavior to be consistent every time we click on an issue key. If someone doesn't like/use the Issue Detail View, getting sent there is very annoying.      (on) *What if clicking on the card, opens Issue Detail View, and clicking on the issue key, as it does everywhere else, opens the View Issue screen.*  * Good GUI uses consistent behavior  * The issue keys, as they do across Atlassian products, go to the View Issue screen  * The card, as a feature of Agile Boards, goes to the Issue Detail View, a view which also is a feature of Agile Boards""",Suggestion,AgileBoard
67606,"""As a scrum master, I would like to have ability to print smaller Issue Cards to save space on physical board.  Mini size could be half of Small size.""",Suggestion,AgileBoard
67615,"""As a user, I would like to be able to sort the listed view of issues shown in the Agile Reports in JIRA.    Sorting the Issues by date added to the Fix Version for instance, is confusing. An ability to sort the list by Issue Rank would be much more valuable and less confusing for our teams.""",Suggestion,AgileBoard
67650,"""Hello    h3. Feature suggestion  It would be great to have the ability to switch between different swimlanes on the fly, rather than having it being a board setting.    h3. Use case  As a scrummaster, I need to have a board where I can see the sprint with swimlanes based on stories, so I can quickly spot what subtasks remain for each story.    However for the daily scrum I need another board where swimlanes are based upon assignee. I had to create two boards, and each time I introduce a configuration change in one board, I have to copy it over to the other one.    h3. Conclusion  Other people or situation might also shed light on the need for other swimlanes, based upon the same board. Basically, to me, the swimlane should not be a hard coded setting in the board, but a quick filter that you can change any time depending on your current and ever changing need.""",Suggestion,AgileBoard
67656,"""I know you ask that I search before adding an issue. However, search is very liberal and not easily filtered and so my issues are rarely addressed by search returns. I am giving written feedback as a last ditch effort to get some help, and even finding this took upwards of 5 minutes (way too long for a feedback box).    My main issue is that documentation explains features from a development perspective, which doesn't serve me as I am a business user and would like features discussed more broadly so that I may relate them to my work.    Documentation is either very sparse or not tagged well because, like I said, searching hardly every addresses my needs. I am rarely able to use documentation to solve my problems, and the learning curve on this product is steep so I've spent several months just fumbling around trying to """"figure it out."""" This product is so full of features and functional that it (and those who created it) deserve better documentation to help customers actually use and work with it. As things are, I use it sparingly because I never really know where to find things on a board designed for developers when I am in a very different business field. The features are also named and labeled using industry terms like """"scrum,"""" which means nothing to me and requires research before I can know to look for something else, which is often also not labeled in a straight-forward manner.    At the moment, I am trying to learn the differences between various project types (for about the 5th time; thought I understood, but somehow I am constantly finding new info that wasn't available before...) as well as how to delete or back up data, none of which simple searches have been able to explain.    I may not be developer-style computer savvy but I am no Luddite either, and this should be way easier. One good start would be to include a text box in the page-specific feedback area (instead of the multiple choice of three vague options). You'll get better page-specific feedback that may allow your teams to address needs better than via a separate task or suggestion that is now removed from its context... Another good place to start would be to consider that development language does not always translate across industries, and when you claim to address regular business needs, your help language should reflect that.    I'm sorry if this sounds harsh. I like the product overall; I'm just very frustrated that help is so hard to come by. I can't even call anyone...""",Suggestion,Documentation
67720,"""As a Agile Team (Scrum, Kanban) I'd like to specify swimlanes for """"Projects"""" to group my issues by projects on multiple project boards.""",Suggestion,"Board configuration,AgileBoard"
67790,"""As an engineering executive, I'd like an Epic Burndown Gadget so we can track progress across many epics via Dashboards or Confluence Report Pages.    *_Additional Comments:_*  _Atlassian has really under invested in Gadgets and Dashboards.  For example, I've been waiting literally years for you to add the ability to have Story Points as the """"value"""" field for the two dimensional filter.  Similarly, when you redesigned Jira and added the Reports section, all reports should have been made available as Gadgets via Dashboards.  For any meaningful implementation, its inefficient to have to go to each project/board to view reports.  Dashboards/Gadgets are a very powerful capability.  I'd think improving this area would be low effort/high reward._""",Suggestion,Gadget
67823,"""In the Project Releases page there are two quick filters, Released and Unreleased.    !172266.png|thumbnail!    As a user I would like to add new quick filters to this page and/or change the quick filters query.    """,Suggestion,Release Hub
67915,"""As a Solution owner for big projects, I would like to have the ability to create a new custom text field for Acceptance Criteria, which would allow me to add test cases to my stories without cramming it up into the story description field.    """,Suggestion,AgileBoard
67938,"""h5. Suggestion  As a JIRA Agile user, I would like to change the Swimlanes text colour to emphasise and highlight current/high priority swimlanes.    h5. Problem Definition  For example, for a project - TEST, I have multiple swimlanes for both current version and the past versions. It will be great to have the text colour change (the current version and the past versions swimlanes' text are the same in colour) in order to make them more visible and to differentiate them from the past versions swimlanes.""",Suggestion,AgileBoard
67989,"""h5. Problem Definition  Currently a JIRA admin cannot add multiple working days on a single operation.    h5. Suggestion  As an admin, I want to be able to add a span of non-working days instead of having to enter them one-by-one.    For example, our company closes between Christmas and New Year's.  I want to be able to enter the 4-5 week days that are typically between those two days without having to click {{Add Date}}, select a day, and repeat until all of them are added.  """,Suggestion,Board configuration
68088,"""h3. Problem Definition  As an Admin, I want to add a new panel on an Agile Board like product (which is a custom field). Currently we have only *Epics* and *Versions* on the panel.    h3. Suggested Solution  Ability to add custom fields to the Agile Board panel.""",Suggestion,AgileBoard
68089,"""We use JIRA onDemand and have purchased the JIRA Agile add on - we love it but we do have one small niggle.    We understand that Scrum performs best when you have physical boards and co-located teams, we aren't in that position. We are globally distributed over multiple timezones and our team members perform their Daily Scrum via Skype or Google Hangouts etc and from experience it seems that many companies are doing the same thing - Scrum is going distributed.    One thing we  have realised is the importance of the team being able at a glance see how their Sprint is going - and if we had a physical board we would be able to easily tell how many """"stickies"""" were either in progress, or to - do.     Now with the Active Sprint view - it does give that information - but you have to scroll up and down for it, and it's hard to get that """"big but quick glance picture"""".    We have experimented with other tools such as RealTimeBoard - that has a great zoom and  the teams felt that using that tool it bridged the gap between the virtual board and a physical board. However JIRA is better at managing a backlog than RealTimeBoard with all it's filters, searching, tagging, bug raising etc so we have stuck with it.    I would love it if the Active Sprint view was effectively a canvas that you could scroll, pan, and zoom in/out on (though I appreciate that it's a biggy...:) .    Keep it up.    Thanks    Damien""",Suggestion,AgileBoard
68090,"""As a JIRA admin, I would like to grant other users permissions to change the configuration of an existing board.    Currently, I have to add the JIRA Administrator Global permission, so the users can configure the board.    It would be nice if I had a separate permission in the board, so other users can configure the board without being a JIRA Administrator.""",Suggestion,Board configuration
68131,"""As an avid user of Agile Boards, I would like the ability to use color/styling/etc. on the Quick Filter names, so that it is easier to use multiple Quick Filters.    See the attachment for an example of how we use large numbers of filters.    !pic.JPG|thumbnail!    """,Suggestion,AgileBoard
68192,"""As a agile user I want to click on an JIRA issue on an agile board to see the full issue in a javascript frame without having to reload the whole page. I really want to see the whole issue in this frame as if I would reopen it in a new browser tab but without having to load the whole page.    In addition I want to have an option to disable the current sidebar issue detail view on boards.    (i) Trello for example has implemented it that way.""",Suggestion,AgileBoard
68235,"""As a JIRA Administrators, I'd like to be able to set the timeout for the rapid boards. Sometimes users are in remote locations where it's excepted that the load times will be very high.""",Suggestion,Board configuration
68446,"""As a project manager I want to be able to define column constraints based on JQL: In its current version issue count is too restrictive. Instead I want to be able do define a query. The min and max should be defined as the number of items returned by that query.""",Suggestion,AgileBoard
68520,"""As an agile user I want a shortcut to flag an issue because the we work a lot with this and right click every time is kind of a pain.   F( for flagged) isn't in use f.e. ;)     Would be a great relieve for us.    Thanks in advice.""",Suggestion,AgileBoard
68554,"""As an administrator I want to be able to tell users to make a cleanup amongs all agile boards.  The problem is that users tend to create a massive amount of boards for testing purpose and then never delete them.    I suggest that on the 'manage boards' page there is a filter to find all boards you are administrator of. This would make it very easy for users to find what boards they have created or are responsible for to make a decision if it is in use or not.""",Suggestion,AgileBoard
68588,"""As a team member  I want an easy (one or two click) way to mark an issue as Blocked / At Risk / Going Back on our board  So I can quickly communicate a precise status to our team.    On our physical board we used to use a sticky to indicate a story was blocked, turn it to an angle to indicate """"at risk"""", and turn it upside down to indicate it would not be completed.  We are currently using the Flag to indicate that one of those 3 is the case.  It would be great to be able to flag stories with more detail (different colors / type of flag, one click for a label, etc.).    """,Suggestion,AgileBoard
68589,"""As a Product Owner   I would like a hotkey for the Backlog Search in Agile Plan mode  So that I can search for stories with less switching between a keyboard and mouse.""",Suggestion,AgileBoard
68591,"""As a board user I would like to be able to quickly change the Swimlane configuration without having to go into the board configuration screens.  Just offering the more simple choices would be good, Epics, Assignee, Etc., leaving our JQL would be fine.    We like to have the Swimlanes by Stories most of the time and then use quick filters to show issues for a particular user. However it would be more useful if you could just switch to By Assignee or Epics more quickly. """,Suggestion,AgileBoard
68647,"""For a project with a vary team size (ramp-up, ramp-down) or a bigger team it is very inconvinient to limit the max WIP of a lane.  This sounds very mechanic from the automotive industry where the production line has a max. of roboters which you can place along with.    In software engineering discipline I often see junior developers tackling more issues parallel which rather has a slow down effect. (Context switching too much parallel work, a single issue could have been moved on to the review and tester team...)    Therefore I would find it useful to have an option telling the max WIP limit on this lane is 2 per user. So that if I have 8 developers the total max on lane basis would be 16. But I'm guranteed that the junior developers concentrate on finish the tasks the tackle before fetching another one.""",Suggestion,AgileBoard
68679,"""As a new JAG user, I would like the ability to backdate previous sprint information into JIRA. This will allow me to use JIRA's charts and estimation functionality much faster. As well as expand the scope of my reporting long term.""",Suggestion,AgileBoard
68692,"""Scenario where this would benifit us.    Our triage process has a kanban board with three columnns.  Awaiting Triage, Clarification Required, Accepted.  It is not acceptable for something to remain in Awaiting Traige column for more than two days.  It is not acceptable for something to remain in Clarification required for more than 5 days.    Right now the 'age' is useful to indicate when something has broken this SLA, however I'd like the ability to make this more 'in your face' by highlighting the card in a bright colour.    Of course the number of days would need to be configurable per column.""",Suggestion,AgileBoard
68777,"""As a Scrum Board (Plan View) user, I would like to have a third option in the left margin to expand a Component column that behaves just like the Epic and Version columns. That way, I can click on a component and the board will be filtered on that component, just like it does for Epics and Versions.""",Suggestion,AgileBoard
68778,"""As a scrum or kanban board user, I would like to be able to transition my issues between statuses that are mapped to the same column so that I don't have to use the workflow controls to do the transition.    In JIRA Agile v6.6.0, if multiple statuses are mapped to a column, it is impossible to transition a story from one status to another in the same column even though there is valid transition.    In my case, I've created a Kanban board with the columns Not Started - Analysis - Development - Testing - UAT - Done. There are two statuses mapped to each of the middle for columns (e.g. Ready for Analysis and In Analysis, Ready for Development and In Development, etc.). I am showing the status on the card layout to eliminate ambiguity. But I can't move the card from """"Ready for Analysis"""" to """"In Analysis"""" with drag and drop. I have to use the workflow controls instead.  Which renders the whole board pretty unusable. """,Suggestion,AgileBoard
68781,"""As a JIRA-AGILE plugin developer I want to manipulate the listing of stories in the PLAN mode of a SCRUM board to _inject_ a bar representing a release between the stories to get an quick overview over the release-plan.    At the one side there are some stories estimated with story points in my SCRUM backlog.  At the other side I manage releases/versions in JIRA, too.  Each release/version has a *fix release-date*, also stored in JIRA.  The JIRA backlog """"PLAN view"""" is a very great view and my single point of planning. I don't want to do some excel exports etc...    Regarding to the current team velocity it is possible to calculate which stories are able to be """"in"""" a specific version and which not. Nothing new so far...    I want to write a plugin that shows exactly this information in the story list of my backlog. Each version should be placed below the story at most be possible to deliver within this version as a """"fake story"""" or as a bar - see attached example for detail.    *My request*  It would be very nice if you provide some extensions to hook into the enumeration of the stories of the backlog to inject a version-bar between two stories.      *Why I want to develop this?*  In the Product-Owners game there are some issues coming up every day there seems to be _more important_ than the planned issues. That's ok so far - but as a PO i want to keep track which issue will be descoped if I add the new one.  => In this case the issue above a version will fall below the version bar. This is a great visual feedback.  => In order to this i'm able to drag&drop another - in the meantime _not so important anymore_ - issue below to bar to make a visual release planning.""",Suggestion,AgileBoard
68983,"""As a scrum master (or product owner) I want to understand WHY scope changed during development of a release.    The Release Burndown report helpfully shows scope changes as """"added to version"""" (see attached screenshot), but leaves me to manually figure out what was added or edited to result in that change.    Suggested solutions:  # turn the caption """"added to version"""" into a link to the issues that caused the version scope to change. This would greatly reduce the effort required to understand why scope changed.  # add a list of the JIRA issues numbers which were added/edited to cause the scope that was added to the version.  """,Suggestion,AgileBoard
69041,"""As a sprint planner I would like to filter the cards in the plan mode of a scrum board by a single assignee in order to fine tune the ranking for that particular user.    * A very nice way would be by clicking on one of the new profile icons of the involved users above each sprint, which are available since Jira Agile 6.6.  * The expected behaviour would be similar to the behaviour when clicking a version or an epic  * The filtered result should show each card that comply to the following condition: (issue is assigned to the selected user *or* has subtasks which are assigned to that selected user).""",Suggestion,AgileBoard
69072,"""As a JIRA user, I would like to see my sprint burndown chart based on number of subtasks.  We are using story points as estimates, but in a sprint we have only few storys with many tasks. The current burndown based on story points is useless, because we don't want to add story points to the tasks. The burndown shows only few steps as we are resolving our tickets. If we could see the number of remaining tickets in our sprint, we would have an accurate burndown, similar what we had in classic greenhopper.""",Suggestion,AgileBoard
69109,"""In 6.6.0 the option to customise cards in the new Agile boards was added, however the field I need to add is unavailable, it appears only fields which are available on all ticket types are available - this is a regression from the Classic boards, where I could add this field.    *Repro steps:*    # Add a custom field.  # Ensure it's setup to work with a project that uses a JIRA agile board.  # Customise the Agile board and try to add the newly created custom field.  > It works!  # Modify the custom field to only be applicable to one ticket type, Story for instance.  # Now try and add the custom field to the ticket.  > The field is no longer available.    The use case is that sub-tasks have a different set of fields to regular issue types.    Ideally if a field is not available for an issue type then it just wouldn't appear, but if it just had 'EMPTY' that would be fine also.    I've put priority as major as this will block us moving from the classic boards.""",Bug,AgileBoard
69110,"""The 6.6.0 Agile boards added the 'Workload by assignee' table, which is amazing and should solve a lot of our problems regarding dealing with specialists on the boards.    We use Original Time Estimate as our primary estimation statistic, as opposed to Story points.    I need this tool to tell me how much time people are taking on in to the sprint, NOT how much the issue had assigned to it in total.     If I had a ticket with a 2 day estimate that rolled over from the previous sprint with 4 hours remaining on it, then that's only 4 hours that individual has on them in them in the next sprint - that's the information I need to be able to see.    Set this as a blocker because this is absolutely key to our workflow and will prevent us moving from the classic boards.""",Suggestion,AgileBoard
69125,"""We have been using the Classic boards but as these are now on the chopping block I'm looking at the new boards.    The new burndown graph is too noisy and is confusing for some stake holders/managers who used to occasionally check burndown charts on the Classic boards.    I have a team of 11 who are, fortunately, very good at logging time - this means that during the day there's a very tight clustering of data points, which makes it almost impossible to meaningfully inspect the points, but that's then followed by a flatline representing out of office hours.     If there was an OPTION to consolidate each day in to a single data point that'd be ideal. I've attached a screenshot of the classic vs new chart (using same team data, though sprint for new chart starts a day later).    """,Suggestion,AgileBoard
69166,"""As a Agile sprint planner, I want to see the sum of the task and subtasks time in the bullet field when planning my sprint    When in a board, """"work"""" panel, and agile configured as """"Estimation statistic"""" based on estimate and time tracking activated, I want to see the sum of the subtasks and task time in the bullet of the backlog.  """,Suggestion,AgileBoard
69167,"""As a Agile sprint planner, I want to see the remaining time in the bullet field when planning my sprint.    When in a board, """"work"""" panel, and agile configured as """"Estimation statistic"""" based on estimate and time tracking activated, I want to see the remaining time in place of the original estimated time in the bullet of the backlog.    Because I want to be able to plan an issue reopened based on the remaining time and not on the original estimated.""",Suggestion,AgileBoard
69194,"""Hello,    In my JIRA Agile Scrum Board I have set my Estimation Time Tracking preference to _Remaining Estimate and Time Spent_ however when viewing the Epic Burndown chart it appears to only support burning down via the Original Estimate.    So, as an Agile user I would like the Epic Burndown chart to honor my Scrum board's Time Tracking preference so that I can see time being burned down when my developers log time against a ticket within the epic, and not just when the ticket is closed / original estimate is set to 0h.    """,Suggestion,Board configuration
69264,"""As a team, we work on a mix of stories and bugs.      Some individual bugs can take a long time to resolve (multiple sprints).  Is it possible to determine the amount of time logged to a ticket *in a certain time period * (typically, the last sprint).    Current reports show total amount of time spent (e.g. Workload pie chart gadget), but I'd like to be able to also see e.g. """"time spent on bugs in the last sprint""""    Possible?  """,Suggestion,Gadget
69311,"""As a <USER> I need the ability to tie Sprints to specific """"teams"""" to marry them together for Velocity charting, and other metrics.    Acceptance Criteria:  * During the creation of the Sprint entity, demonstrate that the sprint can be categorized with a """"team"""" enitity (or label).  * In the board reports, demonstrate that the Velocity report has the ability to show only the sprints that share the same """"team"""".  * On the different JIRA Dashboard """"Wall Board Plugins"""" related to JIRA Agile, demonstrate that they can be filtered to only use sprints with a specific """"team"""" categorization.        *More detail:*  I have multiple JIRA projects that represent different """"Pillars"""" (i.e. different types of work).  I have Scrum teams that each have a _primary_ pillar, but that can perform work on any pillar during any sprint based on the company's goals at any given time.     Each scrum team, while potentially working on the same application as a different team, is it's own stand-alone body, complete with DB Devs, App Devs, QA-Testers, <USER> and Product Owners.    The problem I have is, since teams aren't isolated to Pillars, we generally have a larger unified backlog that serves the whole department, and we use quick filters to show pillar specific tasks if a team is only going to work within a specific pillar.    I have a unique sprint for each team, in order to track velocity, capacity, and success on an individual team level. This is because they estimate on within their own team.  Thus, Velocity should be unique among teams.    Unfortunately, Since each scrum board must include issues from every Pillar (i.e. Jira Project) since a team can work within any of them, the Velocity report shows ALL sprints, rather than just the ones for the Team I'm interested in.    I currently have no way to """"tie sprints together"""".   I *could* filter each board by a ticket field, but this would mean I need to manage that field for _every_ ticket.  ANNOYING.  Thus, the Sprint label would simply this process.   """,Suggestion,AgileBoard
69337,"""As a Jira Agile Enterprise user, I would like to be able to customize what fields appear on cards in the Jira Agile Kanban board work view.  Currently, I am only able to see Issue ID, a picture of the assignee, issue subject line, and dots indicating age of tickets.  I would like to be able to configure cards differently from one board to the next, and see fields such as:  1) Date Created  2) Reporter      """,Suggestion,AgileBoard
69366,"""In prior versions of JA (I believe 6.2 and below?), one could quickly drag  an issue in a sprint to a different, off-page position on the Plan view of the rapid boards using the multitouch features on the Mac trackpad.    For example, if I wanted to grab and issue and drag it two pages down to reorder it in a spring, I would simply do a """"hard"""" click on the trackpad to select the issue, and then simultaneously do a two-finger swipe with my other fingers in order to flick and very quickly scroll the sprint up or down, and then release the hard click of the issue when it was in the right place.    In JA 6.3+, the two-finger swipe no longer works when I have the hard click depressed to """"pick up"""" an issue. In order to accomplish the same task, I need to drag the issue down past the edge of the window, and then wait while it s.l.o.w.l.y drags down.    This problem is made more infuriating because dragging an issue off the end of the sprint in Safari causes the sprint to scroll down only a tiny bit...and it then stops completely. I need to jiggle the trackpad to keep moving the issue down again, which is a very, very slow operation, and it stops moving whenever I stop moving the pointer.    This makes it take 5-10X as long to reposition a single issue if one has a lot of issues to move over.    The same problems occur in Chrome as well.""",Bug,AgileBoard
69435,"""As a product owner I want to be able to communicate the status of my project as easily as possible without spending much time explaining the charts.    I have received some feedback that the version report diagram would be faster and more intuitive to understand if the secondary y-axis was colored in the same color as the data to which it refers. The only percentage data which is displayed in the version report is the amount of unestimated issues and so the secondary axis could also be colored in red to show this relationship.""",Suggestion,AgileBoard
69436,"""As a product owner I would like to see a rough estimation of the impact which the unestimated issues will have on the predicted date of completion in the version report.    Obviously, this will only give a very rough estimate because the assumption that the unestimated stories will have a similar complexity as the ones which have already been estimated will not always be valid. However, it's the a best guess. This rough additional projection would simplify the visualization and communication of the projected release date of the version.    Limitiations:  If the amount of unestimated issues is too high, this projection is highly unlikely to be precise. Accordingly, the projection could remain hidden until the  amount of unestimated issues has dropped below a certain threshold (e.g. 30%).""",Suggestion,AgileBoard
69468,"""From a support case:    {quote}  So, for usability, I wanted to break the Resolve and Close drop zones out into their own columns, but still have them both counted as """"Done"""" for the purposes of the Backlog and Reports.    Also, our process currently uses the transition between Resolved and Closed as an indicator that additional (yet optional) testing has been performed. I guess this is really a process thing, surely we should just make the additional testing mandatory, or not track it at all. So we'd still like Resolved to be counted as """"Done"""" for the Backlog and Reports, but have the option of seeing Issues transition to the 4th Closed column.  {quote}""",Suggestion,AgileBoard
69476,"""As an Administrator, I want the ability to configure a listener for starting a Sprint so that I can automate workflow events and send notifications to other teams.    This is important because our workflow goes from In Backlog > Committed > In Development > Accepted. When we start a sprint, we want to automatically commit to all issues that are In Backlog.    It's also important because we want to update issues that are linked to CRM tickets so stakeholders are aware that we've begun working on their issues.    We also want to send emails to other team members to let them know we're working on linked issues. For example: the web code team just started a sprint that includes a new web service the client team needs. We want to change the priority, and notify the client Product Manager and ScrumMaster so they can be prepared for the scrum of scrums.""",Suggestion,AgileBoard
69529,"""As a user of JIRA Agile I have completed a sprint. I now clone an issue from this completed sprint.    I'm now unable to remove the cloned issue from the completed sprint.    GHS-9786 seems to be relevant to this. However it continued into a support issue. But yet knowledge on confluence or on answers.atlassian.com is missing about this.    How the heck do we do this?""",Suggestion,AgileBoard
69534,"""As a <USER> I would like to be able to define what statuses are used to determine the 3 blocks on the Sprint Health Gadget using custom JQL.    Currently the gadget uses the first column for blue, last for green and all others for yellow.    I would like to be able to select each block, and use JQL to choose which of the status labels gets grouped in each of the blocks. See diagram for how I would like to group my labels. """,Suggestion,Gadget
69535,"""As a user of JIRA Agile, I would like my sub-tasks to inherit their parent issue's Epic Link (similar to how they inherit the sprint field) so that I can use epics as filters.     For example, currently, if I use the swimlane configuration to sort my Agile Board by Epic, the sub-tasks all appear in a swimlane by themselves at the bottom, called """"Issues without epics."""" """,Suggestion,AgileBoard
69544,"""As a agile team member I want to have the status of an issue regarding (for instance) INVEST criteria visualized on the scrum board's planning page in order to quickly see which issues are ready for being included into the next sprint.    In detail:  Our company has chosen to visualize the result of checking an issue for INVEST criteria by using the *traffic light plugin* (""""de.polscheit.<USER>plugins.traffic-light_status""""). After a team member has checked all criteria and found them acceptable, the traffic light is switched to green, signalling the issue is ready for sprint. This works well in the normal issue detail view but  - neither can the traffic light be visualized directly in the list of issues to be ranked and placed into the spring in sprint planning mode  - nor can the traffic light field be added to the issue detail view shown right of this list if an issue is clicked. It is simply not in the list of available fields, even if I configure it as """"global"""" (all projects, all issue types).    I am aware that we could, in principle use a plain text field or yet another issue status to hold the same information, but a traffic light visualizes the """"sprint readiness"""" much better imho.""",Suggestion,AgileBoard
69741,"""As a scrum master, I would like to be able to rapidly re-sort the scrum board based on epic, assignee, story point estimates, issue updated date and so on. Currently, the only option is swimlanes, which are fixed for a given board and affect everyone.    I would also like to see a total at the top for each scrum board column, which is helpful when the number of issues in a column becomes longer than a page.""",Suggestion,AgileBoard
69754,"""As an admin, I want to set a default template that all newly created Agile boards follow so that I can avoid having to configure boards manually each time.    As an example, our company uses a 4 column layout for Agile boards so that we can differentiate between *Resolved* and *Closed* Stories. Every time I create an Agile board for a project, I have to change it to have 4 columns, change the default filter, setup the same quick filters and make other small configuration options that are consistent across all projects. I would like to be able to change the default setup (or create a new template board) that can be used for all new Agile boards.""",Suggestion,AgileBoard
69853,"""As a Scrum product owner I want to find Epics in my Product Backlog in the same list as User Stories, so that I have an overview of the complete backlog and know which parts of it have to be broken down.    Acceptance criteria:  - Epics are displayed in the list of user stories and are filter and sorted just like the stories  - If I click on the epic, the User Stories underneath will be shown  - Epics have a different look than User Stories (colour, Symbol,...?), so that I can visually distinguish them from User Stories  """,Suggestion,AgileBoard
69854,"""As a scrum product owner I need to see the details (description, estimation, comments,...) of epics so that I can inform me about the epic and understand better, what the Epic is about.    Acceptance criteria:  - If I click on an Epic in the left column of the agile planning board, it's details will be shown in the right column of the board.  - The epic detail view looks and behaves like the User Story detail view.""",Suggestion,AgileBoard
69900,"""We use stories (with story points) to manage our sprints, and sub-tasks for the actual to-dos within a sprint (without any estimation). When we configure the agile board to use """"Issue Count"""" for """"Estimation"""", the burndown chart shows the stories completed. It would be nice to (maybe optionally) see the much more fine granular sub-tasks burning down.    I would have thought this was a quite common practice, but I haven't found any similar requests. Some colleagues misuse the time tracking instead, by entering the number of open subtasks as estimated days.""",Suggestion,AgileBoard
69911,"""As a Product Owner, I want the stakeholders in my product to be able to rank the issues that they find important to our product independently from other stakeholders.     Details:  We manage our stakeholders by allowing them to rank their requests in order of importance. Our sales team will have a different list of priorities to our support team for instance. In JIRA Agile Classic, we do this by given them access to a planning board for their issues and associate the context to a Ranking field (e.g. """"Sales Rank"""", """"<USER>Rank"""", etc.). This allows our stakeholders to drag and drop issues into the order that they find important. These lists are used as input to our sprint planning meeting. The independent ranking fields mean there is no accidental interference between the various stakeholders' lists when moving issues up or down in their own list.    With JIRA Agile, this is no longer possible (due to the fact that there is only one global rank field). We tried this once, but each time a stakeholder changes the ranking of an issue in their list, it has a side effect of changing the rank in other lists (due to the fact the ranking is global). This led to prioritisation chaos.     The lack of support for our use case(s) forces us to use JIRA Classic. Once JIRA Classic ceases to exist - the way we work will no longer be supported by JIRA at all. We are thus forced to stay on an older version of JIRA.     We would like to be able to configure a board to have a specific ranking field other than the global ranking field, just like you can in the current Agile Classic view.  """,Suggestion,AgileBoard
69958,"""As a <USER>  I want to enter org-wide days off only once, and share them across all teams,  So that I do not have to enter the same set on each Agile Board.    AC:  A. In a central config, enter Standard Working Days once org-wide.  B. In a central config, enter Non-Working Days once org-wide.  C. On any Agile Board, optionally include org-wide Standard Working Days.  D. On any Agile Board, optionally include org-wide Non-Working Days.  E. On any Agile Board, optionally include extra or override one-off Standard Working Days (per current feature).  F. On any Agile Board, optionally include extra or override one-off Non-Working Days (per current feature).    -Johnny""",Suggestion,"Board configuration,AgileBoard"
69998,"""I am using an Agile Board which is used to display several projects. I have one quick filter set for each of these projects.  So, when I use a quick filter to see only the User Stories of only one of the projects, I still see all Epics of all projects. As an Epic is only a task part of a project, I would be expecting that the Epics would be filtered as well.    The result is that I end up with a very long list of Epics and I have no way to filter them out.""",Suggestion,AgileBoard
70001,"""As a Program Manager I would like to have access to a basic Agile Board Report that displays a table of information on the no. of open tasks per person per sprint along with the total task hrs for the tasks of that assignee.    This request has come up from tens of people within my organisation and is crippling our ability to have quick access to this information without having to perform an extract of information from a sprint to excel where the data is transformed and then placed on a wiki for the team to view it.    Table Column headers required;  i-Assignee  ii-Total Task Count  (for that assignee)  iii-Original Estimate (hrs)   (for all tasks assigned to that assignee)  iv-Time Spent (hrs)   (for all tasks assigned to that assignee)  v-Remaining Estimate (hrs)   (for all tasks assigned to that assignee)  vi-Remaining Days in Sprint  vii-Original Assignee Capacity to the Sprint  vii-% Assignee Capacity Remaining    The first 5 pieces of information comes directly from <USER>but the last 3 are outside of <USER>  In an ideal world I would like to define all 8 pieces of information in <USER>but if the first 5 could be available I would be a great start.    """,Suggestion,AgileBoard
70079,"""As a PM I would like to be able to also (in addition to Version and Epic) to be able to filter backlog (planning view) by issue components.""",Suggestion,AgileBoard
70087,"""I'm a big fan of your keyboard shortcuts, they make live easier and working with the board faster. (y)    So:  Suppose that I have two Quick Filters: """"A"""" and """"B"""".  """"A"""" is turned ON and """"B"""" is turned OFF.    I now want to switch to """"B"""".    Instead of turning """"A"""" OFF *and* turning """"B"""" ON by myself.... :(    ... I just can (_for example_) {{Shift}}-click """"B"""" (pressing {{Shift}} while clicking) and as a result """"A"""" is automatically turned OFF (of course by that I mean automatically turning all Quick Filters OFF that were turned ON before my interaction).  :) :) :) :) :) :) :) :)""",Suggestion,AgileBoard
70160,"""This may relate to GHS-3922 although it's not clear.  Is there any future plan to allow multiple avatars to show on a single card?  Either via multiple assignees or by flagging another field as a trigger to display an avatar?      I'm not looking for a definite delivery date, just an understanding if this is on the roadmap.  This is something my company would love to see for pair programming and shared testing support between our QEs and SWEs""",Suggestion,AgileBoard
70221,"""As a sprint master I would like to have the possibilty to use epics as a predefined swinlane fllter on a Kanban Board type. This possibilty allows me to track epics through phases outside of the sprints.""",Suggestion,Board configuration
70231,"""As a  scrum master I want to be able to have full control over the order in which fields are displayed in the RapidBoards """"Issue Detail View"""" so that we can surface information on an order that is more relevant to our team, not the way determined by Atlassian.    In this context, we have a custom field called BDD scenarios and we'd like to display this BENEATH the description field, but there is no way of moving these around. This is one example obviously. Ideally we'd actually want all of this information to be displayed in the order we want and way above the General Fields information, for us that isn't as relevant as the Description and BDD scenarios but we they are useful enough to have them available in this view.""",Suggestion,AgileBoard
70242,"""As a scrum master, I want to understand why we have such a high scope change rate. It's difficult to get a handle on the reasons for scope change, just looking at either the sprint or the burndown reports. There's no way to filter or sort those tables, and """"scope change"""" data isn't available in Jira's reporting, as far as I can tell. """,Suggestion,AgileBoard
70297,"""Customer would like to request on the following:    {quote}  """"As a user of JIRA Agile, I would like to be able to rank epics and stories.  Given two epics A and B where A is more important to finish first due to business priority than B, I expect epic A and ALL stories contained within epic A to have higher priority/ranking than epic B and all its stories. It seems that the current version of JIRA Agile does not support this since ranking of stories is left untouched when dragging epics up and down in the Plan view.  Also when creating filters I want to group stories within their epics sorted in priority order. Thus, I would like stories grouped by epic, then sorted on epic rank and within each epic, on story rank.""""  {quote}""",Suggestion,AgileBoard
70298,"""As a user of JIRA Agile I have completed a sprint, the completion process informed me that incomplete issue would be moved to the top of the backlog. However the issue shows as being part of a completed sprint and is not visible to other boards. The only workaround is to create anew issue.    I would like the ability to remove such issues from a completed sprint.""",Suggestion,AgileBoard
70340,"""As a board user, you currently can filter on (active) sprints, and on quick filters (pre-defined by the board-owner/manager).  I would like to be able to filter on the Epics of the board, on the Versions used in the board (Fix Versions field of issues), and on the Users (assignees) used in any catrd on the board.  These filters should always be there (for example in the menu where currently you already can expand or collapse swimlanes).  I know some of these filters things could also be 'done' by using swimlanes, but our users find swimlanes very uncomfortable.  T know these filters could be implemented in Quick Filters, but then they are not dynamic: we have to adapt them all the time new Epics, Versions or Users are used. """,Suggestion,"Board configuration,AgileBoard"
70350,"""Version 6.3 uses certain CSS classes which have the same name as classes in jQueryUI. There's no problem until there's a plugin (in our case, Structure), which use jQueryUI and integration with GreenHopper.     The integration requires us to include certain web resources on the Agile board page, including jQueryUI. This results in unexpected greying out of the whole Sprint sections, which was immediately noticed by the customers and reported as a bug --- see attached screenshot.    To reproduce:   1. Install the latest version of GreenHopper and Structure  2. Create a Scrum board, and a Sprint on it    Since iQueryUI is a pretty standard library which can be used by other plugins as well, I would suggest namespacing JIRA Agile CSS in some way to avoid the collision.    Thanks!  Igor""",Bug,AgileBoard
70352,"""Boards with swimlanes are sometimes hard to read. As a board user, I would like to have an option to activate or de-activate swimlanes (like the option in the menu to expand or collapse all swimlanes).""",Suggestion,"Board configuration,AgileBoard"
70360,"""As a scrum master, I'd like to be able to group tickets by project in work mode.    We run many concurrent projects from the same board. Currently we use epics for projects, but there are some significant disadvantages to that since the rest of <USER> plug-ins, etc. don't really know anything about epics, but they do know about projects & versions.    (Really, I'd like to be able to create swimlanes by pretty much ANY field, and I'd like to be able to dynamically change them on the fly without going into """"configure"""", but that seems like a significantly larger ask.)    {{original Suggestion included by Version but this is now tracked in GHS-5720}}  """,Suggestion,AgileBoard
70381,"""As a user I would like to be able to enter the start and end date of a sprint manually rather than being forced to use the datepicker.    When starting a new sprint or amending the start/end date of a running sprint I always have to select the date and time by clicking in the date selector popup. For the date itself this is not much of a problem, but for the time component this is a real pain as it can mean heaps of clicks to get the minutes and hours right. Instead of clicking so many times I prefer to simply enter date time.""",Suggestion,AgileBoard
70406,"""As a Project Manager I need to see the Remaining Estimation directly in the Plan Mode of my Scrum Board.    Currently there is only the original estimation. I would like to see the remaining estimation directly beside the original estimation (directly beside the 2d / 5h in the attached screenshot).    Of course I know that I can see the remaining estimation of each story if I click on a story. As well do I know that the remaining estimations of all stories within a sprint are cumulated.    I would really love to see this feature in further versions - as well would the customers of //SEIBERT/MEDIA love to see this feature.""",Suggestion,AgileBoard
70407,"""As a Project Manager I need to change the sequence of my planned Sprints in the Plan-Mode of the Scrum Board.    This is really important for me because I wish to create sprints in the far future and break them down to smaller planned sprints when time goes on.    i.e:  * Sprint KW1-KW2 2014  * Sprint KW3-KW4 2014  * Sprint February 2014  * Sprint March 2014  * Sprint Quartal 2 2014  * Sprint Quartal 3 2014  * Sprint Quartal 4 2014    Many of the customers of //SEIBERT/MEDIA wish to have this feature in order to be able to plan the tasks continuously - as a rolling plan. I would love to have this feature as well. I really hope that you are going to implement it in the near future because it allows project manager to use the agile board even for the more """"classic"""" project management which still many of our customers love. ;-) Even for the Agile Planning this feature would bring a lot of advantages.    Please don't hesitate to contact me for any questions. I would really love to see this feature!     Cheers  <USER>",Suggestion,AgileBoard
70431,"""I as a Product Owner or <USER>want to see a report generated by Greenhopper/JIRA Agile which shows the projected completion for the current backlog items so that I can better understand what the overall project status is today.      Further thoughts:  - able to select which epics to include as """"total backlog""""  - calculation based on story points  - configurable average velocity (how many or which past sprints to consider for average)  - utilize average velocity to calculate projected completion  - show how many sprints until completed  - show optimistic and pessimistic line  - configurable optimistic and pessimistic calculation (percentage)      I'm doing this today in Excel based on data I'm extracting out of JIRA.  This is the most important chart for our project and helps everybody involved to better understand where we at and take actions early to influence the project for the better.    Attaching an image of the chart I'm creating in Excel at the moment.""",Suggestion,AgileBoard
70451,"""I find that one of the most useful tools for our software development team is the Version Report chart.  Since the addition of this one chart, we're able to much more accurately forecast the completion of a release and extremely accurately communicate the effect of scope creep on our completion date.  We have been able to determine when it's necessary to add resources to a project and communicate to upper management exactly why they need to make a priority decision on all open projects due to release schedules.    However, in order to do this, we currently create our own trend lines with a sheet of paper held up to the monitor or projector.  I've attached a screenshot that explains why this is necessary.    First off, being able to annotate this graph as I've done in this screenshot would prove invaluable for project status reports as well as """"lessons learned"""" post-project analysis.  I'm thinking GreenHopper stores as many points on the graph as is requested as well as an annotation for each.  These points are stored per version tracked in GreenHopper.    I feel like the """"project start"""" date should show the entire progression (graph) of a project.  However, there needs to be an option for a progression line start date separate from the project start date, as the June re-specification and reallocation of developers to the project requires a new velocity trend line.  I already did this in March, and have been considering doing it again, but if I do this, the new graph loses the historical nature as annotated in the screenshot.    Another cool idea is a """"scope creep trend line"""".  I just noticed today that even back all the way until early mid-april, our scope creep has been extremely linear.  This, combined with current (recent) velocity could possibly create a """"true"""" completion date where the two lines I drew intersect?    One thing I didn't show on the graph was the ability to speculate on an increased velocity over your current velocity.  This could have been used after the spec redesign in late June and the addition of more developers to the project.  I'm thinking a box on the right that says """"Speculate an additional X story points per week"""".  This would cause the velocity trend line slope to increase starting today, causing an upward kink in the velocity trend line with an adjusted final completion date.    I know this is a huge request, but I feel like this adds a significant amount of benefit to an already great feature.  These new features will hugely help us when communicating outside our group and committing to release dates.""",Suggestion,AgileBoard
70459,"""As an admin user     I would like the ability to restrict the """"Remove from Sprint"""" option to specific groups / roles    So that I can ensure the Product Owner is the only group / role that can remove issues from a sprint""",Suggestion,Board configuration
70471,"""As a <USER>and/or PM I want to be able to get easy access to filtering the Board backlog (Plan view) to fast and easily being able to drill down the options.    Possible alternative solutions:  - Being able to mark some quick filters as """"Planning filters"""" which does not make sense on the Work view  - Get access to doing custom filtering like JIRA's search view within the Plan view      Personally I would prefer second option as the needs for filtering always change depending on context and adding Quick filters in the middle of planning is not efficient. """,Suggestion,AgileBoard
70478,"""As a scrum master or engineering manager, I would like to understand what portion of my team's velocity is new features versus other types of estimated work.   --------    In our company, like many, we have not settled the debate on whether to estimate bugs with story points.  There are good reasons for both approaches and I'll leave it at that.    For the teams who choose to estimate bugs, it would be of great benefit to be able to configure the velocity chart to exclude bugs and other estimated issue types besides business stories in the computation of velocity.      Better yet, the fraction of a team's time devoted to bugs is a valuable metric, and a stacked bar chart showing the component issue types of any given sprint velocity would be incredibly valuable.""",Suggestion,AgileBoard
70511,"""The Kanban board has a link called release. Clicking this will take all items in the done column and assign a fixVersion and remove them off the Kanban board. It would help if you could select items for a release. This could be achieved by using checkboxes to select the items to release and thus as a Release Manager I can select which items I would like to include in a particular release.    There should also be a select all check box that I can select all items.""",Suggestion,AgileBoard
70538,"""As a manager, I only want issues entered against a version that will actually ship. However, some teams want to have intermediate milestones within a release (such as a sprint or drop) and use that on their Agile boards. I want some ability to define a milestone and use it in addition to the version (like the hierarchy of versions that was available in the Classic board, but not an actual version that would show up in the Version drop down).""",Suggestion,AgileBoard
70594,"""As a user, I would like to be able to Favorite boards. I envision this behavior being similar to favoriting Dashboards in JIRA.""",Suggestion,AgileBoard
70618,"""As a project administrator or board owner I need to be able to be able to add/remove Statused by using the """"Add Status"""" button from the board Configuration window.    Currently this button does appear only for <USER>administrators.""",Bug,AgileBoard
70727,"""As a project administrator and agile board owner, I want to force developers to choose a specific resolution value from a defined set upon entering a specified ticket status so that we don't forget to set the proper resolution so we may query tickets for duplicates, fixes, won't fixes, etc.    The team is using the GreenHopper simplified workflow for Jira ticket status.  It would be nice if between status changes from """"In Progress"""" to """"Ready"""" a pop-up appeared that allowed the developer to modify the Resolution from """"Incomplete"""" or """"Unresolved"""" to """"Fixed"""", """"Won't Fix"""", """"Duplicate"""", etc.  Right now, the workaround is to create a subscription to a filter that warns us when tickets are in the """"Ready"""" or """"Done"""" status, but have """"Unresolved"""" or """"Incomplete"""" as their resolution field value.""",Suggestion,AgileBoard
70741,"""As a user  I want to be able to abandon epics rather than marking them as done  so I can better differentiate work that was actually done, rather than just ideas.      Currently I have the ability to mark an epic as """"Won't Fix"""" through the JIRA ticket status, but there is no GH equivalent. I think it's important to be able to differentiate between stuff that was *really* done, and stuff that was abandoned as a nice idea.""",Suggestion,AgileBoard
70748,"""As a scrum master, I'd like to be able to mark issues that have been added after the sprint starts as part of the original scope. Very often, there are a few straggler tickets that come in shortly after I've clicked """"Start Sprint"""". Ideally there would be a way to have those not be marked as scope change. They really aren't and having them marked as such reduces the value of both the burn down chart and that marking in other places.""",Suggestion,AgileBoard
70761,"""As a product owner, I want to use the epic gadgets in a confluence page summary or <USER>dashboard summary to communicate the epic's overall status.     This helps to fulfill a need of showing overall status of a product roadmap and portfolio. """,Suggestion,Gadget
70764,"""As a scrum master, I need to be able to easily see how much work is assigned to each developer (or group of developers) in planning mode so that the team members can evaluate whether they have a reasonable amount or not.""",Suggestion,AgileBoard
71034,"""Using the Agile Board as a Scrum Project it will always show all the Epics of the whole project. The filter applied for Component or Version do not filter the Epics Issues.     But on the Classic Board, the Epics are filtered correctly.    I need to filter the Backlog on the Board showing only Epics from a specific version or component.""",Suggestion,AgileBoard
71035,"""We observed this behavior today for several developers in our production system and I was able to reproduce it in the staging system as well as for the latest JIRA + GH.     h3.Steps to reproduce:  # Prepare a SCRUM board for project 'DEMO'  # Create a new sprint and start it  # Remove 'Schedule Issues' permission for 'Developer' from permission scheme  # Login as a sample user with 'Developer' role for project DEMO  # Drag any issue from the backlog into the active sprint    h3.Outcome:    *Setup #1 (JIRA 5.1.5, GH 6.1.3.1)*  # Green box saying the issue has been added to the active sprint  # Red box saying permissions for ranking are missing  # Issue disappears from the backlog and after a refresh (F5) either:  #* appears in the active sprint  #* {color:red}still appears in the backlog <- In this case, opening the issue in the JIRA issue view, the Agile tab shows it in the active sprint (!!!){color}  # {color:red}The burndown does not show the scope change (\!){color}    *Setup #2 (JIRA 5.2.10, GH 6.2)*  # Green box saying the issue has been added to the active sprint  # Red box saying permissions for ranking are missing  # Issue disappears from the backlog but appears in the active sprint after a refresh (F5)    *Expectations:*  - I would expect that a regular developer cannot add an issue to a sprint since this is actually a way more severe operation that ranking it.  - The issue should not disappear from the backlog / sprint even if an error message is thrown  - The reporting seems to miss the scope change in some cases (I am still trying to reproduce this on GH 6.2 - I might upgrade our staging system using a copy of the production data)""",Bug,AgileBoard
71112,"""As a scrum master, I would like to be able to edit fields directly from the 'detail view' of the 'Plan' or 'Work' screens.    My team doesn't use a numeric system to score tasks, so I have a custom drop-down field with the possible options. This has a lot of downstream usability effects (I'll drop another feature request with some suggestions for that particular bee's nest), but the biggest pain-point we're having right now is that we have to open each issue individually to score it during sprint planning. I see that a few of the fields are editable now, but it would be awesome if we could allow or disallow editing on all of the fields instead of having a few hard-coded. """,Suggestion,AgileBoard
71179,"""As a user I'd like to have a quick filter for filtering issues by state: not started, in progress, done. It would be great to use the red, yellow, green labels in the top of the sprint: click on one of them for filtering, click again for removing the filter again. The labels should be buttons.""",Suggestion,AgileBoard
71180,"""As a greenhopper user in the agile board in plan mode I'd like to be able to filter or sort all issues by issue type. I know there is a search field but it doesn't work that smart as I expected. """,Suggestion,AgileBoard
71186,"""As a greenhopper administrator I would like to be able to configure which fields are displayed on the cards on the kanban rapid board, so that relevant information is visible without clicking on the tasks.     Specifically I would like to see the time tracking fields (original estimate, remaining estimate, etc). These are also impossible to add to the detail view on the rapid board. I would like to be able to add them there as well. """,Suggestion,AgileBoard
71190,"""For some reason """"Sample Sprint 2"""" has a nice green border when you go to drop an issue in there, but the """"Sample Sprint 3"""" doesn't get any sort of drop area queue, so it doesn't look like your action is going to succeed.  I'm not actually sure why one gets a drop area hint but the other one doesn't - it makes no sense at all.""",Bug,AgileBoard
71238,"""As a person who creates agile boards, I want a way to provide a description on the board to other viewers so they know what my intended purpose for the board is and they have access to any information I feel is relevant for them with regard to the board.""",Suggestion,AgileBoard
71244,"""Scenario:  Product Owner and team plan using story points, stories on the sprint are then broken into sub-tasks and completed over the sprint.    As a user   I want to see a burndown by issue count when estimating using Story Points  So that I can avoid having to use time tracking, and have a clearer picture of what my sprint is completing      I would anticipate that the current configuration view for the Estimation tab would allow;     Estimation Statistic: Story Points   Time Tracking: Issue Count      Note: """"Issue count"""" means anything that has a JIRA ID and specifically doesn't exclude child types like sub-task.""",Suggestion,AgileBoard
71246,"""As a product owner  I want to gauge how stale a point estimate is on a story  so I know if the team may need to revisit an old story on the backlog to keep it fresh      h3. Acceptance     * Can I specify time thresholds to indicate age brackets where a story point estimate is, for instance, """"fresh"""", """"stale"""", """"rotten""""    * Am I shown that a story point is stale within the Planning board?   * Am I shown that a story point is stale within a ticket view?   * Am I shown that a story point is stale when exporting tickets from JIRA?   * Does the age of the story point get updated when the value is changed?   * Can I """"freshen"""" an existing estimate, without changing its value, to indicate it continues to be correct?  """,Suggestion,AgileBoard
71330,"""As a project admin I would like to be able to """"split"""" an existing sprint on the planning board into multiple sprints to save time and effort when rescheduling lots of tickets.  Or in other words: I would like to be able to insert a new sprint in between two existing (planned) sprints.    At the moment we have quite a few sprints already planned for the future (sometimes > 10), but they are constantly refined up until the moment when we start the actual sprint. This leads to situations where we need to do some major re-prioritization or reshuffling of tickets.  Right now we can only manually achieve this by adding a new sprint at the end of the planning board, dragging all items from the previously last sprint to the newly created sprint, and then move the tickets from prior sprints one sprint down respectively. Also we need to manually update the names of all previously scheduled sprints accordingly.  In order to save time it would be great if we could automate this task by inserting new sprints in the middle or splitting existing sprints.""",Suggestion,AgileBoard
71335,"""As a scrum master i would like to see on the burndown chart the number of hours that the team is ahead or behind schedule in a sprint. A """"hours from guideline"""" would be nice. Could be in the mouse over event.""",Suggestion,AgileBoard
71508,"""Currently, there is no way to export, print the existing """"Backlog"""" of issues in a Scrum Board from within GreenHopper not committed to a sprint.  I would like the ability to export, and/or print the current ranked backlog in GreenHopper for multi-product owner review and ranking reconciliation (more than one product owner means they all can't move backlog items around with cancelling out their own moves part of the time).  I can envision a scenario where a drop down menu appears next to the word backlog, from which a user could select """"Export to Excel"""" (with existing ranking intact), or """"Export as a PDF"""" (with ranking intact), or even possibly """"Export as HTML"""" (once again, with ranking intact).""",Suggestion,AgileBoard
71633,"""As a scrum team member I want the priority of my stories to be reflected on the sprint board. The same way they are prioritized on the plan board.""",Suggestion,AgileBoard
71688,"""As a rapidboard user, I want to have different colors for the standard deviations. Especially on an projection screen, the current blue shades are not easily distinguishable.""",Suggestion,AgileBoard
71809,"""{{As an admin I'd like to set the default value for the Estimation field on new Greenhopper boards}}    As an admin I'd like to be able to set 'Original Estimate' (for example) as the default value for the Estimation to be used on every board created.    Currently the Estimation always use 'Story points' and we need to change the Estimation manually for each board created.    It would save a lot of time if that could be set as a default value on Greenhopper configuration so newly created boards are already with the desired value.""",Suggestion,AgileBoard
71941,"""As a scrum master I would like to be able to create a fixed set of subtasks that I can upload for a certain story, so that I can speed up the process of populating the scrum board when starting a sprint.""",Suggestion,AgileBoard
71959,"""As an admin, I'd like GH Rapid Boards (Scrum and Kanban) to support multiple workflows in the board's columns configuration mapping""",Suggestion,Board configuration
71968,"""As a team lead, I would like to enter the total team capacity for a sprint (the number of hours they have in total to work during the course of the sprint) and then see a line on the burndown that shows REMAINING TEAM CAPACITY based on total original time minus time logged on issues.    It's great that I can see the how much WORK is left, but I need to be able to compare it to how much TIME the team still has available to them.""",Suggestion,AgileBoard
72022,"""As a Kanban rapid board user, I would like to have the same approach offered by Scrum rapid board - plan view.  Basically, it would be great to be presnted with the same view as plan for scrum (epics on left, backlog center and detail on left) but without sprint/create sprint and so on, maybe replaced with a facility to move to """"in progress"""" status topmost backlog issues as we can do when we create a sprint in scrum mode.  """,Suggestion,AgileBoard
72192,"""As a techie user, I want to be able to quickly move issues to the top of my piles (sprint or backlog) or to quickly force rank across the entire pile with as few mouse clicks and drags as possible (I'm a developer - I hate mice!).""",Suggestion,AgileBoard
72281,"""As a JIRA Admin, I want to be able to use some custom status field as a base for columns separation. The system status field is used for all development tickets (Open, In Progress, In QA, Complete, etc).    Meanwhile, Epics have an additional set of statuses as they represent a Release (Piepelined, Approved, In Ideation, In Execution, Under Review, Closed). So I would like to be able to create a Release Board with columns based on those custom statuses.""",Suggestion,Board configuration
72390,"""1) One existing issue A with subtask(s) attached to ongoing Sprint backlog  2) Create new issue B  3) Add issue B to ongoing Sprint backlog  4) Change parent of subtask(s) with estimates from A to B  5) Burndown chart acts as if that was a scope change    I would think, that subtasks already assigned to the Sprint wouldn't change the scope? """,Bug,AgileBoard
72410,"""For agile boards, there exists a report mode. In this report mode I can see my open and my last finished sprint report. However, this is not enough.    As a product manager I need to be able to access and see more sprint reports, i.e. all for my current release.     It would be nice if I could configure how many old reports I'd like to see there or how many sprints back or how many sprints back based on a date/time difference from today in the past.    I then want to see those old sprint reports in the combobox, select and display them.      """,Suggestion,AgileBoard
72431,"""As a <USER>I want to automatically create a link between an issue and an Epic when I associate the issue to the Epic in order to have a stronger traceability.""",Suggestion,AgileBoard
72612,"""Have created Sprint but now I have no permission to do manage sprints:    # I've created one Board using only Project A (that I'm admin),  # I've created one """"Sprint 1"""" and put some issues inside,  # Then... I've changed the filter to include Project B (that I'm not admin),  # I put some more issues inside """"Sprint 1"""", from Project B,  # I want to manage Sprint (create more, complete, change dates) => No permission,  # So, once again I change permissions to include only Project A,  # I want to manage Sprint (create more, complete, change dates) => No permission.    Now I have created a Board and a """"Sprint 1"""" and can't do anything, so:    # I deleted the board,  # I've create a new board,  # The """"Sprint 1"""" was already there, and again, I can't change it.    I'm managing the work of a specialized team across different projects that I'm not administrator. For me, the administration permission requirement doesn't make any sense.""",Bug,AgileBoard
72715,"""As a <USER>I would like to have cumulative remaining hours on Rapid Board Work Mode, just like Classic Task Board.  This will be really helpful for a <USER>to determine how much each of team member is committed for the Sprint.    See attached screenshots for a better understanding:    !Classic Task Board.png|thumbnail!    Has the cumulative remaining hours per assignee as you can set on filters;    !Plan Mode - Rapid Board.png|thumbnail!    Has the cumulative remaining hours, but it is based on the Sprint and doesn't change if selected a Quick Filter;    !Work Mode - Rapid Board.png|thumbnail!    You can count manually but this would be a hard work if you have loads of issues per user.""",Suggestion,AgileBoard
72717,"""The scrum board has an instant filter in the plan mode, where you can just start typing reduce the amount of issues displayed. I would like to have this functionality available in the work mode as well, so it can also be used for kanban boards.""",Suggestion,AgileBoard
72768,"""As an administrator I'd like to configure the count of menu items displaying the recent boards in the Agile menu.    Reason: We are all using 5-10 boards all the time, however, just some of them are displayed in the Agile menu as menu items in section """"Recent Boards"""". In GH 6.0.0 it was easier to navigate to other boards with the combo box that popped up when I clicked on the board name. This disappeared in GH 6.0.3.    """,Suggestion,"Board configuration,AgileBoard"
72798,"""{{As an agile board user, I would like to export the reports into a common format to show my product owner, who doesn't have access to my JIRA, so that he/she can see the sprint progress.}}         It would be extremely useful to be able to export the many great JIRA Software reports into at least one common format (e.g. PDF, excel, etc.). Our project's product owner is not local to us at all and cannot access out JIRA instance on our closed network. The only means we have of doing this currently is via a browser """"Print or """"Print Screen"""". Both of these solutions require major browser window formatting in order to work and have it appear barely readable.""",Suggestion,AgileBoard
72911,"""As a team-member or product owner  I want to see issues in the ongoing sprint ordered according to priority  so that I know what issue to fix next.    When I view the current sprint in my """"Work-tab"""" (rapidBoard.jspa?...view=detail) I want to view all my items ordered with highest priority on top. This is true for issues without subtasks. If however, I have subtasks among the issues, those issues together with the subtasks are displayed on top. I cannot figure out their priority in relation to the other issues.  """,Suggestion,AgileBoard
72929,"""As a scrum master, I would like to restrict users who can add/remove a ticket from the sprint. Have a separate permission to enable/disable users who can change the rank of an issue.""",Suggestion,AgileBoard
73051,"""Several Scrum teams work according to the same sprint periods. All teams contribute to the same sprint results and finally the same release. So, Sprint definition wrt. name needs to be consistent across the teams. As an administrator I would like to define the sprint names centrally so that the individual team Product Owner can choose from the names of unreleased Sprints when going from Planning to Work Mode.""",Suggestion,AgileBoard
73086,"""So that I can do any filter I want on the Rapid Board/KanBan Board/Scrum Board,     As a user, I want to be able to type any JQL into a text box to filter the board issues on-the-fly,    Without being restricted to Quick Filters defined by the administrator.""",Suggestion,AgileBoard
73174,"""Original summary: {{As a Supporter, I would like to be able to sort attachments by Date Uploaded in Rapid View}}    When using GH as a Kanban board for support, I want to be able to sort attachments in Detail view so I can quickly identify the most recent attachments for the end user""",Suggestion,AgileBoard
73253,"""As a user i want to be able to group the many quick filters i have so that i can get a better overview instead of the rapid board wraps the quick filters and they end up taking many lines.""",Suggestion,AgileBoard
73282,"""{{As a ScrumMaster, I'd like to have BurnUp Charts available on the Rapid Board}}    When determining team velocity, I'd like to view statistics as a burn up chart, instead of a burndown chart.""",Suggestion,AgileBoard
73307,"""As a user I want to create Rapid Board columns that do not map to an existing workflow status. E.g. to get a story approved by product design.    I believe this should be possible by tracking the virtual status in another hidden custom field similar to the Sprint field.""",Suggestion,AgileBoard
73410,"""Given that we already force-rank stories in a Backlog.  And we estimate story points.  We should be able to provide a per-sprint, story point """"Velocity"""" and have GreenHopper automatically plan sprints with tickets by walking down the backlog.    As changes are made to the backlog, these sprints should be re-planned automatically, rather than requiring manual effort to move stories in and out of sprints.    To get an idea how this should work, look at Pivotal Tracker.  While their tool has a long way to go for Enterprise use, they've done a good job of making backlogs and sprints almost manage themselves.  The contents of sprints will change many times to adapt to changing requirements and circumstances, as reflected in a well-maintained backlog.      With all GH's additional data and configurability, GH could automate sprint planning even better than Pivotal Tracker does, reducing the effort required to just maintaining a solid backlog.    Further, once there has been some progress down a backlog, it is possible for GreenHopper to calculate our velocity empirically, replacing our manual """"default"""" velocity with a more justifiable, data-driven number.    I had to write a Jira plugin to provide us with Velocity, Iteration Views, and Release prediction, but I'm not asking for any of that for now.  It just seems silly to have to manually drag stories around to """"create"""" sprints.      Sprints will occur in reality, whether we construct them in GreenHopper or not.  Let's make it so those sprints are both planned and tracked automatically, just by having a backlog and burning it down.  :)  """,Suggestion,AgileBoard
73489,"""1. Open a rapid board, planning view  2. Click on any item in the list of items  3. The """"pre-view"""" of the cliecked item is displayed to the right, but it's not possible to configure what fields to include in this display.    As we have some really important infromation in custom fields (e.g. the actual """"user story"""" text 'As a Player I want to...'), we currently need to open all issues in new tabs anyway during planning - it's sufficient to only use the rapid board planning view.""",Suggestion,AgileBoard
73546,"""Was {{As a user, I would like compact cards on the Rapid Board}} but broadening to cover requests for larger cards too.  ---  As a user, I cannot fit all our Rapid Board cards on my oversized screen.  Moreover, my colleagues dislike scrolling with a passion!  Please may I request compact cards for the Rapid Board.  It's the only feature that's preventing our development teams from switching from the Task Board.""",Suggestion,AgileBoard
73627,"""There are two variations encountered (see below for a different perspective):    * Cloning an issue part of an already finished sprint yields the clone to be considered part of that finished sprint, i.e. the clone shows the originating sprint within _Completed Sprints_ in the _Agile_ panel, which is obviously rather irritating already.  ** (!) Furthermore it is included in the sprint retrospective report as well of course, which actually screws up reporting with wrong data, thus can be considered a major bug in this regard.    * Cloning an issue part of an active sprint yields the clone to be considered part of that active sprint as well, i.e. the clone ends up in the _To Do_ column on the Rapid Board, which I consider not to be the proper default at least (see below).  ** (!) It is definitely a bug in the current GreenHopper version though, insofar I'm unable to edit the clone in this regard, i.e. it is impossible to remove an issue from a sprint (which would be a helpful option to have anyway of course, if only to fix accidental additions).    (?) However, a different viewpoint and/or use case might ask for exactly this behavior, which would ask for a respective option 'Clone Sprint' similar to 'Clone Links'. Options are best avoided of course, as long as a reasonable default is provided; in this regard I 'd consider it more appropriate to not include the sprint in the clone operation, because it is pretty easy to move the issue into the sprint later on (usually there are several edits to be done anyway).""",Bug,AgileBoard
73648,"""As a RapidBoardUser I want to be able to show the sum of all issues remainingestimate, so I can see which workload I have.  COS: - Not show if equal - Only when tracking is enabled for a board""",Suggestion,AgileBoard
73707,"""As a Rapid Board user I would like to be able to see sub-tasks of top level issue types if they are not included in the filter my rapid board is based on.    The main problem being addressed here is that it is not possible to have a filter which includes user stories, but not their tasks and still have the tasks shown on the rapid board.  A actual example of when this is required is the following. A company has X number of development teams that each have a rapid board. Each user story has a 'team' field which is tagged with the team currently working on the user story. The filter the rapid board is based could look something like 'team = """"teamname""""'. This filter would result in X amount of user stories being listed. With the current solution it isn't possible to list the subtasks of those user stories.    With the planning board this was not an issue as it would filter on fixVersion, which would include the subtasks.     A workaround for this would be to simply tag the subtasks with the same team as the parent. While this is entirely possible, the hierarchy would break as soon as the team is changed on the parent.    Alternatively the JQL could be more advanced and support queries such as """"issuetype = Task AND parent.team='teamname'""""""",Suggestion,AgileBoard
73920,"""{{As a Rapid Board user, I would like to use Quick Filters on the burndown chart}}    Quick filters make it really easy to see which tasks are left for specific teams working on a sprint. I would like to be able to see a burndown chart just for that team in a similar way.    Similarly, I could use a quick filter to show my personal burndown chart, or the burndown chart for a given component/priority etc.""",Suggestion,AgileBoard
74024,"""I want a quick filter on the rapid board which allows me to filter by an assignee of my choice. Currently there is only a quick filter for issues assigned to the current user. As a project manager or lead, it is useful to be able to easily see what work is assigned to who.    Instead of a simple button, the user would be presented with a dropdown list or a text field.    Swimlanes could be used for this, but I want to use swimlanes for other purposes while filtering by assignee.    More generally, it would be useful to be able to define quick filters with a variable parameter. That would solve this case and probably others.    {panel:title=Please Note}  This issue refers specifically to having a select style quick filter where the options are a list of existing values, in this case the assignee    This issue is minor because most teams can achieve the same outcome today by using creating JQL quickfilters of the form """"assignee = <username>""""  {panel}""",Suggestion,AgileBoard
74198,"""As a Rapid Board admin - I want to be able select which vertical column my horizontal swim lanes should span across - So that workflow status can represented in both horizontal and vertical planes on a Rapid Board.       Comments: The motivation for having this functionality, is a highly configurable Rapid Board (Kanban board), using the most of 'white space' on the board it self. A typical scenario for use Rapid Board is on a big screen or projector in a project area. Having this will allow you to display more of the actual workflow on one screen, regardless of workflow complexity. Attached is a simple visual representation of what I'm looking for.      """,Suggestion,AgileBoard
74247,"""As an agile user, I would like to burn down story points similar to how I burn down hours - on a daily basis. The current story point burndown only calculates completed issues - I would like to track points in a similar fashion to hours on an issue.""",Suggestion,AgileBoard
74366,"""As a user, I would like to see how many issues match each status in the rapid view I am currently configuring, so I can make more informed decisions about which statuses should be mapped to which columns""",Suggestion,AgileBoard
74451,"""These look powerful and I'd be interested in pulling them into Confluence as a gadget.""",Suggestion,AgileBoard
74473,"""As a GH administrator  I'd like to be able to specify issue count contraints per column and/or per swimlane    For example, I have 3 columns:    TODO, IN PROGRESS, DONE    and two swimlanes:    Expedite  Everything Else    I'd like an explicit max of 1 issue for TODO/Expedite, max of 5 for TODO/Everything Else    This could be taken further by having swimlane constraints override column constraints (so we can only have 1 expedite issue on the board at any time)""",Suggestion,AgileBoard
74590,"""As a RB user I would like to set WIP limits for swim lanes""",Suggestion,AgileBoard
74828,"""As a JIRA Administrator,  I want to have the colours that I customized on the Cumulative Flow Diagram in the Chart Board show up in the dashboard gadgets,  So that the team can tell what the colours mean and have a seamless Cumulative Flow experience.    Repro:  # Customize your Greenhopper Cumulative Flow colours by click the colours in the legend and setting them (see attached).  # Add a cumulative flow gadget to your dashboard    Expected:  Colours follow what you just set up on the chard board.    Actual:  Colours are some weird, random, hard to fathom set of default colours.""",Suggestion,Gadget
74880,"""As of GreenHopper 5.4.1, the Agile boards are scoped to a single project at a time. As an assignee, I want to see all my issues across all projects.    One way to address this story would be filtering the issues on an Agile view by a JQL filter and removing the Project scope limitation. See GHS-1849.""",Suggestion,AgileBoard
75455,"""As a greenhopper user I want to be able to configure the card view gadget by project or filters rather than just project and version""",Suggestion,Gadget
75485,"""Hi,    currently when I change column constraints for one project, it is valid for all <USER>project. But of course project constraints could be different depending on several factors (team size...)    So, as a GH Admin I would like to be able to configure column constraints per projet. """,Bug,Board configuration
75660,"""I was editing the iframe status by going Tools > Configuration and then toggling the iframe setting.    I did not realize there was another personal setting through Tools > User Preferences that would then override the Project setting. This functionality makes sense to me now that I think about it, though it might be nice to include a note in the Tools > Configuration explanation of the iframe setting.    I would change it from:    """"Use JIRA IFrame: Enables the modal JIRA standard Issue view popup.""""    to    """"Use JIRA IFrame: Enables the modal JIRA standard Issue view popup for the selected project. Note: this setting can be overridden by individual users User Preferences settings for this project.""""""",Suggestion,"Board configuration,Documentation"
76283,"""Every Jira project has a list of project members, I want my assignee board to be able to be configured to just show those assignees or else there is just too many assignees diplayed on the board to be useful.  Even better, I'd like to be able to filter the assignee board by roles, so I can create various roles within assignees for """"Designers"""", """"QA"""", etc. and get an idea how those sub teams are allocated (not as important a request as the 1st one)""",Suggestion,AgileBoard
77010,"""As a JIRA administrator, I want to be able to disable priority values ​​in priority schemes in order to keep values ​​for old tickets and use new ones for new tickets. It would be great to have this functionality similar to disabling values ​​for select list custom field.    """,Suggestion,System Administration
77091,"""As a Jira admin, I'd like to have the ability to disable specific reports like """"Epic Report"""" or """"Control Chart"""" under the _Reports_ menu (_Project sidebar > Reports_).""",Suggestion,Dashboards & Reports - Reports
77128,"""Hi team,     I know Jira keep inactive status if user reported, been assigned, commented, project lead, component lead.          As a user I want to a configuration option in user directory connection. Maybe, while connecting to AD, Crowd If checked the button, Jira always keep the user even if not reported, been assigned, commented ...""",Suggestion,User Management
77156,"""As a product manager I want that my team members can create, share and edit project dashboards and filters, that last for the time of the project (my product), because currently every time a team member leaves the project or even company and his account is removed, he takes his created dashboards and filters with him and the remaining team members need to recreate in some of their accounts the dashboard and filters, which is a lot of expensive work.         So my suggestion would be to create project related dashboards and filters, that can be maintained by users of a certain project related right. Those dashboards and filters have than a live cycle along the project and not the user account.""",Suggestion,Project Administration
77216,"""Provide the ability to have the ‘Add User’ button removed in Jira so that project-admins cannot just add users to their projects outside of our formal entitlements request process that involves Crowd and other internal tools.    We have been discussing internally our risk item where Jira by default has the ‘add-user’ button which in our case allows project-admins to add users to their project outside of our internal Horizon access request process.     or review – internally here what we are doing as a ‘compensating control’, is that we are hiding the button and also have a cleanup script that removes permissions that were added directly in Jira outside of the formal request process (permissions in Jira that are not in Crowd).    Our risk team has asked a couple of questions that I need your help with answering which are the following:   # Do you know the official RFE (request-for-enhancement) ticket for this specific ask from us regarding the Jira add-user button?   # Can you please open an almost identical RFE ticket for Confluence for removing/disabling the ‘Edit-Permissions’ button in Confluence that provides Space-Owners a similar ability to add permissions outside of our formal Horizon access request process?""",Suggestion,Project Administration
77237,"""Hi recently we have set up a mirror for Bitbucket but we are finding many issues that makes it not usable. Please find the list below.   * When we setup a mirror we would like to do all the automation also using it. Not half automation code to use mirror and half to use server. What is happening is mirror supports only few actions like clone etc. But rest api does not work on mirror.    ** For example in code we will be cloning a repo from mirror but when i want to deploy and download a raw yaml file for download we will have to use main server. Rather than confusing users we have to tell them to use only server for all automation   * For mirror to work both main server url and mirror url has to be accessible to end user. For some scenarios if user only has access to mirror or only access to server then most of the mirroring features will not work""",Suggestion,ADGS Design changes
77260,"""As a user I need to query the status from JIRA as it was on a specific date. This comes in handy for instance when preparing reports and meeting notes that require a snapshot of a search in JIRA without having to work this around by taking a screenshot and pasting it as an image.""",Suggestion,JQL
77282,"""As a user I want a quick way to link issues without having to pull my hands from the keyboard, mouse over to a menu option and have to scroll through all the options to find """"link issues."""" I want to be able to use a hotkey / keyboard shortcut to immediately access the functionality.    By comparison, I rarely if ever add a label, but I regularly link issue. Why would we have a hotkey for labels and not for links?""",Suggestion,Issue
77283,"""As a Jira administrator, I would like to have the possibility to setup a time limit for loading a Board, to prevent their impact on  performance. When the chosen time will be reached. the board should stop loading and a message is displayed to the user asking him to contact Jira administrator to optimizing the loading time of the board """,Suggestion,Project
77379,"""As a Jira admin, I'd like to be able to monitor the integration status between Jira and GitHub through the Jira UI, especially on cases where the sync process may be stuck. It is possible for the DVCS syncing process with GitHub to get stuck without any errors being thrown in the log, as the sync itself is still trying to finish. Knowing the status of syncing would make it easier to determine if there is an error and where it is""",Suggestion,Application Links
77441,"""As a Jira user writing automation rules    I want a rule validated prior to save    So I can create automation more effectively         Acceptance criteria:   * At save, the smart value syntax is validated. Errors are shown next to the rule components.   * At save, the rule components are validated for missing parts. Errors are shown next to the rule components.   * Bonus: where smart values can be entered, auto-completion is provided to reduce entry errors.   * Bonus: where null values can short-circuit smart values, warnings are shown     """,Suggestion,Project Administration
77453,"""As a Scrum Master putting together data for the Sprint Review, I want to be able to show a burndown that shows the burndown of initial scope along with the burndown of scope creep, so that when my team finishes everything they committed to at the start of the Sprint and they Bring more work in, it communicates that we've not only met our Sprint Commitments but met them.    The chart then would show the idealized line and then the burndown of the work that was present at the start of the Sprint. When Items are brought into the Sprint, instead of a single burndown line going up, it branches and moves upward (this line would track the increase/completion of items outside of the scope of the Sprint commitment). If all outside of scope items are finished, the line merges with the burndown and goes away.""",Suggestion,Dashboards & Reports - Reports
77455,"""As a Jira Administrator, I would like to prevent issues from being included in an Epic if they are in a particular status (such as *In Progress*).  """,Suggestion,"Issue,Admin"
77461,"""As a Jira Data Center user, I want to be able to filter my *Active Sprint* and *Kanban* boards in the same way I can filter my *Backlog*, by using the filtering text box currently only available in the upper left corner of the backlog. That filtering text box should also be available on *Active Sprint* and *Kanban* boards. """,Suggestion,Project
77465,"""As a typical scrum master, project manager, team lead, I would like the ability to filter the results of my query in a number of ways related to Changelog History data so that I can gather and report on relevant change events, as well as develop metrics to measure and improve.    Specifically I need the ability to use JQL to filter issues by changelog history events in the following ways:   * Changes to a specific field or list of fields (both system and custom fields; could be a single field or a comma-separated list of fields)   * Changes made to a specific field match or are similar to <text>, e.g. ~""""<text>""""   * Changes made to a specific field equal an option or comma-separated list of options for that field   * Changes made to a specific field from one option to another option, e.g. from <option A> to <option B>   * Changes made within certain time frames, e.g.: -1w, EndOfMonth(), before or after <date>, between date range, etc.""",Suggestion,JQL
77466,"""As a typical developer, scripter, or user of ETL (extract, transform, load) tools, I need to ability to include parameters specific to Changelog History data in the REST API call so that I can limit the volume of JSON data returned.    We are a large enterprise using the REST API to pull Jira data into Alteryx to perform ETL functions in preparation for loading the transformed data into Tableau for reporting. Currently there is a large volume of Changelog History data returned when using """"...?expand=changelog"""". Quite often the vast majority of changelog data is not needed or wanted, so we need the ability to limit what changelog data is returned in the following ways:   * Changes to a specific field or list of fields (both system and custom fields; could be a single field or a comma-separated list of fields)   * Changes made to a specific field match or are similar to <text>, e.g. ~""""<text>""""   * Changes made to a specific field equal an option or comma-separated list of options for that field   * Changes made to a specific field from one option to another option, e.g. from <option A> to <option B>   * Changes made within certain time frames, e.g.: -1w, EndOfMonth(), before or after <date>, between date range, etc.""",Suggestion,REST
77478,"""As a Jira User    I would like to bulk restore archived issues (based on filter criteria in  Archived Issues view)    So that I can easily restore archived items """,Suggestion,Bulk Operations
77518,"""*As a* JIRA Core user    *I want* a means of seeing and managing all the 'children' under a 'parent' issue type in one place. This exists for Epic issue types via the 'Epic Link' field but does not exist for Initiative or Feature issue types, which seems fundamentally flawed.    *So that* the issue hierarchy for a project is completely transparent and reporting of the same is vastly improved.    **SEE COMMENTS UNDER JRASERVER-47584, WHICH PRESENT GREATER ARGUMENT TO SUPPORT THIS CHANGE**""",Suggestion,"Issue,Admin,Admin,Dashboards & Reports - Reports,Project"
77532,"""As a member of the UX team, we work closely with dev teams often. Because we're on separate teams, we manage tasks on separate Jira boards even though cards flow through parallel processes.     As a member of UX team alongside a dev team, I'd like to move stories to the UX Jira board so we can manage and track the story before handing it back to development.    Currently, cloning a matter exists but it's not directional; the card appears below the cloned Jira card. Instead, I want to be able to clone a Jira Card specifically to a different art board.    It would be awesome if I could track that same Jira card between both boards from one, but baby steps.    I also want to do this in bulk.""",Suggestion,"Documentation,Bulk Operations"
77601,"""As a Jira Administrator, I want to limit the fields that users are able to Bulk Update so that I can preserve data integrity of calculated, read-only, or other system-generated fields.    We have several system-generated fields that are """"read-only"""" for users, and others that we don't want users to be able to modify once an issue reaches a certain state. Bulk Update allows users to bypass the controls we have implemented and edit any field value. We would like the ability to restrict the list of fields available in the Bulk Update operation.    (Similar to JRACLOUD-34281)""",Suggestion,"Admin,Bulk Operations"
77701,"""As a Jira admin, I would like to have the ability to look for exact matches in the summary field using the equals operator (*=*). Currently, the (*~*) operator, will provide partial matches, but that is not enough for certain operations, especially when combining Jira with the additional functionalities offered by Automation for Jira.""",Suggestion,JQL
77783,"""As an analyst, I would like control the Jira project key prefix so that I can more easily see what type of issue it is.         For example in one project, PROJA-### and PROJB-### keys are allowable.""",Suggestion,Project
77801,"""As a user I would like an action to be able to delete a specific worklog (from a specifc author at a specific date) in Automation for JIRA.     Today I can only add a worklog. """,Suggestion,Project Administration
77915,"""The mobile app doesn't display the card as yellow or add a flag to the card. Unless a comment was added with the flag you cannot see which issues are flagged, and even so that requires clicking through the card.         I'm trying to convince my teams to stop using a """"Blocked"""" status and to use the flagging functionality instead. This is going to hurt my case.""",Suggestion,Mobile
77979,"""I'm using Kanban on a project with multiple deliverables. We don't use versions for anything. Our team's issues collected in the """"done"""" column forever. The team thought this was weird, since if they're """"done"""", why aren't they disappearing?    The kanban board sub-filter is the answer. The default behaviour for kanban is to show issues with no `fixVersion`. Since we don't use those, they'll collect there forever.    (Aside: what would it mean to """"release"""" a project with no versions? Does that make sense?)    The other weird thing about the filter is its mental model. The correct model: things that match the query will be *visible* on the board; everything else will be *hidden* after a period of time. The problem with this, however, is the context in which the filter is changed. If you're like my team, you're going to change it when your """"done"""" column gets heavy... which means you're going there to *hide* issues, not *show* them. As a result the query can seem a bit nonsensical at first -- I accidentally made everything show up as """"done"""" the first time.    I think one of two things could be done here:    * the description of the kanban sub-filter could be updated to more clearly state that it makes issues visible, and anything that *doesn't* match it will get hidden.  * the field's purpose could be inverted; it would be about selecting issues that are """"done"""". This shift can drastically simplify the mental model for end-users (no double-negatives to think about; you're selecting a discrete set intersection, rather than defining two sets and taking the difference).""",Suggestion,Project
77988,"""As an admin we would like to have the option to manage the """"Filter and Dashboard Sharing"""" settings for all users. Instead of digging around the database to get the required data.     Sort like the Global permissions page but the settings are shown in     * !43211.png|thumbnail!     I was able to get the data using the SQL query shown in the picture attached. However, I would like the option to control settings like this from the UI.     * !12333.png|thumbnail!   """,Suggestion,"Dashboard & Gadgets,Filters & Dashboards - Permissions"
77997,"""As a Jira Admin, I'd like to see the 'Print Cards' option under the 'Board' menu dropdown enhanced. Right now the choices are 'Small, Medium, and Large'. It would be great if there was an option (also) for placing cards into standard Avery Label templates. I would think a relatively small number of these would suffice.""",Suggestion,Project
78044,"""In Jira Cloud, when you link an issue in another issue comments, it shows next to the issue key the summary and the issue status.    !Issue status.png!    As a user, I'd like to have the same functionality in Jira on-premise (Server/Data Center).""",Suggestion,Issue
78121,"""{panel}  *I'm running Jira Server 7.2 ATM.*    *If this is possible in newer versions please let me know. I haven't found that information.*  {panel}       Much in the same way the other time functions like startOfDay() can be given a parameter it would be very nice to be able to use *now(-x)*.     Use case:   * I want to set up an """"issue collector"""" that avery certain ammount of time returns every created issue since in the last specified ammount of time that also matches a series of conditions.    Let's say:   * On monday mornings notifies of every matching issue created since I last left office. That would be automate a task executing every monday at 8h with the following JQL as a filter *""""createdDate >= now(-2d 14h) AND _extra conditions_""""*   * During office hours execute an automate task every half hour with the following JQL as a filter *""""createdDate >= now(-30m) AND _extra conditions_""""*   * From tuesday to friday notifies of every matching issue created since last day. That would be automate a task executing everyday at 8h with the following JQL as a filter *""""createdDate >= now(-14h) AND _extra conditions_""""*     """,Suggestion,JQL
78191,"""As a Jira admin ,  I want to limit the creation of issues types (for users) in relation to user groups without using plugins .""",Suggestion,Admin
78211,"""As a Jira administrator, I would like Jira to generate independent issue keys for different issue types, for instance, if we consider Project A and the task, story and epic issue types:    The first task created would be PATASK-1, the second PATASK-2, and so on. However, the enumeration should be independent for each issue type, so when the first epic created would still be PAEPIC-1, and the first story created would be PASTORY-1.    That change would be highly beneficial as it would allow my teams to identify an issue's type by just looking at its key.""",Suggestion,"Issue,Admin,Project"
78212,"""In very big Jira implementations, enabling Webhooks on """"Update tickets"""" events could kill Jira's performance, because the JQLs engine starts to be flooded with tons of queries.         As a user, I would like to narrow the webhook scope before executing any JQL with some fixed parameters, like issue type list, project lists, the priorities...     """,Suggestion,Webhooks
78298,"""As an administrator, I would like to have a parameter for this endpoint to only return audit records for a specific project.""",Suggestion,REST
78343,"""h1. User Story    *AS A* Jira ticket creator    *I NEED* to be able to see what has been updated in history    *SO THAT* updates are easy to identify (via some form of visual identification of modified text.  h1. Acceptance Criteria  h2. Questions""",Suggestion,Issue
78353,"""We have the possibility to flag the issues. However, in our company it happens quite often that the stuff is getting flagged, and then nobody feels responsible for it anymore.    In order to at least make this problem visible, I would like to have the data at which it got flagged available as a field (so that I can also show it on the board, or easily find it in the issue details).    At the moment I can only find this information in the history of the issue. Which can be very time consuming, depending on the history. And also, it cannot be used as a kind of """"visual trigger"""" during the Daily Standup Meetings.""",Suggestion,Issue
78385,"""As a project administrator, I would like to set up default """"Configure Fields"""" settings at the project level to hide any optional fields, while still allowing changes to these settings at the user level.""",Suggestion,Admin
78439,"""As a Jira user who needs to create a lot of filters and grant Editor permissions to the same set of users, I'd like to count with a user preference for my new filters to be automatically editable by my selected option (Group, Project or User), so that I don't have to manually add the same editors over and over again.    There's a similar preference for auto-sharing filters (well, just with _public_ or _private_ as selectable options).""",Suggestion,User Management
78440,"""Currently, it is possible to link a Confluence page to a Jira issue through the usage of application links. This link will tell users that there is a page linked to the issue, and to access the page's contents you have to click on the link and access Confluence. However, as a Jira admin, I would like to be able to directly display the contents of a Confluence page in the *View Issue* page.""",Suggestion,Issue
78451,"""As a Jira project administrator, I want to edit all workflows, screens, fields, schemas, etc even when these entities are shared across projects in case I have admin permission in ALL that projects    Scenario 1.    Given I'm a project admin in project Pa & Pb    And workflow Wa is shared between tickets in projects Pa and Pb    When I click on edit workflow Wa    Then I see admin controls to edit workflow      Scenario 2.    Given I'm a project admin in project Pa and don't have admin permissions in project Pb    And workflow Wa is shared between tickets in projects Pa and Pb    When I click on edit workflow Wa    Then I see a note that Jira admin helps is required to modify workflow""",Suggestion,Project Administration
78491,"""h3. Problem Definition  Upgrading UPM  triggers all apps/applications to restart because of the licensing API provided by UPM.    h3. Suggested Solution  As a Jira Server admin I want to update UPM, but I don't want all other apps/applications to depend on UPM.""",Suggestion,UPM (Universal Plugin Manager)
78492,"""h3. Problem Definition  Apps downtime during the app upgrade process in Jira Server ends with users not being able to use the app.    h3. Suggested Solution  As a Jira Server admin I would like apps/applications/plugins updates to never affect my end users work.  Apps upgrades cause no downtime for the app end users in Jira Server.""",Suggestion,UPM (Universal Plugin Manager)
78496,"""h3. Problem Definition  In order to know what are the implications of hitting the _""""Update""""_ button (precisely I would like to know how many plugins/apps/application will be temporary disabled).    h3. Suggested Solution  As a Jira Server admin I would like to know, what will be the impact of updating an app in Jira before I hit app/application """"Update"""" button.""",Suggestion,UPM (Universal Plugin Manager)
78502,"""As a system admin    I would like to have the creation date associated with the user    So that I can pull metrics on my user community over a specific period of time    e.g. user community growth for the past year""",Suggestion,REST
78535,"""Once you run a JQL search for issues, you can choose to display the """"Parent Link"""" column, which will display the link between initiatives and epics, for instance.    !Screen Shot 2019-09-04 at 09.55.48.png|thumbnail!     As a Jira admin, I would like to search for all Initiative issues from a project and be able to obtain their child links in a search query result.    Currently, """"Parent Link"""" is available as a column to include in the search result. However, """"Child Link"""" is not available to be added as a result column.""",Suggestion,JQL
78546,"""h3. Problem Definition    As an administrator, I want to see projects that have not been updated from 6 to 12 months so that I can archive or delete them.  h3. Suggested Solution    NA  h3. Workaround    Check when issues in a project were last updated.  {noformat}  select id, pname from Project;  select Max(updated) FROM jiraissue where project IN (<project_id>);  {noformat}""",Suggestion,Project Administration
78677,"""h3. Issue Summary    When setting a property through {{BaseEntityPropertyService#setProperty(final ApplicationUser user, @Nonnull final SetPropertyValidationResult propertyValidationResult)}} and skipping the security check, it's not possible to do so without getting an error (You do not have permission to browse issues in this project) because there’s no way to pass EntityPropertyOptions    h3. Environment  * Code base can be seen in {{master}}, I'm guessing it's 8.2 or 8.3    h3. Steps to Reproduce  From the app developer  {quote}  If you want to set a property and skip the security check it is not possible to do so without getting an error. The problem here is not the null user value itself but the fact that setProperty() has no way to pass EntityPropertyOptions).    Here is the problem:    Setting a property is a two-step process:    # Call validateSetProperty() with property input (skip security check possible - method takes EntityPropertyOptions)  # Call setProperty() - if input is valid (skip security check NOT possible, because *method implementation does not take EntityPropertyOptions and ends with a call to getProperty() version without EntityPropertyOptions)    Because step 2 has no option to pass EntityPropertyOptions to skip the security check trying to set a property without security checks will always fail (Error will be: You do not have permission to browse issues in this project.)  {quote}    h3. Notes  * Issue originally raised in Developer Support.  * Discussion added as an internal comment    h3. Workaround  From app developer:  {quote}  We simply ignoring the permission error - however, this is not good because we cannot tell the user if there is actually a problem with the set property operation in our plugin upgrade task  {quote}""",Bug,Java API
78699,"""When adding a User Picker field to a JQL column, as a user I'd like to be able to display both user names and user's full name (from the user profile).""",Suggestion,JQL
78722,"""We have the following use case:    We are Using Jira and have 2 Confluence instances (A and B) linked with application links. Now in the Jira activity stream the activity of both Confluence instances are shown. As an Administrator I want to disable the activity stream for a specific linked application (confluence instance B) for all users. Because in this Confluence instance B nobody has a user and when a user creates a dashboard with activity stream gadget in JIRA, the activity of Confluence B is always active.    With this active activity the performance of Jira is worse. Everytime a user opens a dashboard with activity stream active Confluence B activity in Filters, the activity stream gadgets need up to 15 seconds to load. When the user disable the Confluence B activity in Filters, the gadget loads in 3 seconds.""",Suggestion,"Dashboard & Gadgets,Application Links"
78727,"""As a project manager, I would like to have the possibility to show multiple projects on a roadmap. This gives me the possibility to consider their progress and releases and dependencies together with their priorities at the same time.""",Suggestion,Project Administration
78787,"""h3. Summary  As a Jira user I would like to be able to select specific fields to export in the json exporter for both issue and search results.     """,Suggestion,Jira Importers Plugin
78824,"""First off: I prefer to use the text mode and I would protest if it wasn't there. It's a shame that Atlassian hasn't implemented Markdown natively as part of Jira and that there seems to be no _official_ plugin supporting it. Markdown is one of the reasons I love Bitbucket.org.    Anyway, given that we as users are stuck with the Confluence Wiki Markup (as it was at least called in the past), I am running into problems time and time again. This ticket is about _that_.    I am using all kinds of programming and markup languages and with Jira I am struggling to format something as simple as (in words) """"backslash, 'usepackage', opening brace, closing brace"""" inline. Most of the problems occur when attempting to format something inline, even when something like {{\{noformat\}}} (<- the rendered HTML suggests that the braces are not even part of the <tt> element created by the double-braces) will work.    Case in point:    {noformat}  \usepackage{}  {noformat}    Now let's try to make this properly render inline with Confluence Wiki Syntax:    h2. First attempt    Trying to escape the """"special"""" characters:    {noformat}  {{\\usepackage\{\}}}  {noformat}    ... even though the current documentation doesn't even mention the backslash any longer.    Outcome: {{\\usepackage\{\}}}    NB: that line break after """"Outcome:"""" above isn't from me.    h2. Second attempt    Adding a leading backslash:    {noformat}  {{\\\usepackage\{\}}}  {noformat}    Outcome: {{\\\usepackage\{\}}}    h2. Third attempt    ... let's at least get those braces to show up somehow:    {noformat}  {{\\\usepackage{{}}}}  {noformat}    Outcome: {{\\\usepackage{{}}}}    ... argh. Now the closing brace is no longer part of the monospaced text.    h2. Fourth attempt    ... maybe by escaping the closing braces somehow?    {noformat}  {{\\\usepackage{{\}}}  {noformat}    Outcome: {{\\\usepackage{{\}}}    ... argh.    h2. Summary    ... can we agree that this appears to be an arbitrary/unpredictable outcome? I _really_ hope someone at Atlassian can see how frustrating this user experience is. Since the problem still exists with the Jira version used for this tracker, I assume I needn't give a Jira version. But if it matters: {{v8.1.0#801000-sha1:2e1cd1b}}.    I've been fighting these issues (on and off) since Jira 5.x. And it's not just LaTeX code which cannot be properly formatted inline (which is really a must-have inside bullet lists), it also affects several constructs in C and C++, though I don't remember the issues from the top of my head (and cannot look them up as I don't have access to that Jira instance any longer).    Any chance that the technical audience (as opposed to the mouse-pusher faction) can see that fixed? How about supporting MarkDown as a first-class-citizen in the Jira world, for example?    btw: this is an in-house instance of Jira. Hope this was the correct project I picked. If not, please move the ticket around.""",Suggestion,Issue
78887,"""h3. Issue Summary    When a node is suddenly terminated (ie not shutdown gracefully), {{ClusterManager}} API still return the terminated node as active    h3. Background    From the app developer  {quote}  I have 2 node cluster of JIRA 7.9.0 (node1, node2). Each node is a docker. When I turn off node2 by stopping the docker container the other node still sees second node as active.    I've checked that in {{clusternode}} table node2 is active. I've checked that in {{clusternodeheartbeat}} table beat data are not updated when node2 is down.    The way I am getting node data:  {code}  clusterManager.findLiveNodes();  return clusterManager.getAllNodes();  {code}    When instead of killing node2 I shut it down gracefully (/etc/init.d/jira stop), then node2 status in {{clusternode}} table is updated to OFFLINE.    Jira should make use of {{clusternodehearbeat}} table to determine whether node is OFFLINE    Tables data after node2 was terminated - not gracefully  2.1) clusternode  {code}  node_id,node_state,timestamp,ip,cache_listener_port,node_build_number,node_version  node1,ACTIVE,1559562446911,30b2e7efd175,40001,79000,7.9.0  node2,ACTIVE,1559834524044,bb7ae1186aa1,40001,79000,7.9.0  {code}  2.2) clusternodeheartbeat  {code}  node_id,heartbeat_time,database_time  node1,1560462378461,1560462378462  node2,1559852296782,1559852296782  {code}  3) I killed a note to simulate a failure in DC cluster. In my case I stopped a docker on which Tomcat with Jira node was placed.  {quote}    h3. Environment  7.9.0    h3. Steps to Reproduce  # Have 2 nodes  # Terminate node 2 instead of shutting down gracefully    h3. Expected Results  When node2 is down other nodes should see it as offline.    h3. Actual Results  When node2 is down other nodes see it as online.    h3. Notes  * I haven't personally replicated this. I'm moving this in this project for more expertise    h3. Workaround    Currently there is no known workaround for this behavior. A workaround will be added here when available""",Bug,Java API
78925,"""Kanban-Boards usually contain serveral different service classes as well as different task types. Therefore it might be required to have different information on those tasks visible on the cards. As a user I would find it very helpful to be able to configure card layouts for each issue type in the board. Or be able to use JQL-queries to differentiate card layout likewise to configuration of card colors.""",Suggestion,Project
78934,"""h3. Issue Summary  As an administrator, I want to use a single fieldConfigurationScheme for new projects. However, when we have a large number of projects using a single fieldConfigurationScheme, it takes a very long time to associate a new project with the fieldCongurationScheme.    {{fieldConfigSchemeManager.updateFieldConfigScheme(...)}} takes as a parameter a list of associated projects (i.e. all of them), and updates the database for each project one-by-one (com.atlassian.jira.issue.context.persistence.FieldConfigContextPersisterWorker.store(....) ) also flushing the fieldConfigScheme cache each time.  h3. Environment  Large Jira Instance, >2000 projects, using a specific fieldConfigurationScheme    h3. Steps to Reproduce   # create >2000 projects   # create a fieldConfigurationScheme   # associate all projects with the fieldConfigurationScheme   # create a new project   # set *DEBUG* for the *com.atlassian.cache.event* package   # associate new project with fieldConfigurationScheme and see in the logs that *com.atlassian.jira.issue.context.persistence.FieldConfigContextPersisterWorker.configContextsBySchemeId* cache is flushed 2000 times    h3. Expected Results  Persistence is handled efficiently, i.e. only add the association for the new project instead of updating all associations, hence causing a single cache flush.  h3. Actual Results  {{*com.atlassian.jira.issue.context.persistence.FieldConfigContextPersisterWorker.store(....) *}} is called once for each project that's already associated with the same fieldConfigurationScheme.    h3. Workaround  None""",Bug,Issue
78951,"""As a manager of Jira issues, I sometimes need to include items in a Jira report that give me an overall picture of the status or attributes of linked objects.    As a manager, I need to be able to report inconsistencies such as why a Zendesk issue is Closed but the Jira issue is sitting in Open status.   * Report on Zendesk issue number, status and other fields from a Zendesk linked issue. Allow a reference like Zendesk(key) to return a list of tickets to which the current object refers to.   * Report on Bitbucket status such as whether commits or pull requests exist and how many of each. Allow a reference like Bitbucket(commits) which would return a list of commits related to the key of the current object.   * Report on fields from a parent-linked object such as a Story referring to an Epic. Allow a reference like parent(status).""",Suggestion,Admin
78980,"""* As an Admin I want documentation for deploying Jira on Google cloud Platform. """,Suggestion,Documentation
79008,"""Use Case:    As an administrator I would like to see some capacity or load value for each node. This would be the available capacity or the used capacity on that node.    I would like this value to be available so that it can be used in real time to determine the load on each individual node.   * This could be used in a load balancer to direct traffic to nodes with available capacity  * This could be used in external integrations that retrieve data from JIRA. The external integration could check for available capacity prior to making requests.""",Suggestion,Data Center
79032,"""Had a recent call with Bloomberg who are in process of upgrading to 7.13.  Couple of pieces of feedback on the 'Sprint Goals' field that their user community provided that I thought I'd share:    1/ They asked if there was any plans to expand the sprint goals field to be more than a single line?  In the past, they have create a ticket per sprint with a series of bullet points.  The are having to now list out the goals separated by '|' for groups wishing to use the functionality.  Thus, they would like the sprint goals to be multi-line    2/ Can sprint goals be searchable?  They'd like to identify common goals across multiple sprints.         I understand these request may be more 'Jira Align/Portfolio' but I was asked to raise them.     """,Suggestion,Project Administration
79088,"""I can't find all that I would like to know about Tempo in your website    I would like to know if this is sold as a separate product or if is part of the Jira new gen projects    Also if it is integrated I would like to know if the tasks in the Kamban are inmediatelly translated into time allocation in Tempo     And if once we have Jira licences we can use Tempo      """,Suggestion,ADGS Design changes
79155,"""As a Jira admin, I'd like to able to integrate Jira and JetBrains Upsource.""",Suggestion,Application Links
79205,"""User sometimes will leave a comment within the wrong issue. As an admin, I would like the ability to move this comment to another issue.    Ex:    Issue A exists. Has Comment X.  Issue B exists.    I need to move Comment X from Issue A to Issue B.""",Suggestion,Admin
79280,"""As a Board Administrator I want to edit the filter query and sharing of the board without being tied to a Saved Filter.    Ideally there would be radio buttons to choose from “Board Limited Filter” or “Saved Filter” and an option to copy a “Saved Filter” to the “Board Limited Filter”. Then if it was a “Board Limited Filter” any of that Board’s Administrators could edit the Filter Query / Sharing Permissions of that board without affecting anything else.    Currently when I add a Board Administrator I also have to remember to give them Edit permission to the Saved Filter. Also I can't stop someone else from using a shared filter for their board or widgets meaning I could change the filter and break their board/widgets purpose. It’s an Administrative / Change Management nightmare.          """,Suggestion,"Project,Filters & Dashboards - Permissions"
79288,"""As a Development team member I would like to be notified when another JIRA user replies to my comment so that we can appropriately collaborate within JIRA.         Currently the ability for an original commenter to be notified of a Jira ticket reply is limited by the notification scheme. Original commenter should be added to the notification scheme to allow for comment replies to properly notify the users who commented on their reply.""",Suggestion,Issue
79408,"""*Background:*    Issue history shows changes that occur, along with the date/time and the user making the change.    At times it can be difficult to determine the exact source of the change (method used to make the change)    For example an issue can be transitioned in many ways:   * Transition button on issue   * Dragging issue on board   * JIRA Service Desk automation causing transition   * Third party plugin performing transition   * Rest API    *Requested Change:*    As an admin I would like to identify the source of a change I see in an issue's history.  * This would be useful when a change occurs and the user associated with that change is unsure of how it was performed (or claims they did not perform the change).    Example of how this could work:   * Ability to enable debug or trace logging for issue history (changeitem)    * Log a stacktrace when issue history is being updated. This stacktrace could be reviewed to determine the source of the change.   ** An improvement would be identify the lines that the admin needs, and only log those lines from the stracktrace   ** Additional improvement ideas:   *** Make this logging show in the UI   *** Capture this level of detail by default (instead of having to enable debug logging)""",Suggestion,Issue
79491,"""When sharing a filter with Public, anonymous users will have the ability to read the JQL syntax of the filter. Part of the JQL may include usernames, project names and other sensitive data.    As an administrator, I'd like the JQL syntax to not be visible to anonymous users.""",Suggestion,JQL
79531,"""As a Jira Administrator, I want to be able to display the field description in a View Issue screen. Currently, the field description is only visible in the Create Issue screen.""",Suggestion,Admin
79796,"""It is very common in true Kanban use to utilize """"Done"""" sub-columns under various activity columns.  The Jira Software does not allow that.    It should not be that hard to allow creation and usage of a """"Done"""" sub-column, for example, under """"In Progress"""".  Currently, this requires adding a brand new column, so if there are 3 columns between """"To Do"""" and the final """"Done"""", each of those columns requires a duplicate """"Done"""" columns, so we go from 5 to 8 columns.    A sub-column would simply divide the total column width.  Yes, it would of necessity shrink the cards, but info displayed on the cards could be adjusted accordingly.    It appears there was a decision made at some point to not do this, but I've never seen it explained or defended.    Please consider this.  Also, we are using Jira Server, and have no current plans to go to Jira Cloud.               """,Suggestion,Project
79800,"""Suggested Change:     In JIRA Cloud time tracking looks different from how it looks in JIRA Server. I would like the cloud look in JIRA Server.     Note from user:    {quote}  I saw on the Atlassian website that Jira Cloud has a different view of the bar chart in time tracking (see below). This is simpler and more intuitive than the view I am currently using with multiple progress bars.  {quote}    *Cloud*   !cloudView.png|thumbnail!     *Server*   !serverView.png|thumbnail!   """,Suggestion,Issue
79804,"""The comments section of Jira easily gets convoluted with multiple conversations happening simultaneously.     As a Jira user, I would like to be able to leave a reply to a specific comment on a ticket and not have my comment appear at the bottom of the thread so that my comment can be directly associated with the comment to which I am responding rather than just getting added to the bottom of the list.     Note: this functionality exists in Confluence (shown below)    !image-2018-11-09-10-03-43-477.png!          """,Suggestion,Issue
79830,"""As a Jira Administrator, I would like to be able to configure Jira to display the description of Priorities directly in the screen while creating / editing / transitioning issues. """,Suggestion,"System Administration,Project"
79831,"""As a Jira administrator,    I would like to do all administration via the REST API, namely setting the """"Allow Gravatars"""" option,    so that we can set options without having to log in, and to trace what options where changed when.""",Suggestion,REST
79849,"""As a Jira Administrator, I would like to be able to disable automatic issue link rendering in certain fields.    In some scenarios, it can cause confusion.    Example:    You have a hash field and the value added contains a jira issue:    Field: {{9ahadf29anag-adn9heng-tst-139bla}}    On this case, if if there is a TST project, the value will contain a link to TST-139.""",Suggestion,Project
79868,"""as a system admin I would like to have the options to copy / import parts of one boards to another ,     for example - copy the queries in the colors , or copy the swimlane queries .         it is useful incases in which 2 boards already exist and we want to make one to be configured the same as the other .""",Suggestion,Project
79929,"""As as Jira Administrator I would like to have the same ordering for my """"Granted Roles"""" for every permission.     For example, for these two permissions the same roles have a different order. It would make it a lot more readable if the roles are ordered alphanumeric:     !image-2018-10-16-08-55-11-499.png!""",Suggestion,Project Administration
80035,"""As an admin, I'd like to be able to edit other user's preferences such as e-mail preferences, """"My Changes"""", Language, etc.""",Suggestion,User Management
80101,"""As a Program, Product, or Engineering Manager, I'd like to produce a report that lists issues that have been added to or removed from a fix version within a defined date range (e.g. between weeks or months),  so I can   # Have discussions with my teams on why things are being added or removed    # Produce monthly reports to share what has been added or removed to my stakeholders""",Suggestion,Dashboards & Reports - Reports
80116,"""h3. Problem Definition  As an admin, I want to be able to export Jira data (issues, comments, ...) that was only created after a given date. This lets me copy deltas only between two Jira environments.   h3. Suggested Solution  Introduce a feature to export data generated after a given date. The exported data should be imported normally on top of existing data.   h3. Workaround  Leverage the database's incremental backup features.""",Suggestion,Migration
80361,"""As a user I would like to have the ability to start a thread on a Jira that is being worked on by several several users so as to avoid lengthy comment strings that are not relevant to all users. """,Suggestion,Issue
80497,"""As an administrator, I would like to monitor Webhook queue like the mail queue feature and also check if there's any webhook is stuck or throwing errors. It's also very handy if I can restart the Webhook processor on the fly whenever it's not working as expected.""",Suggestion,Webhooks
80675,"""h3. Problem Definition  Current status Jira DC recovery from scratch:  # Create a new instance of Jira node: node1  # Deploy Jira to that node1 and connect to DB.   # Start Jira  # Since there is no index  #* you need to run Full reindex   #* or recover index from a snapshot.    This is a manual and time-consuming process. In case of AWS deployment with CloudFormation, when it's expected to redeploy the whole cluster for every upgrade problem becomes more important.     h3. Suggested Solution  As a JIRA Datacenter Administrator I want to do an automated cold recovery from a snapshot. That includes the following steps:  * Periodically create index snapshot at _Shared_home_  * Add option (or JVM flag) for automatic index recovery during start-up   * If the option is set:  *# search for index snapshot  *# pause start-up   *# restore index snapshot  *# continue Jira start when the restore is done   * Log all the steps during the process and preferably expose status through REST API     h3. Workaround   None    h3. Related   * JRASERVER-66649 - to choose for index recovery vs full reindex   * JRASERVER-37896 - for standalone Jira""",Suggestion,"Data Center,Indexing"
80740,"""As a user, I'd like to be able to configure Swimlanes in the Backlog board view.""",Suggestion,Project
80799,"""As a Jira Administrators, I would like to have the ability to retrieve groups from LDAP before users login for the first time, so Project Administrators can have Project Roles and Permissions already configured beforehand.""",Suggestion,User Management
80847,"""As a user, I'd like to be able to select custom fields of type Date Picker in the Two Dimensional Filter Statistics gadget, X and Y axis.         Currently this is not available.""",Suggestion,Dashboard & Gadgets
80904,"""As a Programme Manager I am interested in reporting on and taking action on those issues that are blocking progress across the various work-streams that I run.    I have mandated across the programme that as soon as sprint items are blocked that a corresponding blocking issue is created and the """"blocked by"""" field is updated on all impacted tasks.    I was looking for a report that can be provided to me on a daily basis that provides a summary """"aged blockers"""" with the list of blocked tasks. This will both allow me to ensure my PM's are on the case as well as giving me advanced visibility of problem areas or high impact issues.    I have had one of my developers try to put together a jscript filter for this but although it seems possible to see the impactedby field in drop downs it is not possible to produce a summary report, or any kind of """"Blockers"""" management dashboard.    I am told that there are 3rd party plug-ins that may provide this, but am not in a position to authorise the upgrade to a corporate Jira web instance.    I would be grateful therefore if someone can advise on this as it seems (to me) to be a fundamental requirement.                    """,Suggestion,Issue
81087,"""I know the summary may sound confusing but it was the best way i could think of describing what i want to do. To better explain, i have the following example of what i would like to accomplish.    I have 3 issue types in JIRA Service Desk - Event, Incident and Known Error.    I want to be able to display a list of all Known Error issue types having the same value as a specific field or group of fields within created Event and Incident issue types, in the actual Event and Incident view screens (possibly using JQL to get the list of Known Errors)    Is there a way i can do this within JIRA natively or possibly an add-on that can assist with getting this done?""",Suggestion,"Issue,Issue,Admin"
81207,"""As a project administrator I would like to be able to add multiple groups at once (bulk add) to a single role at once by copy-pasting a text list of groups in the input field.""",Suggestion,Project Administration
81351,"""As a tester / developer / business analyst / customer    I want to see which area is the most """"infected"""" (where are the most of the bugs)    And I want to get rid of duplicate bugs    And i want to see bugs connected to their stories    And I want to see stories grouped together by their subject to follow changes easily         So please implement Natural Language Processing to group / cluster issues based on their title, type, description, labels and comments.     This would also improve issue search itself (filtering)    Most of the multinational companies use Jira in English, so English IT dictionaries and/or existing Jiras would be good as teaching data.         This would be a major improvement, so may be an extra service for some additional fee paid.""",Suggestion,"Bulk Operations,Issue,Issue,Dashboards & Reports - Reports"
81489,"""As a Jira Administrator, it would be helpful if there was an option to deactivate users through Jira, even if they are coming from an LDAP directory.    An example use case, which isn't covered by revoking application access, is if I want to disable a Service Desk customer when the application is configured to allow access from everyone in Jira's user base.    Currently, the only way to cover that scenario is by making changes to Active Directory - like the user membership - and filtering out users not matching a user object filter.""",Suggestion,User Management
81589,"""h3. Definition  I would like to have a report in JIRA where I can see how many times an issue has been transitioned from Status A to Status B.     As an example,  ABC-1 Open to In Progress = 4  ABC -1 In Progress to Closed = 1    """,Suggestion,Dashboards & Reports - Reports
81659,"""As a board administrator, I would like to be able to grant access to administer my board to a project role because JIRA groups have a much higher burden of administration and so we don't use them.""",Suggestion,Project Administration
81753,"""As a user, I would like to be able to choose the fields that are shown according to the *Configure Fields* option per Issue Type.""",Suggestion,"Issue,Issue"
81891,"""In recent versions, the default JIRA footer (that shows 'version - About JIRA - Report a problem') is no longer displayed on the view issue screen. This makes that in service desk issues, the """"Click to add comment"""" box is displayed on the very bottom of the screen, and sometimes is almost covered by the browser link bar, impairing usability for users with small monitors (see screenshots attached). The same behavior doesn't happen if the issue does not have enough comments to fill the page.    As a user, i would like to be able to configure the minimum space between the comment bar and the bottom of the page (or the width the footer) in order to prevent this behavior from happening.""",Suggestion,Issue
82092,"""I am running a project which is integrated with Innotas and the effort logged in JIRA directly flows into. Now I need a way by which I could control the effort logging only if certain fields are populated correctly. A mix of both custom fields and OOB fields.      It will be good if I have the ability to restrict effort logging based on values populated in certain fields.""",Suggestion,Issue
82133,"""*Use Case:*    As a JIRA admin I would like to restore the XML backup from production into my test environment. I would like to keep certain elements from my staging instance rather than importing the production values.   * If the production values are imported I must go and change them to back to the staging configuration.     Examples of configuration that I would like to keep:  * Look and Feel  ** Color scheme in staging  ** Logo used in staging  * Announcement Banner  * Base URL  * License    The following are additional configuration items that could be retained:  * Application Link configuration  * User directory configuration  * Mail server configuration    *Suggested Change:*  On the restore screen provide checkboxes where the admin can select certain configuration elements they wish to keep.""",Suggestion,Migration
82215,"""h3. Preconditions    * Link JIRA 7.3.x with Confluence 5.9.x or higher.  * Ensure that JIRA is talking with Confluence via REST.  ** This is the default behaviour in JIRA 7.3.x, but can be toggled by dark feature.  ** Go to {{/secure/admin/SiteDarkFeatures!default.jspa}}, and check that """"{{confluence.issuelinks.use.rest.api.disabled}}"""" *is not present*.  * Ensure there are two user accounts can log in to both JIRA and Confluence.  ** I'll call them """"bob"""" and """"alice"""".  * Create a project in JIRA whose issues can be browsed by anonymous users.  * Create a space in Confluence whose pages can only be seen by authenticated users.    h3. Steps to reproduce    # As Bob, in Confluence,  ## Create a page that is accessible by any logged-in user. Call it something like """"Logged-in test"""".  ## Create a page that only bob can access. Call it """"Page by Bob"""".  # As Bob, in JIRA,  ## Create a new issue in the public project.  ## Link to both the """"Logged-in test"""" page and the """"Page by Bob"""" page.  # Log out of JIRA.  # View the issue as an anonymous user.    h3. Expected results    ...    h3. Actual results    * In the UI, the anonymous user sees a """"Page failed to load"""" message for the Confluence page link with a spinner that spins forever.  * In the JIRA log files, the following message will appear: {noformat}Error occurred while generating final HTML for remote issue link: java.lang.RuntimeException: java.io.IOException: Failed to load Confluence Page from remote server Caused by: java.io.IOException: Failed to load Confluence Page from remote server Caused by: com.atlassian.sal.api.net.ResponseException: Status Code: 404, Status Text: Not Found, Errors: {}, Error Messages: [The requested Confluence content could not be found.]{noformat}    h3. Workaround    Enable the """"{{confluence.issuelinks.use.rest.api.disabled}}"""" dark feature for the JIRA instance. This will cause JIRA to use XMLRPC to communicate with Confluence. Confluence's XMLRPC responses do not return 404 responses for unauthorized requests, so the errors aren't raised on the JIRA side.""",Bug,Issue
82217,"""As an Administrator I would like to be able to disable public access to JIRA, so the users will have to login before they can browse projects, search issues or navigate to system dashboard.    *Workaround:*  In JIRA 7.2.10 the possibility to disable public access for anonymous users was added, however it is still in labs state.    In order to disable public access for anonymous users, administrator needs to add a darkfeature public.access.disabled.   Here are the steps required for adding a dark feature in Jira:  * Login as an administrator and go to: [BASE-URL]/secure/SiteDarkFeatures!default.jspa  * In the Enable Dark Feature text field add *public.access.disabled*""",Suggestion,"Dashboard & Gadgets,Issue,Filters & Dashboards - Permissions"
82241,"""h3. Problem Definition    Additional unwanted data is currently being exported to CSV that wasn't previously included when exporting to Excel. As a user, I'd like to have more granular control over exactly what columns are included in the export.   h3. Suggested Solution    Provide a dialogue and allow users to choose which fields they'd like to include in the exported data. For example, users may choose to omit Issue Id or Component information.  h3. Why this is important    Many organizations have established reporting built around the data originally included within Excel exports. Allowing users to choose what columns to include in the CSV will allow them to use existing workflows without modification.     """,Suggestion,Issue
82267,"""As a user, I would like to type the JQL query directly into the gadget, instead of using an existing saved filter.     This would allow users to easily modify the filter for that particular gadget, instead of having to modify the saved filter (which may be used by other gadgets) or create a new filter each time.""",Suggestion,Dashboard & Gadgets
82295,"""As an admin, I would like to be able to change user/group preferences from his profile; specifically Language and Time Zone. It would be useful when they are having problems with it. """,Suggestion,User Management
82391,"""JIRA has block format styles for code and for preformatted text. Unlike many other issue trackers, it does not offer a dedicated inline style for code (like Markdown's `code`). But it has a 'preformatted' inline style that can be (ab-)used for short, inline code fragments, to at least get a monospaced font style.    One huge problem with this approach is that the preformatted inline style still applies nested markup and - more annoyingly - emoji replacement, {{as can be seen in _this example_ (:imaginary code snippet:)}}    I would suggest to either introduce a new inline style akin to {{{{noformat}}}}, or better yet, make the inline preformatted style work that way by default (of course the dream would be to have a dedicated inline `code` style).""",Suggestion,Issue
82450,"""I'm working as a QA accessibility tester.    I often have problems navigating around the Dashboard using a screen reader. I find that I have to set up filters to access components of projects within JIRA.    There used to be issues navigating the page, problems seeing what were open/closed etc  but I can see they are improved.          """,Suggestion,Dashboard & Gadgets
82706,"""h3. Summary  When the user has configured a Select list (Single Choice) custom field with options containing the character """"İ"""" (Azerbaijani language), JIRA will throw an error when attempting to search for the option.    h3. Steps to Reproduce  # Create a new Select list (Single Choice) custom field named *DEMO*.  # Add the character """"İ"""" as a new option. Eg. """"*Problem with İ*"""".  # Associate the custom field with the issue screens.  # Create a new test issue with this option """"*Problem with İ*"""" for *DEMO*.  # Go to the Issue navigator and search for: *DEMO = """"Problem with İ""""*.    h3. Expected Results  The issue navigator will return the issues with """"*Problem with İ*"""" selected for *DEMO*.    h3. Actual Results  No issues are returned and JIRA throws the error """"*The option 'Problem with İ' for field 'DEMO' does not exist.*""""     h3. Notes  Tested it with the *Dfile.encoding=utf-8* and *-Dsun.jnu.encoding=UTF-8* startup parameters therefore it does not seem to be an encoding issue.""",Bug,JQL
83643,"""JIRA 7.2 changed how customfield gets rendered using the `NEW_CONTENT_ADDED` event. It was a pain before but now, it is unmanageable and causes a lot of issues in IE/Edge and sometimes in Firefox (browser freeze). For a simple static field, rendering is completely done on server and it works fine. However, for fields having complex logic on the client side, it's not working at all.    Before 7.2, it was possible to apply different hacks. In 7.2 and beyond, freezing issues are unavoidable, especially since you started to be able to browse issues on the project view.  There is no solid and simple way (at least that I am aware of) to properly determine when your field is being rendered or discarded from the screen. In order to initialize my field on the client-side, I need to rely on JIRA.bind. One of the major problem is that as the user browse issues, I keep attaching watchers and I am never notified when the field is removed from the page so I cannot remove the watchers. This causes a multiplication of watchers which ends up freezing the browser. Also, sometimes, call backs do not get called in specific views (e.g. Dialogs).""",Bug,UPM (Universal Plugin Manager)
83914,"""h3. Summary    I create multiple issues using the """"Create issue"""" screen with the """"Create another"""" option enabled. I enter the """"Sprint"""" information to an active sprint and I receive a warning that the scope will be affected, which is correct. After creating the issue, I enter information for the next issue and I remove the """"Sprint"""" value to leave it empty. I still receive the same warning... I'm also able to receive the warning if I set a future sprint.  h3. Environment    JIRA 7.1.9   JIRA 7.2.6  h3. Steps to Reproduce   # Click on the *Create* button to open the """"Create issue"""" screen in a pop-up   # For the Sprint field, select a Active Sprint   # Check the """"Create another"""" option   # For the next issue, select a Future sprint or leave the Sprint field empty    h3. Expected Results    When creating the second issue, the """"Sprint scope will be affected by this action"""" message will disappear, as an Active sprint is not selected  h3. Actual Results    The """"Sprint scope will be affected by this action"""" message will not be cleared at any time.  h3. Workaround    Do not use the """"Create another option"""". This will close the pop-up window and clear the message.""",Bug,Issue
87323,"""h3. Summary    When a re-indexing node goes down (terminated abruptly or crashed), all the other nodes are not able to tell that re-indexing is no longer in progress. There is no impact for normal JIRA functionality. This is UI problem and you will be not able to run reindex.  h3. Environment   * JIRA Data Center   * Apache Load Balancer    h3. Steps to Reproduce   # Setup a Jira Data Center with atleast 2 nodes   # Start re-indexing from one node   # Access the re-indexing administration page from the other nodes to see the Re-indexing progress summary   # Abruptly terminate the JIRA process on the re-indexing node   # Access re-indexing administration again on the node that is still online    h3. Expected Results  This should show that the re-indexing process has been terminated, and should allow you to restart re-indexing if needed.    h3. Actual Results  The online node(s) are not able to detect that the re-indexing started in the other node has been terminated.  You will see the error in UI: '_Lock JIRA and rebuild index' option is unavailable while other indexing operations are in progress_.'   !indexing_not_available.JPG|thumbnail!     h5. Notes  * When user starts *FullReindexing* JIRA schedules the job at specific node and add _non-cancellable_ task into Global cache. This cache is synced with each node.  * When node dies, value still present in the cache and all nodes see it and believes that index is still running.   * Whole cluster shutdown is needed to flush the cache  * Looking into the database from our test we can see that there is an entry in the {{replicatedindexoperation}} table such as the following for each re-index triggered:  {code:sql}  mysql> SELECT * FROM replicatedindexoperation;  +-------+---------------------+---------+----------------+-------------+--------------+--------------------------+-------------------------+  | ID    | INDEX_TIME          | NODE_ID | AFFECTED_INDEX | ENTITY_TYPE | AFFECTED_IDS | OPERATION                | FILENAME                |  +-------+---------------------+---------+----------------+-------------+--------------+--------------------------+-------------------------+  | 10500 | 2015-11-19 20:00:11 | tnode1  | ALL            | NONE        |              | FULL_REINDEX_START       |                         |  | 10501 | 2015-11-19 20:32:02 | tnode1  | ALL            | NONE        |              | FULL_REINDEX_END         | IndexSnapshot_10500.zip |  +-------+---------------------+---------+----------------+-------------+--------------+--------------------------+-------------------------+  4 rows in set (0.00 sec)  {code}  ** There is another entry when this reindex ends. For the killed re-indexing node there is a start with no end entry.    h3. Problem mitigation   To partially address the problem and mitigate some of the possible scenarios, we implemented 2 new features:   * JRASERVER-68885 - Remove the stale indexing Job associated with the current node on startup  * JRASERVER-68616 - As an JIRA Datacenter Administrator I want to delete the reindexing task from offline node    Please check them for more details.     h3. Workarounds  # Shut down all nodes in the cluster.  #* This is needed since JIRA keeps cluster reindex status in memory (see Notes section). Leaving one available and restarting all others will keep the in-memory cache.  # Start nodes        """,Bug,"Data Center,Indexing"
87471,"""The Task List macro does not work on the Confluence Page gadget on a JIRA dashboard. When attempting to check a box on the Task List, the gadget appears to be hung.    This was initially reported as a Support issue. The Atlassian support rep that I was working with recommended that we submit a Bug. See statement below.    {color:#205081}I've have been testing the integration of the Task List macro with the Confluence Page Gadget and I've seem to face the exact problem. I'm not sure of the exact reason this integration isn't working but it's seems it is somehow associated with the bug listed earlier as the request to add the gadget appear within the logs. It may be possible that the integration between these two have never worked and that is why it doesn't appear within the working macro list. At this point, the best option we have would be to raise a separate bug ticket specifically for this issue amongst our bug tracking site{color}""",Bug,Dashboard & Gadgets
87942,"""On of my developers reported an issue which so far can only be confirmed between 2 specific projects when a specific link type is created.     There are 2 ways in which I've encountered this issue:  1) Under Project B user creates issue B-1 & while on the Create Issue screen links the issue as """"is contained by"""" to issue A-1 in Project A. I personally watched this & also saw the Linked Issues field with """"is contained by"""" & """"A-1"""" there. I also saw in the Activity tab of A-1 where the user created the link. After a couple of hours the user went back to B-1 & the Linked Issues field was missing. The field was also missing on A-1, however on A-1 you can still see the link in the Activity tab.  2) While creating issue B-2 _I believe_ I also included a link to A-2 as """"is contained by"""". Later I noticed that B-2 & A-2 weren't linked, so I created the link. After another couple of days I noticed that B-2 & A-2 weren't linked, so I created the link, but this time I remembered that I had already done this so I checked the Activity tab in B-2 and could see both of the times I linked these 2 tickets after creating B-2.    Note: This doesn't happen with Project C or D, where C & D are structurally copies of B (set up the same, with same roles, permissions, issue types, & workflows).""",Bug,Issue
89056,"""# Run the following JS from JIRA when logged into JIRA as an admin but without WebSudo authenticated for that user: {code} jQuery.ajax({   type: 'POST',   url: AJS.contextPath() + """"/rest/api/2/field"""",   data: JSON.stringify({""""name"""":""""Jack"""",""""searcherKey"""":""""com.atlassian.jira.plugin.system.customfieldtypes:multiselectsearcher"""",""""type"""":""""com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes""""}),   dataType: """"json"""",   contentType: """"application/json"""" }); {code} # (BUG) You will get a 20x response and the field will be created. I would expect some kind of 401 websudo error.""",Bug,REST
89769,"""Tempo Planner, Folio and Books are disabled on JIRA 6.4 after restart.    We had to remove the JIRA 6.4 compatibility for these products. Is there a workaround that we can publish to our customers that they can use until a fix is released?    It appears as everything works when we run our environment with atlas-run-standalone. Then if we restart it (just ctrl+C and run it up again) the products are not enabled. We have to manually install these plugins again in the UPM and that is quite sub-optimal.    Additionally, JIRA 6.4 server is affected. We set up a JIRA 6.4 instance to verify that this problem was only limited to using the atlas-run-standalone command but the atlassian-jira-6.4.tar.gz file provided by Atlassian must be broken since we can access everything in the Admin section except the Add-ons section. There we get a 404 error despite having the correct URL.    When one of our Timesheets developers first tried to see if the plugin worked on JIRA 6.4 last November, he experienced these difficulties and contacted Bradley Ayers from Atlassian. He gave us this answer:    """"After having a chat with the plugin system team, I've come to the conclusion that the 6.4-m09 milestone is broken, and it's something we'll need to fix before 6.4 final is released. If this is blocking your development (e.g. you need to test JIRA startup behaviour), there's a work around you can use until we fix this within JIRA:  Make a copy of tenant-smart-patterns.txt, add your other plugin key to it, and then set the system property atlassian.plugins.tenant.smart.patterns to point to the new file.""""    This workaround fixes our problems, but we thought that it should have been fixed by now. Can we do something else than this workaround? What should we tell our customers who are running our products on JIRA 6.4? Should we tell our customers to add the artifact ids to the tenant file as a short term solution?    Thank you very much, I hope you understand that this is quite urgent.""",Bug,Infrastructure & Services - Application Lifecycle
90411,"""Steps to Reproduce:  - Create a Field Configuration named Bug and add *Affects Version* as a required field.  - Create a Field Configuration Scheme named Bug Scheme and associate issue type to *bug*   - Add the Bug field configuration to the *Field Scheme* for an example project  - You should now have a project with *Affects Version* set as required for issue type bug, and this field is not required for all other fields.   - Create an issue with a type other than bug (eg. Story)  - Add a subtask to this issue and attempt to move it to issue type Bug. You will notice the *Affects Version* field is required, however there is nothing populated in the drop down.    !example.png|thumbnail!    Expected Result:  - The dropdown should allow you to set an *Affects Version* for the task and the subtasks.      Workaround   If you set the required fields to the desired value prior to the move, the operation will complete successfully.    I've tested and reproduced this behavior against JIRA 6.3.8 and JIRA 6.3.13""",Bug,Bulk Operations
91777,"""I recently tried to insert various regular expressions as samples inside an issue description, and noticed that the \{\{ curly brace markup \}\} (used for inline monospaced text; as opposed to block monospaced text using the \{code\} and \{noformat\} markup) behaves a bit strange when using curly braces inside the text (such as a regex).  This might be related to JRA-38048, but that one was closed since it seemingly related to just the inconsistent preview. However, the problem also shows in the actual issue description (and possibly other fields/comments).    Maybe a quick sample:  {noformat}Use the regex {{\[A-Z0-9\]\{4,20\}}} for your validator.{noformat}  Expected would be that the string """"\[A-Z0-9\]\{4,20\}"""" is shown as monospaced text. What actually happens is that """"\[A-Z0-9\]\{4,20"""" is monospaced, followed by a single {{\}}} - and this looks a bit odd considering that the first of the 3 braces was escaped, while the latter 2 were not.  As a user, I assumed that escaping also breaks the use for the markup parser, leaving \} as verbatim brace while the rest \}\} is used as end-marker for the inline monospace.    Trying out other ways of escaping does not seem to work correctly (such as adding backslashes, spaces or other characters that might be hidden in output):  - {{\[A-Z0-9\]\{4,20\}}} (monospaced, but ends early)  - {{\[A-Z0-9\]\{4,20\}\ }} (not monospaced, extra backslash and space)  - {{\[A-Z0-9\]\{4,20\}\\}} (not monospaced, extra newline)  - {{ \[A-Z0-9\]\{4,20\} }} (not monospaced, extra space)  - {{\[A-Z0-9\]\{4,20}}\} (monospaced, but ends early; this is actually matching the output as I'd be expecting the last brace to be used as-is)    {noformat}- {{\[A-Z0-9\]\{4,20\}}} (monospaced, but ends early)  - {{\[A-Z0-9\]\{4,20\}\ }} (not monospaced, extra backslash and space)  - {{\[A-Z0-9\]\{4,20\}\\}} (not monospaced, extra newline)  - {{ \[A-Z0-9\]\{4,20\} }} (not monospaced, extra space)  - {{\[A-Z0-9\]\{4,20}}\} (monospaced, but ends early; this is actually matching the output as I'd be expecting the last brace to be used as-is){noformat}""",Bug,Issue
91806,"""We recently discovered that anyone with the 'Watcher' role (either from being added as a Watcher or being a Component Watcher) will receive email notification regardless of their 'Do not notify' setting (i.e. from user profile preferences).     The 'Do not notify' setting did prevent emails when a user was only the Assignee (and not a Watcher or Component Watcher) – so it does work in some situations.    Our notification scheme sends emails for the Issue Update event for reporter, watcher, assignee, and some project roles.  I would assume that 'Do not notify' should override ANY of these reasons for notification if the issue change is triggered by the user that would be notified.    I see a couple other similar issues but nothing that specifically mentions the failure due to Watcher status. JRA-28523, JRA-39021.    Thanks""",Bug,Issue
92214,"""One of our JIRA users, who is not a global admin, created a new Dashboard and about 4 new filters. He did not choose to share the Dashboard or filters so I assume they are private. He wanted to share his dashboard and doesn't have that permission when he """"edits"""" the dashboard. Therefore, as a global admin, I logged in and was going to share the Dashboard for him. However, when I search for the Dashboard it's not found. When I search for the filters he created they are also not found. As a global admin, why should those be hidden from me?    I also had another of our global admins test this and he is having the same experience I am. I'm not sure where to go from here.""",Bug,Dashboard & Gadgets
92311,"""On JIRA CSV import with mapped Issue Keys, any existing issue that gets updated has all the comments duplicated. I would think that JIRA should instead be testing any comment fields in the CSV against the existing issue (timestamp, author, and text equivalence), and not importing a comment if it already exists""",Bug,Jira Importers Plugin
92373,"""I'm trying to create a custom bulk edit plugin.    All the other JIRA bulk classes make calls to BulkOperation.perform() (AbstractBulkWatchOperationAction, BulkDelete, BulkTransitionIssueOperation) or BulkMoveOperation.perform() (BulkMove). BulkOperation and BulkMoveOperation are part of the SPI.    But BulkEdit calls BulkEditOperation.perform(). BulkEditOperation is part of the Jira core classes, not part of the SPI.    As a result, the only way to create a custom Bulk Edit operation (say to add or delete labels) is to extend BulkEditOperation, which results in brittle code that can break whenever the implementation of BulkEdit or BulkEditOperation changes (as it did between revisions of Jira 6).    For verification, look at the implementation of BulkEdit.doPerform().""",Bug,Java API
92574,"""Hi,    I have set up security for a client in a way that you have Users | Developers & Administrators, where Developers can do everything Users can + some extra permissions (like creating issues etc). Administrators can do everything that developers can + some extra permissions (like administering the tool). I have done this in all the 6 tools (JIRA, Confluence, Stash, Bamboo, FeCru & Crowd) & it all works, but I have stumbled upon a bug.    I added jira-administrators to the 'JIRA Administrators'-permission & 'Create Shared Objects'-permission. As a consequence, I can't add jira-administrators to the 'JIRA Users'-permission as this is not needed for administrators.    To be able to create a dashboard however, an account has to have the 'JIRA Users'-permission. This makes it impossible to create a dashboard as an administrator with the current permission settings.    As a temporary workaround, I added the people inside jira-administrators also to jira-users. But this is not the way I like it. I would like the jira-administrators to only be placed in the group jira-administrators and be able to do everything a user & a developer can + administration tasks.    Kind regards,  Dimitri """,Bug,Dashboard & Gadgets
92665,"""The Cancel link has an invisible area around it where if you click you still trigger the """"cancel"""" action, for me seems like it is treated like a button without borders and background.    This is confusing and can lead to unexpected behaviour because the indivisible area of the Cancel link touches the Create button and even a small slip of the mouse when you want to hit Create will close and cancel without any prior warning, causing the end user to loose all the data filled for the new issue.    As you can see in the attached screenshot, the mouse looks like is hoovering the """"Create"""" button, but in fact the Cancel link (button) is highlighted and will be triggered on click.    I would recommend to make the Cancel act as a link and only clicking on it's text should work. This will create a small """"eventless"""" space between them, or at least add a few pixels between the Create button and the Cancel one so it's harder to click cancel by mistake.""",Bug,Issue
92703,"""In the below code - projectTemplate.getScreens() returns back a Long that contains an IssueTypeScreenScheme id. This code has worked for ages for me but in JIRA 6.2+ something change and whenever a Issue Type Screen Scheme is deleted - the object is still returned (instead of returning null or throwing an Exception). The object returned has all of it's method's as null.  I've been able to work around it, but I wanted to report it in case somebody else has any issues. What's really bad is that I was able to assign this semi-null object to a project which broke my project. :(    {code}    if (( projectTemplate.getScreens() != null ) && (projectTemplate.getScreens() >-1 ) )    {     IssueTypeScreenScheme issueTypeScreenScheme = null;     try     {      issueTypeScreenScheme = this.issueTypeScreenSchemeManager.getIssueTypeScreenScheme( projectTemplate.getScreens());     }catch(Exception e) { logger.debug(""""Error looking object up"""");}                  }  {code}    """,Bug,Java API
92889,"""1) Create custom fields using the Multi User Picker and User Picker.  2) Navigate to the specific workflow step.  3) Navigate to Validators.  4) Add the custom fields as required field as a validator.    Upon performing the workflow action specified, the user isn't prompted to enter a user and the action completes.    The more specific example: We'd like to notify the developer once we close an issue. So I've created a """"Notify"""" field using both Multi User Picker and User Picker and placed that as the last step of our workflow (Close Issue). On that step transition, I've set up a validator to make that field required (only during that step) but if the field is left empty, JIRA doesn't indicate the field is required. Is there a fix for this issue?  """,Support Request,Issue
93121,"""We set up a collector to create bugs from an email account at our company.  I noticed that the bug number does not increment contiguously as would be expected (BUG-3, BUG-4, BUG-5, etc.)  Instead it seems to vary wildly.  Real issues: TEMP-1428, TEMP-1431, TEMP-2672, TEMP-3019, TEMP-4987, TEMP-5392, TEMP-7474, etc...    The collector was doing this in the main project so I pointed it to the temporary project to quit burning through insane amounts of issue numbers.  No one is manually creating issues in the project so there's no other way to account for this behavior.  I've tried to browse to some of the issues in between (TEMP-7300) but it does not exist.    The run up from TEMP-1428 to TEMP 7592 was only 5 days (March 29 to April 2).    I'm not filing this as a support request because I can't see any possible reason this would ever be desired behavior regardless of configuration.""",Support Request,Issue
93210,"""This issue was discovered while running on Tomcat 8. While earlier Tomcat versions may hide the issue (I haven't checked) my investigations into this issue haven't found anything that would suggest it is Tomcat 8 specific.    When making a request to Jira, multiple instances of the attached stack trace are observed.    A little debugging and looking at the Jira source code identified the source of the problem as the relative URLs used in /ui-aui-layout/icons.less. These relative URLs are all of the form """"../../../images/...""""    There are two problems.  The first is that all those URLs should start """"../../images/..."""" to correctly locate the required image relative to the icons.less file.  The second issue is that to """"resolve"""" this relative URL, the LessTransformer simply prepends """"/"""" and then uses that value with ServletContext/getResource(). This is guaranteed to fail as any such lookup is relative to the context root.    I am currently running with a local modification to icons.less that replaces all instances of """"../../../images/..."""" with """"/images/..."""" which works around this issue.    I know Tomcat 8 is not a supported platform yet but I wanted to provide you with a heads up of this particular issue. Also, it looks like this will affect earlier Tomcat versions as well.    To date, the ASF has found running Jira on Tomcat 8 a fairly smooth process. There have only been two issues to date and while each took a little time to fully understand, both have been fairly simple to work-around.  """,Bug,Infrastructure & Services - Application Lifecycle
93530,"""_as_ user with """"create project privileges"""" but not in group """"jira-administrators"""",  _I want to_ have a clear method how to create a new project  _so that_ I can create a project in the proper way.    *Reproduction*  * Log in as a user with create project capabilities, but this user may not be in the usergroup 'jira-administrators'.  ** Dashboard appears.  * Go to pull-down 'projects' and select 'Create Project'  ** Selection screen projecttype  and key and name appears.  * Fill everything in and press 'create'.  ** Error appears.    *Reason:*  When creating a new project, there is no administrator, only 'jira-administrators', no users and no developers added in project roles.  Because of this, a privileged user cannot enter the project in user mode.  In 'Jira admin' mode I am able to prepare the project further, but in user mode I am not.     *Suggestions*  # Remove the 'Create project' possibility in user mode, only allow creation of projects in 'JIRA admin' mode.  # When creating a new project in 'user mode', please automatically add the privileged user as project role 'administrator' in the newly created project.    """,Bug,Project Administration
93898,"""Hello,    Refer to my screen-shot. I'm trying to use _WasClauseImpl_ for a plugin implementation, the generated clause doesn't pre-append a blank when using the _Operator.AFTER_. As a result the query is not valid and make execution fail.    Again, following the screen-shot  {code}  wasClause=status was """"No Test""""after """"2013-07-01  {code}  it should be generated as  {code}  wasClause=status was """"No Test"""" after """"2013-07-01  {code}  Notice the missing blank in between _""""No Test""""_ and _after_  """,Bug,"Issue,Java API"
94103,"""Setting {{User Membership Attribute}} allows to gather membership and is respected during full sync.  Unfortunately when user is logged in, his groups are cleared.  Before login (after full sync):  !beforelogin.png!    After login:  !afterlogin.png!    My local ldap config (I've usted {{postalCode}} from my LDAP as a group marker):  !ldapconf.png!""",Bug,User Management
95252,"""*Steps to reproduce*    * Set -Duser.timezone=EST or -Duser.timezone=America/New_York as a JVM input argument in setenv.bat file.  * Create an issue and set the due date to 3/Oct/13 for instance.  * View this issue in the issue view, issue navigator and the database.    *Expected Result*  * The due date shows the correct value consistently wherever the issue is viewed and is stored in the database jiraissue table as 2013-10-03 00:00:00    *Actual Result*  * *In the Issue View:*: The due date shows correctly as 3/Oct/13  * *In the Issue Navigator:* Under the due column, the date for this issue is 2/Oct/2013  * *In the database:* The due date for this issue is recorded as 2013-10-02 23:00:00    *Some Notes*  * It appears that the problem may be related to daylight saving time, so I'm not sure if it's reproducible all year round.   * Because 'due date' is a date field and not datetime, it is expected that the time in the database entry should have 00:00:00 for time, however because the time is one hour off it is recorded as {actualDueDate -1day 23:00:00} in the database. Which eventually translates into a whole day difference when read from the database back into a date field.   * Invariably, the more troubling issue is the inconsistency with the date that so far was noticed only with due date field and not other date fields like Created, Updated, Commented etc.    *Workaround*  Removing the timezone JVM argument -Duser.timezone= eradicates the problem somehow, this should be ok if the JIRA Server is set to the same time zone. JIRA can be configured to use the default system time.""",Bug,Issue
95383,"""One of our admins came by today and said that he couldn't create a new project after we upgraded JIRA to 6.0, he never had this problem before. I said """"_phe_"""" and selected projects in the right drop down menu and clicked the """"Add Project"""" button. Nothing happened.  This kind of baffled me so naturally  I started up firebug to see what happened and I saw the following error    {code}  TypeError: this._enableWebSudo is not a function  {code}    It seemed to be an authorization issue but I couldn't figure out why it would happen now after upgrading. I realized, though, that I had never been asked for my admin credentials when going into project so I went in somewhere else in the admin page where authorization was required, filled in my password and after that I could create a project.    So, the problem seems to be that if you go straight to projects and press """"Create Project"""" then nothing happens. There should at least be a message saying """"You are not authorized"""" or something because our admin was really confused.    *How to re-create:*  Go to projects without having authorized yourself as an admin and press Create Project. Nothing will happen.    *Expected result:*  If I'm not authorized to create a project either ask me for my password or tell me I shouldn't press the button. :)    *Workaround*    This can be worked around with either of the following:        Disable websudo as per our Configuring Secure Administrator Sessions.      Gain websudo access first by accessing a restricted area of JIRA before attempting to create the project (for example access Administration > System).""",Bug,Project
95420,"""e.g.  Username is: user@example.com  Display Name is: Example User    when I mention the user I would expect to see """"Example User"""" as a link to his profile.    What actually happens is, that the following is displayed:  {code}[~user@example.com] {code}  which generates a mailto link instead of a link to the users profile.""",Bug,Issue
96160,"""I've used a label named Sanity  then, I've used two other labels named : Automated, Manual. each issue that has the automated or the manual label has also the sanity label    The filter I used has 49 issues in total:  35 issues have two labels: Automated, Sanity  14 issues have two labels: Automated, Manual    ->The total issues counts 98 (35*2+14*2)    The pie chart plugin counts the total issues correctly     I'm us    """,Bug,Dashboard & Gadgets
97107,"""I would like to be able to set the default issue security setting that will be set when issues are added to the system. Currently the issue are being added as a reporter set to my account and anyone can see the issues unless I manually set them to a higher setting. """,Support Request,Issue
98011,"""From JRA-27353    Steps to reproduce:    1. install JIRA  2. setup LDAP directory - create test data (see below)  3. add LDAP directory in JIRA configuration with option """"Enable Nested Groups"""" enabled  4. synchronize data  5. change name of """"sub_group"""" to """"Sub_group"""" and update uniqueMember parameter of parent_group to match changed name  6. synchronize data again - here crowd should fail on inserting data into database (CWD_MEMBERSHIP table)    I was able to reproduce it with HSQL and PostgreSQL.    Test data - two groups with relation:        parent_group, uniqueMember= {sub_group}      sub_group    LDIF export of my test data (I've configured LDAP to use root entity dc=atlassian,dc=pl):  parent_group.ldif    version: 1    dn: cn=parent_group,dc=atlassian,dc=pl  objectClass: groupOfUniqueNames  cn: parent_group  uniqueMember: cn=sub_group,dc=atlassian,dc=pl  description: Parent group    sub_group.ldif    version: 1    dn: cn=sub_group,dc=atlassian,dc=pl  objectClass: groupOfUniqueNames  cn: sub_group  uniqueMember:   description: Child group    Also to see effect of exception which you should get in step 6 you may add user and set membership using uniqueMember attribute - this user won't be added to any group because of synchronization fail (user should be created, groups also, but no user membership in group will be created).    """,Bug,User Management
98990,"""h1. Problem statement    On a responsive mobile site, where """"pinch-to-zoom"""" is disabled, the Issue Collector popup is completely unusable.    h1. How to reproduce the issue.    The site we're developing has the following configuration:    {code:html}  <meta name=""""viewport"""" content=""""width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no"""" />  {code}    When you click the Issue collector tab/button, the popup appears centered in the viewport, but the width and height are fixed at sizes way too large for the window.      h1. Proposed resolution    Make the pop-up form's height and width, and layout respond to the browser frame (i.e. adaptive/responsive web design).    Introduce media queries at various standard breakpoints to offer alternative layouts for smaller screens.    h1. Workaround    I've been able to achieve what I need using the following snippet of CSS:    {code}  /* Issue collector tweaks for smaller screens */  @media screen and (max-width: 740px) {    .atlwdg-popup {      left: 0 !important;      top: 0 !important;      height: 100% !important;      width: 100% !important;      margin: 0 !important;      padding: 0 !important;    }  }  {code}    UPDATE: The pop-up is still too wide, the email field is still partially hidden and the text-area doesnt wrap below its field label, but since these elements are placed within an i-frame I have no control over their theming from my application.    As a side note, this issue appears to affect *All popups produced by Jira*, and this simple code tweak could be released for a quick win on mobile devices. ;)  """,Bug,Mobile
100247,"""# Create an assignable user called """"z"""". This should be the only user that has a """"z"""" in their username, full user name or email address.  # Open the assign dialog. Don't open the drop down.  # Type in """"z"""". You should now have only one user in the """"All Users"""" group.  # Clear the """"z"""" from the search.  # (BUG) Hit the down arrow. You will notice that the """"All Users"""" group is now first rather than the """"Suggestions"""" group.     !swapped.png!    I would expect the """"Suggestions"""" group to always be first.  """,Bug,Issue
100571,"""JIRA activity stream items from yesterday (not today) are wrong formatted, e.g. """"Gestern um k:52 'U11r'"""" which should something like """"Gestern um 11:52 'Uhr' (note, I'd swapped """"h"""" and """"11"""").    This bug was already in prior 5er releases, but was not fixed in 5.0.5, so this issue now. """,Bug,Issue
100756,"""# Restore [^data.zip]. # Go to """"Issue Types"""" in Admin. # Select the """"Issue Types Scheme"""" tab. Note that the Projects for """"Default Issue Type Scheme"""" is """"Global (all unconfigured projects)"""". # Edit the """"Default Issue Type Scheme"""" # (DEFECT) Note that the """"Shared By"""" lists """"XSS"""" as a project using the scheme (which is correct).  I would expect the data on both pages to be correct.  """,Bug,Admin
101029,"""I have created the following filter:  {noformat}  assignee=currentUser()   AND fixVersion=earliestUnreleasedVersion(MYPROJECT)   AND status not in(Closed, Resolved)   {noformat}  When I run it most of the issues are really from the correct version, but some are leftover tickets from the previous version (which has already been released). We use GreenHopper, and this query is used to view all items in the current sprint. So, the expected Fix Version is _Version 1.0, Sprint 2_, but some issues have the Fix Version _Version 1.0 Sprint 1_. No matter what the specific versions are, I would have expected all tickets matched by that query to have the same version.    When I add the following condition to the query:  {noformat}  fixVersion!=latestReleasedVersion(MYPROJECT)  {noformat}  The unwanted issues disappear, and I get exactly what I wanted.""",Bug,JQL
101747,"""If a new context is created for a custom field and if the applicable context is selected as a given project, the search screen still shows it for all projects (even after clicking refresh search).  If at the root level of custom field if the applicable context is selected as a project, the search screen filters it well.    To Reproduce:  1) Create a Custom Field Test Ajay Goel  2) Choose applicable issue types as All Issue Types  3) Choose applicable context as Global Context (will talk about it later)  4) In Associate field Test Ajay Goel to screens, select any screen  5) In View Custom Fields Screen, click configure next to """"Test Ajay Goel""""  6) Click """"Add new Context""""  7) In Configuration scheme label give name """"Test Ajay Goel for Project A""""  8) In Choose applicable context, select project A  9) Add another new context and repeat 6-8 for Project B  10) Go to Issue navigator, select project C, click Refresh Search and expand Custom Fields     Expected behavior:  Test Ajay Goel should not be visible    Actual behavior:  Test Ajay Goel is visible.    Please note that after having created two contexts for this custom fields for project A and B, at the root Custom Field level, I cannot set the applicable context to project A and B (they are not there in the list).  The same this is true the other way round as well.  If I create a custom field and set at the root level the applicable context, I can not add a new context for those two projects.    Basically, I need to create a custom field that is only visible to two projects but they look differently (say have different title or options) between those two projects.  """,Bug,Issue
101765,"""1) Start from an empty project  2) Create one issue  3) Set FixVersion for this issue to one unreleased version available in your project, say, version V1  4) Clone your issue, creating a snd one.  At this stage, you have two issues, with fixes due for version V1 in your project. Both have been created today.  5) Launch a request on history of your project, asking the issues whose fixVersion have been set to V1 today. You will get only the first issue, the clone, snd, issue, will not be listed in the result. This is wrong.    I see that as an inconsistent behavior as, as a user, if I know there was no issue associated to a version yesterday, and I'm seeing two issues currently associated to this very version now, I'm expecting to retrieve these two issues when asking which issues had their fixversion set to V1 today. And issues created by cloning are breaking this assumption (probably because they are created without history information, whatever the history information found in original issue), leading to results of query on history that will be seen as incoherent when compared with queries on current state of issue database.""",Bug,JQL
101777,"""We are currently using JIRA to track and manage defects raised by multiple teams. Each team has a JIRA project - and users are generally permitted to raise issues within their own projects. Issues can be moved between projects to reflect whoever currently has responsibility for each particular issue.    When an issue is moved between projects the reporter changes (for some reason). It is important for us that the reporter remains the same irrespective of which project the issue resides within.    I would appreciate any help or explanation that anyone can provide of why this is happening - and how to stop it happening. As I cannot seem to determine the problem / solution by myself.""",Support Request,"Issue,Issue"
102119,"""This is a problem that causes emails to get stuck in the pop server and arises when there is a splitregex setting in the handler params for mail services and the email has an empty body and attachemnt.     h5. The error  {code}  2011-11-23 23:42:05,252 QuartzWorker-0 WARN ServiceRunner    POP3 Service [com.atlassian.mail.MailUtils] Unable to extract text from MIME part with Content-Type 'application/msword;   name=""""FINAL IDDI Manual September 2010 (2).doc""""  2011-11-23 23:42:05,299 QuartzWorker-0 WARN ServiceRunner    POP3 Service [service.util.handler.RegexCommentHandler] Failed to split email body. Appending raw content...  java.lang.NullPointerException   at org.apache.oro.text.regex.PatternMatcherInput.<init>(Unknown Source)   at org.apache.oro.text.perl.Perl5Util.split(Unknown Source)   at org.apache.oro.text.perl.Perl5Util.split(Unknown Source)   at com.atlassian.jira.service.util.handler.RegexCommentHandler.splitMailBody(RegexCommentHandler.java:46)   at com.atlassian.jira.service.util.handler.RegexCommentHandler.getEmailBody(RegexCommentHandler.java:35)   at com.atlassian.jira.service.util.handler.AbstractCommentHandler.handleMessage(AbstractCommentHandler.java:78)   at com.atlassian.jira.service.services.mail.MailFetcherService.run(MailFetcherService.java:186)   at com.atlassian.jira.service.JiraServiceContainerImpl.run(JiraServiceContainerImpl.java:60)   at com.atlassian.jira.service.ServiceRunner.execute(ServiceRunner.java:47)   at org.quartz.core.JobRunShell.run(JobRunShell.java:195)   at com.atlassian.multitenant.quartz.MultiTenantThreadPool$MultiTenantRunnable.run(MultiTenantThreadPool.java:72)   at org.quartz.simpl.SimpleThreadPool$WorkerThread.run(SimpleThreadPool.java:520)  2011-11-23 23:42:05,314 QuartzWorker-0 WARN ServiceRunner    POP3 Service [com.atlassian.mail.MailUtils] Unable to extract text from MIME part with Content-Type 'application/msword;   name=""""FINAL IDDI Manual September 2010 (2).doc""""  {code}    Need a windows client machine. (eg: Windows 7 in testing)  And possibly an Exchange server, I'm not sure if other mail servers will have the same fault. They may format the pop3 response differently...      h5. Reproduction Method:    From Windows 7  # Create a dummy document (eg) test.docx  # Right-click test.docx, select Send to | Mail recipient. This should open an email window.  # Add the recipient (eg, in our case) Jira  # Add the issue tag to the subject line, eg, [JIR-484]  # Delete all message content  # Send    h5. Note:  It's the absence of a main message body that causes the problem.    Using Outlook and deleting the message body did not produce a blocked email. Presumably, Outlook puts a bit of """"empty"""" html in the message body. Unsure what other mail clients would be able to produce a """"bad"""" message - would need to do plain text, with and empty message and an attachment.    The service is configured like this:    {code}  Our mail set up uses the POP3 service in a comment only mode with the regex comment splitter:  usessl: No SSL  popserver: DHHS-POP3-jira.issues  handler.params: reporterusername=jiraadmin, createusers=false, splitregex=""""===|\s*_*\s*From: JIRA (by:"""", bulk=delete  forwardEmail: youremailaddress@atlassian.com  handler: Regex Comment Handler  {code}    This basically splits off:  # anything after a === character sequence (people put this in their signature at the chop point to remove history, etc) and  # the jira notification so people can reply to notifications with a quick comment.""",Bug,Scheduled Tasks
102961,"""I would really like to be able to allow designated project leads and project role members the capability to edit/manage custom field options such as a select list's options without having to grant them jira administrator permissions.  Limiting such editing/management to custom fields specifically, and only, used for the project in which said person has such permissions would be key as to not affect other projects where the same custom field may be used.""",Suggestion,Admin
102965,"""I installed a custom field plugin to our Jira 4.4 server using the in-app uploader.  I then added that custom field to all issue types.  I then did an export, and tried to do a project import into a dev server where I'd installed the plugin manually.  The dev server cave an interesting message that the data had a custom field installed but that the plugin for that field had not been installed. (See screenshot, which also showed errors from greenhopper)    I checked the XML backup file, and there was no <PluginVersion> tag for the plugin I had upladed.  I restarted the server, did a new backup, and the new file had the <PluginVersion> tag correctly included.  Since this was a dev situation, there was not a big problem.  However, if I'd wanted to do a project import into production, I'd have had to tweak my xml backup file by hand to recover my data.    (I'm marking this as major because it can result in backup files which cannot be used for project import.)""",Bug,UPM (Universal Plugin Manager)
103261,"""In case of collaboration with customers via JIRA, it would be very helpful to extend JIRA's rights management to Dashboard Gadgets. For example, it is possible for a customer to add the Group Timesheet Gadget and display the worklog summary of any group he selects (even, when he is not member of that group).  As a JIRA Admin, I would like to manage the availabilty of Gadgets per Group/Role, so that confidential information can be kept inside our organization.""",Suggestion,Dashboard & Gadgets
103369,"""Currently, as far as I can tell, there isn't a numeric field type that requires users to pick from a fixed list of options (using radio buttons, a select list, etc).    This would be very useful - often user stories are assigned a value from a fixed range of options (0, 1, 2, 3, etc).    If an alternative field type is used to achieve this (radio button type, for instance), the field is not numeric - and cannot be used to generate graphs or as a greenhopper story points field.    If there is an existing way to achieve this (using a plugin), I would love to know how!""",Suggestion,Issue
104374,"""h3. Summary    IssueInputParameters doesn't create a comment during transition or adds a duplicate comment when the transition has a screen  h3. Steps to Reproduce    Try to add a comment to an issue programatically using IssueInputParameters.setComment() during a transition having no transition screen. For example, I've created a Groovy script that I run via ScriptRunner. This script runs in a post function. In the script, I'm setting a transition comment. Here is the relevant section of the code I'm executing:  {code:java}       final String SYSTEM_USER = """"system"""";  User systemUser = ComponentManager.getInstance().getUserUtil().getUser(SYSTEM_USER);  ...              IssueInputParameters issueInputParameters = new IssueInputParametersImpl();              // comment not showing up              issueInputParameters.setComment(""""All issues depended on are either resolved or closed."""");                TransitionValidationResult transitionValidationResult = issueService.validateTransition(systemUser,                 linkedIssue.getId(), ALL_WAITING_ISSUES_RESOLVED_ACTION_ID, issueInputParameters);                if (transitionValidationResult.isValid())                 {                 IssueResult transitionResult = issueService.transition(systemUser, transitionValidationResult);                   if (transitionResult.isValid())                    {                    log.debug(""""Issue """" + linkedIssue + """" successfully transitioned out of WAITING ISSUE status"""");                      // See above, comment submitted with transition is not showing up, so we add one ourselves.                    linkedIssue = issueManager.getIssueObject(linkedIssue.getKey());                    commentManager.create(linkedIssue, SYSTEM_USER, """"All issues depended on are either resolved or closed."""", true);                    }                 else                    {                    log.debug(""""Issue """" + linkedIssue + """" transitionResult not valid"""");                    }                 }              else                 {                 log.debug(""""Issue """" + linkedIssue + """" transitionValidationResult not valid"""");                 }              }      {code}  h3. Expected results    Comment is added to issue along with other fields.  h3. Actual results   # Comment is not getting added to the issue for transitions that have no screen. I then separately add the comment using the same user account, and that is successfully added to the issue.   # When there is actually a transition screen and skipScreenCheck is set to true on the IssueInputParameters, duplicate comments get added.""",Bug,Java API
104612,"""Hi,    I have been working on integrating a Test management tool (www.testlodge.com) with JIRA using the REST api and just have a few questions that I was hoping you could help with.    1. Creating a ticket - Is there a way to make a POST request to create a ticket? If not is this a feature that will be added anytime soon or is there a work-around? This is one of the most important actions and without it the API is of no use.    2. More general feedback as its not critical, but it would be nice to have an action that allows me to list all the available projects in the account / or that the user has access to. I guess I would expect the response to only contain summary data such as name and project key.    Thanks  Scott Sherwood""",Suggestion,REST
105005,"""A customer was asking for integration between JIRA & Basecamp. Different departments in the company use different tools and thus the need for some form of integration.    I'm creating this ticket so customers can vote for it and add ideas for how the integration could work.     One integration idea:    * Replicate basecamp issue in JIRA and retain link between them  * Resolving issues in JIRA result in completed tasks in Basecamp  """,Suggestion,UPM (Universal Plugin Manager)
105296,"""When setting permissions on a comment, clicking the padlock icon once brings up the menu. Clicking it again does not dismiss it.  I've marked it as major because: * I personally find it very annoying,  * It's a regression from 4.1.2,  * Every other drop-down menu in JIRA closes on the second click (with the exception of Create Issue, which has a Cancel link).  * Now that the permission value is no longer displayed, the users will need to open and close the menu just to see what its value is set to.""",Bug,Issue
105612,"""As a Development leader/(ScrumMaster)/Testleader, I want to be able to edit sub-tasks at the same time as I change the main-task. At least on these fields (and custom fields also):  * Priority  * Release version  * deadline  * Comment  * Released date    *TEST*  Use a filter with maintasks only. (Some of the main-tasks containing subtasks). Goto Massupdate, and tick off a few tasks. Somewhere in this dialog it should be a field to tick off *Update also sub-tasks*, change for instance [Release version] this shall be updated on all tasks and connected sub-tasks  ;-))""",Suggestion,Issue
105619,"""In our Jira 4.1.2 database there are a few issues that are invalid. (They're victims of Jira 3 Bulk Move bugs like JRA-18453. After a move to another project their security level is still set to the old project and thus invalid. But I'm not filing on that bug as I believe that has been fixed in Jira 4. I'm just stating this as an example of how invalid issues still live in our Jira 4.1.2).    If a user runs a JQL function that returns JiraDataType.ISSUE and the function's results hit one of these invalid issues, they expect to get all their results. If there was an invalid issue, there should be perhaps a warning message that says, """"Issue ACME-8242 is invalid,"""" and the rest of the results are returned.  Instead they get 0 results and an error message that says only, """"A value provided by the function 'jqlFunction' is invalid for the field 'issue'.     !invalid-issue.png|thumbnail!    I refer to this bug in a related JQL Improvement request, JRA-22256.""",Bug,JQL
105751,"""DefaultServiceManager method getServiceWithName does not return the loaded service instance, it returns a new instance of the named service.     This can easily be replicated by attempting to access the lastRun time of the service returned via this method it will always return 0.    DefaultServiceManager method getServiceWithId correctly returns the loaded service instance.    As a workaround i'm calling getId on the service container returned by getServiceWithName, then using this id with getServiceWithId to return the loaded service container.""",Bug,Scheduled Tasks
105790,"""For some reason the instance that I'm running is very slow.  I've gone through all of the help documents and forum posts related to optimizing the speed, but I can't seem to get anymore performance with JIRA, which is sad because it really slows me down sometimes.    It would be nice if there was a way I could quickly create multiple issues, say five or ten, where I just fill in the summary and part of the description, then I can come back later to round out the details.  This would at least allow me to quickly get the ideas out of my head and into JIRA""",Suggestion,Issue
106083,"""The step for """"Changing the Windows user that the JIRA service uses"""" is confusing customers. It tells to locate the *Apache Tomcat* service but if JIRA 4.1.1 installed as a service, the name of the service is *Atlassian JIRA 4.1.1 JIRA*. I've attached the screenshot for this, [^screenshot-1.jpg].""",Suggestion,Documentation
106261,"""I would really like to have a Resolved vs. Closed Chart gadget in JIRA, very similar to the Created vs. Resolved chart.  As a tester with a large chunk of bugs that need to be verified/closed, I need to see if we are making progress, or if bugs are still being resolved faster than we can verify/close them.    Is there any easy way for me to copy the Created vs. Resolved chart and change it to Resolved vs. Closed?  Is there another gadget available that I could use for this?    I also see some value in having a Created vs. Assigned chart to see how fast developers are accepting issues.  Maybe it would be worth it to have the """"Created"""" and """"Resolved"""" be options to select in the gadget while editing it?""",Suggestion,Dashboard & Gadgets
106317,"""Hello Support Team,    it would be very helpful for me, if you could give me a bit more Information about """"indexing"""" and re-indexing in Jira Version 4.0.2.    I've read a lot of pages in the internet regarding this, but I can not found a solution to handle re-indexing as a service.    My questions are:    1. Is there a default for re-indexing or not?  2. Which steps are to do, to setup a re-indexing as a service if there is no default?    Many thanks for your support      """,Suggestion,Scheduled Tasks
106591,"""Build a new gadget that would be added to one's dashboard that would show all issues that have been updated in some way that they """"typically"""" would have been notified on via email (reporter, watcher - assignee would be optional).    Instead of sending an email (or as an option, one could choose to only send notifications, only load into this gadget, or do both).  As the user reviews the change to an issue, they would check off that they are done with it, and it would fall off the gadget and no longer be displayed.    I am desperately trying to reduce our reliance on email, and although email notifications are great for some things, it would be far better if most of this information was just contained in the JIRA tool itself, and one could go to the dashboard, review this gadget for a list of all issues that had some change to it that have yet to be reviewed, and then check it off in the gadget since you have now reviewed it.  This would allow a dramatic reduction in emails being sent.    Perhaps additional functionality would be an option to have a digest sent from this gadget on some period - say daily or weekly - to remind the person to go to review this gadget, but this would be 1 email a day or week as opposed to the at least ~70+ that I get now each week due to people updating an issue in some way that I either reported or that I am a watcher on (I am in a leadership role, so I am tied to a bunch of issues in our system).    I believe the ability to send out an instant email notification for someone who was just assigned an issue should still be allowed, but perhaps optionally.  I personally would leave this set to instant when I am assigned to an issue, and I would ask my team to do the same, but some users might want this to be an option.    I know there is a """"to do"""" list macro for Confluence that is similar to this request.  The difference is that this would be dynamically built by the JIRA gadget, and it would have to retain (persistent) all changes that you have not reviewed since the time the gadget was added to the dashboard.  Once the item was checked, it would be permanently removed from the gadget and no longer display (just like when I delete my notification email out of my inbox never to look at it again), but obviously all historical data is still retained in JIRA.""",Suggestion,Dashboard & Gadgets
106777,"""I've been trying to see if there was a good way that would allow me to accomplish the following:    1. As an admin, I go to JIRA and create a new project. This action also results in the creation of a new space in confluence and the appropriate groups in crowd.    2. A more integrated version would allow me to identify an existing SCM repos or create a new one; link it with Fisheye, link a bamboo plan etc.    This is similar in concept to GForge where as I create a new project, I create new wiki's, SCM, tests etc.    Doesn't look like there are any existing plugins to achieve this. I think it would be a cool capability to have in JIRA""",Suggestion,Project
106791,"""I find it hard to reply to a comment in JIRA now because the add comment box is at the top of the screen, while the comments (including the most recent one I want to reply to) are at the bottom.    Two requests for further UI work in 4.2:    - put the comment box back where it was - at the bottom of the screen, rather than the top  - put an 'Add Comment' button at the bottom of the comments (as requested in JRA-11682).    This follows a pretty standard usability guideline: put controls near the content they act upon. Reply and add comment links should be somewhere near the comments, not up in a toolbar that has a bunch of unrelated functionality.""",Bug,Issue
106795,"""I'm using greenhopper and have set the issue type schemes to """"GreenHopper Scrum Issue Type Scheme"""" on 2 projects.   I'm trying to move an issue of type """"story"""" between these 2 projects with the """"move this issue"""" operation on the issue page. It fails at first step (clicking next after selecting the new project, and letting the issue type as is) with this error message : """"The issue type selected is invalid."""" No error or exception in the logs.    Strange things are :  - issue types are correctly recognized (trying to move to a non scrum project forces to change the issue type)  - moving to a default issue type succeeds (bug or improvement, but not epic which is also a greenhopper issue type)... but of course I loose all the extra fields (business value, story points...)  - bulk moving the issue (from tools/bulk change on the issues list page) succeeds and keeps all data (issue type, extra fields...)    So I have a workaround by """"bulk"""" moving the issue I want to move, but it is an inconsistent behavior that you might want to fix.   Sorry if it has already been reported but I couldn't find it in your Jira.""",Bug,Issue
106829,"""As an IT Manager, by having to add users to the Administrators group in order to edit and manage workflows is prohibitive to the administration and security of our Jira environment.  While I want users to create, manage and edit workflows, I do NOT want them creating or modifying accounts (which comes with the Admin priviledges).    Editing/Managing/Creating workflows should be a project scheme, or have the permissions seperated out from the Administrators group.  I.e. have a global """"Workflows Administrators"""" group, or something like that.    Thanks,  -Adrian""",Suggestion,Project Administration
107100,"""I'd really like the ability to add new customfield options to custom fields, as we have one custom field containing many hundreds of customer names, and adding new entries to this list is a bit of a hassle.  Currently only administrators can do this, but we'd like to be able to do it through Jelly scripts and have them executed as a bulk process when added to the system.    Open to other suggestions of how to achieve this without the use of an administrator""",Suggestion,Issue
107205,"""We have severe performance problems with users at client sites who use IE7 and have high-latency SSL connections. As this affects a large portion of our 1000-odd users, this prevents us from upgrading to 4.0.1. This is a disadvantage of the gadget architecture, and I cannot see it going away soon.    I'd like to air a half-baked idea: Would it be possible to construct an *optional, alternative dashboard* rendering engine, which would render the gadget, including all its data _on the server_? It would act as a kind of """"proxy"""" to the user's browser, in fact emulating a browser and sending one stream of fully rendered HTML to the client's browser. As this would all happen in one request between client and server, latency and JavaScript performance would not matter. Within the gadgets, all the same mechanisms and calls as in 4.0 would take place, but on the server, with virtually zero latency.     As said, this would be an optional alternative Dashboard, perhaps best controlled through a user setting, and/or an URL parameter.    It would keep all the API and advantages of open gadgets, without the performance penalties incurred in legacy / constrained environments.    Best regards,  Martin """,Suggestion,Dashboard & Gadgets
107226,"""There are cases in JIRA where data submitted in forms is sent via an ajax call.  If the user is on an internet connection that is unstable, such as a train or mobile, the calls may not complete correctly.  However, the user interface does not allow the data to be re-submitted.  This means that the user needs to copy the data they wished to enter, reload the whole issue and then try again.  This can lead to frustration.    See these quote from a mailing list I am on:    {quote}  [the network] caused all sorts of havoc with JIRA, which has no error handling at    all. If the AJAX call didn't work, you couldn't click on the button to    try again, you have to cut and paste your comment somewhere else,    completely refresh the page, paste the comment back, then submit again    hoping it will work this time.  {quote}    In reply, another user listed a similar experience:    {quote}  I've done the same thing many times when I've tried to use JIRA on a train. I end up with a word document containing my comment for each issue and then have to copy/paste them into JIRA when on a stable connection.   {quote}""",Bug,Infrastructure & Services - Application Lifecycle
107230,"""Currently, when I add a new custom field, it will be available in all field configurations, and I need to go through all the field configurations to hide the field.    Actually, what I want is that it is added as a hidden field.     """,Suggestion,Issue
107455,"""I just did a JQL search where I wanted to filter by component. When I did """"component = """", I got a dropdown with suggestions, but the particular JIRA instance I was looking at had hundreds of components. The autocomplete results only showed components starting with letters """"A"""" and """"B"""". There was no scrollbar, or even a visual indication that more results might be available. If I typed the first letter of the component, that did start listing components starting with that letter, but I think the UI here is unintuitive and not user friendly.    A scrollbar would be a start, but still probably wouldn't scale to hundreds of elements. And, as an aside, it would be even nicer if JQL pre-filtered the components based on the project I selected. That would obviate the problem somewhat since most projects don't have hundreds of components.""",Bug,JQL
107648,"""When a Two Dimensional Filter Statistics gadget is configured to show statistics on a Filter that does not match any issues, the message """"The filter for this gadget did not return any issues"""" is displayed.  I would suggest that either the word """"filter"""" should link to the Issue Navigator view for that filter, or that the name of the filter should still be displayed as a clickable link.""",Suggestion,Dashboard & Gadgets
107754,"""Activity streams are great !  They could get better with the introduction of hierarchical activity streams :-)    The way that most JIRA instances get configured are (from what I know)    Projects -> (Versions, Components) -> Parent Issues -> Subtask issues -> (Worklog, Commits, Comments & Edits)    For instance if I'm looking at a certain parent issue (such as a story), I would like to see all activity related to that issue, including the subtasks.      If I'm looking at a version (or component) I'm interested in what has happened with that version.  Currently the activity stream either shows to much (like in the activity stream gadget) or too little (like in the activity stream panel of an issue).    It might be an idea to base the activity stream on a filter (?)      """,Suggestion,Issue
107971,"""As a JIRA Administrator    I'd like to have the ability to change the field type of a custom field    So that I can edit an existing custom field instead of having to create a new field to replace it, recreate all of the configurations for that field and migrate the data""",Suggestion,Admin
108217,"""My instance of Jira currently manages ~80 projects.  Project roles are used extensively for permission schemes in these projects, and default project role membership is set by using user groups.  When the membership of one of those groups changes, it does not cascade retroactively into pre-existing projects (and should not according to docs).    There should be a method of force-sync or force-refresh, however.  I don't create many new projects, but I do change group membership (both add and subtract) somewhat regularly; administering the addition of 1 user to ~80 projects by method of manually adding that user to each project is quite cumbersome.    I'm suggesting that a method exist for a single project, as well as a global method to refresh all project roles in all projects from the default membership assigned in the project role browser.""",Suggestion,Project Administration
108245,"""When creating an issue, the Component select box contains """"Unknown"""" as well as a set of components.  Today I found myself creating an issue against a project with which I'm unfamiliar, so I selected """"Unknown"""" because I didn't know, and one of the components I thought might be relevant.    When I submitted the form I was told:    {panel:bgColor=#fcc}{color:red}*You cannot select """"Unknown"""" with a specific component*{color}{panel}    I was also told that I had to reattach my attachments, which was pretty annoying.    If I'm not allowed to select """"Unknown"""" and a specific component, then don't let me do it, or at least instruct me near the box that I'm not allowed to do it.  I'd be happier if the system just threw the """"Unknown"""" away if it has no meaning.      (I guess I would be less annoyed by this if the uploaded files weren't thrown away, see [JRA-16335])""",Bug,Issue
108486,"""Using keyword search i am not able to search a particular keyword as a last entry. Meaning, i have added comment saying that """"Verification_Pending"""". This comment may be in many places in the comments entry.     I need only the issues which has """"Verification_pending"""" as the last comment. If this comment is added in middle, that issues should not be listed.""",Suggestion,JQL
108832,"""Some portlets & gadgets have a feature to show """"Major versions"""" only. At the moment this only excludes . (e.g. 4.0.1 but not 4.0-m1 or 4.0 m1). Is is possible to make this be a bit stricter and treat 4.0-m1 or 4.0 m1 as minor version. Maybe you could make it configurable through some kind of global props.    I would also understand if this gets closed as a won't fix since it's all a bit of hack anyway :)""",Suggestion,Dashboard & Gadgets
109187,"""I am actually not sure if this is working as intended or a bug but I thought I'd file it just in case.      If you create an issue in one time zone, and then change the time zone of your Jira instance to an earlier time zone and create a new issue, you will find that your second created issue has an earlier create data than the first one.  So, the first issue's create date is not representing itselft itself in the new time zone.""",Bug,Issue
109704,"""I would like to have a custom field that can display not only the current value of the field, but also its previous values. This is vaguely similar to, but also somewhat different from the Change History. I have a specific problem to solve, but I also think there might be other uses from this functionality.    The specific thing I'm trying to do is in a workflow that has a review process. The review cycle may be executed an unknown number of time (usually more than once). Each time through the process, I want to capture the name of the person who is reviewing, an indication that they have signed off (or not), and the date & time. I initially thought I would just be able to create a custom field for each of these three bits of information, but then I realised that if the issue needed to be reviewed again, the next reviewer would """"over-write"""" the previous reviewer's field values.     Although the changes would be logged in the issue's Change History, it might not be easy to find them. What I really want to see is all the previous values of JUST these three field, all in one place. Effectively, this would be a table with each row showing the reviewer, their sign-off (or not), and the date.    Firstly, is there an existing way of doing this? I'm relatively new to JIRA, and I'm still learning how to configure it.  If not, is there a third party extension or p[lug-in? I've searched and haven't found anything, but I'm not sure what to call this type of functionality: audit history? field versioning?     Where could this sort of functionality be used? Anywhere where more than one user has in input.  Reviews, approvals, and sign-offs - all the sorts of activities where you're expecting parallel inputs  Voting  - eg. you want to record a score from 1 to 5 from multiple users.  Budgeting - eg. various team leaders can enter the anticipated costs from their own team.    thanks,  P""",Support Request,Issue
109779,"""When I delete a project version, it prompts me to swap the """"Affects Version/s"""" or """"Fix Version/s"""" field values to another valid version.  It does this correctly, however I would expect that any custom fields which are """"version"""" type would also be changed from the deleted version to the new version I have chosen.  It seems they are not changed, so the custom field values show up as blank after the version is deleted.    As a workaround, I guess I can run a SQL query to change the customfieldvalue for all issues in the project.    Thanks,  Bill""",Bug,Issue
110024,"""All validators must pass in order for a transition to complete. Currently an issue will fail as soon as one validator fails, without running subsequent validators. The disadvantage of this is that it staggers the error messages.    For example in my transition I have several validators, in this order:  1. Fields Required (About a dozen fields must be filled out)  2. Date Compare (I want to validate some date fields)  3. Comment Required (Using a custom plugin I ensure a comment is entered)  4. User Picker Validator (Using a custom plugin I ensure a user picker has a minimum number of users selected and that they have been selected from a particular group)    When a user submits a ticket in violation of all four validators they only see the error from 'Fields Required'. When the required field is added and they resubmit they will then see the error from 'Date Compare', and so forth. The overhead of running all validators is low and, while not logically necessary, having the ability to display all error messages simultaneously would enhance usability. I receive complaints from users who dislike having to keep correcting errors only to resubmit and receive a whole new set of errors.  """,Suggestion,UPM (Universal Plugin Manager)
110191,"""As administration I need to be able to set the comment view by options depend on roles and to restrict the permission of change it (edit or readonly) so that when the user belong to that role or group the comment field within the issue page set the defual viewable by option and set its view mode depend on the logged in user.  As an example i have 3 parties with different roles  > first role currently logged in then thier comment is viewable by all and they don't have permission to set it to any other option.  > second role thier comment viewable by the third role only  > third role have the permission to set it to all or any other roles""",Suggestion,Project Administration
110331,"""I'd like to automatically transition an issuetype to new type, WIBNI as a workflow transition.  I've tried this through a jelly script, but this has no effect (don't think it's allowed)  How would I transition this automatically    I.e. user reports a bug, which we determine to be an enhancement request, and transition to this type directly.  Is there any other way to acheieve this?""",Suggestion,Java API
110624,"""I own a dashboard that is not a favourite. When I click to display 'System Dashboard' directly I get the following message for the dashboard:    {quote}  'You are viewing the 'System Default' dashboard, as you have not added any dashboards as favourites. To display your preferred dashboard here, click Manage Dashboard and add one or more dashboards as favourites.'  {quote}    I would expect the dashboard to display an error message of the form:    {quote}  The dashboard that you are viewing will only be displayed temporarily, as you do not have it added as a favourite. To display this dashboard permanently, add it as a favourite.   {quote}    since I have asked to display the page explicitly.    """,Bug,Dashboard & Gadgets
110691,"""When moving issues with bulk change, you are warned that versions are not retained from one project to the other:  """"Issues not in the project Apogee Portal will not retain values for Affects Version/s"""";   """"Issues not in the project Apogee Portal will not retain values for Fix Version/s"""".  (Besides, I think the Retain check box there is therefore obsolete).  But more importantly, you are not warned for custom fields which also use that version list (Version picker field).  The retain check box is available, so you assume these values will be retained.  But since it uses the same values as Affected version and Fix version, of course it is not retained and written with the value you selected (default Unknown).  I would expect the same warning as for Affected/Fix version.  The same is valid for Assignee/Reporter that is not a member of the new project. This field is filled in as Anonymous.  Understandable, but there was no similar warning as for Versions.    Consequence: after I did this Move, I had to correct all the Verified in Version values manually.""",Suggestion,Bulk Operations
110725,"""When I create a custom field, I would like the ability (on certain types) to be able to associate an image to the CF - this is similar to how the 'issue type' field works.     As an example, I have a field called 'health' - this may have red, orange, green as 'values' but I would like to associate an image to help distinguish.""",Suggestion,Issue
110838,"""If JIRA would allow me to create an actual hierarchy of issues (not just sub-tasks, it should go down to sub-sub-sub-sub...tasks), then it will have everything I need for Requirements Management in JIRA    The features I would need are:  - ability to set """"parent"""" issue when creating an issue  - ability to view Project issues as a tree  - automatic issue resolution when all child issues are resolved    Example of issue hierarchy:    + ROOT Issue (Type: User requirement): Manage Customers     + Child Issue (Type: Technical Requirement): Ability to Create Customers         + Child Issue (Type: Task): Dialog for entering customer data         + Child Issue (Type: Task): Dialog for entering customer data     + Child Issue (Type: Technical requirement): Edit Customers     + Child Issue (Type: Technical requirement): Delete Customers  ....    """,Suggestion,Issue
110879,"""Most of the charting portlets have a """"Days Previously"""" field that allow you specify a window on the chart in question.  In many cases I find that I'm not interested in a window on my filter - my filter defines the window exactly as I want it.  I'd like to be able to leave the """"Days Previously"""" field blank and have it simply show all the information within the filter.  The initial date can easily be worked out from the earliest creation date of issues within the filter.    As an example, I'd like to use a Created vs Resolved of issues Fixed For my next release.  Our releases happen on an approximately quaterly basis but in the first week of the project I'm only interested in looking at that week's progress and towards the end of a slipping release I want to be able to see the whole of the release's progress and not risk the """"90 days previously"""" clipping off some of the issue creations that happened early on.  At the moment, to acheive this I have to edit my window each time I look at the chart recalculating the appropriate number of Days Previously.""",Suggestion,Dashboard & Gadgets
111006,"""We are trying to find the function for blocking mail from anonymous senders, creating comments into open issues in JIRA.    The funcion we mainly need this for is to prevent an possible flood of comments into issues created by the """"Out of office"""" setting in outlook this summer holidays. The scenario ( and I would be surprised if you have not heard this before ). :    A person is a project member, he is on holiday and has enabled the out-of-office function in his outlook. Another project member is making a comment in the project which produces a mail to all watchers of the project. The outlook of the person on holiday bounces back a mail to Jira that he is on holiday. This mail ends up as a comment in the project and Jira sends a confirmation back to the user... ...    How do we stop this?    //Michael Friman""",Suggestion,Scheduled Tasks
111020,"""I would love a counter, that starts when you press """"start progress"""" and then stops the count when pressing """"Stop Progress"""". It will sum up the spent time concurrent with the work being done. - Ergo: If I go into a task five minutes in, I will see five minutes spent. If I go in ten minutes after that I will see fifteen minutes spent. When the """"Stop progress"""" is pressed, the sum up will stop.   Then you only need to enter the """"extra"""" time spent on an issue. - And the actual spent time when working on the issue, will automatically be filled out.   It would save us coders a LOT of clicks and entering when working on small bugs and issues.    Just to be crystal clear:  This is regarding starting / stopping the progress on an issue when a coder / supporter is working on an issue. Today he/she has to manually enter the time spent wheter the start/stop progress is pressed or not. - we would like a sum of the spent time to start when pressing start progress. - This way the reporter can follow the time spent and the assignee does not have to manually fill out this field afterwards since that part is done by JIRA automatically.    We could REALLY use this feature.    Thanks a bunch in advance. - And thanks for a fantastic tool. JIRA has already made our lives a bit easier.  Have a great weekend and I hope to see this request implemented in the future.  If you have any questions do not hesitate to send me a mail.    /Jens Bay    """,Suggestion,Issue
111168,"""I would like to be able to edit the """"System Default Field Configuration"""" that is used as the Field Configuration Scheme on new projects.  There are changes that we would like to make Globally, but currently we must change the scheme for _every single project_ we create.    As an optional bonus, it would be nice to be able to specify which Field Configuration Scheme to use during project creation, instead of having to go back and edit that setting.    Thank you,  Adam Bevers  Network Technician  Frasca International, Inc.  217-344-9200 x243""",Suggestion,Admin
111327,"""I am trying to convert a Sourceforge XML dump into Jellytags.    As I need to create several users from my previous dump, I was about to write, in my script, a random password generator.    But CreateUser could do it ! Creating an user with a clear-password is not that good, for privacy concerns.    Thanks :)  Nicolas""",Suggestion,Documentation
111491,"""Right now the default Due date field just allows me to pick a day as the actual due date. I would like to be able to choose a day and time as the due date for each issue.     I can create a custom field to do this but that wont allow me to hook it in with the Schedule Issues permission scheme.     Please let me know if there is a way to do this or to add it as a feature.      """,Suggestion,Issue
111645,"""I am currently using a trial of JIRA Professional.    I have created a user that is in the Jira-Users group only. I have also create 3 dashboard pages for the user. One default dashbord page and other two pages are specific to certain projects.    However when i try to add any portlets to the third page they do not appear on the thrid page. Rather they are always added to the first page which. This is not the case with the second page.     So far the only workaround i have found is if i move the page i want to setup to first position then add all the portlets i want then move it back to third position. I was also getting NPE in the apache persistance layer you use when trying to delete some portlets on a page. Unfortunately i don't have the stack trace but will add to this bug if it happens again.""",Bug,Dashboard & Gadgets
111655,"""It's nearly impossible to work with the Full Configure Dashboard when you have multiple items of the same portlet type.      I need to use about 5 or 6 different """"Two-Dimensional Filter Statistics"""" reports in my dashboard, and all the configure UI says is """"Two-Dimensional Filter Statistics"""" for each one.  Not very helpful.  It would be better if:    # the item gave a """"hint"""" of the assigned filter.. like """"Two-Dimensional Filter Statistics: *Open in 2.0*""""  # there was a """"short name"""" associated with each filter type, so it wouldn't take up so much space in the Configure interface.  Or, just make the boxes a lot wider.  :-)""",Suggestion,Dashboard & Gadgets
111683,"""Hi Rosie and Andrew,    I have just added some content to the Logging and Profiling page of the JIRA documentation, on how to make Profiling permanent (i.e. still enabled after restarting JIRA). I committed it to the trunk as revision 48955.    It was a moderately substantial addition, so could one of you please review it and clean it up if necessary. I'm confident that it is technically correct, but it might need some technical writer love and care.    Thanks.    Kind regards,  Ian      """,Suggestion,Documentation
111739,"""I raised one support issue JSP-17119 about index timed out exception some days ago, with help of Zhen Yueh, Lean, I found out the problem and would like to suggest JIRA make improvement on Index Optimization part:  JIRA introduced daily index optimization, it was set to 0:00 every day. The optimize method in DefaultIndexManager will accquire index lock, all other changes (e.g. create, edit) won't get index lock until optimization done. In our case, optimization took 75517 ms, while the INDEX_LOCK_WAIT_TIME is only 30000 ms, that means, within about 45s after optimization started, any changes won't be indexed and definitely got index timed out exception.  We'll change INDEX_LOCK_WAIT_TIME to greater than 75517 ms, but I would like to suggest JIRA to be improved to enlarge or ignore INDEX_LOCK_WAIT_TIME when optimizing since for normal issue index, more than 75s should be recorded as an error, while not when optimizing.""",Suggestion,Indexing
111775,"""QA has reopened closed issue that was assigned to released and archived Fix Version.  I wanted to reassign it to one of planned releases.   As result Issue is assigned to two versions, see screenshot.    Also side bug of this bug - issue assigned to two versions. Is it correct?""",Bug,Project Administration
111780,"""When I grant permissions to certain actions in the permission scheme I can add multiple criteria (based on role or group etc) but I can't seem to be able to grant permission to combinations of these criteria. For instance if I have the role """"tester"""" and """"manager"""" I'd like to be able to grant permissions for certain actions to those users who are flagged as acting in both of those rules rather than the current system where I'd have to create a role """"test manager"""" (or something similar).    Taking it a bit further it would be nice if I was able to define permissions with operators:  Project Role (Tester) AND Project Role (Manager)  Current Assignee OR Group (Administrators)  (Project Role (Developer) AND Project Role (Manager)) OR Group(Adminstrator)""",Suggestion,Project Administration
111869,"""Please do not mark this as a duplicate: I am not talking about setting remaining estimate to zero.     I would like to be able to change the remaining time for an issue as part of a project management review of the remaining issues. In otherwords, not as part of logging work, but as part of a general housekeeping/restimation process. I would like to be able to change the new estimate to any value including zero, without having to fill in the 'time spent' value. I used to be able to do this in 3.7, and I can't do it since upgrading to 3.11.    We are using JIRA as a PM tool a great deal, and this new feature of work logging is quite disruptive.""",Suggestion,Issue
112175,"""why the comments field content can be output when exported as an Excel file.    but save it as a word file, it can be included.    i'd like to see this feature!~~~Thanks""",Bug,Issue
112234,"""I need to have the ability to set the working days other than the default.    For example we (in Israel) do not work on Fridays but we DO work on Sundays.  In addition, we have our holidays which should consider as non working days while several other holidays which might be considered as a non working days are totally regular working days for us.    The meaning of lacking this feature is that I cannot keep accurate track of the time it takes to handle a bug reported to my Jira.  This makes the Jira less useful for our needs as a utility that supports project management.    {panel:title=Atlassian Status - 24/June/2010}    Since GreenHopper now serves most of this functionality with regards to tracking time and reporting this information, we will not tackle this issue against in core JIRA, but recommend users to look at GreenHopper.    GreenHopper (5.0) currently allows you to track non-working days (now defaulted to Saturday and Sunday) and takes those into account for reporting purposes. The ability to choose other days of the week as non-working days is currently an open feature request in GreenHopper. Please track this at GHS-806.    {panel}""",Suggestion,Project Administration
112274,"""I developed a simple text renderer that does nothing more than wrapping the comment text into <pre> tags. The renderer is correctly recognized by JIRA.    However, when I want to change the comment field to use the renderer, I get the following error:  ---  Errors  """"Tried to add a renderer type: fixed-text-renderer that is not know to the system.  ---    Steps to repeat (fresh install):  * Copy plug-in (attached to this bug report) into edit-webapp/WEB-INF/lib  * Build, deploy & start server  * Complete setup wizard  * Log in as admin  * Go to Administration -> Field Configuration  * Click on 'Configure' for the default configuration  * Click on 'Renderers' for the comment field  * Select 'Fixed-Width Text Renderer' and click 'Update'  * Confirm update  => Error    """,Bug,Issue
112525,"""A few problems (maybe even more then 1 issue)    We had the following resolutions: waiting for a meeting and on hold.  We changed the workflow so that they became statusses.  Problem is that you can't put the resolution on unresolved when deleting the resolutions.  Also the resolution can't be changed with reopen or edit.    I gues that I want the possibility to change the resolution as an editable field and that it shifts to unreolved if you reopen it.    For the moment they have resolution fixed (even if they are still open!)    All this is caused by the fact that when copying workflows the postfunctions and conditions aren't copied (I don't remember in what version I copied them so maybe it's already resolved)""",Support Request,Bulk Operations
112712,"""I found some defect in work of web method addComment(string in0, string in1, RemoteComment in2) of RemoteComment class.  When I tried to set author of new comment, author was not set as I wanted, but current user:                    JIRASoap.RemoteComment rc = new global::JIRA_API_Class.JIRASoap.RemoteComment();                  JIRASoap.RemoteUser ru = new global::JIRA_API_Class.JIRASoap.RemoteUser();                  rc.body = strBody;                  rc.roleLevel = """""""";                  rc.author = """"stella"""";                  JiraService.addComment(strSessionId, strIssueKey, rc);    It is C# source :)    Then I downloaded the latest version of jira-rcp-plugin (version 3.8-1) source and fix some files, to add this feature. You can see these files in attachment.  My code marked with """"// THIS WAS ADDED"""" comment.    I hope this helps you. Thank you for good product ;)    P.S. Sorry for my english""",Suggestion,UPM (Universal Plugin Manager)
112759,"""Currently you can only watch a singgle issue at a time. I would like a new feature that allows a user to assign themselves as a watcher of multiple issues (all at the same time).""",Suggestion,Bulk Operations
113038,"""To reproduce:    - login with Internet Explorer to your Jira Enterprise 3.7.2 application  - Go to an issue which as some sub tasks  - You will see a numeric list of sub tasks: 1. Some title; 2. Some Other title and so on.    With Firefox 2.0 or Konqueror 3.5.5 (I tested with these 2 browsers) I see that the number (in my example: """"1."""" and around """"2."""") has a red background color if trhe issue is open. If the issue is closed/resolved: the number has a green color.    With Internet Explorer 6.0 and 7.0  there is no background color.    I've just upgraded from Jira 3.4.3 Enterprise. In version 3.4.3 Enterprise the background colors where shown correctly so I guess this CSS/HTML bug is present from a version newer than 3.4.3.""",Bug,Issue
113539,"""Dear Atlassian,    I would like a report which would display a snapshot of the progress of any version.  The x-axis would be time, and the y-axis would be number of issues.  Then, with a color-coded legend, the number of issues in each workflow state would be indicated as a bar for each day.    Attached is a screenshot from our JIRA system (UnresolvedIssueReports.jpg), which displays the total number of issues in our 2.20.1 release, which of course increases over time.  Below that, in green, is the total number of 'resolved' issues.  Since the only issues which change from 'Unresolved' to 'Resolved' are those which are closed or reopened, we have difficulty tracking the progress of the project.  I need to see how many issues are submitted, open, fixed, testing, passed, reopened, or closed.  Since we only close issues before the release if they are duplicates, or 'won't fix', there is no way for me to track the health of the project.  Additionally, since the reopened issues automatically get the 'reopened' resolution, we could be in trouble, believing that resolved issues are good.  We get around this by setting passed issues to have a non-unresolved resolution, but I'ld like some more granularity.    I am also attaching a screenshot from an Excel report that sort-of does what I expect.  Note the colored bars indicating total number of items in each status.  Of course, I would really want to order the bars in a workflow that makes sense (Submitted > Open > Fixed > Testing > Passed > Reopened (Failed) > Closed) and then you get into the issue that the workflow statuses must be numbered by an administrator, so that they aren't just sorted alphabetically.  You can see from the screenshot how easy it is to watch the number of submitted items decrease as the number of testing and passed items increase.  If items get moved to future versions (as they do frequently), then you can easily determine the number of issues in the release from the top line.    I could prose on, but I don't want to submit more than one idea per issue.    Thanks  grace@geospiza.com  """,Suggestion,Dashboards & Reports - Reports
113903,"""Currently, we have an issue security scheme that specifies a security level of """"Reporter plus support."""" Of course, the """"Users/Groups"""" for this level is """"Reporter"""" plus the group of users that comprise the support folks. This has worked really well for us.    Some of our """"customers"""" have more than one user that submits issues, and they would like to see all the issues reported by their """"company.""""    Rather than try to manage this with separate groups (one for each customer, I would like to try this:    We need something like a """"Reporter Domain"""" user/group and when this is specified, security checks would work similar to the """"Reporter"""" user. But instead of comparing the id of the current signed-on user with the id of the reporter of the issue, compare the e-mail domain of the signed-on user with the """"reporterdomain"""" custom field (that is implemented by one of the Atlassian supplied plug-ins) in the issue.    We would like to see this as both an issue security level and as a transition condition.    Clearly the execution code change is very small. The trick is going to be getting this configurable, translated, documented, etc.    A portion of this is already implemented in that we already have the """"reporterdomain"""" field configured.     Of course, using e-mail for this introduces it's own set of requirements on registration. For example, there needs to be a flag in configuration to require e-mail verification. That is, it should not be possible for someone to register with an e-mail address of tom@mycompany.com and then be able to look at all issues reported by anyone at mycompany.com without at least going through mycompany's e-mail system first.  """,Suggestion,Project Administration
114056,"""Some times I need search for several users as Assignee or Reporter (but don't search for existing group)""",Suggestion,JQL
114179,"""A custom field not associated to a project (via screen) is presented when the project is selected.    I understand from Jira support that the project / screen associations won't take affect unless you explicitly   context that custom field to those projects using those screen schemes.    To me, this logic isn't correct.    If a custom field is associated to a project, but is not added to the screen(s) associated to the project, that field won't be used for that project, and hence, should not have anything to search for against the selected project and shouldn't be presented at all.    In my implementation, I have created many custom fields which are open to all projects; but I confine what fields to be used for which project by associating the custom fields to screens.    Currently, if I need the fields to be presently correctly at filtering, I need to recontext the fields to the specific projects.  But that also means if I add a new project using an already defined screen, I also need to update the field to associate to this project.  Imagine I have 20 custom fields for a project using the screen with these 20 custom fields defined.  I am set to make mistakes that way.    The logic would be to take into consideration the screens associated with the projects selected in filtering, and recalculate the search menu.  Only taking projects into account (besides issue types) is not sufficient; must include screens as a factor as well.    Thanks.    """,Bug,Issue
114363,"""We have two projects that share a common custom field (cascading select) between them. Each project has a distinct context for the field. When we move an issue from one project to another, the """"Move Issues"""" screen doesn't pop up the shared field as one that needs to be updated as part of the move process. As a result, the moved issue now appears in the new project with an invalid, left over from the original project value. My users have to remember to go back and edit the moved issue after the fact to update the shared field's value to be valid in the new project.    For example,   *Cascading SelectA*  Context: Project A, All issues types  Options a-1, b-1, c-1    Context Project B, All issue types  Options: x-1, y-1, z-1    The cascading select field has different contexts for each project.  The problem is that when I move an issue from Project A with a value of """"a-1"""" to project B, it retains the value of """"a-1"""" and I'm never prompted to map it to the values appropriate to project B.     This is a fairly serious problem because we define work queues in our different divisions based on this cascading select.  If an issue is moved from one project to another with an invalid value in this custom field, it is, in effect, invisible to the folks in the destination and will languish.""",Bug,Issue
114794,"""Hi,     After some deletes or modifications, I encountered some service crushes. Also, sometimes, I'm able to create tasks but can't see them. I don't have any message.   After a lot of investiguation, I discovered that it was a problem of Indexes.   Is this problem present only in Standalone Demo versions ?    Thanks,   Slim.""",Support Request,Indexing
115021,"""I want to change the """"assignee"""" field of a number of issues, because the assignee is no longer employed, but stil gets mail from Jira ;-).  When i try to bulk edit these issues, the """"Edit issues"""" option is disabled, saying taht """"NOTE: You do not have permission to edit the selected 1 issues or at least one issue has a status that forbids editing.""""    However, when i start edting the issues one-by-one, it works fine, so i think that proofs that i do have enough permission and the all the issues are still in a state that allows editing.    As with all """"bulk edit"""" bugs, this is a very irritating bug, because it results in lots, lots of work.  """,Bug,Bulk Operations
115350,"""I'd like my subtask to , as a default inherit the parent's component.  presently this has to be added in.  alternatively I'd like to be able to assign the component in the subtask add screen under the parent task.""",Suggestion,Issue
115363,"""I'm wondering about how to correctly remove a user from JIRA. We have a collegue who has quit and he has no longer need to access our JIRA system. If we try to delete him we get a warning saying the user has asigned tasks. Do we have to reasign all his tasks(they are all closed/resolved) to another user or can we just go ahead and delete him and the tasks will go to unasigned?""",Support Request,Project Administration
115431,"""I want to create a custom  field not at the issue level, it should more be like the """"Components"""" or """"Version""""   filed provided by Jira ,i.e in addition to these two fields at the product level, I wanted another filed called """"Features"""" and then    create these """"Features"""" remotely irrespective of weather an issues is  being created or not and when the issues is created it should contain  a dropdown containing these """"Features"""" as a list from where the user  can select an affected """"Feature"""" for the issue.   """,Suggestion,Issue
115604,"""I visited the new User Default Settings screen and changed the default from text to html.  Then I pressed on """"Apply"""" and received the message:    Change Users Default Email Preference  Are you sure you would like to change all users who currently receive 'text' email to receive 'html' email instead? A total of 12 users will be affected. Note that this change cannot be undone, and should only be performed if you really know what you are doing.   I'm not sure what 12 users it thinks have the text setting (there should be none since my preferences-default.xml was always set to html), but I went ahead with the update. However, if I perform the update again, I get the same message again.  Which means it never really updated all 12 text users to html, or (the more likely scenario), there aren't really 12 users set to text in the first place.""",Bug,Project Administration
115908,"""Not sure if this functionality has been fixed, or is indeed intentional, but I thought I'd raise it in case it was an oversight. Our project's workflow does not allow editing of closed issues. However, when deleting a version, the system told me there was 1 issue to be fixed for that version, and offered me the chance to change the version for that issue. However, it was a closed issue, so I shouldn't really be able to change the version on a closed issue. I wonder if this is intentional so administrators don't get stuck?""",Bug,Project Administration
115944,"""Prior to upgrading to 3.3.1-#97, I had the ability to use bulk changes to make adjustments to closed issues whose attributes were set incorrectly. Now I cannot.  The specific example is the the """"Fixed Version."""" Sometimes a developer will forget to set this value or set it incorrectly. Since I use this as part of some quality metrics I need to update these fields. I also track """"ReOpens"""" to issues as a quality function of the development process, so while I can reopen an issue to fix the problem, this reduces the meaning of reopen. Since I have full administrative rights, isn't it reasonable that I be able to make this kind change?""",Support Request,Bulk Operations
115998,"""I was hoping we could add a new filter by date function. I would love to be able to create and save a filter where it would always be able to query for all tickets """"assigned to me"""" and """"due today""""   Currently in version 3.2.1 there is no way to save """"due today"""" as a criteria, you always have to specify a date. I would love I didn't have to specify an exact date and I could use whatever the """"current day"""" is as the criteria instead.  Thanks for considering this option.""",Suggestion,JQL
116527,"""Often I find myself wanting to create a filter that displays some arbitrary set of issues. This is particularly useful in conjunction with confluence. To give you an example, if I wanted to display on the wiki a set of issues that will be reviewed as a meeting.   To facilitate this, I would simply like to be able to type into the quick search """"JIRA-104 JIRA-49 JIRA-287"""" and have it return a list of those three bugs. Additionally, I would find it useful to do the same thing on Bugzilla ID.  As an alternative solution, but in my opinion not nescessarily a better solution in all cases, a custom field can be created, such as a keywords field. This field could be populated with specific keywords that can be used to perform the search, such as """"keyword: meeting-102"""". """,Suggestion,Indexing
116557,"""I would like to know how the backup service runs and what causes it to create a zip file. Currently it has a folder and zip file format setup with a 720min cycle time. I have looked at the data that is in the backup folder and it's not clear how the schedual works or what causes the back to run.   Can I please get some more information on how this works.  Thanks Dustin Baker """,Support Request,Scheduled Tasks
116856,"""We plan to implement Jira in our company and for this we must prove that regular updates from our old issue tracking system are possible. For this I am writing a Jira service that should get all the issues that have changed in the old system and create/update the issues in Jira.  For the update I have to check whether an issue has already been imported or not, thus I need to search a Jira issue by a custom field value. After studying your APIs for some hours the best I could come up with is the following code - which unfortuately tries to instatiate an interface ;-)      private Issue jiraSearchByTicketId(Integer ticketId) {         SearchRequest sr = new SearchRequest(true); // search without security checks         CustomFieldParameter cfp;                  cfp.setCustomField(cfTicketID);         cfp.addValue("""""""" + ticketId);         sr.addParameter(cfp);         try {             List issues = issueManager.execute(sr, adminUser);             return (Issue) issues.get(0);         }         catch (SearchException e) {             return null;          }     }  Any ideas/hints whatsoever to do this?""",Support Request,Scheduled Tasks
117178,"""I'm not sure if this is issue is with JIRA or with the jTDS classes.  When a custom field of type number is created, the values are saved in instances of the customfieldvalue table in the NUMBERVALUE field. This field has a data type of """"numeric"""". This is a data type intended for floating point numbers that stores the values in numeric formt (rather tha a binary floating point format). It can be defined with values for """"precision"""" (the number of digits) and """"scale"""" (the maximum number of decimal places to the right of the decimal point).  The default values for precision and scale are used (18 and 0 respectively), which effectively means that only integers values can be saved in custom number fields. The value for scale should higher (possibly 18 but I'm not familiar enough with MSSQL to comment).  The workaround is to edit the table design manually using Microsoft Enterprise Manager and set scale to a higher value.""",Bug,Issue
117271,"""This has happened to me a couple of times, and I think this would be an excellent improvement.  I have deployed an application (for bugfixes and unplanned enhancements) twice between planned releases, but the items that I have been working on that were released are still marked as being fixed in an upcoming version.  If you could allow users to specify the """"fixed"""" version as an """"Unreleased version"""", then then when a version is released, the system would check through the list of issues to see which ones had """"Unreleased"""" specified as the fixed version and set it to the recently released one.  That way, if I have release version 0.8, and I'm tracking issues to be released in 0.9, and I have a few sudden critical bugs that need to be fixed and released I can slip in version 0.8.1 and release some bug fixes and have anything that was fixed to that point tracked as being fixed in 0.8.1 instead of 0.9.  From an implementation perspective: Creating or editing an issue:  Selecting """"unreleased version"""" as the fixed version records fixedVersionId as -1.  Release a version: When an administrator (or lead) releases a version, loop through issues for the project, if fixedVersionId = -1 then set fixedVersionId = justReleasedVersionId.  Questions or comments: jira@pollensoft.com""",Suggestion,Project Administration
117455,"""I am just testing out 3.1.1 and of course this was the first issue that I wanted to verify, and I must say, I'm extremely dissapointed by the solution...  A mask edit field with a date format as a mask???  The open source calendering component that JIRA uses, with one small change allows the date AND time to be entered.  Why was this now used?  It would be consistent with the data field, and very easy to use.""",Suggestion,Issue
117636,"""1. New filter 2. Select component to search by 3. Press 'View >>' When results shown the previously selected component is no longer selected.  This is very annoying as I want to minimize search result by adding some more criterias and re-search but I have to re-select the component every time. """,Bug,Indexing
118074,"""I would like to be able to search for issues that have  - no value set in a specific field  - the value in a number field larger than a specific number  - the value in a number field smaller than a specific number  - the value in a number field NOT equal to a specific value (could be a combination of the two previous)  - no due date set  - a due date earlier than a specific date OR no due date set    These capabilities do not seem to be there through the issue navigator.    I would recommend that you take a look at IBM/Rational ClearQuest, which provides a very flexible interface for defining searches/reports, where it almost exposes the SQL query directly. Maybe you should keep the current navigator as an easy tool for beginners, but augment it with a more flexible tool for administrators and advanced users?""",Suggestion,JQL
118106,"""Hi there,    Is there a way to locate an issue based on its history??    For example, can I run a query to show me all issues that were re-opened at one point in time?? The issues *could* be closed now - but I would want to see issues that had a certaiin status at one point or another.    (Bugzilla has a 'Bug Changed' search field  where you can do this sort of thing - locate bugs according to a field change)    Thx!     BTW - we really like the product!    --Bill""",Suggestion,Issue
118344,"""Hi, atlassian. I made a custom field that connects via ODBC successfully, but I have to pass the 'jira-ProjectCategory' parameter  to the query, cause we use the ProjectCategory as a connection between our existing architecture. To do so I have to pass the parameter to the CustomerOrderViewHelper object. I'm trying to retrieve the Project Category this way:  CustomerOrderCFType extends StringCFType      public Map getVelocityParameters(GenericValue issue) <---     {         Map map = new HashMap();         System.out.println(""""DEBUG:beforemapinserting1"""");         try{ ---->       GenericValue project = ManagerFactory.getProjectManager().getProject(issue);             GenericValue category = ManagerFactory.getProjectManager().getProjectCategoryFromProject(project);             String cName = (String)category.get(""""name"""");             String catName = cName.toUpperCase();             map.put(""""customerOrderViewHelper"""", new CustomerOrderViewHelper(catName));         }      ...., but it raises a null pointer exception, of course, because issue is null! At that point it seems to me that the issue object in  getVelocityParameters(GenericValue issue) is meaningless. How can I use it, if is it always null.  However, is there any other way to get the ProjectCategory.  Thanks for your help. LukaS.""",Support Request,Issue
118366,"""  *Original request description:*     As JIRA user who creates issues and subtasks I want this process to be as easy as possible. Therefore I want the subtasks to assume the values of fields from the parent issue, so that I do not have to enter these values manually. But at the same time I want to be provided with a choice of the fields which will be used for propagation, because clearing redundant fields is going to also cost me some time. Among the propagated fields the following should be allowed for selection as a minimum: affected version, fix version, components and optionally: severity and labels.""",Suggestion,Issue
118614,"""Let me start with a custom field that we have available on the issue level.  We have a Bill As custom field to help us when we generate invoices.  We generate all of our invoices with the data in JIRA as this contains all of our times via worklog.  So an issue may be listed as Development - Billable, Development - Non-Billable, Support - Billable, Support - Non-Billable, etc...  What we've been finding though is that for a given issue, the majority of it might be Development - Billable, but the client may call up and as a quick question or so in relation to that ticket and we may want to log 15 minutes as Project Management - Non-Billable.  That's not exactly the best example but I think it gets the point across.  It would be awesome if we could have a custom field available for the individual worklogs so we could set that per log that was entered.  That would help us from having to adjust time after we generate the report to make the report look right and from having us to create a lot of other issues that relate to the one.  What would be even better is if that custom field could default to the main selection from the issue custom field.  In other words if we have selected Development - Billable for the issue.  The worklog custom field would default to that selection but could be changed.  I'd be happy with just the first part though.""",Suggestion,Issue
118656,"""I originally turned on time tracking to look at the time tracking reporting, but didn't change the defaults of 24 hours / day, 7 days / week (and why make that the default, anyway? How about 8/5?) When I went to view the time tracking, the 24 hours of work I had logged showed up as 1 day, so I went in to change the time tracking settings. It just showed me the current settings of 24/7 but no way to change it. I looked all around through the admin pages looking for the place to change this. I even went to the Jira instance on opensymphony.com to look at the admin console there, and saw that these could be set on the time tracking page in the admin pages, since the time tracking had not been turned on. I figured out that I needed to de-activate the time tracking by clicking the """"deactivate"""" button, then I was able to change the hours and days settings and re-activate. This is pretty confusing.""",Suggestion,Issue
118657,"""I was setting up some test issues to look at the time tracking reports, and noticed I hadn't put an original estimate on one issue I'd already logged time against. I went to edit it, and there was no field displayed next to the original estimate label. I was able to set an original estimate on an issue that I hadn't logged any time against.""",Suggestion,Issue
118709,"""We need a custom field that simply displays as an """"Edit"""" link, which when pressed edits the issue concerned.  Other operations could be handled similarly.  > When I'm viewing a list of issues, I often wish to update a particular > issue (e.g. assign it to another version).  In our current bug tracking > system, there is a link """"Edit"""" for each of the issues in the list. So I > can simply click it, update the issue and I'm back at the list. In > JIRA, I have to first to click on the issue, then press Edit link > (which is, BTW is located in absolutely another place of the screen!), > update the issue and then press """"Return to search"""" (again is located in > another place on the screen). That's much less convenient. I think > adding this little """"Edit"""" link for each issue would be a great > improvement (and very easy to do!). """,Suggestion,Issue
118745,"""We sometimes dont want our customers to see who is actually working on what item, as some bad customers then tend to talk directly to the developers (which is in many cases not very good) and overgo the project lead, which he is supposed to talk to.  What i would propose is a new permission in the permission sheme, called """"Sees Assignee"""".  As an alternative you could couple this permission together with the permission to assign someone.""",Suggestion,Project Administration
119131,"""I'd like to see the ability to associate issue field schemes with a particular issue type (not just per project).  i.e. a """"Support Request"""" requires different fields than an """"Improvement"""".  This would make it very easy to use JIRA as a helpdesk application, not just a bug/enhance reporting tool.  """,Suggestion,Admin
119353,"""This strikes us as a bug, maybe you don't agree. Thought I'd report it.  I have an issue assigned to 'me' and I 'start progress' on it. Then, someone comes along as re-assigns it to 'fred'. The issue remains in progress for 'me' and Fred cannot change this, or start progress for himself.  It seems to me that in the ideal world, multiple people could have an issue in progress. But if that fails, then when it is re-assigned perhaps 1) Fred should have permission to stop progress by someone else or 2) the system should automatically do this...  As it is, Fred has to send an email to 'me' asking me to 'stop progress' so that he can start...""",Bug,Project Administration
119601,"""I use JIRA at codehaus.org (as a user, not a committer), and find it frustrating that I can't locate the patches I submit to various projects to see if they've been incorporated.    You can currently search by """"Reported by"""" or """"Assignee"""", but I'd like to see a """"Comment by"""" search option.  This would allow me to quickly see all issues that I've been involved with, and follow-up on them.""",Suggestion,JQL
119737,"""It is possible to configure the Protlets, particularly the """"Assigned To Me"""" can modify the number of displayed items. Unfortunately there is no obvious order in the displayed items. I would expect that is feasible enough to add a way to specify the order as a parameter to the portlet(s). (order by due-date is what i need most, but order by reporter, order by alphabet might make sense as well)""",Suggestion,Dashboard & Gadgets
119793,"""I have both a JIRA and a Confluence installation, Ive had them previously communicating amongst themselves for the authentication. I would like to get them to both talk to Active Directory and use that as a single source of security / group detailing.    I have found a few resources in the forums / mailing lists with fragments of advice on how to integrate Confluence into Active Directory using LDAP, but I have yet been able to put the peices together and get a working installation. (I get various stack traces on Tomcat startup)    What would be invaliuble to users in my situation is a guide (like the """"Integrating Confluence and JIRA"""" documentation guide) taking us through, step by step, what is needed to be done in order to get an integrated system up and running.""",Suggestion,Documentation
119849,"""I'd like to be able, as a user, to add the current number of votes to my issue navigator. Since people who use voting are often going to use voting results to determine what features to add, I would be nice to be able to see that information in the summary. And make it sortable.  """,Suggestion,Issue
120113,"""It would be fantastic if JIRA would time work while an issue is being worked on (between pressing Start Progress and Stop Progress). It would have to """"sleep"""" during off hours, which implies a working calendar (eg, M-F from 9 to 5), and users would need an """"I'm on vacation"""" setting so that they could easily suspend work without having to Stop Progress on every issue. Ideally, developers could have personal calendars and an optional ability to overwrite the recorded work time could be available in a permission (so that an Administrator would have the ability to fix mistakes). This is something several of our developers have asked for and as a project manager, it's absolutely my #1 priority feature request.""",Suggestion,Issue
120410,"""It is possible that the project developers would not want users to edit issues in regards to for which version it needs to be fixed and what the priority is.  I would propose that when a user vote for a project, he can supply a personal priority for the issue.  All the personal priorities would then be averaged to determine the general priority of the user community.  You would thus have 2 priorities, one assigned by the developers and one by the users. The latter could serve as a guideline to the developers on what the communities wishes are.""",Suggestion,Issue
120804,"""I've been having major problems lately using Jira on Resin 2.1.2. Installation was a breeze but these appears to be problems handling transactions. Attached to this message is the error I get when I try to add a new task or issue. Each time I add a new issue, I get an error notification in the browser & a strace in the error logs, but the issues IS entered into the system (if I go back to the issue list and reload I see it there).  Note, I am also seeing this behavior when adding components or versions to a project as an admin.  BTW, my Resin datasourece and entityengine.xml is setup exactly as mentioned in the installation docs.""",Bug,Infrastructure & Services - Application Lifecycle
121365,"""As a user I want to be able to filter plans on main dashboard to see only my favourite plans. """,Suggestion,Dashboard
121409,"""h3. Problem Definition  Currently there's no way to setup notifications based on remote agent / build queue activity  h3. Suggested Solution  Please add notifications based on build activities / remote agent activity  h4. Why this is important   As administrator I need to receive notifications when builds are queued to help determine whether new agents are needed  h3. Workaround   Currently, you can setup monitor queue activity through the UI (Build Activity) or setup a cron job script using Bamboo rest API  """,Suggestion,Notifications
121517,"""As a Bamboo administrator I would like to control users permissions in a regular way via UI separately from Bamboo YAML Specs (At the same time using YAML specs for build control).    Like in Travis or Appveyor.""",Suggestion,Plan configuration
121617,"""As a developer, I would like to have the option to force the creation of a tag, i.e. re-tagging, with the """"Repository Tag"""" task.    At the moment, once a tag exists on the remote repository, recreation of the same tag fails with something like this   {panel:title=Bamboo task log}  simple 22-Aug-2019 11:50:40 Starting task 'Tag the build as latest version' of type 'com.atlassian.bamboo.plugins.vcs:task.vcs.tagging'  simple 22-Aug-2019 11:50:40 fatal: tag '""""7.7""""' already exists  error 22-Aug-2019 11:50:40 Error occurred while running Task 'Tag the build as latest version(5)' of type com.atlassian.bamboo.plugins.vcs:task.vcs.tagging.  error 22-Aug-2019 11:50:40 com.atlassian.bamboo.task.TaskException: An error occurred while tagging a repository  ...  error 22-Aug-2019 11:50:40 Caused by: com.atlassian.bamboo.plugins.git.GitCommandException: command [/usr/bin/git tag """"7.7"""" 813679c581791b975523fb4bd8de38aac91b05d0] failed with code 128. Working directory was [/app/bamboo/bamboo_agent/xml-data/build-dir/RE-CRU-JOB1]., stderr:  error 22-Aug-2019 11:50:40 fatal: tag '""""7.7""""' already exists  {panel}  It is not possible to circumvent this limitation by manually manipulate the local / cache repository, to try removing that tag prior to (running `git tag -l 7.7` returns nothing, so `git tag -d 7.7` would have failed regardless).    The scenario is analogous to updating """"latest"""" tag of docker images. Consider that a branch  """"release-7.7"""" exists, and patch release for that """"release-7.7"""" will be tagged as """"7.7.1"""", """"7.7.2"""",  etc.  For client of this artifact who simply wishes to track the latest 7.7.x they simply look at tag """"7.7"""" and retrieve the latest """"7.7"""".""",Suggestion,"Tasks,Repository"
121630,"""h3. Problem    As an admin, I need to retrieve programmatically the information that links a release to a given build key and vice versa even when there is no artifact published for a build.  h4. Workaround    If the DB access is available, this information can be retrieved with the following query:  {code:sql}  SELECT      CONCAT(dvp.plan_key, CONCAT('-',dvp.build_number)) build,     dv.plan_branch_name,     dv.name  FROM     deployment_version dv     LEFT JOIN <USER>version_planresultkeys dvp     ON dv.deployment_version_id = dvp.deployment_version_id{code}""",Suggestion,REST
121631,"""In the Bamboo deployment plan REST API, there is no link between a release, and the build result from which that release was created. The only exception is if that build result created an artifact which the release can use.      As a developer, I need to programmatically determine which release is linked to which build result.         This information is available in the UI, both from the build result and from the release, so it must be held somewhere. I'd like to propose this information be exposed through the REST API as well. It is important information, and is necessary to help us fully automate our CI/CD pipeline.""",Suggestion,"Deployments,Build,REST"
121674,"""As a developer (_tester_), I would like to have Bamboo integrated with Worksoft Certify.""",Suggestion,"Test,Misc integration"
121761,"""As a bamboo user, I would like to be able to select which variables will be exposed as environment variables while running script tasks.     This is more critical in plans that have multiple repositories linked to it, each repository will create a series of new environment variables. Based on recent tests, a script task can take up to 30% more time to finish if the plan has 60 repositories instead of 1. """,Suggestion,Build
121818,"""h3. Problem Definition    I have a large number of agents, and builds.    I'd like to keep all agents with a certain capability available for jobs that require that capability.    I have a large number of jobs which do not use this capability.  I don't want these to build on the agents with the capability.    There should be an option to set a *Requirement* to """"does not exist"""".  So I don't need to dedicate agents each time I create a new job.     !image-2019-03-22-11-05-25-449.png|thumbnail!     h3. Suggested Solution    Add """"Does Not Exist"""" option to requirements dropdown.     h3. Workaround    # Create a Custom Capability for all agents without capability such as:   !image-2019-03-22-11-07-27-117.png|thumbnail!   # Add the """"NoDocker"""" capability as a requirement for jobs which do not use Docker.      """,Suggestion,Requirements
121856,"""h3. Problem  As an admin, I need to be able to move plans programmatically using REST API, but this endpoint is not available.  """,Suggestion,REST
121935,"""h3. Problem    As an admin I need to get the details from a broken build programatically though REST API.   Bamboo provides a lot of details for a given build through /rest/api/latest/result endpoint but the responsible information is not present there.  h3. Suggestion    Expand the result endpoint to include this information or add a new endpoint for this  h3. Workaround    Use the following query to get the details directly from the DB:  {code:sql}  SELECT  A.*,  """"USERNAME"""",  """"USER_WHO_UPDATED"""",  BUILD_TYPE,  CREATED_DATE,  BUILD_KEY,  BUILD_NUMBER,  BUILD_STATE,  LIFE_CYCLE_STATE,  BUILD_DATE,  BUILD_CANCELLED_DATE,  BUILD_COMPLETED_DATE,  TRIGGER_REASON,  REBUILD  FROM  """"AO_7A45FB_AOTRACKING_USER"""" TU  LEFT JOIN  AUTHOR A ON A.LINKED_USER_NAME=TU.""""USERNAME""""  JOIN  """"AO_7A45FB_AOTRACKING_RESULT"""" TR ON TR.""""LINKED_TRACKING_ENTRY_ID""""=TU.""""LINKED_TRACKING_ENTRY_ID""""  JOIN  BUILDRESULTSUMMARY ON BUILDRESULTSUMMARY_ID=""""RESULT_SUMMARY_ID""""  {code}  /!\ The query above was designed to work on Postgres DB and might need small adjustments to run on other DBMS""",Suggestion,"REST,Responsibility tracking"
122079,"""h3. Issue  Due to the way Quiet Periods are implemented, one of Bamboo's plan execution threads is consumed for the duration of the period.    Plan execution threads are limited to 4 by default, so if quiet periods are set for long periods of time (_e.g. 10 seconds is the default, but users have been known to set values upwards of 5 or 10 minutes_) then these """"_sleeping_"""" jobs can potentially fill up the plan execution queue. This can result in an builds backing up in the queue, manual builds failing to start in a reasonable amount of time, etc    h3. Suggestion  * As a Bamboo administrator, I would like to review thrown REST API how {color:blue}Repository Configuration >> Change detection options{color} are set up so I can prevent Bamboo from running out of resources. For that, it would be interesting having:  ** GET /rest/plugin/1.0/repository/changeDetection  Exposing a list of repositories and {color:blue}Change detection options{color}  ** GET /rest/plugin/1.0/repository/<repositoryId>/changeDetection  Exposing given repository and {color:blue}Change detection options{color}  * As a Bamboo administrator, I would like to review this configuration settings through the Support Tools""",Suggestion,REST
122166,"""As a Bamboo administrator, I would like to build a report based on build / deployment triggering method.    For instance, I would like to have a list of plans that have been triggered:  * by repository changes though a push against repository  * by scheduled trigger  * by remote event  * by remote trigger""",Suggestion,REST
122170,"""Excessive requests against repository server can be made (_unnecessarily_) due to misconfiguration on {color:blue}Plan Configuration >> Triggers{color} tab, preventing builds from being triggered and/or source code checkout from fetching/pulling repository changes.    As an administrator, I would like to review plans that are set up with {color:blue}Repository Polling - Periodically{color}, displaying information about:  * plan  * repository set up in trigger  * its interval    The same information should become available for {color:blue}Repository Polling - Scheduled{color} and {color:blue}Scheduled{color}""",Suggestion,"REST,Triggers"
122339,"""h3. Problem Definition  As an administrator I would like to be able to break out """"configure triggers"""" as a separate permissions for deployment projects.  h3. Suggested Solution  Currently, we can't get to a true segregation of duties using Bamboo. We'd like to be able to set dev teams to have edit permissions for prod deployment environments and only admins be able to configure triggers.  h4. Why this is important   Unfortunately, edit permissions gives you the ability to configure triggers on build plans (even when you don't have the """"deploy"""" permission).             """,Suggestion,Deployments
122369,"""At moment Bamboo provides 2 endpoints to dedicate agents:  * /rest/api/latest/agent/assignment  * /rest/api/latest/deploy/environment/<environment_id>/agent-assignment    In order to dedicate an agent, the minimum requirement is:  * admin permission for plan/project  * edit permission for  environment/deployment project    This may cause a denial of service situation if someone grabs all the agents.    In order to avoid that, as an Administrator I would like to have a security toggle to make the agent assignment """"restricted to admin only"""".""",Bug,Admin
122544,"""h3. Problem Definition  As an administrator, I need to be able to audit who modified the project name    h3. Suggested Solution  Include project rename to the audit log""",Suggestion,Projects
122560,"""As a project level admin I want a way of removing all plans from a project.    Potential ways of achieving that:  * Make 'Delete Project' action remove all plans from that project  * Introduce some sort of project level bulk action screen.""",Suggestion,Projects
122662,"""As a developer waiting for builds in """"My Bamboo"""" I would like to see the time each build completed.         Currently I am waiting for a """"release"""" where a lot of builds have to happened and some of them in order. But the dependencies are not working correctly and most of the builds say """"1 hour"""". I would like to see """"1:03:33"""" for example so I can see which of the many builds ran in what order.""",Suggestion,"User Interface,Build"
122795,"""h3. Problem Definition  As an admin I want to be able to easily remove a user or group from plan/project permissions in batch    h3. Suggested Solution  Implement a bulk remove permission to feature, similar to the bulk add already existing    h3. Workaround  Remove the permissions manually from the database:  # Stop Bamboo  # Remove the entries returned by:  {code:sql}  SELECT * FROM ACL_ENTRY WHERE SID LIKE 'user';{code}  # Start Bamboo""",Suggestion,Plan configuration
122881,"""As a normal plan user (All permissions except admin) I want to link and create JIRA issues.    At the moment this is only allowed with admin permission on the plan.""",Suggestion,JIRA integration
122883,"""h3. Problem Definition  The greatest load by far that CI systems put on SCM systems is inefficiently configured polling for changes. As an admin, I want to know the load my plan configurations are putting on my source control management system(s) so I can identify inefficient configuration and correct it.    h3. Suggested Solution  Add a UI section that surfaces information about which plans are configured to poll for changes, and at what frequency. This should also show the repositories that are linked to that build plan. Optionally this could also include other SCM stats, such as average/daily fetches / clones, whether shallow clones are enabled, etc; and should show an overall picture of the activity of the instance (e.g. """"Your Bamboo server is polling various SCM systems for changes a total of 1,000,000 times per day"""")    This information should also be included in support zips, to enable Atlassian Support to analyse this info.    h3. Workaround  Various manual database queries can be run to gather some of this information, """,Suggestion,"Triggers,Telemetry / Reporting"
122967,"""h3. Problem Definition  As an admin, I want to be able to manage users and groups remotely using REST API    h3. Suggested Solution  Create REST endpoints to manage the most common tasks involving user configuration     h3. Workaround  Use an external user repository and perform the operations directly into that repository""",Suggestion,REST
123066,"""Within Bamboo - as a System Administrator I would like to have the ability to assign, view and manage capabilities assigned to the Restricted Administrator role.    This is to allow the capabilities of the Restricted Administrator role to be refined as needed to meet workflow, process and business needs.     """,Suggestion,Admin
123277,"""h3. Problem Definition    As a Bamboo user I want to retrieve repository information from the Bamboo REST API  h3. Suggested Solutions   # Add a /repositories endpoint that would list repositories used by Build plans.   # Add repository data to /plan endpoint    h3. Workaround    This could be retrieved from the Bamboo database:  {code:title=mysql}select b.full_key, vl.name, substring(vl.plugin_key, locate(':', vl.plugin_key)+1) type, vl.xml_definition_data   from build b   join plan_vcs_location pvl on (b.build_id = pvl.plan_id)   join vcs_location vl using (vcs_location_id);  {code}  {code:title=postgres}select b.full_key, vl.name, substring(vl.plugin_key from position(':' in vl.plugin_key)+1), vl.xml_definition_data   from build b   join plan_vcs_location pvl on (b.build_id = pvl.plan_id)   join vcs_location vl using (vcs_location_id);  {code}""",Suggestion,REST
123311,"""As a manager I want to choose which deployment projects I see so I can quickly tell the current snapshot of what version of code is in each environment of my different deployment projects.         It would be preferable for it be displayable in a Bamboo Dashboard and JIRA dashboard    Other spots that would work would be as a Confluence macro or All Deployment Project favorite.          Dashboard method:    Have a selector for Source build plan and environment and then show the Release field & completed date.    I want to recreate the example below from Confluence in a general table   * I display 4 environments as text   * release field as link (select source build plan, environment)     * change icon (display icon if build was completed within designated interval)    !image-2016-11-04-17-39-28-981.png|width=505,height=226!    !image-2016-11-04-17-46-23-154.png|width=489,height=269!         Or Add a favorite button and filter on All Deployment Projects.. this would be the lease favorite method but would be an improvement.     !image-2016-11-04-17-37-49-336.png|width=511,height=329!""",Suggestion,Deployments
123332,"""h3. Purpose  As an administrator I'd like to have my Bamboo job email the link of the pull request that started the job (along with the link to the job itself and the location of the binaries it built). If this information was available in a Bamboo variable, that would be ideal.    In general, I think having the link to the pull request in the Bamboo job's summary (similar to what we have in JIRA) would be handy for users too. Bamboo currently shows a list of commits but the pull request would have the nicely formatted diffs along with any comments from the code review.""",Suggestion,Notifications
123335,"""h3. User Story    As a Bamboo user, I would like Script tasks to be able to use Shared Credentials in the same way as SSH and SCP tasks.    h3. Suggested Solution  * Add Shared Credentials feature to script tasks""",Suggestion,Tasks
123341,"""_As a Bamboo user_    _I want Bamboo to ensure the entire build-job directory is empty in advance to running a job therein_    _so that builds do not fail due to leftover from previous builds_    Notes:   * Job option """"Clean working directory after each build"""" as a post step seems not be sufficient since it is obviously not be performed in certain cases, e.g. Bamboo / host shutdown and HBK.   * Source Code Checkout option """"Force Clean Build"""" is not sufficient since in the case the checkout is done into a sub-directory """"Checkout Directory"""" (required when build depends on several repositories), this option takes care only of the sub-directory(s).    I just see two possibilities to fix build issues due to leftovers of previous builds   * cleaning up *entire* work directory before / as the first task of every build   * configuring Bamboo to include the build number into the path of the current build directory (preferred)    Are there any option for one or the other?    PS: In our team, we recently discussed removing any created files by our own, however came to the conclusion that it is worse strategy with no guaranty that nothing else is being removed from build hosts unintentionally. If unique build directories are not possible, Bamboo tooling should be used to ensure clean directories beforehand of a build.""",Suggestion,Build
123361,"""As a *Bamboo user* creating jobs by *cloning* from existing   I would like to see hints at all *disabled* original jobs in the select boxes   so that I am warned those jobs may not be working with the latest repo revision anymore.    This would give me the options to either choose such one nevertheless (in order to modify it afterwards) or clone from quite another job.    Example: see attached [^mock-up.png]""",Suggestion,"User Interface,Plan configuration"
123424,"""Hi Atlassian,    I believe that a good feature for Bamboo would be that job requirements understand """"versions"""", as it improves the overall expressivity of the builds.    As an example, I would like to run a build for  - a custom capability """"cmake_version"""" """"strictly greater than"""" """"3.5""""  - and and """"cuda_version"""" """"lower than"""" """"8""""  - and a """"python_version"""" """"greater or equal to"""" """"2.7.12""""  - and a """"boost_version"""" """"greater or equal to"""" """"1.62""""    That would help:  - expressing requirements  - managing software/os versions on agents    Thanks for considering this,  <USER>",Suggestion,Requirements
123460,"""h3. Problem Definition  As a user I want Bamboo to send notifications to Bitbucket Server if I'm using a linked Git repository, and not require a linked Bitbucket Server/Stash repository    h3. Suggested Solution  Send notifications to Bitbucket Server when using Git repository link, instead of only Bitbucket Server/Stash link  This was available in previous versions of Bamboo, before the rebranding of Stash.""",Suggestion,Bitbucket Server integration
123527,"""Please provide a way (via polling or hook) to trigger a build-plan when a new binary is detected in an Artifactory Pro repository.    I need to this to integrate release packages created by a third-party vendor in their own system, with our Bamboo system.    (TeamCity provides this functionality. Jenkins has a workaround that achieves this.)    Note: There is a open request to support a workaround for the lack of this feature: BAM-17404""",Suggestion,"Triggers,Misc integration"
123577,"""Hello,    I'm trying to use Bamboos API to extract some KPIs of my daily work.  Therefore I extracted allready the overall average queuetime of a plan.    As a counterpart to that KPI I wanted to extract the summed agent utilization as well.  Unfortunately (and I really don't get why), there is no real json representation so I've to extract the values from imageMap.    Thats working even though its ugly, but when I try to do the same for AgentUtilization, I get:  {Code}Unknown was utilized 147.0 seconds for January 2015{Code}    I would expect """"_Unkown_ """" to be sth like """"_PROJECT-BUILD_"""". Or am I sth missing and Unknown is correct?    *Update*: I see its also not working in the shown image in the UI    *BTW*.: Even though this is fixed its still ugly. Is there already an open Feature Request, to return the report data as json?  """,Bug,Telemetry / Reporting
123582,"""The request:    For release versioning, allow the version that is set in the Release Versioning page to auto-populate when deploying from a branch that is not the default branch of the Build Plan.     The reasoning:    We typically will release to our lower environment from our 'release' branch so this is the default plan branch. Occasionally, our dev team will manually deploy from the 'master' branch to our STG/INT environment. When choosing to deploy the environment, and then choosing """"Create new release from build result"""" the """"Release"""" field is auto-populated based on the variables in the Release-versioning setup.     However if you update the plan branch to a different one, ie master, it defaults to branchname-buildnumber. I see that this is mentioned on the Release Versioning page (""""Releases from branches will default to using the branch name suffixed with the build number of the build result.""""), but was wondering if there was a specific reasoning/purpose behind that as opposed to just auto-populating the format the users set on this page.    No urgency here. It's just a very minor nuisance that the dev team has complained about a few times so I figured I would toss in a ticket about it.""",Suggestion,Deployments
123642,"""As an admin I would like to be able to get the user who runs a JOB.  I've tried to use the variable *bamboo.ManualBuildTriggerReason.userName*, but this variable is useless in some situations. Its value is set to null every time a build is triggered for a reason different them manual, so if I manually run a JOB after a build triggered by other reason this variable wont be set leading me to the following error:  {noformat}... ${bamboo.ManualBuildTriggerReason.userName}: bad substitution{noformat}     To fulfill the above gap I suggest adding the following variable: *bamboo.ManualJobTriggerReason.userName*. This variable should be set every time a JOB is run, so it could be used for JOBs even when they are manually run after a build not manually triggered.  """,Suggestion,Build
123672,"""My team has a PR-based workflow, where all work is done on private feature branches and reviewed before merging to master.  I would like to be able to create a HipChat notification for my feature branch, to notify me of all builds on this feature branch.    I think the """"Add Notification"""" dialog that is available in the notifications tab  of the top-level plan configuration would provide what I need, if this same dialog were also available for a branch plan.    Thanks for considering!""",Suggestion,"Plan Branches,Notifications"
123680,"""As an admin I want to be able to see the number of users a Bamboo instance has.  The suggested information could be displayed in the *Instance statistics* at *System information* admin page, similarly to products like JIRA and Confluence.""",Suggestion,Telemetry / Reporting
123704,"""On the plan summary page, there is a drop-down selector with a branch icon and a list of branches.  At the bottom of the list is a link """"View all branches"""".  I request to change the text to """"View all branch plans"""" with the following justification:    I had an old PR whose branch plan had expired.  I wanted to re-enable or re-create the branch plan.  Not being familiar with how branch plan expiry works, I wasn't sure exactly how to do this, but I assumed it should be obvious enough how to do this.    I got tripped up, however, by the """"View all branches"""" link.  Assuming based on the name of the link that this would let me see all branches, and from there I could re-enable or re-create the branch plan, I was confused when the resulting page didn't have my branch listed.  I was going to file a bug report saying that somehow my branch had been lost from Bamboo, but first I got a more experienced Bamboo user to check my work.  At that point I realized that the """"View all branches"""" link actually takes you to a list of branch *plans*, and then of course it made sense that my expired branch plan was not listed.    It's true that the linked page does have the text """"Plan branches"""" in bold near the top, but it also has a tab title of just """"Branches"""" and a URL ending in """"..../branches"""".  More consistent naming would help with discovery for other novice users like me.    Also consider renaming the """"Branches"""" tab to """"Branch Plans"""", and possibly include text or a link to aid in discovery of the """"Actions => Configure Plan => Branches => Create plan branch"""" button.    Thanks for considering!""",Suggestion,User Interface
123832,"""Greetings Atlassian Bamboo,    I'm writing to request that on the Build result summary, under Code Commits, that there be an added Range of Builds option. This would enable the user to see all of the Code Commits aka Fixes from the current build to say 4 builds ago for example.    More specifically, as a QA worker, say I wanted to review the Code Commits from  one build after the previous build I tested to the current build. Then an example would be: On Tuesday I tested build 764. On Wednesday I came in and saw that the recent build is 771. So now I would like to review the Fixes that occurred from 765 - 771 all on one page. This saves the headache & time of having to click through each build.    Feel free to contact me if the above is not clear enough or you have some questions on the matter.    Cheers""",Suggestion,"Build,Telemetry / Reporting"
123849,"""I'd like to be able to use docker-compose to start containers on a remote host, as either part of a CI build or as a deployment build.""",Suggestion,Docker
123860,"""As a company IT administrator of our company  I want that all users authenticate to bamboo in two steps (username password  & app or sms).  So that I'm always sure that a employee of our company logs in bamboo instead of a hacker. This makes even my infrastructure more secure. """,Suggestion,Security
124059,"""I am developing a plugin that needs to extract all the changed files from an SVN repository related to a JIRA Version (release). Note that I'm looking for the CHANGED files only.  I was using the BuildRepositoryChanges class to figure out what was changed for a certain JIRA version. However, the content of buildContext.getBuildChanges().getRepositoryChanges() does not represent all commits for all the JIRA issues in a release (FixVersion). It just looks at the revision number since the last build and only has the changes since the last build.    In other words, the BuildRepositoryChanges ignores the JIRA fixVersion/ Release related JIRAs and commits and just contains the changes since the last build just like any other build.    I think the BuildRepositoryChanges should represent the JIRA issues and commits related to the fixVersion / JIRA Relase.    Example scenario:  commit 1 - JIRA issue #1 - fixVersion #1  commit 2 - JIRA issue #2 - fixVersion #2  commit 3 - JIRA issue #3 - fixVersion #1  When I release fixVersion #1, I need commit 1 and 3, and JIRA issue #1 and #3  When I release fixVersion #2, i need commit 2 and JIRA issue #2    To make matters worse, I can't do it myself in a plugin because of the issue described in BAM-17061.""",Bug,JIRA integration
124298,"""h3. Problem Definition  Bamboo terminates Elastic instances after it detects the agent is offline for a certain period of time.  h3. Suggested Solution  As an administrator I would like to configure the offline time of an agent to a higher value before Bamboo shuts down the instance because it detects the agent is offline.  Even if I have the _Automatic elastic instance management_ configured in a way to shut down the instance after a high period of idleness, if the agent is disabled (and I want it disabled for more than 30 minutes and I understand that Amazon will charge me and I don't care), I want to manually configure the time that Bamboo waits to shutdown offline agents.""",Suggestion,Elastic Bamboo
124327,"""Having to use the EC2 ami-id is very burdensome, especially when testing an ami which is undergoing a lot of changes. It would be very nice to be able to simply use the ami's Name which may remain constant while the ami-id changes. Even better, if the user was able to enter a bit of code themselves to always grab the correct value.""",Suggestion,Elastic Bamboo
124604,"""When:  - an elastic image configuration has an invalid (e.g. non-existent) subnet configured, and  - there is a build queued that requires this image configuration    then Bamboo refuses to spin up *any* image configuration to handle any queued builds.    In our case we changed all our VPC subnets and forgot to update one of the image configurations with the new values.    The server logs reveal the following:  {noformat}  2015-06-18 01:36:17,588 INFO [scheduler_Worker-10] [ElasticRunningInstancesOptimizerImpl] 5 elastic instance(s) will be started for those builds that cannot be build on currently connected agents.  2015-06-18 01:36:17,588 INFO [scheduler_Worker-10] [ElasticRunningInstancesOptimizerImpl] 5 elastic instance(s) will be started to run builds that are waiting in a queue. Current queue size is 198, number of builds executable on elastic agents is 183. Bamboo is currently starting 14 elastic instances.  2015-06-18 01:36:17,588 INFO [scheduler_Worker-10] [ElasticRunningInstancesOptimizerImpl] AWS account has 588 elastic instances started by Bamboo server(s) and has 0 spot requests pending, 588 in total. Of these, Bamboo controls 67.  2015-06-18 01:36:17,691 INFO [scheduler_Worker-10] [SubnetCache] query for a non-existing resource [subnet-64fd5a13]  2015-06-18 01:36:17,691 ERROR [scheduler_Worker-10] [ElasticInstancesMonitorJob] Failed to adjust the number of elastic agents.  com.atlassian.aws.AWSException: Error when starting a new instance      at com.atlassian.bamboo.agent.elastic.server.ElasticFunctionalityFacadeImpl.startupAgents(ElasticFunctionalityFacadeImpl.java:218)      at com.atlassian.bamboo.agent.elastic.schedule.ElasticInstancesMonitorJob.execute(ElasticInstancesMonitorJob.java:50)      at org.quartz.core.JobRunShell.run(JobRunShell.java:202)      at com.atlassian.bamboo.utils.BambooRunnables$1.run(BambooRunnables.java:49)      at com.atlassian.bamboo.security.ImpersonationHelper.runWith(ImpersonationHelper.java:31)      at com.atlassian.bamboo.security.ImpersonationHelper.runWithSystemAuthority(ImpersonationHelper.java:20)      at com.atlassian.bamboo.security.ImpersonationHelper$1.run(ImpersonationHelper.java:52)      at org.quartz.simpl.SimpleThreadPool$WorkerThread.run(SimpleThreadPool.java:525)  Caused by: com.atlassian.aws.ec2.caches.Ec2CacheMissException: The resource with ID 'subnet-64fd5a13' does not exist (Service: AmazonEC2; Status Code: 400; Error Code: InvalidSubnetID.NotFound; Request ID: none)      at com.atlassian.aws.ec2.caches.SubnetCache.onResourceLookupFailure(SubnetCache.java:48)      at com.atlassian.aws.AwsOmeCache.filterResources(AwsOmeCache.java:90)      at com.atlassian.aws.AwsOmeCache.filterResources(AwsOmeCache.java:85)      at com.atlassian.aws.AwsOmeCache.describe(AwsOmeCache.java:114)      at com.atlassian.aws.AwsOmeCache.describeResources(AwsOmeCache.java:127)      at com.atlassian.bamboo.agent.elastic.server.ElasticFunctionalityFacadeImpl.getVpcsAndSubnets(ElasticFunctionalityFacadeImpl.java:278)      at com.atlassian.bamboo.agent.elastic.server.ElasticFunctionalityFacadeImpl.ensureSecurityGroupsExist(ElasticFunctionalityFacadeImpl.java:290)      at com.atlassian.bamboo.agent.elastic.server.ElasticFunctionalityFacadeImpl.startupAgents(ElasticFunctionalityFacadeImpl.java:179)      ... 7 more  {noformat}    Valid image configurations could still be started manually and would pick up builds during this time. Correcting the image configuration to use valid subnets immediately fixed the problem.    The impact to us was that our unwitting back-end mistake caused many of our Bamboo servers to lose most of their build capacity, and it was not obvious what the problem was.    I would have expected:  - Bamboo to continue being able to spin up instances from other image configurations to handle the builds that didn't require this one particular (invalid) image config; and  - a visible error in the UI reporting that there was a problem when trying to launch that image configuration.""",Bug,Elastic Bamboo
124644,"""In version 5.8.1 there is the option to check the     'Clean working directory after each build' in Miscellaneous on a build job.      The assumption is that checking this value will clear out the entire job folder's content after the entire build has completed as the name says so.     To my unfortunate surprise the clean actually occurs before the entire project build is actually complete and the way that the project is set up in my instance the entire build fails.    I would like to propose a simple check in the plan configuration   'Clean all working directories after entire build' which would make more sense for the name and would function as a single clean for all of the job folders after the entire build has been completed (whether successful or not).""",Suggestion,Build
124712,"""Configuring Bamboo builds is really like creating a build script, and right now there is no way for me to comment on how and why I have setup my Bamboo """"script"""" a certain way.    Thus, I wish I could add comments at the Bamboo server level, and to projects, plans, and tasks, about how they were setup. It would be wonderful if the comments or notes could be long and *rich*, like JIRA-style description fields (maybe excepting comments on tasks, which I think would usually be short).    By comments at the server level I mean something like a menu or some link on the home page (maybe under the """"?"""" menu), where an admin would click to see a page with notes on the overall project and plan configuration.    On projects, plans, and tasks maybe there would a similar button, link, or maybe a """"hover over"""" control; I'm not sure exactly how or where, but I'm sure you guys can figure it out. ;-)    And of course, this would probably be irrelevant to regular users just looking for build status or artifacts, so the notes would be available only to admin users (or maybe they could be enabled/disabled as an admin would see fit).""",Suggestion,Plan configuration
124736,"""The Xunit xml report has a treasure trove of data for timing data. To know how long each test runs is important data for the task of reducing build times. It is this data that provides the informed answer for the test automation folks.    Bamboo does so easily let us find this data.    In the Build Results data, I expect to sort the test-cases in one of 3 ways:  1) execution order (how it is now, by default)  2) alphabetical name (just to find stuff if I need to by name)  3) test duration (for this specific work of quickening test suites)    When it divides tests into pages, I expect my page number to be consistent when I reorder,  and find the range of tests 50-100 (on page 2 for example) for the newly-selected order. This requirement is not very important. The real gold is just being able to find the long-running tests.    I found there was a previous request in BAM-532 to find longest-running-tests. This sounds great! It sounds like it was done. But where is it in the Bamboo interface? I cannot find it. Maybe all I'm asking for is directions to it. Let me know if so.""",Bug,User Interface
124763,"""A command can easily be added to execute InspectCode but there is no way to visualize the results. Currently our Jenkins build machine does this and I would like to migrate over to Bamboo but without this feature, supporting Stylecop I can't currently justify a commercial solution as a replacement.""",Suggestion,Windows
124922,"""We have a deployment project which, in one task, downloads a large artifact from a source plan. One time, when running this task, the artifact download was taking an unusually long time. Upon inspecting the agent logs, it was timing out and retrying.    I clicked the 'Stop Deployment' button twice (about one minute apart) but the deployment did not stop. Eventually the task was able to download the file and it moved on to subsequent tasks and finished the deployment.    I would have expected that, when I clicked 'Stop Deployment', the deployment:  - should have stopped immediately -- no waiting for a download to time out; and  - the deployment definitely should not have moved on to other tasks after the download completed.    Output from the deployment logs show:  {noformat}  simple 06-Mar-2015 05:30:01 Starting task 'Download release contents' of type 'com.atlassian.bamboo.plugins.bamboo-artifact-downloader-plugin:artifactdownloadertask'  simple 06-Mar-2015 05:30:01 Preparing to download plan result BAMBOOUPGRADE-DOWNLOAD-25 artifact: Shared artifact: [StandaloneTarball], pattern: [*.tar.gz] anchored at: [./]  command 06-Mar-2015 05:35:22 Request to stop '271515650-277512218-312737795' received from mknight  command 06-Mar-2015 05:36:32 Request to stop '271515650-277512218-312737795' received from mknight  simple 06-Mar-2015 05:39:38 Artifact [StandaloneTarball] downloaded successfully in 576s to: /./  simple 06-Mar-2015 05:39:38 Finished task 'Download release contents' with result: Success  simple 06-Mar-2015 05:39:38 Starting task 'Upload Bamboo archive to BAC' of type 'com.atlassian.bamboo.plugins.scripttask:task.builder.script'  ...  {noformat}""",Bug,Deployments
125029,"""When I first ran across the ability to Link a JIRA Issue to a Bamboo Test Case that was generated by JUnit.  I thought that was a cool idea.  So I went ahead created a bugfix issue and resolved the problem.  Now however the same Test Case has broken for a different reason and I need to link a different JIRA Issue to the same test case.  But I cannot because Bamboo Test Cases only support one JIRA Issue per Test Case.  I believe an Improvement would be to make it so multiple JIRA Issues can be linked to a Bamboo Test Case. """,Suggestion,JIRA integration
125033,"""h3. ***Summary***  Due to the infrastructure problem on Amazon (connectivity issue between EC2 and S3), the agents keeps timing out.    h3. ***Feature Improvement Request***  I would be beneficial if there was a better status message to reflect this.""",Suggestion,Elastic Bamboo
125101,"""Since Docker arrived I'am playing around with it quite a bit.   Currently I'm curious how to integrate docker in my deployment process using bamboo.    Bamboo has a bit of a static Environment Configuration, which was fine at a time where I had just a fixed number of environments.  But since docker arrived I can basically create one complete environment (multiple containers) within a small script in no time.   This allows me to easily create multiple environments for different branches.   But in bamboo I would now have to create one environment per container manually.    I would suggest something similar to the plan branches where I have a basic environment and if I want to deploy an artifact I can choose to automatically create a new environment from this template or choose an existing one. This template would contain tasks to create containers, link them, etc      The current workflow looks like that:  - Developer creates new docker container(s) with bash script  - Developer creates new environment in bamboo (basically clones an existing one)  - developer deploys artifact to this environment    ...at some point developer deletes environment in bamboo and (hopefully) removes all running containers""",Suggestion,Docker
125204,"""Hi,    It will be nice if there was a ability to bulk / multi-select and delete plan results.    Currently I have to keep clicking delete for each result I want to delete.""",Suggestion,Build
125445,"""It would very helpful to display the maven modules in the Build Summary tab of a Bamboo build (for Maven 2.x/3.x jobs).    As a first pass, just show the summary as in the end of the log.  Perhaps link each module title to the line number in the log file somehow, so you can jump right there.  You could keep going, and breakdown the maven mojo's that ran successfully/failed as well, again with links to the line numbers in the logs.  And I'm sure more could be done...    Don't let a full feature set block an incrementally added feature though -- anything in this direction would be helpful.    As an added bonus, make Maven's {{--fail-at-end}} option be a first class checkbox on the Job configuration, but its simple enough to do this manually anyhow.    """,Suggestion,Maven
125501,"""As a user I would like to create dependencies on deployment plans.    Example:  Component A has a dependency on Component B.    Component A has its own Stash repository.  Component B has its own Stash repository.    The deployment plan for Component A should know if Component B is already successfully deployed, before deploying Component A.""",Suggestion,Deployments
125565,"""As a continuous integration admin I need to have option to do test build for pull request not from source branch but from source code after merge before I can allow pull request on stash.    Currently pull request trigger test builds on source branch, but sometimes source branch build could be successful, but when code is merged to destination branch all builds start failing because some other merge to destination branch was made before and code is inconsistent state.    It is really useful when I do pull request, Bamboo will do local merge of source and destination branch and then trigger build on this merged state. If this build is not successful then it will not allow pull request merge in stash.""",Suggestion,Bitbucket Server integration
125631,"""When adding a child plan under the Dependencies tab, the combobox is too narrow to display the entire plan name.  As a result, I have to hover over each to determine which one I need.  There is plenty of horizontal real estate to make this UI element much wider.""",Suggestion,User Interface
125750,"""I have a build plan with a number of stages. The initial stages run automatically and the final job triggers events outside of bamboo before completing. This stage goes green at this point as it does not wait for a response but the process that has been initiated elsewhere is still running.   The external process completes later and it then calls the bamboo api to initiate the next stage in order to complete the remaining stages of the pipeline.    So currently, we have to set the later stage as a 'Manual' stage to achieve this. However, this means that a logged in user can click the stage run button and start the second set of stages before it is ready to proceed.    I would like an option to have a manual type stage that can be triggered through the API, but does not display a run button on the web interface.  I would call this a 'Remote Triggered' stage and be a check box on the stage config like the 'Manual' option.""",Suggestion,"Triggers,Stages"
125827,"""I was allowed to create a plan from Existing Projects. I was prompted to select from this list.  It created the plan successfully, however I'm never allowed to update the description.    This does not seem consistent (as much as I regret somebody used this character in a project name).""",Bug,User Interface
125841,"""I'd like to be able to quickly look at the status of all of the plans that contain a branch. It is easy to see the status of the master branch on the allPlans page or the Wallboard, but I'd like to see the status of each of those plans on branch LABEL-123 for example. Adding this as a filter option to the allPlans and Wallboard would be great.""",Suggestion,"User Interface,Plan Branches"
125855,"""h1. Overview    The ability to flexibly manage / register Deployment environments, where these environments are created/instanced in 'the cloud' and exist as part of an elastic infrastructure.    h1. Background    Having switched from SVN to Mercurial/Git we have found that using Branches to develop and test on has become a best-practice in our teams. One impact of this change, however, is that we now find that we are restricted on the number of physical test environments in which we can deploy to.    As a result, we've begun investigating using AWS in a way similar to the 'elastic agent' concept by which we create a 'snapshot' test environment build, and then create test environment instances from that snapshot which are then deployed to as though any other test environment.    h1. Implementation Thoughts    I could see this being done in one of two ways:  # Follow a similar approach to Elastic Build Agents where users...  ## Create and register AMI's with Bamboo  ## When a new Environment is required, Bamboo can manage the initialisation of an instance based on that environment  ## When the Environment is no-longer required, Bamboo can manage the shut-down/hibernation/termination of that environment  ## Value-Add features under this approach would be to have the ability to snapshot the Instance at given points in time so that 'child'/'related' instances can be created at any point to allow Developers and QA teams to work in tandem without necessary disrupting one-another  # Provide API Access to the Deployment administrative functions which can be used by an Instance-triggered script to...  ## Create an Environment definition in Bamboo on Boot  ## Remove the Environment definition from Bamboo on Termination/Shut-Down""",Suggestion,"Deployments,Plan Branches"
126063,"""We have a plan ({{SOLR-LOADOFFERS}}) that consists of multiple stages and jobs, and, in addition to that has a child plan ({{SOLR-SOLRREPLICATION}}).    We ran into the issue that the child plan was not executed although the parent plan was successfully executed. It seems that the scheduled backup that is run at 1:00 AM every morning, changing the system state to {{PAUSING}}, effectively disallowing the child plan to execute.    This happened silently. There was no mentioning of a failed plan. The child plan  just didn't get executed. While I can understand to not run new jobs during a    scheduled backup, I would think that dependent jobs are an exception.    Is this intended behavior? Is there an upgrade we can do to fix this?    """,Suggestion,"Triggers,Import / Export"
126065,"""As a build engineer, I would like the set of tasks available in build plans and deployment plans to be analagous.    They may have functionality limited [like reporting, UI visualization] but they should be accessible to be executed.    As a couple examples:      VCS Tagging / Branching tasks would be useful, in order to create a tag once something was deployed to an environment that matters to prevent tag/branch spam.  This can be done cmd line, but if the functionality already exists in bamboo... why not leverage it!    Third party plugins like the Artifactory plugin expose incredibly useful tasks to push into a repo / pull from a repo.  Both actions can be seen to be useful in the context of deployments.  (For example, A deployment pipeline that goes from Artifactory/Repo -> Test -> UAT -> Prod ... the first deployment project would deploy to artifactory, the subsequent ones would pull from the repo and deploy to servers.)""",Suggestion,"Tasks,Deployments,Plugins"
126279,"""As a user, I would like for the command tasks to also be able to process shell substitutions (wildcards, variables an so on) and so on. As in certain tasks, using a command task is more appropriate than using Script tasks, for example:    * Command issue automatically creates requirement for job-to-instance matching mechanism.  * Command gives the user the choice not to care about where executable file resides. On different instances (started from different instance images) same executable may reside in different places. With Command the user just needs to set instance image capabilities.    As suggestion is to add a checkbox in the bottom section of Command task option. This checkbox should say *Process shell substitutions (wildcards, variables and so on)* or something similar.    """,Suggestion,Tasks
126354,"""Let's imagine I have an issue type like Business Requirements or Change Request. It has an """"Approved for Deployment into Production"""" status. I would like to condition the deployment like this:    1) Link that requirement to the build, manually (I can do that, now)  2) If that issue is not in that status, then Bamboo won't let the user to deploy the related release""",Suggestion,"Deployments,JIRA integration"
126395,"""If a job fails with test failures, you should have the option to only re-run the failed tests and not the entire job.    How could this be done? Well, I imagine as part of your plan configuration, you could specify extra options for rerunning failed tests. These options would simply get tacked on to the the maven task of your choice (via a dropdown).  Bamboo would provide you with a variable that contains a comma delimited list of the test classnames.    Essentially, I would add the config: -Dit.tests=$\{bamboo.failed.tests}    Then when I click rerun failed tests, bamboo puts this as a maven param and boom, my job now only reruns the failed tests, not the entire test suite.""",Suggestion,Test
126404,"""I'm using Git.    If I create a plan that has a trigger of any kind (say, one that causes the build to kick off once a day at midnight on the master branch), and I enable automatic branch creation on that same plan, then every time someone creates any new branch (that is, pushes a branch that previously did not exist) in my Git repository Bamboo will immediately start a build for that new branch.    This is obviously very bad and not what I want.  What I want is for (a) the build to run on the master branch automatically once every day at midnight, and separately (b) branches to be created automatically by Bamboo so that users can manually start builds on their personal branches.    Having Bamboo automatically start builds like this means I can't use automated branch creation on any plan that has a trigger.""",Bug,Plan Branches
126513,"""I have set up stash to use the """"feature branch"""" workflow.  Bamboo is triggered anytime a dev pushes commits to his/her branch. When the branch plan is finished, stash doesn't receive a build status update.  If I build the master branch (by running the plan or pushing to master), stash receive the build status update.  As a consequence, no pull request can be merge to master because i use the """"at least one build succeed"""" enforcement.    I am sure that stash doesn't receive the update in the branch case but does in the default case, see the attached log.    It sounds like a bug to me, or maybe i'm missinng something.    Bamboo: version 5.1.1 build 3902 - 12 Sep 13  Stash: 2.8.1""",Suggestion,Bitbucket Server integration
126661,"""As a developer I would like to push a branch to a git server and have it validated against a bamboo plan which notifies me and/or to my stash instance in the time that it takes to run the plan (or reasonable wait on a queue).    At present I need to wait up to 5 minutes for the """"Automatically manage branches"""" feature to recognise the new branch before the plan is started.    Please make the detection of new branches time configurable.""",Suggestion,Plan Branches
126693,"""As an Bamboo admin, I want to be able to filter my agents by OS, or other property that they may have.  Now you can't get an overview of agents types...""",Suggestion,Admin
126713,"""We just added an artifact download task to a build.  We have a build plan that generates a shared artifact.  Another build plan wants to download that artifact.  The artifact download seems to fail when the downloading build is kicked off as a child build of another plan.     When the artifact download works, we see this in the log:  {code}  simple 21-Aug-2013 01:20:03 Starting task 'Download Backup Client' of type 'com.atlassian.bamboo.plugins.bamboo-artifact-downloader-plugin:artifactdownloadertask'  simple 21-Aug-2013 01:20:03 Preparing to download plan result CAV-BKUPRELEASE-15 artifact: Shared artifact: [Stash Backup-Restore Client Release], pattern: [stash-backup-distribution-*.zip] anchored at: [target/checkout/distribution/target]  simple 21-Aug-2013 01:20:05 Artifact [Stash Backup-Restore Client Release] downloaded successfully in 2s  simple 21-Aug-2013 01:20:05 Finished task 'Download Backup Client'  {code}    When the artifact download doesn't work, we see this:  {code}  simple 21-Aug-2013 01:44:34 Starting task 'Download Backup Client' of type 'com.atlassian.bamboo.plugins.bamboo-artifact-downloader-plugin:artifactdownloadertask'  simple 21-Aug-2013 01:44:34 Preparing to download plan result CAV-MASTER-1549 artifact: Shared artifact: [Stash Backup-Restore Client Release], pattern: [stash-backup-distribution-*.zip] anchored at: [target/checkout/distribution/target]  error 21-Aug-2013 01:44:34 Unable to download artifact Shared artifact: [Stash Backup-Restore Client Release], pattern: [stash-backup-distribution-*.zip] anchored at: [target/checkout/distribution/target]  simple 21-Aug-2013 01:44:34 Task configuration:  simple 21-Aug-2013 01:44:34 sourcePlanKey -> [CAV-BKUPRELEASE]  simple 21-Aug-2013 01:44:34 artifactName_2 -> [Stash Backup-Restore Client Release]  simple 21-Aug-2013 01:44:34 artifactId_2 -> [455147648]  simple 21-Aug-2013 01:44:34 localPath_2 -> []  simple 21-Aug-2013 01:44:34 Task runtime configuration:  simple 21-Aug-2013 01:44:34 runtimeArtifactIds_2 -> [0]  simple 21-Aug-2013 01:44:34 resultKey -> [CAV-MASTER-1549]  simple 21-Aug-2013 01:44:34 securityToken -> [0144bcf6f97751172b4b0b3fa05bcad9efb9b93c]  simple 21-Aug-2013 01:44:34 artifactName_0 -> [Stash Backup-Restore Client Release]  simple 21-Aug-2013 01:44:34 artifactPattern_0 -> [stash-backup-distribution-*.zip]  simple 21-Aug-2013 01:44:34 artifactId_0 -> [455147648]  simple 21-Aug-2013 01:44:34 localPath_0 -> []  simple 21-Aug-2013 01:44:34 artifactLocation_0 -> [target/checkout/distribution/target]  simple 21-Aug-2013 01:44:35 Finished task 'Download Backup Client'  {code}    See how the """"Preparing to download plan result"""" line changes?  It looks like the task is getting confused about where it should be downloading the artifact from.  In the """"broken"""" log, it says the artifact will come from """"CAV-MASTER-1549"""", which is the parent build that kicks this build off and not the source of the artifact.    I'm not 100% certain about the child build being the cause, but it seems to be one thing common in the failed builds:  !bamboo-artifact-dl-failure-builds.png!""",Bug,Artifacts
126875,"""The only option I can find is the Build Expiry (Admin and/or per Plan) which has a time period and/or number to keep.     Previously I've been managing the Artifacts out of Bamboo and had them Expire every 3 hrs leaving 1 and the only reason for the artifact was to transition between stages. With the new Deployment Projects I cannot expire them anymore because I want manual deployments and they need the artifact.  I would like a built-in task that I can included into a Job or Deployment to delete the Artifact(s).""",Bug,"Artifacts,Deployments"
126876,"""I have setup two distinct deployment configurations to have the exact same configurations to cover 100% of my plans (all of mine have 2 plans per project). I was able to setup all my plans the same as well and been able to clone the plan and simply change the Subversion URL, but I can't seem to clone a deployment.     This way would also result in a large number of deployments with no change except the plan and I'd have to control a shared version using variables. Ideally I could apply a single deployment to multiple plans.""",Suggestion,Deployments
126994,"""In Bamboo 5, there is now a dropdown button labelled """"Administration"""" in the top right corner. This button provides 1 click access to some commonly used Administration pages, as well as a catch-all link that bring the user to the main Administration splash page.    I find that, by a long way, the administration page I use most frequently is the Agents page (ie. /admin/agent/configureAgents!default.action). I would find it very helpful if there was a link to enable me to click straight through to this page.""",Suggestion,User Interface
127000,"""I would like to be able to have a UI to customize the name, location and contents of the build-stamper plugin.    Currently with a Maven build I have to configure each project individually and add command -D variables to every build. Which looks horrible and is difficult across multiple builds.  I'd like to be able to configure the plugin to tell it what and how to create the build-number file. Additionally it would be nice if the location and the name of the file would be configurable, say naming it build.properties and putting it in my maven target/classes directory.    I might try this my self as I see the code is out there as a sample""",Suggestion,Plugins
127307,"""Currently, the Branch Updater and Gatekeepr merging capabilities are hard wired to the Plan Configuration/Branches page. These merging mechanisms should be available as tasks under the plan jobs section.     This would allow a build job to:  # control when merging occurs  # perform possibly multiple merges  # initiate merging as a manual task after other tasks have completed   # execute error response tasks on merge failure.     In short, I need more control over merging than the hard-wired mechanisms. I need the ability to implement change flow between branches and between repositories and designated places within the software construction process. I like CI merging however the current bamboo merge mechanisms are too limiting.""",Suggestion,"Plan Branches,Repository,Repository"
127313,"""If the checkout task includes a branch in a repository that does not exist (someone forgot to make it, mispelled it etc...) I would like the checkout task to fail instead of reporting it as a system error.""",Suggestion,Repository
127315,"""As a bamboo admin, I would like the value of a password within a commandline to be masked in the metadata view.    Steps to reproduce:  # Create a global or plan variable which contains the string 'password' (in my example 'libraryPassword')  # Create a script/commandline task with a commandline containing a reference to the plan variable, ${bamboo.libraryPassword}  # Start a build.  # Open the metadata view.    The password is shown in it's unmasked version as part of the commandline, but masked on every other place (logs and the variable itself).      My work around for now is to create a second script which reads the variable as a environment variable and calls the first script.    As a new user to bamboo, this security flaw is not obvious, certianly not when the variable settings page states that any variable containing the string 'password' will be masked.""",Bug,Tasks
127349,"""If I quarantine the tests which failed a build I'd like the build to turn green. As a workaround I can re-run failed jobs to get the expected result.""",Suggestion,Test
127352,"""As a developer I would like to retrieve the child plans of a plan via the REST API""",Suggestion,REST
127380,"""As a user I'd like to see the output of agents build logs converted to the bamboo server text encoding (or utf8)    For example, I extracted one line of a msbuild build log report contains both correctly encoded char (the date) whereas the output itself has the same letter replaced by a """","""" (comma)    {noformat}   21-févr.-2013 09:53:42 7>  Cr‚ation de """"PropertyEditor.dir\Release\PropertyEditor.unsuccessfulbuild"""", car """"AlwaysCreate"""" a ‚t‚ sp‚cifi‚.  {noformat} """,Bug,Infrastructure
127424,"""I'm having problems copying files to the network, because of permission issues.  And it would be nice to be able to run the scripts as a particular user. i.e.    DOMAIN/user  pwd: ****  """,Suggestion,"Tasks,Windows"
127433,"""I'm working on replicating the issue, but to wit:    We upgraded to 4.4 a week ago, and have branch detection for several Github-based projects checking for (release)/.*     Noticed that when I branched to release/february, the auto-detect picked up the branch, but had the actual Github branch set to develop, not the release branch, and more strangely, seemed to be picking up multiple copies of the same branch (release-february, release-february 1, release-february 2, release-february 3, etc.)    I saw at least 8 branches get created before I turned off the auto-detection, so unsure whether this was an issue with cross-branch merges I was doing, commits to the branch, etc.     As noted above, I'm working on replicating this more thoroughly. Also wondering if this might be related to BAM-12770 and that the bamboo/github integration is out of date.    Just recording the issue for posterity.    """,Suggestion,Plan Branches
127478,"""I'm working on making a plugin that will display the test history for a given plan, but came across this exception:  {code}  2012-10-04 09:52:12,674 ERROR [TP-Processor10] [PlanTestHistoryDisplayPlugin] Error : Parameter summary does not exist as a named parameter in [      SELECT t FROM TestCaseResultImpl t        join t.testCase as tc        join t.testClassResult as tcr        join tcr.testClass as tcl        join tcr.buildResultsSummary rs        join tcl.plan as plan        WHERE rs.chainResultsSummary = :chainResult        AND t.state = :state        ORDER BY tcl.name, plan.buildName            ]  2012-10-04 09:52:12,676 ERROR [TP-Processor10] [PlanTestHistoryDisplayPlugin]    AbstractQueryImpl.java.setParameter at 342  2012-10-04 09:52:12,676 ERROR [TP-Processor10] [PlanTestHistoryDisplayPlugin]    AbstractQueryImpl.java.setParameter at 398  2012-10-04 09:52:12,676 ERROR [TP-Processor10] [PlanTestHistoryDisplayPlugin]    TestResultsHibernateDao.java.doInHibernate at 159  2012-10-04 09:52:12,676 ERROR [TP-Processor10] [PlanTestHistoryDisplayPlugin]    HibernateTemplate.java.execute at 370  2012-10-04 09:52:12,676 ERROR [TP-Processor10] [PlanTestHistoryDisplayPlugin]    HibernateTemplate.java.executeFind at 341  2012-10-04 09:52:12,676 ERROR [TP-Processor10] [PlanTestHistoryDisplayPlugin]    TestResultsHibernateDao.java.getTestsForChainResultByState at 155  2012-10-04 09:52:12,676 ERROR [TP-Processor10] [PlanTestHistoryDisplayPlugin]    TestsManagerImpl.java.getTestsForChainResultByState at 126  {code}    Here's how I'm invoking testManager.getTestsForChainResultByState(..)  Note: I just started working on this plugin, so it's rough  {code}  List<ResultsSummary> buildresults = resultsSummaryManager.getLastNFailedResultsSummaries(plan,5);              List<TestCase> testCases =  new ArrayList<TestCase>();              for (ResultsSummary build : buildresults) {               ChainResultsSummaryImpl chainResults = (ChainResultsSummaryImpl) build;               List<TestCaseResult> testCaseResults = testManager.getTestsForChainResultByState(chainResults, TestState.FAILED,0, 99);               for (TestCaseResult testCaseResult : testCaseResults) {                testCases.add(testCaseResult.getTestCase());               }              }  {code}  """,Bug,"Plugins,Infrastructure"
127485,"""Task: Add a new repository. A git repo on stash.a.c with a custom branch.    This has many problems...    I have a reference repo that I am copying. It claims it is a """"Git"""" repo as the Source Repository field. BUG1: It should report """"stash master on stash.a.c""""    When not knowing this, selecting Git and filling in the appropriate fields then clicking save, apparently has no affect. BUG2: Validation errors not visible but are hidden in the advanced disclosure. BUG3: No visual clue is given that the operation failed. BUG4: Navigating to another location in the UI does not warn that config will be lost(I did this and had to start again).    On finally checking the Source Repository drop-down and figuring out that it was a shared repository...BUG5: Config of shared repos is only available to admin users(which luckily I am one).    BUG6: It *should* be possible to override fields of a shared repo. In particular the branch.    I'm suggesting the UX could be improved here somewhat.""",Bug,"Bitbucket Server integration,Repository"
127570,"""I'm setting up calabash tests to be run on a mac machine using xcode and the iPhone emulator.  Anyways I'd like them to show as Test result no as a separate task/job.  That's messy.      And I have a very similar problem with my javascript test that use grunt.  """,Suggestion,Test
127613,"""h3. Summary (Original content by the reporter)  When checking out code, the 'Source Code Checkout' task has an option named 'Force Clean Build' to clean out the directory prior to checking out the code.  I've discovered that although this option is checked, the code directory is not being consistently cleaned out before fresh code is being checked out.    Files that have been deleted from the repository are still present in the per-plan directory on each agent that has run the plan, even after several manual runs of the plan have been run on the agents.  This causes confusion and delay to the builds as outdated information is used.    Please ensure that 'Force Clean Build' actually does what the documentation says it does, that is, remove the directory before running the build.    h3. Steps to Reproduce  # Install latest Bamboo v5.9.4 (OS does not matter)  # Link to a Git repository  # Create a plan with 2 stages:  ## Stage1: It has a single job with 'Source Code Checkout' task  --- It checks out the source code from the linked Git repository  --- It enables 'Force Clean Build'  --- It shares every file as an artifact (i.e. copy pattern: {{\*\*/\*}})  ## Stage2: It has a single job with 'Script' task  --- It has a artifact dependency to be shared the artifact  --- It executes {{$ find .}} to check the working copy of the repository  # Execute the plan  # Delete a file from the repository  # Execute the plan again. The log of Stage2 tells that the deleted file is still accessible    h3. Expected Results  The deleted file is not accessible any more    h3. Actual Results  The deleted file is still accessible     h3.Workaround  Delete the working directory manually before the plan is run ({{<BAMBOO_HOME>/xml-data/build-dire/<PROJ>\-<PLAN>\-<JOB>}} directory). You may do it in another task which is executed prior to 'Source code checkout' task""",Bug,Build
127657,"""As a user of the MSBuild plugin who has to define multiple environment variables, I would like to have a larger input field for the Environment Variables. Ideally, this would be a multi-line input""",Suggestion,User Interface
128138,"""The /latest shortcut on a build plan is a nice mechanism to get details on the latest build, both from a UI as well as an API perspective. That said, something I'd say is critically lacking is the ability to GET /latestSuccessful or /latestFailed without having to load all the results and parse the resulting JSON output.    Regardless of 'latestFailed', about which I'll reference a few yet-to-be-filed issues shortly, it seems like the ability to rapidly select /latestSuccessful is a significantly missing chunk of functionality.""",Suggestion,REST
128153,"""If I go to the plan branches page (which lists all branches, like on allPlans dashboard) you can hover your mouse over branch name and a 'title' attribute will appear. The 'title' is currently populated with issue-linked-to-branch description. This come very handy if you don't remember what branch you are after - hovering your mouse over a branch name gives some context.    I'd like to have the same behavior when dropping down the breadcrumbs branch selector - that is, when I hover over some branch name I'd like to see (as a 'title' popup) the branch description.    See attached screenshots.""",Suggestion,JIRA integration
128158,"""Steps to reproduce:  1) use REST api to add a 'test' label to a plan (that has no such label)  1a) in the UI the plan won't be displaying 'test' label  1b) while I'd expect it will   1c) note the label was added to the db table buildresultsummary_label  2) edit configuration of that plan and save it (for example changed the plan name from """"Plan name"""" to """"Plan name2"""")  3) observe the label appeared in the UI""",Bug,REST
128301,"""I itegrated Bamboo with Jira and I am currently using Jiras userrepository for authentification with Bamboo.  At the moment I can only grant permissions to Bamboo plans based on users and groups (both managed in Jira). I would like to grant permissions for Bamboo Plans based on Jira Project roles. This would simplify user managment as all permissions could be controled by Jira project admins simply by adding and removing users from the roles in there projects.""",Suggestion,"Admin,JIRA integration"
128344,"""On the plugin management page, if I click the pause button, followed quickly by the cancel pause button, Bamboo will pause then unpause as expected.    However if a build is running at the time, then it will pause itself again later once that build finishes. I'm not sure if a build just has to be running, or requests for builds have to be coming in as well for this to occur.    Attached a screenshot of it going from """"RUNNING"""" to """"PAUSED"""" without going to """"PAUSING"""" first.    I think this happens because the queue watcher doesn't get cancelled properly when resumeServer() is called. The queue watcher spins in a loop waiting to transition the server from """"PAUSING"""" to """"PAUSED"""". When you call resumeServer(), it sends a cancel to the watcher.    I'm unsure, but I believe that since the watcher has a second callable inside it that blocks when build requests are being made, the watcher is cancelled but the inner blocked callable hangs around and executes anyway, transitioning it from """"RUNNING"""" directly to """"PAUSED"""".""",Bug,"Admin,Plugins"
128370,"""In Bamboo 4.1, the 'Assign Responsible User' for a build result that is broken needs a permissions setting. For example while logged in as a user (UserA) with full permissions to a plan (View, Edit, Admin, etc). I assign responsibility to UserB .    UserB ONLY has View permissions on the plan. If I log in as UserB I can remove the assignment of responsibility from the build and assign another user.    I would not think a user with only VIEW permissions should be able to alter ANYTHING on the build, including build responsibility.""",Suggestion,Responsibility tracking
128474,"""The new branch functionality shows great promise.  But, and there's always a but, it doesn't handle plans with multiple source repositories.      For an example setup, I have code stored in the following svn locations:    /svn/libraries/core/trunk  /svn/libraries/widget-creator/trunk  /svn/applications/magic-8-ball/trunk    To create a clean build of the magic-8-ball application, I need to check out all three in order and build them.  If I want to branch the magic-8-ball application for release and have it depend on specific versions of its libraries, the source directories might be:    /svn/libraries/core/branches/3.14  /svn/libraries/widget-creator/branches/12345  /svn/applications/magic-8-ball/branches/1.2    From a functional perspective, each of the source repositories in a plan needs to be able to have a separate source repository path when building a given branch.  This path for the default build, this other path when building the '1.2' branch etc.    On the UI side when configuring a Bamboo Branch, the current location for specifying a different source repository when building for a branch is partially non-intuitive.  To my mind, especially if Bamboo's concept of Branch builds becomes more central to Bamboo's operation, having the different source repositories for Branches as additional options in the Configuration -> Source Repositories -> Edit Source Repository tab would make more sense.  For Branch (pulldown) use [this] repository information with [x] same password or different password etc.  I really like the idea of not having to retype the same repository password multiple times.  """,Suggestion,Plan Branches
128490,"""As an administrator I'd like to hook an ebs volume into bamboo ondemand rather than a snapshot, as I need a consistent state of dependencies between elastic instance restarts. I've hooked a snapshot in as per your documentations however I have builds that rely on each others build. For example plan 1 builds and installs as a plan1.jar via maven. plan2 relys on the existence of plan1.jar. Plan2 is then triggered by plan1 and builds succesfully and the elastic instance shuts down. A developer then checks in code to plan2 and it fails, as the elastic instance has no plan1.jar installed.""",Suggestion,Elastic Bamboo
128602,"""Sometimes, when a job in my build fails, I know as soon as it has failed that I want to run it again (e.g., dependency on external infrastructure that was down at the time the job ran, but is fixed now).    In some cases, other longer-running jobs are still in progress, so the """"Rerun Failed Jobs"""" button doesn't appear. This means that I need to wait for everything else to finish before I have a chance to rerun the job that has already failed.    It would be better if I could kick off another run of jobs that have already failed while other jobs are still in progress.""",Suggestion,Build
128742,"""On the admin page, I'd like to see (near the top) a easy way to pause/unpause the server. Other views (like the upm) can still include this on the header of their views, but as an admin I shouldn't need to think """"oh I need to pause the server, let me load the plugin manager view, as I know it has pause feature on the top).""",Suggestion,Admin
128928,"""This is something I use heavily when determining why builds failed.  The result navigator used to be for the job so this was possible.    That said the plan status widget was always plan only, so not sure what the solution is, but I need one.""",Bug,User Interface
128940,"""From customer;  bq. I would like to be able to determine whether or not a task failed by looking in the log file for specific error messages (reqular expression search). If the string is found I want the task to be failed. Is there a direct way to do this within Bamboo or do I need to go and add a final task everywhere that searches for the expression to all jobs? Curious since this is an out of the box feature of luntbuild and I was wondering if bamboo has anything similar.    """,Suggestion,Tasks
128947,"""I have a quite fragile build that I would know would fail by just looking at the first couple hundreds of lines in the jira.log - there is no way to read them before the whole build finishes (which is much later)    I would love to have ability to see my target/jira.log online in the same way I see the mvn log.    Such log file might be defined as a special case of artifact that would be streamed on Plan/build log page in addition to the maven log""",Suggestion,"User Interface,Build"
129427,"""When looking at test case summary, I would like to get a table with last N builds (N=20 or configurable) with explicit information whether given test case passed, failed or was absent in given build.    As a bonus I would love to get a mouseover with detailed build info - changesets, failed test count etc - but I can implement that with SpeakEasy myself later if you can't ;-)    I need this info to diagnose:  - whether this test seems to fail randomly  - whether this test disappears in some builds and reappears later.    Due to BAM-9933 I would love to have this info on plan level as well as job level - so if this test executes in JOB1 in some builds and JOB2 in others, I would like to see all executions in one place.""",Suggestion,Test
129449,"""Feedback from customer;  {quote}    If I click a link to a specific passing build, and click on the Tests tag, it brings me to a screen that has a summary, such as: """"210 Tests in total 1 minute taken in total."""" I can not see the tests run unless I click into a job.    If it were a build which had failing tests, or newly passing tests I would get a summary of new failures or successes. This is great except the """"Summary"""" tab already has this information. In fact, more information.    ....the Tests tab should show information on tests. The extra click in an already overly """"clicky"""" application should be eliminated.  {quote}    I agree with the customer, the plan level test tab has no data - might as well not have an empty test tab.. (see screenshot.. )""",Suggestion,"Build,Test"
129501,"""Feature Suggestion....    Provide the Bamboo Admin the ability to lock or freeze the available capabilities on an agent. In the Admin Agent page, if I click Edit link next to an agent I can edit name and description. I would also like to have a checkbox that says something like """"Lock Capabilities"""". If checked, then the capabilities cannot be changed on that agent.     Per my understanding and testing of Bamboo 3.2, I create an agent on a remote machine. It reads environment variables to determine capabilities such as MSBuild.exe's, Ant, Maven, JDKs, etc. and then displays those as the capabilities for that Agent in the Admin Agent section.    If as an admin I delete/remove those capabilities in the Admin > Agents > View (for a specific agent) page, then if the Agent Server gets rebooted (due to power loss, maintenance, patching, etc) it makes the original capabilities available once again for Plans to use.    Use Case For Why This Feature is Needed :     If I have 2 agents, Agent A and Agent B both running on separate OSes/Servers, I may want Agent A to be configured with Java build tools (Ant, JDK, Maven, etc). I may want Agent B configured with Microsoft build tools. Due to a variety of reasons it may be true that Agent A and Agent B both have other tools on the actual OS, but they shouldn't necessarily be shared as capabilities in the Agent.    Say a development shop does 50 Java build projects and 4 Microsoft build projects. It's true that both Agent A and Agent B could be configured to support the same capabilities, but an IT owner of Bamboo may want to ensure that the Microsoft build teams have a Bamboo Agent available and it is not constantly being utilized by the vastly larger numbers of Java teams. Thus, the Bamboo admin could strip Java-related capabilities from the Admin >Agents >View page so Plans cannot use Agent B for Java builds. However if the Agent needed to restart for above mentioned reasons then the 50 Java Plans could overtake Agent B and monopolize the Agent B. (Obviously the IT owner could strip environment variables off the server running Agent B, but the tools may actually be needed for other reasons by people directly accessing those servers, but not through Bamboo).    Based on the description the Agent itself does not need to be modified and can function as it does, however the main Bamboo server would need the """"lock"""" feature implemented.    """,Suggestion,Admin
129636,"""* Maybe I want to deploy to a different server than my plan dictates.  I can't do this because prior builds will have the plan level variables set and you can't change the variables on the fly to take effect  * Maybe I want to change my maven version on my release tag on my manual stage (my real selfish request here)  * Maybe I want to call the release tag something else and I normally have this set in the plan variables (also selfish)    We are stuck right now on the plan level if a build has run and we want to go back and run a manual stage that we are stuck with the variables of that past run.  A parameterised manual stage FTW!  :)    [~<USER> also thought this was a good idea  :)""",Suggestion,Stages
129863,"""We are using Bamboo for Maven releases, we use the Bamboo 3.1 tasks with build plan variables. We have to include the username and password as variables so we can release under end-user credentials. Unfortunately Bamboo displays all the variables of the build plan. Maven correctly hides the password with ***** in the output log but bamboo does not.    As an user I want to provide my credentials as plan variables without them showing up in the bamboo build log and build summary.""",Suggestion,Tasks
129874,"""Spots instances have been a great money saver but this saving gets absolutely crushed on bamboo crashes and restarts because of the connectivity loss to amazon EC2 agents.  These agents will remain running and continue to charge us money.    We need to allow for us and customers to shut these instances down that """"have never connected to this bamboo instance""""    Navigate from Administration -> Instances -> You can view all elastic instances that are running on your account      Please see the attachment on all my instances that I get charged because Bamboo does not clean these up on startup/shutdown (another feature I'd like as an optional config setting).""",Suggestion,Elastic Bamboo
130578,"""When saving a repository modification, the plan overrides the old build definition prior to running a comparison to see if the repositories are different. Since the plan has already overrode its previous state, Repository.setReferencesDifferentRepository(boolean isDifferentRepository) is always fed false.     I'm seeing the bug in:     {code:title=DefaultBuildDefinitionManager.java|borderStyle=solid}  public void savePlanAndDefinition(final Plan plan, final BuildConfiguration buildConfiguration)  {code}      """,Bug,"Repository,Repository"
131086,"""h3. Problem Definition  A user has a customized image in Amazon and a reference in Bamboo with all capabilities set. I need to create another image from that one, which some modifications. Currently, I have to:  # Start the image in Amazon, modify it and save the new image  # Create a new image in Bamboo and set again all capabilities, as the capabilities are not copied from the image I had originally.    h3. Suggested Solution  I would like to be able to clone a previous """"Elastic Image Configuration"""" in Bamboo. This way, to make a new configuration based of an old one, I'll avoid to  manually copy over all of the custom capabilities.     On JBAC this can be painful. """,Suggestion,Elastic Bamboo
131258,"""If you're running your tests against a web application running in Cargo, there is an application server log produced as part of the build. Because they include messages and exceptions directly from the application, application server logs can actually be much more helpful in debugging build failures than the build logs and test output which Bamboo automatically captures.    At the moment, we configure these log files as build artifacts so they get stored with the build. However, I'd like to see application server logs become """"first-class citizens"""" in Bamboo. For me, that would mean:     * Application server logs should be part of the """"Logs"""" tab in Bamboo, in addition to the current build and plan logs.  * Application server logs should be available while the build is running, so you can see what is happening on the app server during the tests.    At the moment, interacting with the application server logs as artifacts is workable but clunky. The above two features would improve things dramatically.    The application server log location would be optional and configurable on a per-plan basis. When running with Cargo, the default location is 'target/output.log' in the webapp module of the project, so this should probably be the default value.    Potentially you could combine this feature request into a generic """"track arbitrary log files during builds"""" request, because Java app server logs are just our particular instance of this general problem.""",Suggestion,"Tasks,Artifacts,Infrastructure"
131837,"""I would like to be able to automatically perform a manual process I sometimes do:  {code}  ant build.something  [if passed]  ant build.something.else  [if passed]  ant deploy.the.result  {code}    The point is that step2 operates on results of step1 - no clean in the mean time.    I would like each step to have separate UI in Bamboo as if it was a normal standalone build.  In my case, ant build.something builds the jars and performs unit tests, build.something.else just does exactly the same unit tests but with different classpath.""",Suggestion,Stages
131951,"""As a bamboo administrator  I want to describe a build plan  so that others may understand the purpose of a build plan    - This could be implemented by allowing administrators to leave comments on build plans""",Suggestion,Admin
132422,"""I'm not sure if it's browser specific but on Safari 3,   1. go to all plans screen   2. click on the favourite button several times across different build plans  Then it becomes in a state where every single build plan has a grey star. You have to refresh the page to get the correct display of favourites.""",Bug,Dashboard
132513,"""I would appreciate, if user defined dashboards where supported, as already known from JIRA and Confluence.   This would allow to view different sets of projcets, plans, and reports.""",Suggestion,Dashboard
132574,"""The """"Completed Build Results"""" screen currently has the following columns:  Status, Build Number, Reason Completed, Duration, Test Results,Operations    If the name of the Agent used to execute the build was added to this column list, it would be very helpful.  I can see 2 immediate benefits:    1. If there is a problem with a Build Agent, such that builds are failing on 1 agent but succeeding on others, it would be easier to recognize the pattern.  2. Recognizing that it is running slower on some build agents than others is useful.    I'd actually hope that you'd consider adding the """"Build Agent"""" to many screens and also to the reports, but I'm sure that is too ambitious so I'll stick with this smaller scope.  """,Suggestion,Build
132786,"""Writing automated UI tests for plugins is very easy for products that support import without restarting the application, between each test you can do an import and then you're guaranteed to have a clean new instance of the app, no need to worry about cleaning up after yourself, creation of test data is very fast and simple.    Bamboo doesn't support this.  I'm not sure how Bamboo's web tests ensure that dirty data isn't carried between tests, but if you've written a simple plugin that just requires one build plan to be set up in a particular way, you're not going to want to have to write 50 lines of code that go and click through Bamboo's build plan creation process, and then write more code to delete it all at the end of each test, so that you can write a few lines of code to test your plugin.  You just want to create your test dataset, and then write a simple bit of code that imports that data.    I understand that Bamboo has a lot setup and running in the background, and that an import has the potential to cause everything to go crazy, but in a functional test scenario, you have control over everything that's happening, you can ensure that there are no remote agents, no builds running, no repository polling etc.  Even if there's a hidden importWithoutRestart option on the import page, not for production use, this would be massively helpful for plugin developers.""",Suggestion,Import / Export
132997,"""Request updateAndBuild (preferred) or a new API that does the same to cover 2. below.  Linked issues cover 1, but I thought I would all the information here.  From BSP-1361 so it can be closed.    Here is what I believe the correct behavior for updateAndBuild and this should be and documented as such:  # From configured trusted IP or, if blank, source control server - no authorization required on call  ## if there are source changes then trigger build and return current message (this is the way it works now )  ## if there are no source changes then no build is triggered and return informational message (not an error!!!) that there are no source changes and no build triggered (the change is to give the info message on the api)  # From any other system require authorization  ## if and only if successful do the same as above.  ## If not authorized then return an error message  """,Suggestion,"Triggers,API"
133111,"""I would like it if I could not only trigger additional build plan upon completion of a build, but to have another plan triggered before a build is started. An example would be having a build which wipes and re-images a box with a new database. I have a wipe image repository plan as a child plan of this since the 2 are linked, but I would like to be able to stop and start apache before the plan is being run so someone can't accidentially hit the database or image repository when this build is going on. I currently have to manually stop apache before and restart after. It seems that Bamboo should be able to take care of this. I don't want to run a stopApache plan everytime which triggers the db build -> wipe images -> start apache. I would like to have the database build as the one that is clicked. Also, conditionals for plans to execute when a build fails would be useful.""",Suggestion,Triggers
133132,"""Not sure the best way to implement this but I would like to have a daily or cron build strategy that only triggers if another build is currently not failing.    At the moment my daily build has to be triggered manually which isn't ideal as it should be done out of hours, the only other alternative is to set it as a daily build. However if one of our other commit builds is not completing then the daily build will always fail. This fills up the bamboo screen and out email boxes with fails as well as having other disadvantages.""",Suggestion,Triggers
133154,"""The default for new users seems to be no notification. As admin, I'd like to either be able to set the default notification prefs for new users or be able to edit an individual user's prefs from the admin interface. As is stands, our users don't know if they broke the build by default.""",Suggestion,"Admin,Notifications,Admin"
133264,"""Plugins should be able to specify when they'd like to execute.  This is especially useful for pre-build plugins that may depend on other plugins having executed before them.  For instance, I would like to specify that the jiraversions plugin runs before the pre command runner plugin so admins can pass the jiraversions custom data to the pre command runner.    This could be implemented as a new collection setter. .i.e:  public class JiraVersionsPreBuildAction {      //a list of strings with the format pluginKey:pluginModule       List<String> beforePluginModules = new ArrayList<String>();      beforePluginKeys.add(""""com.sysbliss.bamboo.plugins.postbuildcommand:preCommandRunner"""");           this.addExecuteBefore(beforePluginModules);  }      """,Suggestion,Plugins
133269,"""I understand the low-hanging fruit of the JUnit test results format.  Unfortunately, it doesn't seem to be documented.  It's also lacking as a format and Bamboo's interpretation of it is occasionally surprising (see BAM-2627 )    I would love to have one of:  - API for interacting with test results  - Formal documentation of test result format  - Better test result format, formally documented    If you aren't using JUnit, Bamboo's ability to deal with tests is greatly reduced.""",Suggestion,Test
133358,"""Bamboo has a l{{og4j.properties}} file set up to log all messages to a ConsoleAppender and a RollingFileAppender    In the former case they get logged to the wrapper log in {{$BAMBOO_HOME/logs}}, and in the latter case dumped in whatever directory {{wrapper}} is run in (since the path for the log file isn't set in {{log4j.properties}.  Currently this means I have to give my build user write permissions to the {{wrapper}} directory, which I'm not overjoyed about.    What I'd like to see is:  * A single RollingFileAppender (actually I'd prefer a DailyRollingFileAppender to make finding logs easier)  * All log files in $BAMBOO_HOME/logs - e.g. {{wrapper.log}} and {{atlassian-bamboo.log}}  * No ConsoleAppender  * BAM-2803 fixed so I don't get remote agent spam in my logs  """,Bug,Infrastructure
133461,"""I'm using TestNG and have one test class that takes a parameter. In my TestNG output, each usage of that test is shown as a different test, due to the change in parameter.  this is correct.  the JUnit XML file that TestNG outputs has one entry for each test case, as it should.  However, the entires for these tests that, in TestNG, are parameterized, are all identical.  Bamboo seems to kill the duplicates and only show one of them (possibly someone's using a Map instead of a List?)  I've attached my JUnit file and a screenshot of what appears in Bamboo""",Bug,Test
133659,"""The plan is setup to retrieve the module 'comp-test' from CVS.  'comp-test' is setup as an alias module, retrieving 'components' and 'lib'.  This results in the following directory structure:     $BAMBOO_HOME/data_20/xml-data/build-dir/LW-C2T/comp-test   $BAMBOO_HOME/data_20/xml-data/build-dir/LW-C2T/comp-test/components   $BAMBOO_HOME/data_20/xml-data/build-dir/LW-C2T/comp-test/lib    The builder has the 'Working Sub Directory' set to 'components', and runs an ant task.    The 'Specify Custom Results directories' is set to: 'components/all/testData/xml/*.xml' .  In Bamboo 1.2.4 this worked.  In Bamboo 2.0, I need to prepend that by 'comp-test/' in order for it to find the tests.  Not sure which way is right (ie, if it worked because of a bug in Bamboo 1.2.4), but it should somehow be compatible with the Bamboo 1.2.4 data (if by upgrading it because of an upgrade task, or otherwise).    """,Bug,Test
133823,"""My company develops J2EE applications.  Our current application server of choice is WebLogic.  We are in the process of migrating from Weblogic 8 to Weblogic 10, and some of our apps have made the jump, while others have not.    Weblogic's J2EE implementation is part of the application server, not part of the application.  This means that it's not in our source respository as a JAR file, and it has to be included in the classpath at build time.   (Side note: there are actually about a dozen JARs that need to be included in this manner.)  Since we're using two different versions of Weblogic, each build plan needs to be able to specify which set of JARs to include.    Our old CI application (AntHill) provided us a way to specify a set of JARs to include for each build plan.  You drop JAR files in a directory on the build server, and Anthill creates a checkbox on the build configuration page for each JAR in that directory.  This solution is a little bit awkward.    In Bamboo, I have remedied this problem by creating multiple installations of Ant, and putting the Weblogic JAR files in Ant's lib directory.  Two installations of Ant for two versions of Weblogic.  Then I configure a builder for each installation, and name them """"Ant 1.7 - WLS 8.1"""" and """"Ant 1.7 - WLS 10.0"""".    This works, but it would be much nicer if I could define a library for each version of Weblogic, and specify a list of JAR files to be included in that library.  Ideally, I'd like a Bamboo admin to be able to upload JAR files via the web interface.  Then I could specify which libraries I want to include in the classpath when I set up the build plan.""",Suggestion,Build
133872,"""Often, build artifacts are textual log files that can benefit a great deal from compression.     At the moment, it is very inconvenient having to wait to download a 2M log file, which under compression would be less than ~100K.     My suggestion is to provide a TGZ option next to each downloadable artifact.     As a first cut, I'd be happy if _all_ the build artifacts were rolled into one zip - as this would already produce a bundle that is far smaller than one of the log files I download at the moment.     """,Suggestion,Artifacts
133892,"""Bamboo has undocumented restrictions with how you can use global variable substitution for a plan's goal.  In our case, we have a batch of plans that all share the same set of verbose configuration options.    I defined a global variable, {{OPT.DBSETTINGS}} to represent a series of options that are common to several builds.  This looks something like:    {{-Ddb.host=our.db.server -Ddb.port=5001 -Ddb.dbname=xe}}    It's a fair bit longer than that, but you get the idea.  I passed this as a goal to our maven 1 builder like this:    *Goal:* {{${bamboo.OPT.DBSETTINGS} full}}    This doesn't get passed to maven in the way I'd hoped. Instead of:  ||Property||Value||  |db.host|our.db.server|  |db.port|5001|  |db.dbname|xe|    ..maven receives:  ||Property||Value||  |db.host|our.db.server -Ddb.port=5001 -Ddb.dbname=xe|  |db.port|_not set_|  |db.dbname|_not set_|    Other global variables of the form {{-Dpropertyname=value}} get through as expected.""",Bug,Tasks
134206,"""Bamboo has a tab for each build called """"Changes,"""" which allows you to see what has changed since the previous build.  This is a great feature.  Sometimes however, we wish to know what has changed from 10 builds ago.  This is often the case during release cycles or drops to QA, where we need to generate a report of what has been fixed/updated/added since the last drop.    This feature should allow me to select two arbitrary builds (from the same plan) and show the changes between them.    A suggested implementation in the UI would be to add functionality to the current """"Changes"""" tab.  It should allow the user to select any build (i.e. other than the immediately previous build) from which to show the changes.  The UI would then be updated and display changes like usual, only they would span a gap of more than 1 build.""",Suggestion,Telemetry / Reporting
134297,"""Bamboo uses a ton of disk space, with no easy way to keep it to a managable level.  The logs for running it are constantly inflating with output of checking the repositories for updates.  Fortunately it seems to limit the filesize to 20MB, and keeps only 5 levels of history for a total of 80MB of logs.  That's an upfront cost that should never change.  It would be nice if these logs grew at a slower pace (or if there was an option to reduce the amount of logged info.)    Build output can grow beyond reason, though.  It's often useful to have a continuous build that has logging turned on, so you can see what's going wrong when something goes wrong, but each build can then balloon in size.  We're at a point where Bamboo writes ~1GB per day.  We've temporarily set the builds to expire every 2 weeks, but that isn't an ideal situation -- we really want to keep them around forever (or close to forever).    Some suggestions for managing ballooning disk space:   - Archive builds older than X days, unarchived upon request (spidering disabled, etc..).   - Store build output in zipped format, unzipped upon request.   - Enable finer-grained expiry controls pt 1.  (Per project, plan, on-demand build, etc..)   - Enable finer-grained expiry controls pt 2.  (Options to expire only: selected artifacts, log output, test results, etc...)    I'm sure there's a ton of other things that can be done too.    Thanks!""",Suggestion,"Artifacts,Performance"
134349,"""<USER>mentioned a build server that he used a long time ago (Continuum?) had a neat feature where the original committer who caused a build failure would be """"on the hook"""" until the build passed. In addition, anyone who committed to a failing build and didn't make it pass (and therefore could have broken more stuff) would also be """"on the hook"""" until the build passed. Once the build passes, everyone that is on the hook gets """"let off the hook"""".    This notion seems useful, particularly for notifications. It would be great if IM notifications only went to people on the hook. The current notification scheme only allows committer notifications, which isn't as useful when there are multiple commits while the build is failing.    It would be also good to have a telemetry screen with mug-shots of those users who are """"on the hook"""". This gives the team an idea of who is (or should be) fixing the build at the moment.    This could possibly be developed as a plugin; I'm not sure how customisable these things are in Bamboo. It's probably a matter of adding some new metadata to a failing build (a list of users who are on the hook), and exposing that for use by notification schemes.    If it can't be a plugin, I think it would be a useful addition to Bamboo core.""",Suggestion,"User Interface,Notifications,Triggers,Telemetry / Reporting"
134381,"""Sometimes, when the number of tests changes, I want to understand the differences between the tests in two different builds. Perhaps you could show this in the interface, but I could do it myself if there was a way to get a complete list of the tests for a build.  """,Suggestion,Test
134423,"""The 1.1 release notes mention that Bamboo defines build metadata which can be passed to a builder. bamboo.buildNumber is given as an example. What other metadata is available? I can't find any other mentions in the manual, forums, etc. I'm immediately interested in my svn repository version, but would love to see a complete list.""",Suggestion,Tasks
134450,"""Bamboo should show SCM changes for multiple cvs modules?     I'm using bamboo to build the overnight of a project spread across several cvs modules. The product has a master ant file that calls the builds for the other modules and incorporates all the elements. I'd like to have bamboo watch all the modules for changes and report them.    """,Suggestion,"Build,Repository"
134486,"""I've created a plan that hasn't yet been built, and was browsing the tabs to see what information was available.  The 'Completed Builds' tab has the following text on it:      """"Bamboo builds everything whenever the Subversion source code repository changes.""""    ...but the build strategy for plan for my project is """"Manual builds only"""".""",Bug,User Interface
134644,"""It appears that some of the AJAX breaks IE 7 when you're logged in as any kind of registered user.  I see it on the Plan Summary page as both a normal user and an admin.  I've told my user to stop using IE ;) but that doesn't seem to be a permanent fix for him.""",Bug,User Interface
134808,"""I've noticed that repository changes are not associated with a user unless a repository alias is added. This is true even if the user ID and the repository alias are the same.""",Suggestion,Repository
134857,"""As a 'manager' of a development project, I have gotten to the stage where I want to switch off my immediate email notifications for build failures (as there are too many), but I still want to keep on top of how the projects are progressing.  Information like (for each project):    - How many builds today?    - How many committers?  - How many builds passed / failed?  - Average time to fix a failure?  How many hours today was the build failing?    Of course with click-throughs to all the appropriate places.    I know that I can get this information online, but an emailed notification would also be useful.    <USER>",Suggestion,Notifications
134958,"""0.8 looks great guys.     Just wanted to point out though that the username and other links displayed in the top right are now quite dull in colour, making them partially harder to see. Just thought they might be better in a brighter colour (like white). This would also match your other app headers (ie jira).     Admittedly fisheye has a slightly grey links, but the mouseover as white. The username would be clearer in white though.    Thought I'd throw that in here. Keep it up, well impressed with the ui.""",Suggestion,User Interface
135014,"""i'd like to be able to define a threshhold on build time that makes the build count as a failure.  Probably defined as a percentage of average.    I currently have a build that locks up horribly from from time to time.      It would be great if it could be automatically killed and counted as a failure whenver the build time gets to twice the average, or 30 minutes past the average, or even 100 minutes (regardless of the average).""",Suggestion,Build
135018,"""Currently, I have three separate numbers that indicate the version of the software being built:    1) The Subversion revision number  2) The Bamboo build number  3) The application build number (which is held in a properties file that Bamboo updates on every build)    I want all of these to come from Subversion.  This means that:     - Bamboo should obtain and hold the revision number when it does an 'svn up'   - Bamboo should use this as the build number   - Bamboo should (ideally by default) pass this number to Ant so that I can use it as a substitution value during the build.    """,Suggestion,Repository
135164,"""I'd like to be able to use a different filter for the number of builds displayed on the project summary page and results page.    I'd like to see all builds on the summary page because the are grouped and do not take long to display, and it's a nice coverage of build history. However I don't want to see them all individually on the results page as after 800 it takes forever to load the page - I'd personally keep this down to 30 or 60 days max.""",Suggestion,User Interface
135194,"""One of the primary reasons for selecting Bamboo over Zutubi's Pulse or Jetbrains TeamCity would be the potential for tight integration between the build server both Jira and Confluence.  As a start it would be nice to see what Jira issues were included in each build.    I could continue on the theme and say it would be nice to publish artifacts in Confluence either autmoted with each build or as a designated push especially around release time.""",Suggestion,Telemetry / Reporting
135218,"""Not sure how to mark this one up. If i kill a build partway (say my 2hour build is cut at about 10 mins) it says it's successful. This wouldn't be a problem if it then meant the build queue timer was then updated to include this, thus becoming miles out of what it should be.    I'd like it be marked with something like suspended, or not all tests run - kinda like a 3-state boolean :).    Actually this is quite open. Personally I'd say a build stopped short is a failure as you can't prove your code works unless you run the entire suite. Thus it should be marked as a failure. However, I may be narrow-minded in this thinking....""",Suggestion,"User Interface,Build"
135396,"""As a Confluence editor, I need a button to highlight (change the background color of) content, like the button I have to change text color, so that I can make draw attention to content easily and make it more readable.""",Suggestion,Editor - Fabric
135526,"""When editing a Confluence page, all the Jira tickets/issues and/or filters appear as gray boxes that you cannot read. This is frustrating when one is running a meeting and has lots of Jira issues and filters on a page. As one types notes/actions and meeting notes, all the Jira issues and filters revert to grey boxes.    What I have been doing whilst running meetings, is to present my published meeting page in the Zoom meeting and then on my second screen, type the notes, meetings and actions but this is frustrating as every 5-10 minutes I need to publish my edits and notes and then refresh the shared meeting page that is being shared live in the meeting.    As I use many Jira filters and Jira issues in a meeting page, it is also quite painful when I need to copy and edit a meeting page, again, as the Jira issues appear only as a grey box with an issue number and one cannot read what the issue is so one has to have a second page open to keep reverting to what one is actually looking at.    This seems quite painful for two very clever systems - is there a way around this?""",Suggestion,Integration
135779,"""When I go to add a link in Fabric, it insists on changing my text to a widget with the name of the page I'm linking to. Instead, the default should be to preserve the text I've selected.    As a workaround, I can use Ctrl+Z to get the text I want, but it's awkward to have to do so.    Why can't Fabric be like every normal rich text editor on the planet? Creating these widget/card things should *not* be the default behavior.    More generally, I feel like Fabric is constantly fighting me and violating every established usage paradigm of text editing. Please study other text editors (including the legacy Confluence editor which was decent) and rationalize the behavior.         !image-2020-07-09-20-14-54-429.png|width=666,height=341!""",Suggestion,Editor - Fabric
136287,"""Currently, Users that have not logged into Atlassian and are detected as Anonymous users can try to directly access pages in a <USER>instance that allows anonymous access to some pages.    When they try to directly access pages that they cannot otherwise see, then they get the error indicating that the page or space is restricted. This can be confusing for some users who expect to get access and do not notice that their little user icon is currently set to be the """"login door"""".    I would like to propose that we trigger users who are not logged in as any Atlassian Account and try to access a Confluence page that does not otherwise allow access to anonymous users to prompt for login or otherwise indicate that the use might want to try logging in.""",Suggestion,"User - Global / Space Permissions,User - Authentication"
136539,"""Recently Atlassian enabled 2FA for domain accounts which is a good first step...however you've only addressed half the problem.    What about all the accounts on a domain we don't manage?    As an <USER>for my organisation, I want to force ALL users to have 2FA enabled if they plan to access our sites. So I would like to see a setting added to Jira, Confluence, <USER>which is:   * Force non-domain accounts to have 2FA enabled    Until they setup 2FA on their Atlassian account, they can't access our site.""",Suggestion,User - Authentication
136648,"""I've spent an increasing amount of times answer questions from users who are trying to access pages when they aren't logged in and it's just listing it as a restricted space. Our cloud instance does not allow anonymous viewing, so a user must always log in to view information. But there's no clear indicator to inexperienced users that they're not logged in if not prompted - which is SUPER annoying as an admin. I'm wondering if something changed, because this has only been an issue recently.""",Suggestion,User - Global / Space Permissions
138141,"""h3. Description  As a Confluence user, I would like to be able to create links with custom URL protocols that are used by other applications for eg. Hansoft.  Currently, Confluence does not work with custom protocols and the link is never created.    h3. Why is this important  Companies tend to use different applications that use different protocols for their interactions. Having Confluence work with those can improve productivity, ease of use and more seamlessly jump from one app to another.     Confluence pages with skype links which will open Skype and start call  Or a list of slack channels list with their descriptions where every channel name is a link pointing to slack and will open this channel immediately for you""",Suggestion,Editor - Fabric
138188,"""h3. Issue Summary    Currently, Confluence doesn't have the capability to use a paragraph indentation by using the *TAB* keyboard key and even copying and pasting this formatting from an external source (like an e-mail or Word/PDF file text), it doesn't keep the original format.  h3. Environment      * Confluence Cloud  h3. Steps to Reproduce    By copying from an external source (you can use the attached file as an example - [^Paragraph indentation.docx]):   # Copy the paragraph from the file;   # Paste it in a Confluence page (an existing one or not, in the tiny or fabric version).    By trying to insert it manually:   # Write a 3 lines paragraph;   # Press the *TAB* keyboard key, in the first paragraph line.    h3. Expected Results    The copying example:   * The paragraph should come with the same indentation than the source    The manual example:   * Hitting *TAB* should get only the first line indented.    h3. Actual Results    The copying example:   * The paragraph comes with a Confluence default indentation.    The manual example:   * Hitting *TAB* gets all the paragraph indented.       h3. Workaround    We do not have a workaround. I've tried to hit the *SPACE* keyboard key instead of *TAB* at the first paragraph line, and it sent me to a new line.""",Suggestion,Editor - Fabric
138235,"""As a user, I would like to be able to create indented action lists  h3. Why is it important    Creating sub-actions are useful to organize bigger tasks that are broken into smaller steps  h3. Suggested solution    Have a similar solution as what was found in the Legacy Editor / Server editor         From CONFCLOUD-59780:  h3. Expected Results    The list should be indented as image below:   !_thumb_315285.png|thumbnail!  h3. Actual Results    The list is not indented, as you can see below:     !current.png|thumbnail!    From CONFCLOUD-65814:  h3. Expected Results    The copying example:   * The paragraph should come with the same indentation than the source    The manual example:   * Hitting *TAB* should get only the first line indented.    h3. Actual Results    The copying example:   * The paragraph comes with a Confluence default indentation.    The manual example:   * Hitting *TAB* gets all the paragraph indented.               """,Suggestion,Editor - Fabric
138311,"""As a user, I would like to be able to *switch between* the new editing experience (*narrow view*) and the old one (*wider view*)  h3. Why this is important    Documentation may not fit on the new page style, for both editing or viewing. Working can become cumbersome. Wasted space on higher resolution monitors (1080p, 4k).""",Suggestion,Editor - Fabric
139362,"""Getting access to page_id is great, but it'd be awesome if the draft_id was available also. I'm making use of it for my Confluence Presentations app.""",Suggestion,Ecosystem
139554,"""Right now, when people PDF export pages, they get nothing from dynamic macros because the content is loaded asynchronously :(   Here's a suggestion:  * Add flag {{pdfExport}} in the descriptor * Use the same URL provided and invoke it the same way it's done with {{staticContentMacros}} but pass a flag or header   As a add-on developer, I'd set the {{pdfExport}} flag to true, and handle the request accordingly (i.e. plain HTML)   """,Suggestion,Ecosystem
139639,"""I'm currently creating a Confluence Connect add-on which makes use of dynamic content macros. Some of them are intended to be inline macros (""""outputType"""": """"inline""""). Unfortunately, their content is not vertically aligned with the surrounding text because the generated iframe does not have """"align=middle"""" set (or the HTML5 equivalent with CSS).  As an example, see the attached screenshot where """"Hello world"""" is the output of my dynamic content macro (note that I've set the size optimally with AP.resize, otherwise scrollbars in the iframe would appear) which is not aligned with the surrounding text.  A possibility to change the rendering of iframes with Connect (beside setting their size with AP.resize) would greatly help here.""",Suggestion,Ecosystem
139718,"""When making a REST API call to Confluence to get a specific page and I mention """"body.view"""" in the expand parameter, I should have the rendered format of the page. Instead, I get {color:#d04437}""""MyAddOn is not responding. Wait or cancel ?""""{color} where there should be rendered macros. When I tried the same thing with default <USER>macros, there was no problem, macros are perfectly rendered. The problem is with Atlassian Connect add-ons.   I don't know if this has anything to do with the mysterious{color:#d04437} """": = | RAW | = :"""" {color}parameter, because it is still there (see enclosed picture).  I need to render content on a Web Item to make my add-on work. I can't think of any other way to do this. I even tried the JSON-RPC API.   Thank you""",Suggestion,Ecosystem
139742,"""As a developer I would like to have a REST call that allows me to find out whether or not a specific user has administrative privileges. The main use case for this is that we have functions in our addon that require global admin privileges and we have some that only require space level admin privileges. We need a way to check if the user making the call is authorized to do so.  Suggestions for temporary workarounds are greatly appreciated, if there are specific operations that we could try doing and looking at the response.""",Suggestion,Ecosystem
139761,"""I'm wanting connect to publish a <USER>specific event when a particular general pages module is rendered, I'm planning on the IFrameRenderStrategy publishing an event, say IFrameRenderedEvent, a listener in the <USER>connect plugin then acts as a bridge, determining if the module rendered by the iframe should have the <USER>specific event published for it.  At a high level this feature gets content rendered by connect into <USER>s recently visited dashboard stream""",Suggestion,Ecosystem
139836,"""Content Properties are implemented as a RequiredKeyBean, when they don't really have a key in the same way as other Connect modules. They use individual propertyKeys instead. The normal module key shows up in the field documentation, but not in the example, and as far as I know i've not used the key field before. Should we change the implementation to be a NamedBean instead?   """,Suggestion,Ecosystem
139885,"""As a user I'd like the colour scheme to be reset to the default for <USER>when I disable the theme for a space or uninstall the theme add-on completely. """,Suggestion,Ecosystem
139984,"""As a customer I would like the color scheme selection to revert to the default color scheme if I uninstall the currently activated global theme add-on.  Steps to reproduce:  1. Install a theme add-on 2. Select the theme globally 3. Uninstall the add-on  Expected results: All spaces are back to the look they had previously  Actual results: Theme's default color scheme is still applied and color scheme settings page looks like this   !Screen Shot 2016-11-14 at 16.51.59.png|thumbnail! """,Bug,Ecosystem
140002,"""As a developer I'd like all browsers to get the correct result when querying the scrollPosition API.  We are using the scrollPosition API to set the top of a dialog in our iframe to make it visible for the user when they click a button. In Chrome it works perfectly, but when using Firefox we noticed that the dialog ended up way lower than we expected it to.   This is the snippet of code we use to position the dialog:   {code:javascript}       // Make sure that the popup is visible.       AP.require('scrollPosition', function(scrollPosition) {         scrollPosition.getPosition(function(pos) {           // [...]            console.log('scrollPosition pos.scrollY;', pos.scrollY);            // Set css top to the result of pos.scrollY...           });       }); {code}  And this is the result in the console of Chrome and Firefox respectively:   !Screen Shot 2016-10-05 at 14.01.52.png|thumbnail! """,Bug,Ecosystem
140102,"""Needs more investigation   {quote} @mate the problem was that Balsamiq's P2 and Connect plugins expose the same macro to customers, so when they're both installed, there were failures of some sort (I don't know exactly what - we'd need to reach out to them for further information). Perhaps one thing we could do is ensure in Cloud that all pluggable points first check for Connect-provided functionality before checking for P2 equivalents. I'm thinking that perhaps Confluence has an inconsistent ordering where sometimes the Connect plugin has priority and other times P2 (but that's just a personal hunch). In UPM for example, AC is always checked for plugin state prior to checking the P2 plugins framework.{quote}""",Bug,Ecosystem
140163,"""*Scenario*: I want to use a content property in a custom content (question) to indicate if a question has any answer (child content of question) and search the answered questions thru that property. Initially the question content is created with default value _false_ for the property _atl_cq_has_answers/any_answer_.  I created a custom content via the Confluence REST API with one content property _atl_cq_has_answers_ which is configured in the addon descriptor to be indexed by Lucene:  {code}     """"confluenceContentProperties"""": [{         """"key"""": """"<USER>questions-index"""",         """"keyConfigurations"""": [           {             """"propertyKey"""": """"atl_cq_has_answers"""",             """"extractions"""": [               {                 """"objectName"""": """"any_answer"""",                 """"type"""": """"string""""               }             ]           }         ],         """"name"""": {           """"value"""": """"Unanswered question index""""         }       }], {code}  I want to search by content property thru CQL : {code} space=MOON AND type=""""ac:<USER>questions-io:question"""" AND content.property[atl_cq_has_answers].any_answer=false {code}  That search returns nothing after the content is created. If I update the property via REST API (just increasing the version) then the CQL returns the expected result.  Problem is likely to be in _ContentPropertiesExtractor_ class: {code}     private Iterable<IndexableField> extractIndexableFields(final ContentEntityObject contentEntityObject,                                                             final Multimap<String, ContentPropertySchemaField> indexSchema) {         ImmutableList.Builder<IndexableField> resultBuilder = ImmutableList.builder();          for (String key : indexSchema.keySet()) {             Collection<ContentPropertySchemaField> contentPropertySchemaFields = filter(indexSchema.get(key), notNull());             if (!contentPropertySchemaFields.isEmpty()) {                 Option<JsonContentProperty> jsonContentProperties = contentPropertyService                         .find().withContentId(contentEntityObject.getContentId()).withKey(key).fetchOne();                  if (jsonContentProperties.isDefined()) {                     resultBuilder.addAll(                             contentPropertyExtractionManager.extract(                                     jsonContentProperties.get().getValue(), contentPropertySchemaFields));                 }             }         }         return resultBuilder.build();     } {code}  _ContentPropertyService_ is used to find the properties but perhaps it cannot be found because the transaction is not committed yet.  It should be possible to get the content property off the _ContentEntityObject_.     """,Bug,Ecosystem
140185,"""page.id and other page params are deprecated. However, for a dynamicContentMacro, content.id and other content params do not work. These are supposed to replace page.id and so on.  My plugin macro has a url at this address:  {noformat}/macro-page?space_key={space.key}&page_id={page.id}&content_version={content.version}&content_id={content.id}{noformat}  space.key and page.id work fine, but content.id and content.version give blank results.  I would also really appreciate if it was possible to get the page's title from params, e.g. content.title.""",Bug,Ecosystem
140195,"""As a developer I would like the default values to be filled when fetching the colour scheme using the REST API.  Steps to reproduce:  1. Set a value in the colour scheme settings to blank and save 2. The value should be auto-filled as a default 3. Fetch the colour scheme using the REST API  Expected result The JSON from the REST API contains the default value.  Actual result The JSON contains no value for the set parameter.""",Bug,Ecosystem
141183,"""h3. Summary    Newly installed apps with dynamic macros can't be found in the list to insert into a page.    Other vendors at Atlassian App Week (Amsterdam 2017) have observed this issue in customer instances. Not all instances appear to be affected.  h3. Steps to Reproduce   # Install a Connect app that has a dynamic macro (e.g. Navitabs Add-on, Tableau for Confluence, Numbered Headings, eazyBI for Jira in Confluence Cloud)   # Create a new page   # Try to insert a macro (e.g. """"Tableau for Confluence"""") the app includes and observe that it doesn't appear in the list    h3. Expected Results    The newly added app's macros should appear in the macro list  h3. Actual Results    You cannot find the macros  h3. Impact    *_Affects both old and new editor_*  h3. Notes    Using incognito mode seems to work.   I've done some more digging. There's a request behind the scenes when editing a page that goes to {{/wiki/plugins/macrobrowser/browse-macros.action?detailed=false&macroMetadataClientCacheKey=1}} which contains the list of all the macros. In both the normal and incognito browser modes it makes this request once and then continues to load it from the disk cache for future page edits (hence why newly installed add-on macros were not appearing). See the linked PR for more info.  h3. Workaround   * Clear the browser cache OR   * Work in incognito mode""",Bug,Ecosystem
141644,"""As a <USER>I would like to have an ability to deny a permission from a user or a group.    I have a space that is confidential and should be viewed by all Confluence users expect for one specific user.    The user should be able to view/edit in all other spaces as usual like any other Confluence user, except that confidential space that is allowed for everybody except him.    Currently there is no easy way to do it.""",Suggestion,User - Global / Space Permissions
158814,"""Hello Team,    The option on to enable '*CAPTCHA on Login*' in the Confluence Administration has a sub-option for '*_maximum authentication attempts allowed_*' post which the CAPTCHA will be presented. When the option '*CAPTCHA on Login*' is unchecked the option for maximum attempts allowed is inconsequential and is rather misleading and confusing.    I would request for the option to set '*maximum authentication attempts allowed*' to be grayed-out when the CAPTCHA on Login is disabled.    Kind regards,  Venkat""",Suggestion,Server
159183,"""h3. Summary  As a +<USER>   I want +Confluence to perform permissions checks against an index, instead of directly on the database+  So that +I can have a faster performance on this operation, as with large instances, with thousands of pages, spaces and groups, this can take long times.+    h3. Details  Customer with an environment with Directory containing more than 300K users and 100K groups, where some users can be members of more than 5K groups, permissions checks can take a proportional time to be performed.  This request is to consider the option of having permissions indexed with Lucene or other indexing engine and consult against it, to increase performance        """,Suggestion,User - Global / Space Permissions
160067,"""Hi,    we are trying out if our library is still working in Confluence 7.0, but it seems it isn't working any longer. It still worked fine in Confluence 6.x    We are putting our library in the libs-folder in Confluence, and inside, we register our custom SMTP transport, which so far was always picked up. But now, it doesn't get called any longer.    Can you help us out why this might happen? It looks like our library isn't loaded any longer, but we can't find any hints in the logs why this might be the case. Do you know where this might be protocoled?    If you need further information, I'm happy to provide it of course.    Thanks  <USER>",Suggestion,Server
160153,"""Jira currently notifies via e-mail when a subscription is ran.         I would like an App Notification via the official Jira app as an option too please.""",Suggestion,Server
160642,"""As a Confluence administrator, I would like that the left side navigation to admin functions, at [Base URL]/admin, is shorter and divided into sub sections.    Every time I do global administrative tasks in Confluence, I need to spend time to find where the desired function is located in the long list. The more apps there are installed, the longer the list will be and thus the time to find the function will be longer.    Something similar to what Jira provides would be a great improvement.""",Suggestion,Server
161028,"""As a documentation maintainer, I want to know what searches are failing to find good results in the confluence Search tool.    I would like to see some information on:   # Pages that have not been accessed for a very long time.   # Searches that turned up no results.   # Searches that resulted in a high bounce rate. Eg: User clicks on a lot of pages for a search, but can't seem to find a good result.   # Search results that result in a high bounce rate. Eg: Users often click on this page, but it's not what they were searching for (regardless of whether they were different searches).    This will help me find pages that have been abandoned (1), or search results that are under documented (2, 3), or that show up in misleading search results (4).""",Suggestion,Documentation
161623,"""h3. Summary    As a Confluence <USER>I would like to be able to manage users and groups directly from REST API endpoints. Automation and scripting of user management can be made easier with REST API.  h3. Problem statement    User and groups management is currently not possible via REST API only. Bulk operations, automation or scripting of management actions is not currently possible when using REST API. User management was previously possible with SOAP, which has been deprecated.  h3. Suggested Resolution    Provide methods akin to what as present with SOAP that would allow Admins to manage users and groups via REST API.                   _Previous Ticket Description_   As Confluence is popular tool used by many users, it should provide the API to add users to groups, similar to what Jira Cloud has.""",Suggestion,Core
161835,"""When Confluence is starting up, it scans spring annotations for plugin classes. As part of this scanning process, it tries to ensure that these spring annotation classes are contained only in single jar. Responsible class for this action is AtlassianScannerBeanDefinitionParser.    If for some reason plugin can see spring annotations from another plugin that bundled them, plugin initialisation fails with error  {code:java}  $ find . -name """"atlassian-confluence.log"""" | xargs grep -h """"Cannot execute atlassian-spring-scanner-runtime"""" | sort -u  Caused by: java.lang.IllegalStateException: Cannot execute atlassian-spring-scanner-runtime: plugin has an extra copy of atlassian-spring-scanner-annotation classes, perhaps embedded inside the target plugin 'com.atlassian.confluence.plugins.confluence-baseurl-plugin'; embedding scanner-annotations is not supported since scanner version 2.0. Use 'mvn dependency:tree' and ensure the atlassian-spring-scanner-annotation dependency in your plugin has <scope>provided</scope>, not 'runtime' or 'compile', and you have NO dependency on atlassian-spring-scanner-runtime.  Caused by: java.lang.IllegalStateException: Cannot execute atlassian-spring-scanner-runtime: plugin has an extra copy of atlassian-spring-scanner-annotation classes, perhaps embedded inside the target plugin 'com.atlassian.confluence.plugins.confluence-scheduler-spi'; embedding scanner-annotations is not supported since scanner version 2.0. Use 'mvn dependency:tree' and ensure the atlassian-spring-scanner-annotation dependency in your plugin has <scope>provided</scope>, not 'runtime' or 'compile', and you have NO dependency on atlassian-spring-scanner-runtime.  Caused by: java.lang.IllegalStateException: Cannot execute atlassian-spring-scanner-runtime: plugin has an extra copy of atlassian-spring-scanner-annotation classes, perhaps embedded inside the target plugin 'com.atlassian.confluence.plugins.synchrony-interop'; embedding scanner-annotations is not supported since scanner version 2.0. Use 'mvn dependency:tree' and ensure the atlassian-spring-scanner-annotation dependency in your plugin has <scope>provided</scope>, not 'runtime' or 'compile', and you have NO dependency on atlassian-spring-scanner-runtime.  {code}  The problem here is that plugins from error message have nothing to do with bundling spring scanner. Because of OSGI non-deterministic wiring, these bundles see spring annotation from 'wrong' bundle.    What is causing these errors is the fact that some of the plugins use old 1.x version of atlassian-spring-scanner-annotation dependency. Here is the list of bundled plugins that do it:   * atlassian-authentication-plugin   * ImageEffectsPlugin   * atlassian-webhooks-plugin   * chart-plugin   * confluence-collaborative-editor-plugin   * troubleshooting:plugin-confluence   * atlassian-playbook-blueprints   * synchrony-interop    All of the plugins above (and maybe some more) should be upgraded to use 2.x version of spring scanner annotations. They should also use <provided> scope instead of <compile> or <runtime>. After new versions of these plugins are released, they should be used in Confluence.""",Bug,Server
162076,"""h3. Problem  In some cases when UPM is trying to update a plugin, a deadlock occurs in the database.    The issue can be reliably reproduced with plugins that are published in the Atlassian Marketplace:    - environment: Confluence 6.7.1, 6.7.2, or 6.8.1, PostgreSQL 9.3.5  - install Scroll PDF Exporter 4.0.18  - install Scroll Versions 3.8.5  - update Scroll Versions to 3.8.7 (the latest version at the time of writing)    Relevant logs:    {code}  [p=39237, tx=5396419, vtx=7/1353] LOG:  execute S_4: update BANDANA set BANDANACONTEXT=$1, BANDANAKEY=$2, BANDANAVALUE=$3 where BANDANAID=$4  [p=39237, tx=5396419, vtx=7/1353] DETAIL:  parameters: $1 = '_GLOBAL', $2 = 'plugin.manager.state.Map', $3 = '<map>     <entry>       <string>com.atlassian.confluence.plugins.confluence-onboarding:onboarding</string>       <boolean>false</boolean>     </entry>     <entry>       <string>com.k15t.scroll.scroll-exporter-extensions:scroll.only</string>       <boolean>false</boolean>     </entry>   </map>', $4 = '8'  [p=39237, tx=5396419, vtx=7/1353] ERROR:  deadlock detected  [p=39237, tx=5396419, vtx=7/1353] DETAIL:  Process 39237 waits for ShareLock on transaction 5396429; blocked by process 39238.   Process 39238 waits for ShareLock on transaction 5396419; blocked by process 39237.   Process 39237: update BANDANA set BANDANACONTEXT=$1, BANDANAKEY=$2, BANDANAVALUE=$3 where BANDANAID=$4   Process 39238: update BANDANA set BANDANACONTEXT=$1, BANDANAKEY=$2, BANDANAVALUE=$3 where BANDANAID=$4  {code}    where:  - {{39237}} pertains to vendor transaction's update (select, update from PLUGINDATA and BANADA)  - {{39238}} pertains to {{atlassian.confluence.plugin.counter}}    It is not apparent that there is anything wrong with those plugins. It rather seems that something goes wrong when Confluence tries to dispatch the plugin install event. Further investigation shows that the Gadget Plugin (com.atlassian.confluence.plugins.gadgets), which is a system plugin, is somehow involved in the deadlock. Disabling the Gadget Plugin for the duration of the update prevents the problem.    When the deadlock occurs, the log file is flooded with exceptions. Attached is a sample. Also attached, a PostgreSQL log file showing the events leading up to the deadlock (filtered for the two transactions involved).    ----    I'm assuming that the Gadget Plugin causes the issue, because it is implicated both in the exception stacktraces and the PostgreSQL logs.    However, the problem also seems contingent on the specific versions of those user-installed plugins. I don't excactly know why. Interestingly, the problem does not occur :  - if version 4.1.0 of Scroll PDF Exporter is installed.   - If the Scroll Exporter Extensions plugin, which comes bundled with the Scroll PDF Exporter and contains a number of macros, is disabled, the problem also does not occur. The new version of Scroll Versions bundles a new version of the Scroll Runtime for Confluence plugin, which both Scroll Versions and Scroll PDF Exporter depend on.     The Scroll Exporter Extensions plugin that is bundled in Scroll PDF Exporter 4.1.0 also depends on the Scroll Runtime for Confluence (it didn't before), which means that if 4.1.0 is installed, the Scroll Exporter Extensions plugin is temporarily disabled during the update. Does the Gadget Plugin somehow mess with the macro modules of the extension plugin during the update?    Another theory we had was that the optional require-bundle dependency of Scroll PDF Exporter on Scroll Exporter Extensions could trigger the issue, but I was unable to corroborate that theory. Replacing it with a mandatory import-package dependency did not change anything.    Another observation: the actual update succeeds in that the new versions of the plugins are installed afterwards, even though UPM says that there was an error. But I'm not sure if all events, migration/upgrade tasks can complete successfully in such a case.""",Bug,Server
162145,"""Dear Team,    I was trying to creat my Team space using Confluence, but when I am trying to log in using my DB Email address and password, I am receiving an error as attached with this report. Is there any way I can create the space in Confluence without logging in  or if I Need to log in how can I get rid the issue I have having now ( System Error)?    Thank you so much for your help and prompt support on this.    Kind Regards, Sharmin Sultana               """,Suggestion,Server
162670,"""As a administrator, I want to be able to deploy Confluence in my environment in a fully automated way.   h3.*Problem Definition*  Currently, Confluence only supports the automated install of binaries. When setting up the database connection and other setup options, the administrator is forced to interact with the UI.  h3.*Suggested Solution*  Enable Confluence to have a scripted installation which grabs the necessary information for the setup from a """"setup.txt"""" file and avoids interaction with the UI while setting up the instance.  h3.*Why This Is Important*  In environments where the redeployment or deployment of new services is constant, administrators lose a considerable amount of time when deploying instances.    h3.*Workaround*  None""",Suggestion,"Server,Server"
162706,"""As an administrator, I want to be able to determine app login by granting this as a global permission for certain groups and/or users.""",Suggestion,Mobile
163338,"""As a Confluence editor, I would like to be able to switch to the the JIRA <USER>""""Knowledge Base"""" view because my customers like well-formatted documentation.         A JSD customer is able to see Confluence pages as a Knowledge Base pages.  However, these KB pages are narrower than the standard view.  I would like to be able to switch between the views from the page.  This will make editing the page for customers easier.""",Suggestion,Integration
163352,"""I am having a hard time tracking what plugins are installed for what purpose, who requested it to be installed, <USER>  It would be fantastic if there was a comment field for every plugin on the plugin management page for user-installed plugins.      I would like this feature across all server applications (confluence, jira, bitbucket, <USER>    Thanks  <USER>",Suggestion,Universal Plugin Manager / Manage apps
163762,"""Hi,    Editing features. It would be great if you guys extended Synchrony to be a general purpose RTC service that could be used by add-ons that need real-time collaboration.    For example, our Lucidchart OnPrem for Confluence Server add-on (and similar productivity tools) could use the service as a real-time collaboration back-end to allow simultaneous editing of diagrams. Right now we implement a quasi-realtime collaboration through polling attachment versions.    Please let me know if this is something you would be willing to consider in the future. I'm happy to provide additional details.         Thanks""",Suggestion,Core
170746,"""Problem description:  JIRA links in old Confluence instances can be missing their serverID-parameter.    Steps how to reproduce the problem:  1. Running any version of Confluence between >= 5.1 and < 5.4 (tested with JIRA macro plugin up to 5.3.4).  2. Create an application link to a JIRA server (was tested running JIRA server 6.1.9 but I believe this to be independent from the actual JIRA server).  3. Copy a JIRA link to an issue and paste it into a new document in Confluence.  4. Save the document.  5. Check out the documents storage format.  6. Note that looking at the storage format, the severID-parameter is missing.  7. Upgrade Confluence to >= 5.4 (tested when upgrading to 5.4.3, but I assume the issue persists even when upgrading to the latest Confluence version including 5.8.13)  8. Check out the storage format of the test page.    Actual result:  The storage format does not specify the serverID for that issue to the JIRA server.    Expected result:  The storage format shows the proper serverID.    The problem is here as follows (at least all according to my assumptions/observations):  In Confluence 5.1-5.3.4 there was a bug resulting in the serverID not being added when copy-pasting links directly from Confluence.  There is actually an upgrade task (""""jiraIssueMacroServerParamsUpgradeTask"""") which correctly adds the missing serverID. However, this upgrade task is only run when upgrading from Confluence <= 5.0.x.  Due to the bug in Confluence 5.1-5.3.5 upgraded Confluence instances can however still contain JIRA issues which are missing the serverID.    Therefore I'd suggest to perform the jiraIssueMacroServerParamsUpgradeTask as part of one of Confluence upgrade tasks for one a following version (Confluence 5.9 or 5.10 for instance). That way any older remains of this old bug are fixed correctly and won't cause problems in the future.    The issue is related to CONF-25329 and the support request I had a while ago (CSP-146293).    h1. Workaround  # upgrade your instance to Confluence >= 5.4  # go to admin/do-force-upgrade.action  # manually run """"jiraIssueMacroServerParamsUpgradeTask""""""",Bug,Integration
170883,"""I'm guessing this is related to Confluence 5.9.1-pluginsfour-013 upgrading to the rest3.     Installing an add-on from the Atlassian Marketplace works by searching for it and installing it.     However if I manually upload it (or submit it as a url) the p2 add-on doesn't get installed. I get an error saying """"An unexpected error occurred. Refer to the logs for more information."""" Looking in the logs doesn't show me anything. However looking in the javascript console, I see that a call to /rest/plugins/1.0/?token=[some number] fails with a 403.     Looking at what's being submitted - it's a multipart form data with the content so the data and the response is """"XSRF check failed"""".    Not sure where to submit this. So I'm submitting it both to the Confluence project and to the UPM project and hopefully one of them is the right place since if this goes out to the public add-on vendors won't be able to submit patch releases.""",Bug,Universal Plugin Manager / Manage apps
172637,"""h2. Problem:    Certain cache's have 100% Capacity Utilization so I am trying to allot more space to them but I am unable to.  I go to General Configuration &gt; Cache Management &gt; Advanced View &gt; Adjust Size.  When this prompt comes up requesting the amount of elements to give to the cache, I enter the amount I want but the submit does nothing.  It doesn't do this for all of the cache's but it does for some.  The caches in particular are listed below:    Application Links  Atlassian White List Service Switch Enabled  HipChat discovery cache  HipChat links cache  HostLicenseCache  Notification Server Configuration Cache      I was doing performance tuning and this is inhibiting me from having all my caches with less than 75% utilization.     h3. Workaround:    Located the cache-settings-overrides.properties file in \Atlassian\Application Data\Confluence\shared-home\config. I noticed that when you hover over a cache name in General Configuration > Cache Management , it shows its “real” name. So I just mimicked the way this file created the overrides I was able to do for other cache’s, and did the same thing for the caches that were not submitting the change.""",Bug,Server
172806,"""Yes, this is a duplicate of [CONF-32139], but it is because that issue was marked as Resolved because there was a desire not to delete pre-existing indexes.     I think the proper resolution here is to do some checking in the upgrade task to determine if the index already exists and if so, use that index versus creating a new index. I know that SailPoint did this when we had many hundreds of clients running Oracle who added indexes for performance reasons through the Statistics and Query Analyser tools that improved performance in our application.  This made the upgrade tasks run smoothly to finish without upsetting the clients who had already added indexes by halting their upgrade tasks.    I would like to have this issue revisited as our largest clients are running Oracle backends and will see this issue in their environments. If we checked for the existence of an index on the table and column that we are about to index and then did not do so if it already exists, then the upgrade experience would be smoother overall.""",Bug,Server
173052,"""When I use the JIRA filter macro, and I want to include the entire table of results, I see about 464 results on the page (perfect, all results are shown that match the query).    However, when I try to sort the table by a column by clicking on the column name, I get this error at the bottom of the table:    There was a problem rendering this section: timeout    I also cannot rearrange the columns in the edit mode of the macro.  I have to delete the columns and add the n in the order I want.  It would be nice to arrange them with drag/drop.    Using Confluence 5.5.3 on google chrome  """,Bug,Integration
174842,"""When setting up an application link from a standalone Confluence instance (v 5.5.2) and a standalone Jira instance (6.2.1) running on the same windows server, Basic authentication can be configured, but the forms for OAuth and Trusted Applications do not load. I attempt to configure OAuth by navigating to the application links admin panel and clicking """"Edit"""" on the application link that I previously set up. (I'll refer to this situation as the """"problem case"""")    Application links to jira.<USER>_can_ be configured (with anonymous access), and when I edit the application link, I see the OAuth and Trusted Application forms as expected. (I'll refer to this situation as the """"success case"""")      *What I Expect To See*  ==================  It seems to me that either there is a bug relating to the Application Link editing dialog box, or there is something wrong with the way that Confluence and/or Jira has been configured causing an error.     If the former, I expect to be able to configure OAuth for the application link between the confluence and Jira instance configured as above.    If the latter, I expect to see an error message that would give me some clue about what is going wrong so that as the system administrator, I can correct it.       *What I See:*  ==========  Organized by tabs and buttons selected from the Edit dialog box. See screen shots for details of a few of them :  |                                        | Trusted Application |    OAuth      | Basic        |  | Outgoing Authentication | &#x2717; loading wheel          |&#x2717; loading wheel |&#x2713; form renders  | Incoming Authentication  |&#x2713; form renders           |&#x2713; form renders | &#x2717; loading wheel    *More Details*  ===========  *What _Does_ Work*  ---------------------------  In the problem case, I have successfully used the Jira Issue plugin with Basic Authentication, and confirmed from the Jira log that Jira is being accessed from Confluence using the account configured with the Basic Authentication.     In the success case, anonymously available content from jira.<USER>was loaded.       *What do I mean by """"indefinitely""""*  --------------------------------------------------  I open the dialog box, start a timer, and at 30 minutes the loading wheel is still turning.       *Attachments*  --------------------  Unless otherwise specified in the file name, the screenshots are of the dialog box opened when editing the problem case application link.     *Network activity*  -------------------------  I inspected the network activity (using Chrome developer tools) to see if that gave me any hints - I haven't ruled out configuration problems. In both the problem case and the success case the only network activity once the dialog box has stopped loading is a call every 30 seconds to <base url>mywork/latest/status/notification/count which returns {""""count"""":0,""""timeout"""":30,""""maxTimeout"""":300}. I don't expect this to be very interesting, but what it get from that is that network-activity wise the two dialog boxes (the problem case dialog box and the success case dialog box) are acting in the same way     *If More Information Is Required*  =========================  Basic Authentication doesn't meet our requirements as we have confidential data in Jira that we need to restrict on a per confluence user basis. It is a high priority for us to get the application link configured. If there is any other testing or data collection that I can do on my end to help with fixing this issue, please ask.   """,Bug,Integration
176018,"""The notification list has a width 100% with padding that causes it to spill off the screen on mobile. The CSS will need to be fixed. The CSS might live in Workbox plugin, but I'm not sure.""",Bug,Mobile
176089,"""From Confluence 5.3 onwards it's not possible any more to add macro parameters when calling MacroDefinition.setParameters inside a MacroDefinitionUpdater, when the xhtml-macro definition declares any parameters.    MacroDefinitionUpdaters are used from:  {code}      String body = xhtmlContent.updateMacroDefinitions(ceo.getBodyAsString(), conversionContext, new MacroDefinitionUpdater(){  {code}    This problem appears as soon as the macro has a parameter defined like  {code}      <xhtml-macro name=""""macro-test"""" class=""""test.TestMacro"""" key=""""macroTest"""">          <parameters>              <parameter name=""""name"""" type=""""string"""" required=""""true""""/>  ....  {code}  Even if I add a placeholder in the macro definition for parameter I want updated, it is not added. Works in Confluence 4.3 - 5.2.    h3. Cause    Confluence 5.3 now tries to maintain some structured values for macro params inside a new ac:structured-macro element. The storage macro marshaller for this element treats the MacroDefinition.typedParameters property as canonical in this case, and it only consults the (untyped) parameters property if the typedParameters property is null or empty.    As of Confluence 5.8, the types of typedParameters values can be one of:  * for xhtml-macro declared parameter type """"username"""": UserResourceIdentifier  * for """"spacekey"""": SpaceResourceIdentifier  * for """"confluence-content"""": Link to a ResourceIdentifier   * for """"url"""": UrlResourceIdentifier  * for """"attachment"""": AttachmentResourceIdentifier  * for """"full_attachment"""" (new in 5.6.5 and 5.7): AttachmentResourceIdentifier, fully qualified with the containing page/blog post and spacekey  * otherwise: String    Right now, too many property types are still treated as strings for the typedParameters map to be a permanent replacement. Longer term, the parameters property will be deprecated, and callers will need to migrate to using the typedParameters API.    Note also that handling for the default parameter is now also different: it is stored keyed by the empty string (""""""""), rather than as a separate property. This applies to both the typedParameters and parameters properties.    h3. Workarounds    For all of the workarounds below, you will need to define the macro parameter correctly in your xhtml-macro definition. You cannot rely on your parameter being parsed and stored safely without this. You can <USER>this parameter as hidden if you do not want it to show up in the macro editor.    1. Update the MacroDefinition.typedParameters property to hold the value you want to add/update, using the types as described above. This is the safest and most forwards compatible option. OR  2. Build a fresh MacroDefinition with the MacroDefinitionBuilder containing just the properties you want stored. This option risks shearing off additional attributes the Confluence internals want to store. OR  3. Null the typedParameters, to force the persistence code to fall back to using the old parameters property instead. Note that this option may cause page, blog and attachment links to break again, due to the problems with wikimarkup special characters.    h3. Fix    We could add a way of tracking if the MacroDefinition.parameters property was modified while invoking any of the XhtmlContent macro methods, and if so use it to persist the macro params instead of the typed parameters. Note that this may cause page, blog and attachment links to break again, due to the problems with wikimarkup special characters""",Bug,Core
186316,"""Resolving CONF-19321 has a few documentation implications.    The general fix is that instead of adminstrators.action returning a list of e-mail addresses for the configured administrators of the site it will now give you a form which you can use to e-mail all the administrators of the site. This is preferable since it doesn't give the user any information about who these administrators are (e.g. the old mechanism of supplying their e-mail addresses could make it easier to guess the usernames of administrators for the site).    This mechanism is linked from the 500 error page in Confluence. It is also linked in 3.3 onwards from the template and theme configuration pages for a space.    *Conditions to watch out for:*  1) If there is no mail server configured for the site then instead of a contact form you will receive a message telling you that - """"No mail server has been configured so it is not possible to contact the site administrators via this method.""""  2) If none of the administrators for the site have e-mail addresses you also get a message instead of the form - """"None of the site administrators have e-mail addresses set so cannot be contacted via this method.""""  3) If you have removed the confluence-administrators group then you receive the same message as in point 2. (I know this isn't quite correct but you could argue it avoids giving out specific details the user set up of the instance. There wasn't enough time to add handling for this specific condition.)  4) If SPAM prevention/Captcha is configured then those configuration settings will also apply to this form    *Configuration Options*  1) As mentioned previously, the site wide SPAM prevention settings are used here.  2) There is a new General Configuration setting called """"Custom Contact Administrator Message:"""". This allows you to change the prompt that is presented before the form.  3) There is a new General Configuration setting called """"Contact Administrators Form"""". This allows you to turn the form off. This should be used in conjunction with the previous setting explaining your alternative contact mechanism.    I've attached a few screenshots to help illustrate things.    If you would prefer me to write (or provide a first draft of) the documentation please just get back to me with a location for where it should live and I'll do it immediately.  """,Suggestion,Documentation
188644,"""I wanted to test Confluence 3.0 so I downloaded confluence-3.0.0_01-std.zip, extracted it and ran it, as usual. I told Confluence to install the Standard Site and HSQL.   Afterwards, when I first saw the Demonstration Space it showed many errors of missing macros (plugins). The Dashboard shows many missing macros as well.  I can't even easily access the Administration Menu, because the Javascript libraries are missing. I was able to access the Administration Menu over the known URL for the Admin-Menu. Still, like this, I can't really test the new version, since it even misses the plugin repository and I'm not able to install missing plugins for macros.    This error seems  to have slipped through quality assurance - or am I doing something wrong?""",Bug,Server
190423,"""It would be awesome if I could export a single Confluence page as a PDF file through the Remote API (XML-RPC). Currently the only PDF export option is exportSpace() which is not suited to our purpose.    Ideally, I would like something like this:    String exportPage(String token, String pageId, String type)    where type could be TYPE_PDF. The method would return a URL where the exported PDF could be downloaded.""",Suggestion,Core
192636,"""I'm working on an integration project to manage groups for users in Confluence. I'm able to define groups, add groups, get a users groups, <USER>.. but the one thing that is missing is being able to get all users in a specific group. Here at Rambus, they have groups for each manager (they contain their subordinates). When a manager quits or leaves, the subordinates all get reassigned to the new manager and get removed from the old manager group via my integration. There is nothing in the raw data that feeds the integration that says """"a manager quit""""... I just get a different set of Groups for the given set of users. Example, some one moves out of Testing and into Support, they get a new manager and the integration will remove the user from the old manager and add the the new manager (note that nobody quit... its just a different group assignment). The problem is that I don't have an easy to know when a group is empty and I can delete it as ongoing maintenance (i.e a manager quits and nobody will ever be in that group again). They have dozens and dozens of groups so ongoing it will become an issue with all kinds of empty groups of terminated managers. If there was a       Vector getAllUsersForGroup(String token, String groupName)  method, that would solve the issue. I could use a getGroups call in conjunction with getAllUsersForGroup to perform the ongoing maintenance without any admin/UI interaction (what Rambus is looking for... let the integration project manage all that stuff without any administrator maintenance). Thanks for the help.""",Suggestion,Core
195851,"""In my perl script, I retrieve the child pages of a certain page, then for each child page object, I update the title of the child page then call confluence1.storePage to store the page, and the script hangs for a long time, then gives up.  This happens for almost every page in 2.1.5a.  This exact same script was used before on some earlier Confluence versions (I forgot which ones) and was working on 80% of the child pages, but get stuck on some child pages.  I didn't notice anything special about those child pages that storePage failed on and I ignored the problem.  But now in 2.1.5a, the update failed on every single page.    I did some initial testing and found that whether the page has attachment or not do not affect storePage.  I also removed all attributes of the child page object and leaving only title, space, version (either the previous version number or the previous version + 1), id, content, and it didn't help.  The page content itself does not have anything suspicious (some of the pages are actually completely empty).    Please let me know if storePage updating page title is working fine in 2.1.5a.  If it's a version-specific bug then I'll upgrade later.""",Bug,Core
196293,"""Got this problem from a customer in email. I've sent him the ticket so he can track the status:    Hi Ernest (and <USER>,    No problem on any delay: <USER>Silvers (in San Francisco) was very  helpful in arranging XML backups of the two spaces as well (thank you  again, <USER>).  It is good news that JIRA supports local online docs,  thanks for checking on that for me.    Unfortunately, I have a problem to report.  It would appear the space  restore from XML backup is broken in some way, with respect to  attachments.  Comments appear intact, but all attachments are  inaccessible.    We are using the standalone build#512 (v2.2), on an Apple XServ  running Mac OS X Server 10.4.6 (Tiger), communicating to a remote  PostgreSQL 7.4.7 (debian """"sarge"""") using the latest JDBC driver (501)  from jdbc.postgresql.org.  We are using the default configuration for  attachments, storing them on the fs.    I can fetch the """"system information"""" page for our configuration if  needed.  By the way, our next eval phase is planned to deploy the WAR/ EAR lash-up alongside a JIRA eval, using the Mac OS X Server LDAP for  authentication.    **After restoring either the CONF20 or DOC spaces, and indexing:  - (OK) all pages/hierarchies appear intact -- even the confluence  """"Team"""" name showed up;  - (OK) all attachments LIST under the spaces' attachment tabs  - (FAIL) ALL inline images display as broken icons within any page  that contains them;  - (FAIL) ALL attachments cannot be found;  - (FAIL) accessing any attachment produces server ERROR page (~  """"attachment not found where expected"""")  - (FAIL) /var/confluence/attachments/ contains no new items/directories    I have tried to import the CONF20 and DOC spaces numerous times now  with XML exports obtained by directly accessing the  confluence.<USER>website as well as those exports provided by  you and <USER>  I've utilized both restore space methods of uploading  backup via browser as well as a """"local restore"""" after transferring  the backups to the configured confluence """"restore"""" directory (under / var/confluence/ in my case).    During the various restore attempts I could see the backups were  unzipped into """"/var/confluence/temp/import{name_varies}/"""" where I can  find the """"attachments"""" subdir, containing the usual hash dirs.    At the end of the restore process I briefly saw that a new directory,  """"/var/confluence/temp/import{name_varies}/new.attachments"""" (or  similar) was created, containing the same hash dirs as the apparently- to-be-restored attachments dir.  Once the restore completed the the  entire """"/var/confluence/temp/import{name_varies}/"""" was removed, as I  would expect.    No changes are ever made to /var/confluence/attachments/.  However,  during normal use of confluence, making NEW attachments in other  spaces does work and does change the contents of /var/confluence/ attachments, both before and after the restore process.    Any suggestions you have would be greatly appreciated.      <USER>L.""",Bug,Documentation
198609,"""We expose java.util.Map in the remote API as arguments to methods such as search() and getRenderedContent(). While XML-RPC has a native hash type, SOAP doesn't, so SOAP toolkits don't know how to translate the Map into a SOAP object... with predictably bad results.    We need to replace all maps in the API with beans that have specific property getters and setters. This will necessarily be a breaking non-backwards-compatible change, so we'll have to <USER>a new version of the API (confluence-v2) as a separate plugin. To keep all the breaking API changes in the same place, we should also fix the overloaded methods (CONF-3877) problem at the same time.     (Original description)     Hi    In the remote API one can search using:    Vector search(String token, String query, Map parameters, int maxResults)    I am trying to invoke this from Visual Basic .NET and confess to be a bit new to all this and a bit stuck. I can see an object called Map in the object browser which is added along with the ConfluenceService. But I think this object cannot be instantiated - I suspect it is sort of abstract, as the reference.vb file indicates: Public MustInherit Class Map    So how do I populate this with the space keys I want to restrict the search to. I could subclass Map, but what methods should I add (I tried it - but it did not seem to work). I suspect in Java this would be trivial, but I am stuck in .NET.    Anyone got any ideas - code examples?    Thanks    Chorltan """,Bug,Core
199600,"""This issue started off in the comments of CONF-2308, but is unrelated, so I'm moving it to its own issue.   <USER>  We tried to perform upgrade from 1.2.3 to 1.3 using Oracle as database. Everything went fine until colour schemes upgrade, which produced the stacktraces included here. Any workarounds?   2004-12-01 09:25:52,314 INFO [atlassian.confluence.upgrade.ColourSchemesUpgradeTask] Beginning colour schemes upgrade 2004-12-01 09:25:52,321 ERROR [confluence.setup.bandana.ConfluenceBandanaPersister] Error loading stream for context : com.atlassian.confluence.setup.bandana .ConfluenceBandanaContext@0 java.lang.NullPointerException         at com.thoughtworks.xstream.XStream.fromXML(XStream.java:168)         at com.atlassian.confluence.setup.bandana.ConfluenceBandanaPersister.loadBandanaMap(ConfluenceBandanaPersister.java:66)         at com.atlassian.confluence.setup.bandana.ConfluenceBandanaPersister.store(ConfluenceBandanaPersister.java:47)         at com.atlassian.bandana.impl.CachingBandanaPersister.store(CachingBandanaPersister.java:57)         at com.atlassian.bandana.DefaultBandanaManager.setValue(DefaultBandanaManager.java:50)         at com.atlassian.confluence.themes.DefaultColourSchemeManager.saveGlobalColourScheme(DefaultColourSchemeManager.java:133)         at com.atlassian.confluence.upgrade.ColourSchemesUpgradeTask.doUpgrade(ColourSchemesUpgradeTask.java:52)         at bucket.upgrade.AbstractUpgradeManager.doUpgrade(AbstractUpgradeManager.java:88)         at bucket.upgrade.AbstractUpgradeManager.upgrade(AbstractUpgradeManager.java:53)         at bucket.upgrade.UpgradeLauncherServletContextListener.contextInitialized(UpgradeLauncherServletContextListener.java:27)         at com.atlassian.confluence.upgrade.ConfluenceUpgradeServletContextListener.contextInitialized(ConfluenceUpgradeServletContextListener.java:13)         ...  2004-12-01 09:25:52,326 ERROR [bucket.upgrade.UpgradeManager] Upgrade failed: null java.lang.NullPointerException         at com.atlassian.confluence.setup.bandana.ConfluenceBandanaPersister.store(ConfluenceBandanaPersister.java:54)         at com.atlassian.bandana.impl.CachingBandanaPersister.store(CachingBandanaPersister.java:57)         at com.atlassian.bandana.DefaultBandanaManager.setValue(DefaultBandanaManager.java:50)         at com.atlassian.confluence.themes.DefaultColourSchemeManager.saveGlobalColourScheme(DefaultColourSchemeManager.java:133)         at com.atlassian.confluence.upgrade.ColourSchemesUpgradeTask.doUpgrade(ColourSchemesUpgradeTask.java:52)         at bucket.upgrade.AbstractUpgradeManager.doUpgrade(AbstractUpgradeManager.java:88)         at bucket.upgrade.AbstractUpgradeManager.upgrade(AbstractUpgradeManager.java:53)         at bucket.upgrade.UpgradeLauncherServletContextListener.contextInitialized(UpgradeLauncherServletContextListener.java:27)         at com.atlassian.confluence.upgrade.ConfluenceUpgradeServletContextListener.contextInitialized(ConfluenceUpgradeServletContextListener.java:13)         ...  Dec 1, 2004 9:25:52 AM bucket.upgrade.UpgradeLauncherServletContextListener contextInitialized SEVERE: Upgrade failed, application will not start: null java.lang.NullPointerException         at com.atlassian.confluence.setup.bandana.ConfluenceBandanaPersister.store(ConfluenceBandanaPersister.java:54)         at com.atlassian.bandana.impl.CachingBandanaPersister.store(CachingBandanaPersister.java:57)         at com.atlassian.bandana.DefaultBandanaManager.setValue(DefaultBandanaManager.java:50)         at com.atlassian.confluence.themes.DefaultColourSchemeManager.saveGlobalColourScheme(DefaultColourSchemeManager.java:133)         at com.atlassian.confluence.upgrade.ColourSchemesUpgradeTask.doUpgrade(ColourSchemesUpgradeTask.java:52)         at bucket.upgrade.AbstractUpgradeManager.doUpgrade(AbstractUpgradeManager.java:88)         at bucket.upgrade.AbstractUpgradeManager.upgrade(AbstractUpgradeManager.java:53)         at bucket.upgrade.UpgradeLauncherServletContextListener.contextInitialized(UpgradeLauncherServletContextListener.java:27)         at com.atlassian.confluence.upgrade.ConfluenceUpgradeServletContextListener.contextInitialized(ConfluenceUpgradeServletContextListener.java:13)         ... rethrown as bucket.upgrade.UpgradeException         at bucket.upgrade.AbstractUpgradeManager.doUpgrade(AbstractUpgradeManager.java:107)         at bucket.upgrade.AbstractUpgradeManager.upgrade(AbstractUpgradeManager.java:53)         at bucket.upgrade.UpgradeLauncherServletContextListener.contextInitialized(UpgradeLauncherServletContextListener.java:27)         at com.atlassian.confluence.upgrade.ConfluenceUpgradeServletContextListener.contextInitialized(ConfluenceUpgradeServletContextListener.java:13)         ... Caused by: java.lang.NullPointerException         at com.atlassian.confluence.setup.bandana.ConfluenceBandanaPersister.store(ConfluenceBandanaPersister.java:54)         at com.atlassian.bandana.impl.CachingBandanaPersister.store(CachingBandanaPersister.java:57)         at com.atlassian.bandana.DefaultBandanaManager.setValue(DefaultBandanaManager.java:50)         at com.atlassian.confluence.themes.DefaultColourSchemeManager.saveGlobalColourScheme(DefaultColourSchemeManager.java:133)         at com.atlassian.confluence.upgrade.ColourSchemesUpgradeTask.doUpgrade(ColourSchemesUpgradeTask.java:52)         at bucket.upgrade.AbstractUpgradeManager.doUpgrade(AbstractUpgradeManager.java:88)   I didn't get 1.3 working. I went back to 1.2 and restored the backups that I created just before trying to upgrade.  As for version of xpp, it seems to be correct:    $ jar tf atlassian-confluence-1.3.war | grep xpp   WEB-INF/lib/xpp3-1.1.3.4d_b4_min.jar  I have went through catalina.out and clipped a 120k piece that was generated using the attempted upgrade. I don't think there was anything interesting except the above stacktrace, but I'll attach it here anyway. """,Bug,Server
202552,"""Currently, we do not provide a specific version in the documentation indicating what tag produced that documentation.    For example, if I'm looking at the stable docs, it does not tell me that it is for v0.8.5.  As a reader, that would be potentially useful information to embed in the documentation either on the index page or in the header.    We could potentially integration version in the docs conf.py with our version command in bin/. """,Improvement,Documentation
202816,"""As a user, I want to be able to add metadata to an existing DID in my wallet.    As a user, I want to be able to edit metadata for an existing DID in my wallet.""",New Feature,cli
202896,"""*Story*  As a administrator of an application designed for an Indy Network that enforces a Transaction Author Agreement, I want to use the Indy CLI to view the hash of the TAA on the ledger so that I can put it in the configuration of my application to demonstrate TAA acceptance by my organization.""",New Feature,cli
203064,"""*Story*  As a network administrator using indy-cli to automate administration, I will call Indy CLI in """"batch mode"""" and perform potentially dangerous actions that will prompt a warning requiring user acknowledgement. I want to be able to use a flag that tells indy-cli to always accept a """"yes"""" when acknowledgement is required in this mode.    *Acceptance Criteria*  * A flag exists that similar to """"-y""""  * When the flag is provided at the time indy-cli is called, any operation which would normally require user acknowledgement of """"yes or no"""" proceeds as if the user entered """"yes"""".  * A message is printed to the logs and console that is similar to """"Proceeding with response of 'yes' because of '-y' flag.""""    *Note*  * As an example, see the Apt tool for Debian.""",New Feature,cli
203072,"""*Story*  As a developer using the Indy CLI, I want tab to cycle through logic values to complete my current action so that it is easier to use the CLI.    *Acceptance Criteria*  Tab completion exists for:  * Wallet names  * DIDs  * Previously used payment addresses    Issues are created for further work.""",New Feature,cli
203212,"""*Story*  As a developer on an Indy network, I want the CLI to show me the author agreement before submitting my transactions to the ledger so that I can proactively review and accept the terms.    *Acceptance Criteria*  * Before submitting a write to the public ledger, the CLI will display the current transaction agreement for that transaction type to the user.  * The user will be asked to agree before the write can be signed and submitted.  * The time and date when the user accepted is recorded as part of the transaction.  * The mechanism by which the user agreed is user configurable with a default mapping to """"Click Agreement"""".    *Notes*  * The acceptance mechanism might be an enum / id value instead of a string.  * It is the CLI user's responsibility to ensure that the indicated acceptance mechanism matches the string being used on the ledger that will receive the transaction.""",Task,cli
203336,"""*Story*    As a trustee of an Indy network, I need to be able to use my hexadecimal seed created with the legacy python CLI on the new CLI to import my existing keys into a new wallet. My hexadecimal seed contains unprintable characters, and so cannot be represented as ASCII.    *Acceptance Criteria*   * Use the legacy CLI to create a seed containing non-printable characters encoded as 64 bytes of hexadecimal.   * The new CLI should accept the seed through the command line (not a configuration file)    *Notes*  * Why don't they just upgrade their wallet? Some trustees did not create a wallet. They created and stored their seed without creating a wallet. So they need to provide a seed to perform actions such as changing IP addresses or adding new trustees or stewards.""",New Feature,cli
207444,"""As a user of Fabric Config Library, I would like to replace policies in bulk like *(*ApplicationGroup)SetACLs()*.         *Motivation:*   In the current Fabric Config Library replacing multiple policies in an existing configuration could be a bit tedious.   For example, the following methods could be considered to achieve this at present:   - Method 1: Repeat *SetPolicy()* and *RemovePolicy()* for the group to be replaced.   - Method 2: Use *Configuration()* to get the config struct for the group -> replace the policy fields in that config struct -> then use *SetXXX()* to reconfigure   ** NOTE: some groups does not seem to support this way (e.g., *ordererGroup.SetConfiguration()* is skips updating policies)    By providing functions to replace multiple policies in bulk, users can change multiple policies with a single function call, so users should not need a tedious method like the above two.         *Example for usage of the proposed function:*  {code:go}  a := c.Application()  a.SetPolices(newPolicies) // replace the all existing policies in the configuration with newPolicies in bulk  {code}       *Targets to add the new functions:*   - ApplicationGroup   - ApplicationOrg   - ChannelGroup   - ConsortiumOrg   - OrdererGroup   - OrdererOrg""",Task,fabric
207519,"""Is there any chance that the migration to go modules could be back-ported for the latest 1.x release (1.4.8 at the time of this writing)?    I want to write a chain code with the latest 1.x version, but really dislike the legacy package management of go and the need to manage $GOPATH and other env vars (personal preference).    If I understand it correctly all it would take is a single commit that adds a go.mod file on the 1.x branch and then a release of a minor version that gets tagged so go can pick it up as a version. Also happy to send a PR with this if people support the idea. Please advise.""",Story,fabric
207720,"""The first thing I'll note is, this is going into a feature branch, not directly into master, as getting it right is more important to me than getting it in. I am opening the PR's here so the work is visible and can be commented on, but it will not change the way the Operator works today, as I wont be merging anything to master until all of the work is done.    The second thing is, I am focusing on one part at a time (Docker to start). This means I am okay breaking things related to kuberenetes up front. Once I am done with the Docker portion, I will come back through each PR and refactor each PR to work with Kubernetes. So the NodePortIP missing would be corrected in a refactor of this PR at a later time    Today the operator calls out to several command line tools: Docker, Configtxgen, Configtxlator, YTT, Crtyptogen, and even performing some tasks like moving a file by calling the bash {{mv}} as opposed to using the Go {{os.Rename}}, and until recently when Surya added support for the KubeSDK we also called out to the {{kubectl}} command.    This is highly problematic in that it requires the user to first put tools on their path before ever using our tool. It also prevents us from ensuring version compatibility, i.e., I don't know if they have configtxgen 2.0 or 1.1.    Because of YTT we also write a custom config to disk, over and over again. And we read the config and append data to it and write it to disk, then read it again, and append data to it and write it to disk, all before ever generating the YAML files we want.    Instead these changes will bring about proper config injection and dependency management. Creating in memory config objects where necessary and replacing tools like configtxgen and configtxlator with the {{fabric-config}} to manage config operations in code, and replace the calls to Docker with the Docker SDK, and ultimately remove YTT by replacing cryptogen with calls to the fabric CA using the Go SDK (since we aren't actually testing the functionality of cryptogen the place we get crypto from is irrelevant).    Ultimately this will allow us to remove external dependencies, and make the Operator tool a truly standalone tool.    All of this is in support of an upcoming proposal to convert PTE from Node.js to the Go SDK, now that it supports the high-level programming model and is receiving first-class support now. The change tot he Go SDK will allow us to embed everything we need to perform System Tests in the Operator tooling. Getting the Operator into state where it is able to extended and maintained is important, and today it is very hard to wrap you head around what is happening as a lot of things are managed on disk, rather than in code.""",Task,fabric
207747,"""*1.Condition*   - Use HLF v2.0.0 ~   - Use CouchDB as statedb with cache   - Chaincode use struct as data (The fields are not ordered alphabetically)    *2. Symptom*   - With single peer : It can occurred after calling GetState between 2 invocations of PutState on same key.   Before 2nd PutState, the keys in the query result would be {color:#ff0000}alphabetically{color} ordered.   After 2nd PutState, they would be ordered as {color:#ff0000}fields in struct{color}.   But it's not a big problem.     - Multiple peers : It can occurred one peer calling GetState between 2 invocations of PutState on same key, another peer does not.   On this situation, GetState result from 2 peers would be different.   And if there's an invocation that includes GetState, it would fail with the error 'Inconsistent Response Payload from Different peers.'    !image-2020-06-03-10-44-53-269.png!    *3. Reason*   The cache of statecouchdb loads data when chaincode calls GetState, and update cache when process transaction - only the key is exist in cache.    When loading, the data is marshalled as alphabetically ordererd.( keyValToCouchDoc func )   When updating, the data is marshalled order of fields in struct.( In the writeset )    !image-2020-06-03-10-14-49-414.png!    *4. Solutions*   I make 2 solutions for this symptom.    *Solution 1* : Unmarshall and marshall data before update cache        .Good points - Keep current architecture        .Bad points - Not so efficient    *Solution 2* : Do not update cache - just remove when updated        .Good points - Very simple                                 No need of hold new value in committer        .Bad points - Cache miss can be occurred one more time    I've already test both solutions, and they solve the problem.    I'm ready to contribute. :)""",Bug,fabric
207790,"""This task handles the following scenario:   * Orderer is a member of the cluster - the config block given at join has the self ID of the orderer.   * The config block given at join is number 0 - no onboarding needed.     In this case the join block is first appended to the ledger as a genesis block, and the multichannel.ChainSupport is created in the normal fashion, reading the last-block from the ledger and constructing the consensus.Chain accordingly.  h1. Rationale     {{HandleChain}} starts a chain based on the last block in the ledger, which it gets from the ledger resources.    What we need in order to join a chain in cases when on-boarding is needed (see special case below) is to start a {{follower.Chain}}, with a join-block, when the ledger is empty. The follower chain does not need many of the resources that an etcdraft.Chain needs, as it only pulls blocks (for example, it never signs and never uses the BlockWriter) . Moreover, some things it cannot even create, like a BlockWriter, which need to be based on a tip of a ledger (which we do not have at this point). Rather then stud HandleChain with many """"ifs and buts"""", and extend its interface with the join-block, I think it will be much clearer and safer (not introducing bugs to HandleChain) to define a new method that does exactly the job of joining a channel.    The way I want to think about it, conceptually, HandleChain starts from a valid the ledger, so a failure means that something is fatally wrong (e.g. corruption of the ledger). In contrast, JoinChain is started with input from the user, i.e. the join-block, and errors in processing his request should be propagated back to the user.    A special case is when the join-block has number=0, and then it is appended to the ledger and {{HandleChain}} is called. If the orderer is a member ({{detectSelfID()}} no error), an {{etcdraft.Chain}} is created. Otherwise, a follower.Chain be created. However, this follower.Chain is past on-boarding and is just pulling blocks and observing the config.    When considering CFT, an orderer might fail in the middle of on-boarding a channel. When we recover, we'll find a join-block.Number >= ledger.Height, which indicates a gap between the ledger tip and the join-block. In this case we'll also call JoinChain rather than HandleChain.         Here is the flow of a join request with `{{join-block.Number >0}}`.   * The `JoinChain` is called synchronously from the client request go-routine. If it fails, the request fails. If it succeeds, a follower.Chain is created and started (it has a go-routine).   * The follower.Chain is given a function pointer to a {{createChain}} function just like in the {{inactive.Chain}} and the haltCallback in the {{etcdraft.Chain}}.   * The follower.Chain starts pulling blocks. It is in status {{on-boarding}} until it reaches {{join-block.Number}}, then it changes to {{active}}.   * If at that point the orderer is in the consenters set of the join-block (""""a member""""), the call to {{createChain}} in invoked, triggering the transition between follower.Chain -> etcdraft.Chain. This happens with a call to HandleChain.   * HandleChain has a switch to create a follower.Chain when there is no sys-chan, and an inactive.Chain when there is a sys-chan.   * If the orederer is not in the consenters set of the join-block, it will continue to pull blocks and watch config blocks.   * Note that once the ledger catches up with the join-block, the join-block is deleted.   * When a config block arrives in which the orderer is a member, {{createChain}} in invoked, again, triggering the transition between follower,Chain -> etcdraft.Chain. This happens with a call to HandleChain.    Conversely, here is the flow of an etcdraft.Chain evicted:   * Eviction is detected, halt is called (as usual)   * The haltCallback is initialized to create a chain. Calls HandleChain. HandleChain creates a follower.Chain without a join-block.    So, {{JoinChain}} is normally called by the client request go-routine, and always when there is a join-block. HandleChain is called by internal go-routines, and when the chain is started off the ledger.    The {{JoinChain}} call behaves differently from {{HandleChain}} only when {{join-block.Number >0}}, i.e. there is a need for on-boarding. Otherwise, if it is {{join-block.Number==0}}, it just appends the block to the ledger and delegates to {{HandleChain}}. That is what's addressed in this commit, all the rest is future tasks.     """,Sub-task,fabric
207791,"""The peer fails to join the channel in the following scenario:   # A channel was created by the Orderer Organization (running 3 orderers in RAFT mode).   # A peer organization was added (let's say with 3 peers). Chaincodes were installed, instantiated and invoked several times.   # Over channel lifetime, the orderer nodes in channel were modified to the extent that *no channel genesis block orderers* existed in the channel any longer.   # A requirement occurred to add a *new peer node* in the channel. On giving, channel genesis block, it kept trying to fetch next blocks (starting from 1) by reaching out to orderer defined in the block 0 of the channel.    As expected, the new peer could not fetch any block after block 0! Because all the orderers defined in the channel at the time of its creation were swapped over by different orderers over channel lifetime.    I tried looking online if this issue was officially addressed by the community. But could not find any proposed solution for this. Is this a bug and taken into consideration for development? Or is this addressed already?    ----------------------------------------------------------------------------------------------------------------------    UPDATE:   *THIS SOLUTION DOES NOT WORK AS WELL!*    I found the solution to this problem:    We must pass the Orderer endpoint of one of the orderers that already exists in the channel! The modified command would be:    {{peer channel join -b mychannel.block -o orderer4.example.com --tls --cafile <pathToOrdererTLSCACert>}}    Where, {{orderer4.example.com}} *currently* belongs to the application channel.     ---------------------------------------------------------------------------------------------------------------------    UPDATE 2:    What if instead of swapping the nodes, I need to rotate the TLS certificates of the different consenters in the application channel, such that in  channel lifetime, at some point every consenter has a TLS certificate different from the one present in the channel genesis block. This would again stop the newly added peer nodes to join the channel by blocking them to sync any blocks after the block 0 of the application channel!""",Bug,"fabric,fabric"
207837,"""Recently upgrade-dbs added support for remote CouchDB database.    rebuild-dbs should similarly support dropping CouchDB databases.         As an administrator, I want to drop my peer databases including CouchDB state databases, so that the next time peer restarts it will rebuild my databases.         Acceptance: rebuild-dbs should drop databases including CouchDB state databases. The next peer start should rebuild databases including CouchDB state databases using the channel blockchains.""",Story,fabric
208034,"""As a user of the Fabric channel configuration API, I would like to be able to retrieve the entire channel configuration without manually parsing the config transaction. Now that we have the supporting functions in place for retrieving all dependent fields, we should be able to support retrieval of the entire channel configuration.         *Acceptance*    I should be able to call a function for retrieving channel configuration that returns a config.Channel.""",Story,fabric
208054,"""As a user of the Fabric channel configuration API, I would like to be able to update an application groups's ACLS by adding or removing ACLs.         *Acceptance*    I should be able to call this function to modify a config transaction and then process further configuration updates on the modified config transaction. The examples test should be updated to demonstrate usage of this feature.     """,Story,fabric
208055,"""As a user of the Fabric channel configuration API, I would like to be able to retrieve existing application configuration without manually parsing the config transaction.         *Acceptance*    I should be able to call a function for retrieving application configuration that returns a config.Application.""",Story,fabric
208056,"""As a user of the Fabric channel configuration API, I would like to be able to add and remove application, orderer, and channel capabilities. Consider whether we should allow setting unsupported capabilities outside of the existing ones or if we should just have toggles (ie EnableV20ApplicationCapabilities() and Disable...)         *Acceptance*    I should be able to call these functions to modify a config transaction and then process further configuration updates on the modified config transaction. The examples test should be updated to demonstrate usage of this feature.     """,Story,fabric
208057,"""As a user of the Fabric channel configuration API, I would like to be able to retrieve existing orderer, application, and channel capabilities from a config transaction.         *Acceptance*    I should be able to call a function for retrieving orderer, application, and channel capabilities that returns a config.Capabilties.""",Story,fabric
208093,"""As a user of the Fabric channel configuration API, I would like to be able to retrieve existing policies for an organization without manually parsing the config transaction.         *Acceptance*    I should be able to call a function for retrieving policies for a specific org (regardless of application/orderer/consortium org). The example test should also be updated to demonstrate usage of this helper.""",Story,fabric
208101,"""As a fabric administrator, I want to use a fit-for-purpose library that can modify an existing channel configuration to remove an orderer endpoint from an orderer org's config group.    Acceptance:   Starting with an existing Config block/transaction, I can call a function that removes an orderer endpoint from the configuration. After performing that operation, I can compute a ConfigUpdate transaction that is ready for a signature workflow. An appropriate example should also be added to the example test that demonstrates this behavior.""",Story,fabric
208102,"""As a fabric administrator, I want to use a fit-for-purpose library that can modify an existing channel configuration to add an orderer endpoint to an orderer org's config group.    Acceptance:   Starting with an existing Config block/transaction, I can call a function that adds an orderer endpoint to the configuration. After performing that operation, I can compute a ConfigUpdate transaction that is ready for a signature workflow. An appropriate example should also be added to the example test that demonstrates this behavior.""",Story,fabric
208106,"""As a fabric administrator, I want to verify that a cert was issued by my organization's MSP before adding it to the MSP's revocation list.    Acceptance:   Provided with a config block/transaction, I can call a function to """"revoke"""" a certificate issued by an MSP. This should result in a configuration update that appropriately modifies the {{revocation_list}} attribute of {{FabricMSPConfig}}.    Providing a certificate that has not been issued by the MSP should result in an error.""",Story,fabric
208117,"""As a user of the Fabric channel configuration API, I would like to be able to retrieve existing msp configuration for an organization without manually parsing the config transaction so I can reuse CA certs or other relevant information.         *Acceptance*    I should be able to call a function for retrieving msp configuration for a specific org. The example test should also be updated to demonstrate usage of this helper.""",Story,fabric
208121,"""As a user of the Fabric channel configuration API, I would like to be able to update a consortium org's, consortiums group's, and consortium group's configuration by adding or removing policies.         *Acceptance*    I should be able to call this function to modify a config transaction and then process further configuration updates on the modified config transaction. The examples test should be updated to demonstrate usage of this feature.     """,Story,fabric
208122,"""As a user of the Fabric channel configuration API, I would like to be able to update an orderer org's and orderer group's configuration by adding or removing policies.         *Acceptance*    I should be able to call this function to modify a config transaction and then process further configuration updates on the modified config transaction. The examples test should be updated to demonstrate usage of this feature.     """,Story,fabric
208123,"""As a user of the Fabric channel configuration API, I would like to be able to update an application org's and an application group's configuration by adding or removing policies.         *Acceptance*    I should be able to call this function to modify a config transaction and then process further configuration updates on the modified config transaction. The examples test should be updated to demonstrate usage of this feature.     """,Story,fabric
208124,"""As a user of the Fabric channel configuration API, I would like to be able to retrieve existing application, orderer, or consortium org configuration without manually parsing the config transaction so I can persist values I don't want to change when updating an org's configuration.         *Acceptance*    I should be able to call a function for retrieving an application, orderer, or consortium org's configuration that returns a config.Organization. I can then modify fields on the Organization and pass it to an appropriate update organization function to update the org's configuration. The example tests should also be updated to demonstrate usage of this helper.""",Story,fabric
208125,"""As a user of the Fabric channel configuration API, I would like to be able to retrieve existing orderer configuration without manually parsing the config transaction so I can persist values I don't want to change when updating orderer configuration         *Acceptance*    I should be able to call a function for retrieving orderer configuration that returns a config.Orderer. I can then modify fields on the Orderer configuration and pass it to config.UpdateOrdererConfiguration to update the orderer's configuration. The example tests should also be updated to demonstrate usage of this helper.""",Story,fabric
208126,"""As a user of the Fabric channel configuration API, I would like to be able to retrieve existing anchor peers without manually parsing the config transaction.         *Acceptance*    I should be able to call a function for retrieving anchor peers for a specific org. The example test should also be updated to demonstrate usage of this helper.""",Story,fabric
208128,"""As a user of the Fabric channel configuration API, I would like an example in the library documentation that shows me idiomatic usage of the package to create new channel create transactions and sign them.         *Acceptance*    I can find an example for creating create channel transactions in the GoDoc for the package.""",Story,fabric
208140,"""As a user of the Fabric channel configuration API, I would like an example in the library documentation that shows me idiomatic usage of the package to create configuration updates and sign them. The updates should involve multiple changes to instruct on how state is managed.         *Acceptance*    I can find an example for updating channel transactions in the GoDoc for the package. It should also sign the config update envelope.""",Story,fabric
208146,"""As an administrator, I want to use a fit for purpose library to add a new root CA certificate to an MSP configuration.    Acceptance:  Starting with a configuration transaction/block, I can call a function that adds CA certificate to the MSP configuration. After this operation, I should be able to create a config update transaction that is ready for the signature workflow.    Minimal certificate validation should be done when processing the function. In particular, reject certificates that is missing the KeyUsageCertSign/CA attributes.""",Story,fabric
208147,"""As a fabric administrator, I want to use a fit for purpose library to generate a configuration update that revokes a certificate issued by my organization's MSP.    Acceptance:   Provided with a config block/transaction, I can call a function to """"revoke"""" a certificate issued by an MSP. This should result in a configuration update that appropriately modifies the {{revocation_list}} attribute of {{FabricMSPConfig}}.""",Story,fabric
208204,"""As a fabric administrator, I want to use a fit-for-purpose library function to generate a ConfigGroup representing an Organization. The input to the function should not depend on existing MSP material but should accept directly the necessary cryptographic material.    Acceptance    Calling the new function with appropriate arguments should generate a proto structure that is semantically equivalent to one generated by {{cryptogen}} and {{configtxgen -printOrg}}.""",Story,fabric
208222,"""As a fabric administrator, I want to use a fit-for-purpose library to generate a channel update transaction that modifies the {{max_message_count}} batch size parameter. The channel update transaction that is generated should not be signed.    Acceptance    Calling the new function with appropriate arguments should generate a config update transaction that matches one produced by manually transforming and generating a configuration update using the existing tools (ignoring any signature that may be applied by the existing tools).""",Story,fabric
208223,"""As a fabric admin, I want to take a {{ConfigUpdate}} transaction and a collection of detached signatures and use them to create a complete, signed channel config update transaction that can be validated by the orderer and the peers.    Acceptance:    The generated configuration transaction should be equivalent to one generated by """"building up"""" signatures in order using the existing `signtx` command on the peer.""",Story,fabric
208225,"""As a fabric administrator, I want to use a fit-for-purpose library to generate a channel creation transaction from input data structures. This library should not make assumptions about how any key material is stored and should not require network access.    Acceptance:    Calling a function similar to what we have below returns a proto-encoded genesis block that roughly matches that produced by {{configtxgen -channelID channel-id -profile profile-name ...}} using a prototypical {{configtx.yaml}} from our integration test suite.    Generating a genesis block from the configtx.yaml and an msp folder, and using code implemented in this story, I should get two `proto.Equal` blocks.    {code}  type Profile struct {   Consortium   string   Application  *Application   Orderer      *Orderer   Consortiums  map[string]*Consortium   Capabilities map[string]bool   Policies     map[string]*Policy  }    ...    type Option func(options)    // Options for extensibility  type options struct {}    func CreateChannelTx(channelSpec ChannelConfig, options ...Option) (*cb.Block, error) {    // Implement me  }  {code}    The types that are listed above come from the {{genesisconfig}} package in fabric and appear to be managed through a combination of viper and yaml processing. We do not want to use these packages directly; we want to copy and adapt the necessary structures into our package.""",Story,fabric
208290,"""Current *peer lifecycle chaincode approveformyorg* does not seem to output any information of the specified package ID to both the standard output on the console and the peer logs, when the command is executed successfully.    The following is an example of the current peer logs for approved definition.  {code:java}  Successfully approved definition sequence: 1, endorsement info: (version: '1', plugin: 'escc', init required: true), validation info: (plugin: 'vscc', policy: '12202f4368616e6e656c2f4170706c69636174696f6e2f456e646f7273656d656e74'), collections: (), name 'fabcar', on channel 'mychannel'  {code}       As a Fabric admin, at least, I want to see the output of the specified package ID for approved definition in the peer logs as follows.  {code:java}  Successfully approved definition sequence: 1, endorsement info: (version: '1', plugin: 'escc', init required: true), validation info: (plugin: 'vscc', policy: '12202f4368616e6e656c2f4170706c69636174696f6e2f456e646f7273656d656e74'), collections: (), name 'fabcar', package ID 'fabcar_1:a6b61184ef66929c1b6e6ad657c248e9ba6283080eb59250a93e1706eeffe981', on channel 'mychannel'  {code}  The log message will be useful for the following scenarios:   - The message can help Fabric admins to confirm the applied package ID later without recording the command history with the parameters on their console   - The message can help Fabric admins to quickly notice operation mistakes on the applied package ID especially when environment variables are interpolated in package IDs (e.g., empty package ID)""",Story,fabric
209055,"""*Background:*    Currently in Hyperledger Fabric, new peers joining a channel can only get the orderer locations from the channel genesis block.  Once joined to a channel, any change to orderer locations will be pushed as a channel config block so the peers are able to update.         The problem is, if the orderer locations have changed at any point during the life of the channel, new peers joining that channel have no way to get the current, correct location of the ordering nodes since the genesis block will have old information and the peers don't have access to anything else.         I believe there is work slated for maybe Fabric 2.0 (or later) to allow peers joining the channel to pull the orderer locations (and perhaps other data) from the latest channel config block vs only the genesis block.  This seems like the proper way to solve this long term, but we need a 1.x based solution to hold us over until then and this is what this story is meant to address.         *Proposed Change:*    After some internal discussions we believe that the best solution to this problem until the long term fix shows up is to add a section to core.yaml to provide an additional set of  orderer locations the peer could check when joining a channel if none of the original orderer locations were valid.    I say """"none"""" because my thinking is as long as the peer was able to connect to one of the orderers and join the channel, it could then get the locations of the rest of the orderers from the channel configuration it gets after it joins.  The logic could of course be to check the core.yaml block if any of the orderers listed in the genesis block didn't exist or to check it every time.    I'm told there is precedence here since the same thing was done for TLS certificates, so I suspect the exact same logic would apply.""",Story,fabric
209170,"""We need to announce deprecation of the Kafka-based orderer given Raft provides the equivalent trust model, has additional features and is easier to manage and operate.  Fabric 1.4.2+ has the ability to migrate from Kafka to Raft.    Personally, I'd like to actually remove Kafka support in Fabric 2.0.0 but if not, we should announce and then remove in a 2.x version.""",Story,fabric
209702,"""As a an operator, I would like to use the operations service to get an orderer's/peer's version. For one thing, this will help operator know when a sufficient number of orderers/peers have been upgraded such that a channel can be upgraded to the latest capability. For example, an operator may display an 'upgrade channel capability' button on a channel when a sufficient number of peers/orderers have been upgraded.""",Story,"fabric,fabric"
209781,"""The {{chaincode package}} command includes some logic to filter packages that the ccenv includes. This means that older or newer versions of these packages (used during development) will not be included in the package that's installed to the peer.    As a developer, when the """"auto-vendoring"""" process is performed, I would like my code to be included at all times.""",Story,fabric
210163,"""As a Blockchain Developer, I would like to explicitly set my peer's external endpoint advertised to SDK users in Service Discovery, so that I can use Service Discovery to discover peers without implementing a DNS.         Motivation:    As an application developer, I host my fabric network on a docker network. I use the node Fabric SDK on my application to discover other peers in the network so that I can send endorsement proposals to them.    Currently we use the PEER_GOSSIP_EXTERNALENDPOINT option on the peer for advertising the peer's url in the SDK. However, as the peers are hosted in a docker network (non-host network), the URLs used are only discoverable within the docker network. If I choose to use my host's hostname in that option, the docker containers are unable to bind to that address.    The current work around is to implement a DNS for my client applications to use, and use external port mappings to map to the internal ports the peers are listening on.    We propose to have such an option to speed up development of our applications and networks.""",Story,fabric
210889,"""The following test:    {color:#4c4c4c}EndToEnd reconfiguration and onboarding {color}when an orderer node is joined {color:#ff0000}*[It] isn't influenced by outdated orderers* {color}    in integration/e2e/etcdraft_reconfig_test.go fails frequently in integration testing. There are 3 occurrences of this failure in the first 2 pages of open CR's on Gerrit. I've run up against it 3 times myself today.    Locally, the test takes 53 seconds to run, it has a 60 second timeout, which when run in Jenkins is extremely flaky given the small margin of error. It is very easy to replicate, if you simply focus the test, and do a `go test -count=2` I get the error every time locally.""",Bug,fabric
210974,"""h2. Observed:    After launching network with 4 orderers in the orderersystemchannel, created channel """"testorgschannel2"""" without orderer4 in the consenters list, then orderer4 was removed from orderersystemchannel (automatically, NOT part of the test) and the following was seen in orderer4 logs   {code:java}  Feb 25 13:15:39 orderer4th-ordererorg-7fdfb59d94-6gdds orderer4th-ordererorg 2019-02-25 18:15:39.191 UTC [orderer.consensus.etcdraft] apply -> INFO 7f7e Applied config change to remove node 4, current nodes in channel: [1 2 3] channel=orderersystemchannel node=4  {code}  Noted below are [~<USER>'s observations(from slack):   * What I strongly suspect is going on above, is that the orderer is treating the channel creation tx as a config update (which it is, but to another channel). Likely we have not noticed this before as the updates have not modified the consenter config. I had [~<USER> check the orderer system channel and its config is in fact unchanged.   * I'm coming at this from the outside, but I _suspect_ that based on the way that the blockwriter was written, that Raft categorizes transactions as either """"normal"""" or """"config"""", and that channel creation transactions are being classified as """"config"""" (good, they should be). However, because they are categorized as a config transaction, the Raft code is inspecting the config contained here and realizing that the update made to the template config removed an orderer and is inappropriately applying it as an update to the orderer system channel. I expect that the fix is essentially to detect when the config update is being done to the template and not to the real orderer system channel and ignore it. Of course this is 90+% speculation based on the symptoms   * Based on the fact that when [~<USER> pulled the orderer system channel config and the set of consenters still contained all four (while the log indicates the fourth was removed), I do not believe this is a `configtxgen` related bug         Attached orderer4th log : orderer4th-FAB-13702.log      """,Bug,fabric
211151,"""As a user of Fabric 1.4, LTS, I'd like to benefit from the Kafka health checks introduced into the master branch.""",Story,fabric
211220,"""Recently, Cello restructured the directory structure and moved the playbooks   * From (Old): src/agent/ansible   * To (New):    src/operator-dashboard/agent/ansible    ATD has some hard-coded relative paths to Cello, for example in tools/ATD/roles/tool_pte/fabrictestsetup/tasks/apply.yml. So, ATD cannot work with the latest Cello. I tentatively fixed this issue. I'm planning to post the patch but let me know if someone already has a solution.""",Bug,fabric
211659,"""1.Chaincode instantiate - add this may take a few minutes to complete.    2. Change  This is because it additionally submits an instantiate transaction to the orderer, which will include the transaction in the next block and distribute it to all peers that have joined mychannel, enabling any peer to execute the chaincode in their own isolated chaincode container.  To  This is because it additionally submits an instantiate transaction to the orderer, which will include the transaction in the next block and distribute the chaincode  to all peers that have joined mychannel, enabling any peer to execute the chaincode in their own isolated chaincode container.     3. Change:  Note that instantiate only needs to be issued once for papercontract even though typically it is installed on many peers.  To  Note that instantiate only needs to be issued once for papercontract on a channel even though typically it would have already been installed on all the peers that will run it.    4. networkConnection.yaml contains a reference to Hyperledger Composer ../gateway/networkConnection.yaml  # the """"x-type"""" property with the """"hlfv1"""" value example below is used by Hyperledger Composer to  # determine the type of Fabric networks (v0.6 vs. v1.0) it needs to work with.  #        5. I didn’t see any instructions on how the ../gateway/networkConnection.yaml.  As an application developer,  I would be interested to know where that comes from and if I need to provide it and how to create it.     6.Change  The response needs to deserialized from a buffer into paper, a CommercialPaper object which can interpreted correctly by the application.  To  The response needs to be deserialized from a buffer into paper, a CommercialPaper object which can interpreted correctly by the application.    7. Should `beta` be removed from here:  """"dependencies"""": {      """"fabric-network"""": """"^1.4.0-beta"""",      """"fabric-client"""": """"^1.4.0-beta"""",      """"js-yaml"""": """"^3.12.0""""    },    8. Let’s install these packages with the npm install command – this may take up to a minute to complete:  I think we need to tell them that `npm install` needs to be run from the ../commercial-paper/organization/magnetocorp/application directory""",Story,fabric
211908,"""When operating a system, I would like to be able to observe whether there is an increase of info, error, and warning messages that are emitted by the logging framework as a metric.""",Story,fabric
212526,"""A general purpose smart contract will be useful for most projects. Such a smart contract will enable application developers to easily integrate their application to Hyperledger Fabric.    I've written a general purpose smart contract than can store any key-value to either LevelDB or CouchDB. The idea is to pass the value as a JSON object and insert it to the ledger so that rich queries can be run.    An example with PutState()    User's application provides a key and a value in JSON, and the smart contract unmarshals and then marshals the value before calling shim's PutState() method. This makes the value field to appear in the CouchDB document as a valid JSON object.    So, the user's application can run native CouchDB queries on his data.    I've also added built-in support for Statsd reporting using the same go library as used in Fabric. Each call to GetState(), PutState() etc are counted and reported to the given statsd server.    Please find attached my implementation of general purpose smart contract.    Regards,    // Gokhan   """,Story,fabric
212913,"""When debugging performance problems, it's very important to be able to get more granular times on when a peer is processing a block.    Currently, the only logging at the INFO level is when the peer commits a block.  To get more detail, we have to turn on debug logs which results in a TON of logs and makes debugging difficult.  My understanding is the states are at a high level:        * block arrival via gossip (and orderer if the peer is pulling directly from orderer?)   * when it starts to get processed   * when validation is complete   * when commit is complete to chain   * when commit is complete to statedb    And currently all that is logged is when commit is complete to chain.    Can we get a bit more detail at the INFO level.  Here's a couple of specific proposals:    1 - At info level, log the first time the block shows up at the peer via any method and whenever the peer is done with it.  Given that you are currently logging when the commit is complete to chain, that may be sufficient for an end state and you can just add block arrival.    2 - Keep a single info message per block to avoid overwhelming the info logs, but add more granularity to that log line.  Something like:   """"Block 298: arrived at x (time), started processing y, completed validation z, committed to to chain, aa, committed to statedb bb""""  Obviously this could get long, so you could limit the granularity down to arrival and commit to chain, or some smaller list.         I'm not filing this as a bug, but if it's an easy change, please port it back to 1.1.x and 1.2.x as many customers are still on these versions and are starting to debug performance issues.          """,Story,"fabric,fabric"
213112,"""Here are my notes from the meeting with Gari and Chris:    Customers want to know how to deploy an e2e fabric solution, using the fabric CA client to generate their certs.    How to set things up?    How to modify the network?    Describe `here are the things you need to do...` -   * What info do you need and how do you get it?   * You need an ordering service then you Stand up peers that may or may not be connected. How to set it up incrementally?   * Where is the ordering service, how do I set up the ordering service? How to bring it up?   *  How do I create the genesis block?     * When you setup a peer/here are things you need to know to setup a peer   * Now that I have a remote peer, what can I do with it?   * Where am I connecting it to - an anchor peer in another org? What info do you need to connect it.    * How do I setup an anchor peer   * If you want to use TLS, what do you need to do? How to get the TLS cert?   * How do you configure peers and get them to work together?   * How do I create a channel?   * What info do I need from the orgs (msps)   * How to use Join channel api on the peer?   * How to fetch the genesis block?   * How to install & instantiate chaincode?   * How to use the fabric-ca vs embed own cryptographic material externally   * Guidance on setup for HA   * What monitoring do I need to do?Here’s the processes you need to be monitoring. Here’s your log files…..   * There’s a network running over here - how do I connect to that network?   * We support Kafka - recommended config is: Min 4 Kafka brokers, replica config of 3. Get the Orderer config info from your kafka cluster   * Message hub - Kafka as a service in blue mix - ibm event streams = message hub for icp - need to document - use this for 6 months and then switch over to RAFT     """,Documentation,fabric
213640,"""*Description*:    Offer a TMS system with the ability to construct and verify _issue requests_, i.e.,    # As a TMS developer I would like to implement the part of _TMS Issuer_ interface that refers to the generation of TMS issue requests   # As a TMS developer I would like to implement the _TMS Verifier_ interface that refers to the verification of TMS issue requests          *Success Criteria*: At the end of this work-item:   * the TMS accommodates requests to construct _issue requests_   * the TMS accommodates requests to verify _issue requests_ assuming a mockup of ledger (KVS) as follows:   ** *malformed* _issue requests_ or _issue requests_ that do *not* conform with the TMS rules (e.g., correctness of the tokens created) are *rejected* and have *no* impact to the mockup ledger   ** *wellformed* _issue requests_ or _issue requests_ that conform with the TMS rules (e.g., correctness of the tokens created) are *accepted* and modify the state of mockup ledger """,Sub-task,fabric
213662,""" This is one epic from many to follow to allow for token management enablement of Fabric. Current proposal for the higher level design is TBA soon.         A Token Management System is an abstraction that aims to generalise the set of operations associated to the issuing, transfer and listing of tokens at a relatively lower level. It is essentially a library that can be used by FabToken entities (client, prover peer) to construct the FabToken issue and transfer transactions, to list the tokens owned by a specific user, etc, and to validate/commit FabToken transactions (committing peer side).         At the completion of this epic work a Token Management System (TMS) component is defined and implemented as follows:    * the Token Management System accommodates requests to construct _transfer requests_   * the TMS accommodates requests to verify _issue_ and _transfer_ requests __ assuming a mockup of ledger (KVS) as follows:   ** *malformed* _issue_ or _transfer_ _requests_ or_requests_ that do *not* conform with the TMS rules (e.g., correctness of the tokens created, double-   ** the TMS accommodates requests to construct _issue requests_   ** the TMS accommodates requests to verify _issue requests_ assuming a mockup of ledger (KVS) as follows:   *** *malformed* _issue requests_ or _issue requests_ that do *not* conform with the TMS rules (e.g., correctness of the tokens created) are *rejected* and have *no* impact to the mockup ledger   *** *wellformed* _issue requests_ or _issue requests_ that conform with the TMS rules (e.g., correctness of the tokens created) are *accepted* and modify the state of mockup ledger    ** spending attacjs) are *rejected* and have *no* impact to the mockup ledger   ** *wellformed* _issue or_ _transfer_ _requests_ or _requests_ that conform with the TMS rules (e.g., correctness of the tokens created w.r.t. the ones consumed/imported) are *accepted* and modify the state of mockup ledger    * the TMS accommodates requests to _list tokens_ assuming a mockup ledger          Potential break down of the work involved in this epic into pieces:   # Design    # As a Fabric core developer I would like to provide a Token Management System (TMS) with the logic to accommodate issue token related requests   ## As a TMS developer I would like to design an interface for a _TMS issuer_ that would generate token issue requests and of a _TMS Verifier_ interface that would verify token issue requests upon demand   ## As a TMS developer I would like to implement the part of _TMS Issuer_ interface that refers to the generation of TMS issue requests   ## As a TMS developer I would like to implement the _TMS Verifier_ interface that refers to the verification of TMS issue requests    # As a Fabric core developer I would like to enhance the TMS with token transfer abilities   ## As a TMS developer I would like to design the interface of a _TMS transactor_ that would generate TMS token transfer requests and extend the _TMS Verifier_ interface to allow for verification of TMS token transfer requests   ## As a TMS developer I would like to implement the part of the _TMS Transactor_ interface that refers to the generation of TMS transfer requests   ## As a TMS developer I would like to implement the part of the _TMS Verifier_ interface that refers to the verification of TMS transfer requests   # As a Fabric core developer I would like to enhance the _TMS Transactor_ functionalities with token listing abilities   ## As a TMS developer I would like to extend the _TMS Transactor_ interface with the capability to accommodate list token requests   ## As a TMS developer I would like to Implement the _TMSTransactor_ interfaces for processing TMS list token requests   # As a Fabric core developer I would like to enhance the TMS with the ability to configure itself   ## As an infrastructure developer I would like to be able to build a _TMS factory_ to allow for the setup of a configurable TMS and respective Transactors, Issuers and Verifiers   # Documentation   # System Test         Adding [~<USER>, [~<USER>, [~<USER>, [~<USER>, [~<USER>.     """,Epic,fabric
213900,"""Dave asked us not to use the term SideDB in the docs, but rather use the term 'private data'.    We have 1 reference to sideDB in the private data key concept, so that if someone searches on the term sidedb, they will be taken to the appropriate content.         However, Barry noticed that the search also returned a hit in the Introduction and was confused because there is no doc topic called SideDB.         I'd like to remove the reference to sideDB in the Introduction. This sentence:         Fabric is also working on two sets of features that are intended to improve upon its privacy and confidentiality capabilities, specifically: private data – a feature we call SideDB, and zero knowledge proofs (ZKP).    to    To improve upon its privacy and confidentiality capabilities, Fabric has added support for <link: private data> and is working on zero knowledge proofs (ZKP) available in the future.          """,Story,fabric
213965,"""As a user, I need the ability to perform post-order commit-time operations (e.g. addition/subtraction, range checks) so that collisions on endorsed read/write sets can be avoided  in high TPS systems.""",Epic,"fabric,fabric"
214137,"""When writing chaincode if I call PutState() to create a new key without having first called GetState() for that same key I'm open to the possibility of a race condition going undetected between two different clients, each issuing a different transaction but both thinking the key does not exist and so both calling a create() chaincode function with different values.       As an example, a common pattern seems to have chaincode that looks like this:   {{queryThing(""""x"""") \{}}   {{  //return thing """"x"""" if it exists}}   {{}}}    {{createThing(""""x"""") \{}}   {{  //call PutState(""""x"""") to create thing """"x"""" but do not not call GetState(""""x"""") first}}   {{}}}    {{updateThing(""""x"""") \{}}   {{  //call GetState(""""x"""") followed by PutState(""""x"""") - all good here}}   {{}}}        This seems like a common enough pattern and it's open to the problem above especially if the client has a UI that calls queryThing() first, then based on existence call createThing() or updateThing(). Ideally they could have a single """"createOrUpdateThing()"""" method but people seem to like the above so are open to the race condition...       Really this potential problem should be documented so people can avoid this problem which would be hard to debug. Best practice should be to suggest people always have a matching """"GetState(""""x"""")"""" for each """"PutState(""""x"""")"""" which will prevent the problem from occurring.""",Bug,fabric
214951,"""From J. Yellick    An orderer org may be part of the consortium, but as a best practices, I would suggest that they should not. Even if the same organization is running both ordering and transacting in the consortium, I would suggest it as best practice that they use different MSPs for each so that the default block validation policies and channel reader policies etc. work as expected for clients acting in a non-orderer role.""",Sub-task,fabric
215042,"""Until recently, """"make gotools"""" automatically installed the eight executables in $(GOPATH)/bin, which is why the documentation instructed the user to make sure that directory was on their search path.    With the recent introduction of """"gotools.mk"""", those executables are now simply placed in ./.build/gotools/bin, which I'm fairly sure is not part of the user's search path, suggesting that anything that required those to be on the search path will now fail.    As a perfect example:    *$ make spelling*  *Checking changed go files for spelling errors ...*  *xargs: misspell: No such file or directory*  *spell checker passed*    So what's the resolution for this?""",Bug,fabric
215251,"""As an integration test author, I want to be able to populate a cryptogen {{Config}} struct, and call the {{generate}} function on it to create the relevant crypto artifacts.    I cannot do either of these things however because both the {{Config}} struct and the {{generate}} functions are located in the {{main}} package.    Besides artifact generation, having access to the {{Config}} struct means that I can easily infer the right values of the {{LOCALMSPDIR}} and {{TLS_*}} ENV vars of orderers or peers, and set them when creating a cluster runner.    I suppose that the solution here involves exporting all of these constructs and moving them into a separate package (that a barebones cryptogen {{main}} package would then invoke).""",Task,fabric
215398,"""I'm trying to make a Peer join two channels each with its own Orderer, so I have the following scenario:    - A channel named solochannel with an orderer Solo, using the port 7050  - A channel named kafkachannel with an orderer Kafka, using the port 8050    1- I create the chanels using the command  {code:java}  peer channel create -o orderer:8050 -c solochannel -f ./channel-artifacts/channelSolo.tx  peer channel create -o ordererKafka:8050 -c kafkachannel -f ./channel-artifacts/channelKafka.tx{code}    2- After creating both channels, I start a peer to join them in both channels  {code:java}  peer channel join -b solochannel.block  peer channel join -b kafkachannel.block{code}      This step shows a message saying that the Peer was able to join both channels,  and if I try to list all channels using 'Peer channel list', it shows that the  Peer joined both channels successfully.    3- I open the Log for the Peer and the peer is trying to send information of the  last Channel it joined to the orderer of the First channel.    So for example:  If I join the solochannel first, It tries to send information of the kafkachannel to the ordererKafka:8050 and fails with the message Got error &\{NOT_FOUND}  If I join the kafkachannel first, It tries to send information of the solochannel to the ordererSolo:7050 and fails with the message Got error &\{NOT_FOUND}    This is the Log from the Peer  {code:java}  2018-03-07 16:58:06.071 UTC [deliveryClient] StartDeliverForChannel -> DEBU 6d0 This peer will pass blocks from orderer service to other peers for channel kafkachannel  2018-03-07 16:58:06.074 UTC [deliveryClient] connect -> DEBU 6d1 Connected to ordererSolo:7050  2018-03-07 16:58:06.074 UTC [deliveryClient] connect -> DEBU 6d2 Establishing gRPC stream with ordererSolo:7050 ...  2018-03-07 16:58:06.077 UTC [deliveryClient] afterConnect -> DEBU 6d3 Entering  2018-03-07 16:58:06.078 UTC [deliveryClient] RequestBlocks -> DEBU 6d4 Starting deliver with block [1] for channel kafkachannel  2018-03-07 16:58:06.082 UTC [deliveryClient] afterConnect -> DEBU 6d5 Exiting  2018-03-07 16:58:06.084 UTC [blocksProvider] DeliverBlocks -> WARN 6d6 [kafkachannel] Got error &{NOT_FOUND}{code}    This is the Log from the ordererSolo:7050  {code:java}  2018-03-07 16:58:06.076 UTC [orderer/main] Deliver -> DEBU 1f5 Starting new Deliver handler  2018-03-07 16:58:06.076 UTC [orderer/common/deliver] Handle -> DEBU 1f6 Starting new deliver loop  2018-03-07 16:58:06.076 UTC [orderer/common/deliver] Handle -> DEBU 1f7 Attempting to read seek info message  2018-03-07 16:58:06.082 UTC [orderer/common/deliver] Handle -> DEBU 1f8 Rejecting deliver because channel kafkachannel not found  2018-03-07 16:58:06.082 UTC [orderer/main] func1 -> DEBU 1f9 Closing Deliver stream{code}    This keeps happening forever.    While I keep having this error, I cannot instantiate or upgrade a chaincode on the second channel      I'm having this error for both versions 1.0.5 and 1.0.6.     """,Bug,fabric
215496,"""As a developer I want to construct channel config block outside of *configtxgen* tool.    My proposal is making MSPConfig to interface, which allows to construct your own configs""",Story,fabric
215527,"""I asked this question originally on #fabric-orderer and then on #fabric-ledger, and I was asked to summarize it here. The original question was:     {quote}  """"Is there ever a situation where I may end up with duplicates of the same txnId in the same or even different blocks - maybe as the result of submitting the same transaction twice through the same OSN, or at the same time through multiple OSNs?""""  {quote}     [~<USER> replies that     {quote}  """"yes, that's indeed possible, and only the first validated instance of the same txnId would have a chance of being valid, the others would be marked as invalid""""   {quote}    (which I personally find a bit odd, but, OK). The follow up question I asked [~<USER> and [~<USER> was    {quote}   """"then, how does the block index behave? What does getTransactionById(txnId) return if txnId was submitted multiple times""""  {quote}    [~<USER> confirmed that subsequent submissions of the same txnId would be marked as invalid, but he was not sure about the behavior of the block index/getTransactionById() in that scenario; maybe it returns the last submission of the same txnId? (which I would think is the worst of all possible scenarios).    My personal opinion is that duplicate txnIds should never occur in the ledger.""",Bug,fabric
215570,"""A placeholder feature to improve command output.         The peer channel command (and indeed all subcommands), should   # Provide a clear indication of whether the command has succeeded or failed   # Have some level of helpful INFO messages that return helpful information on command progress.     I'm happy to expand here on what these need to be, but for now, I've just opened this JIRA as a placeholder.""",Story,fabric
216009,"""     Hi,    after the patch campaign on our Hyperledger Fabric cluster, all the VMs of the cluster has been restarted.     After restart, all the containers were stopped an so we restarted the all stack :    * \{Hyperledger Fabric CA and Orderer Solo} on a dedicated consortium VM,   * \{HF Peers, CouchDB and application} on 3 other network participant VMs.     At this first restart everything was looking OK but there was a lot of logs history and so to troubleshoot easily another application issue on a a dedicated participan VMs, we decided to restart again the docker stack but this time by removing and recreating the docker containers to remove the past log (*).     Unfortunately when restarting the Hyperledger stack on this particular node we hit an issue on one of our peers which prevent us to restart our chain on this peer and particularly read the MSP from the configuration block (see logs bellow).     I'm currently looking some way to restart the peer and I see one by removing the peer data and resync the empty peer with the two other still running on our network. Do you see other cheaper way ? Do you have some advice to help us ? And finally do you have some idea on what could be the root cause of this issue ?     Thank you  {code:java}  2017-06-19 23:26:01.361 UTC [ledgermgmt] OpenLedger -> INFO 1c9 Opened ledger with id = elixir-channel  2017-06-19 23:26:01.361 UTC [peer] getCurrConfigBlockFromLedger -> DEBU 1ca Getting config block  2017-06-19 23:26:01.361 UTC [fsblkstorage] retrieveBlockByNumber -> DEBU 1cb retrieveBlockByNumber() - blockNum = [26858]  2017-06-19 23:26:01.361 UTC [fsblkstorage] newBlockfileStream -> DEBU 1cc newBlockfileStream(): filePath=[/var/hyperledger/production/ledgersData/chains/chains/elixir-channel/blockfile_001449], startOffset=[10192565]  2017-06-19 23:26:01.361 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 1cd Remaining bytes=[5377], Going to peek [8] bytes  2017-06-19 23:26:01.361 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 1ce Returning blockbytes - length=[5375], placementInfo={fileNum=[1449], startOffset=[10192565], bytesOffset=[10192567]}  2017-06-19 23:26:01.361 UTC [fsblkstorage] retrieveBlockByNumber -> DEBU 1cf retrieveBlockByNumber() - blockNum = [26847]  2017-06-19 23:26:01.361 UTC [fsblkstorage] newBlockfileStream -> DEBU 1d0 newBlockfileStream(): filePath=[/var/hyperledger/production/ledgersData/chains/chains/elixir-channel/blockfile_001449], startOffset=[10025675]  2017-06-19 23:26:01.362 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 1d1 Remaining bytes=[172267], Going to peek [8] bytes  2017-06-19 23:26:01.362 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 1d2 Returning blockbytes - length=[5546], placementInfo={fileNum=[1449], startOffset=[10025675], bytesOffset=[10025677]}  2017-06-19 23:26:01.362 UTC [peer] getCurrConfigBlockFromLedger -> DEBU 1d3 Got config block[26847]  2017-06-19 23:26:01.362 UTC [common/config] NewStandardValues -> DEBU 1d4 Initializing protos for *config.ChannelProtos  2017-06-19 23:26:01.362 UTC [common/config] initializeProtosStruct -> DEBU 1d5 Processing field: HashingAlgorithm  2017-06-19 23:26:01.363 UTC [common/config] initializeProtosStruct -> DEBU 1d6 Processing field: BlockDataHashingStructure  2017-06-19 23:26:01.363 UTC [common/config] initializeProtosStruct -> DEBU 1d7 Processing field: OrdererAddresses  2017-06-19 23:26:01.363 UTC [common/config] initializeProtosStruct -> DEBU 1d8 Processing field: Consortium  2017-06-19 23:26:01.363 UTC [peer] Initialize -> WARN 1d9 Failed to load chain elixir-channel(Bad envelope: Not a tx of type CONFIG)  {code}  (*) : peers data and conf are configured to be on the host  through dedicated volume ; this procedure has been used a lot of time last years (at least 48 time as we did 48 chaincode upgrade).     """,Bug,fabric
216099,"""When committing a transaction in which the private data has a private read-set but no private write-set, the following warning occurs (after about 60 seconds):    gossip/privdataÅ StoreBlock -> WARN 40b Could not fetch all missing collection private write sets from remote peers. Will commit block with missing private write sets: txID: 4d3adc8ff4e4df0f09a526a7653de5b1a2adfd09b4d0488e2c18236420ffd18d, seq: 0, namespace: mapcc, collection: coll2, hash:    Steps to reproduce:    1) PutPrivateData(""""coll1"""",""""key1"""",value1); endorsement successful; commit successful   2) GetPrivateData(""""coll1"""",""""key1""""); endorsement successful; commit receives the above warning after 60 seconds    I also tried the following and it had the same result:    3) PutState(""""key2"""",value2); GetPrivateData(""""coll1"""",""""key1""""); endorsement successful; commit receives the above warning after 60 seconds    I'm testing in a configuration composed of 4 MSPs, 1 peer per MSP for a total of 4 peers, and 2 (Kafka) orderers. One chaincode (mapcc) has the following collection config:    Collection Name: coll1   RequiredPeerCount=1   MaximumPeerCount=2   Policy includes all 4 MSPs    Attached are the logs (look for the warnings at the end of the file).""",Bug,fabric
216132,"""The CLI command `peer channel create` honors the --tls and --cafile options as expected, and uses a TLS connection with the given CA file to the orderer for channel creation.  However, `peer channel join` and `peer channel list` do not honor the --tls and --cafile options, always attempting to make an insecure connection with the peer.    However, if one specifies the CORE_PEER_TLS_ENABLED=true and CORE_PEER_TLS_ROOTCERT_FILE=path/to/cert.pem environment variables, it does work.    Upon closer inspection, --tls and --cafile only specify the TLS settings for the orderer endpoint, not the peer endpoint.  This asymmetry (commandline options vs env vars) is confusing, probably stemming from the fact that the `peer channel join` and `peer channel list` don't even require an orderer endpoint, so the --tls and --cafile options are useless for those subcommands.    I'd suggest that at the very least, the --tls and --cafile options be moved to only the `peer channel create` and `peer channel update` commands.  Better yet, change --tls and --cafile to something that indicates they're for the orderer; --orderer-tls and --orderer-cafile.  Then also add commandline options for specifying the TLS settings for the peer connection; perhaps --peer-tls and --peer-cafile.    I think part of why this confusion occurred is because the line between the peer acting as a server (i.e. `peer node`) and the peer acting as a client (i.e. `peer channel`) is blurred and makes configuring and using the peer binary very confusing.  This is another argument for fully separating the peer binary into peer-client and peer-server, similar to the architecture of fabric-ca with its fabric-ca-client and fabric-ca-server.    I request that this fix, if made, be also added to the 1.0.x branch.     """,Story,fabric
216540,"""This analysis and design task is intended to satisfy the requirements of FAB-7088. Please refer to that item for a clear picture of what we're hoping to achieve.     The discussion regarding the filtering of blocks began in FAB-5481. The previous decision to create a new type, FilteredBlock, was made in the context of using the eventhub  (and similarly designed Channel Service) to deliver blocks to clients. Now that the peer will implement deliver, which will be used in place of the eventhub to notify clients on a per channel basis, I'd like to reopen the discussion of how blocks should be filtered in the event that peer wishes to do so.     One relevant piece of implementation detail I'd like to call out is that the deliver service currently sends back a response with the following definition:  {code:java}  message DeliverResponse {      oneof Type {          common.Status status = 1;          common.Block block = 2;      }  }{code}  The two filtering options that have been proposed so far (via FAB-5481):   # use the FilteredBlock proto defined via FAB-5481 and add it as a type to the DeliverResponse   # use the same Block proto but filter out fields""",Task,fabric
216686,"""I have on instantiate chaincode that set some values but has no events. What I get is a filteredTx that has a collection of 1 filteredAction.  The FilteredAction is empty .. serialize it to bytes its size is  zero.  This produce when you get the ccEvent as zero byte and not null ccEvent.     What I'd expect is a FilteredAction which has btyes with the ccEvent set to null.    java debug code :   ```  System.out.println(event);                      System.out.println();                      final PeerEvents.FilteredTransaction filteredTx = event.getFilteredBlock().getFilteredTx(0);                      System.out.println();                      System.out.println(""""filteredTx:"""" + filteredTx);                        final PeerEvents.FilteredAction filteredAction = filteredTx.getFilteredAction(0);                        System.out.println();                      System.out.println(""""filteredAction:"""" + filteredAction);                      System.out.println();                      System.out.println(""""filteredAction size:"""" + filteredAction.toByteString().size());                        final ChaincodeEventOuterClass.ChaincodeEvent ccEvent = filteredAction.getCcEvent();                      System.out.println();                      System.out.println(ccEvent);                      System.out.println();  ```  filtered_block {    channel_id: """"foo""""    number: 1    type: ENDORSER_TRANSACTION    filtered_tx {      txid: """"2ba694f6ec2cd1a664a01a7790c5e39091189ef87a16cb9b0311079f75da6fff""""      filtered_action {      }    }  }  channel_id: """"foo""""        filteredTx:txid: """"2ba694f6ec2cd1a664a01a7790c5e39091189ef87a16cb9b0311079f75da6fff""""  filtered_action {  }      filteredAction:    filteredAction size:0""",Bug,fabric
216710,"""As a Fabric network manager, I want to know the performance of my Fabric queries if CouchDB is chosen as the state database.""",Story,fabric
217403,"""As a chaincode developer I want to be able to enable RYW during key retrieval optionally, so that I can access already updated keys during my transaction.         My suggestion would be to enable this via an extension of the API in the fashion of as `stub.GetTrxState(key string)`.    Having this feature would make chaincode development more appealing and explicitly enable access to data already written during the currently active transaction.""",Story,fabric
217469,"""Gossip has an in-memory mapping from PKI-ID to peer identities.  This mapping is used to:  * Keep track of which identities are valid and which are not  * Locate the identity given a PKI-ID in order to perform verification on a message's signature, given only a PKI-ID of the message.    Since x509-based certificates (what is used currently in Fabric) - eventually expire, the procedure that the current code does to remove them once they expire is periodically (once in 24 hours) call *MessageCryptoService.Validate* on all identities.    This has the following shortcomings:   * Once per 24 hours makes it that signatures of peers that their certificates have expired can still be incorrectly validated for a maximal duration of... 24 hours.  * Validation of certificates involves with cryptography computations that are expensive, and since certificates expire only once every few months (or years) - it is almost always a redundant check.    Therefore what I want to do, is that the MessageCryptoService would have an additional method that would tell given a *PeerIdentity* - when (*time.Time*) it expires:  * A zero value would indicate it never expires  * A non zero value would indicate that it expires at a certain time.    Then, when gossip would create a new mapping between the PKI-ID and the identity - it would spawn a timer that would fire at *NotAfter + time.Millisecond \* c* for some small constant *c*, and then the mapping would be deleted.   * If the identity is deleted from memory before that time - the timer would be cancelled, to prevent stale timer contexts floating in the golang runtime.     [~<USER> [~adc] [~<USER> [~<USER>  """,Story,"fabric,fabric"
217493,"""Recall that in the new message flow, the OSNs tagged incoming messages with the config sequence they were validated against, before submitting them for ordering.    If the config sequence advances before the messages are read/consumed from the Kafka partition, they need to be revalidated.    (For the sake of this discussion, let's refer to these messages that need to be revalidated because the config sequence number changed as """"stale messages"""".)    A naive approach to this problem would have us dropping all stale messages that fail to pass the revalidation, and append to the ledger those that don't.    This is incorrect as validation can fail non-deterministically; consider for example the case of a message that is identified as stale by OSN1 because its certificate just expired. OSN2 has a slower clock however, so it identifies this message as valid. OSN1 drops the message, OSN2 adds it to its ledger, and we now have a fork.    We therefore need to revalidate *and reorder* these stale messages before deciding on their fate (commit or drop). This way, all OSNs will be on the same page, and we won't get forks.    However, assume an ordering service consisting of 30 OSNs. Under this scheme, every stale message can be resubmitted to the ordering service (a max of) 30 times (if all 30 nodes find it valid during revalidation). That's a lot of duplicate messages that the OSNs will have to go through eventually, and we have no easy to way to identify them.    Therefore we need a way to have each OSN go """"I've already processed a copy of this revalidated and reordered stale message, I'll pass.""""    Wanted behavior for each OSN:   # DO: Tag the message itself. Add an OriginalOffset field to the KafkaMessageRegular protobuf definition. If a message is stale and found to be valid during the revalidation, populate that with the Kafka offset that corresponds to this message, then submit it for re-ordering.   # DO: Record the last OriginalOffset it processed, in a variable that we call LastOriginalOffsetProcessed. If a message with a non-nil OriginalOffset is received that is <= LastOriginalOffsetProcessed, discard it immediately. Each OSN should record LastOriginalOffsetProcessed on disk for crash-fault tolerance. (Add that piece of metadata to the block similar to how we encode LastOffsetPersisted.)   # DO NOT: Commit the block until all stale messages with OriginalOffset <= this block's LastOffsetCommitted have been posted to the Kafka cluster (ACK'd) successfully. Otherwise you run the risk of not giving all the stale messages a second chance. (Consider the case where an OSN reads a message that is now labeled as stale, its OriginalOffset is set to 18. It is handed over to the Enqueue thread for re-ordering (posting to the Kafka cluster). The main thread in the meantime now reads the following message, which is valid and fills the pending block. The main thread writes this block to disk with LastOffsetPersisted = 19. The OSN crashes before the Enqueue thread manages to post message 18 to the Kafka cluster this successfully. If this happens across all nodes, this stale message will be lost without being re-ordered.)    Additional notes:   # This flow applies to both NORMAL and CONFIG_UPDATE messages.    NOTE:   we must NOT resubmit it if we still have v1.0.x orderer in the cluster, as they couldn't differentiate the resubmission and original tx. Therefore, we should add a switch {{Compatibility.Resubmission}} to turn on/off resubmission. Upgrade path should be:   - Partial OSNs are upgraded to v1.1, {{Resubmission}} is off be default   - v1.1 OSNs behave exactly the same as v1.0.x (still double-validate every tx, and no resubmission)   - Upgrade remaining v1.0.x OSNs to v1.1   - Re-configure the channel to turn on {{Resubmission}}   FAB-5999 is linked to this issue as block to capture this work.""",Sub-task,fabric
217750,"""{code:java}    membersrvc:      image: hyperledger/fabric-membersrvc      ports:      - """"7054:7054""""      command: membersrvc    vp0:      image: hyperledger/fabric-peer:x86_64-1.0.0      ports:        - """"7050:7050""""        - """"7051:7051""""        - """"7053:7053""""      environment:        - CORE_PEER_ID=vp0        - CORE_SECURITY_ENROLLID=test_vp0        - CORE_SECURITY_ENROLLSECRET=MwYpmSRjupbT        - CORE_PEER_ADDRESSAUTODETECT=true        - CORE_VM_ENDPOINT=unix:///var/run/docker.sock        - CORE_LOGGING_LEVEL=DEBUG        - CORE_PEER_PKI_ECA_PADDR=membersrvc:7054        - CORE_PEER_PKI_TCA_PADDR=membersrvc:7054        - CORE_PEER_PKI_TLSCA_PADDR=membersrvc:7054        - CORE_SECURITY_ENABLED=false      links:        - membersrvc      command: sh -c """"sleep 5; peer node start --peer-chaincodedev""""  {code}  Using the above docker-compose-yaml, I can no longer GET localhost:7050/chain or POST localhost:7050/chaincode. I'm wondering if this is a known issue with an update or if it might be something with my OS. This was working about a week ago and nothing has changed in my configuration to my knowledge. I reinstalled all the images, tried mapping 7050 to a different port, etc. but nothing happens. I get no messages from the server when I try to hit that location, and the only thing I get from the Postman side is """"Socket hangup"""". Any help would be greatly appreciated.     As a side note, I have to use hyperledger/fabric-peer:x86_64-1.0.0, as hyperledger/fabric-peer:latest no longer exists in Docker.""",Bug,fabric
217794,"""This JIRA item summarizes issues regarding to addition of new peers and/or new organizations to a channel, and discusses solutions for them and tracks their JIRA items.    Currently, there are several problems with regard to channel reconfiguration, that all stem from the fact that when a peer joins the channel - it is given the first configuration block (the genesis block):   # The certificates (TLS or signing identity certificates) of the orderers cannot be changed, otherwise the peer would not be able to connect to any *ordering service node* (hereafter - *OSN*) or verify the blocks from any OSN.   # The endpoints of the OSNs shouldn't change much - if all are changed then a peer that joins the channel would not be able to connect to *any* OSN at all, and would have to rely on receiving blocks from other peers in order to obtain the latest configuration block that would connect it to the peers.   # When a peer that belongs to an org that wasn't a channel member at channel creation is joined the channel, the gossip layer rejects the join channel and is only updated via direct connection to the ordering service for that peer - FAB-5246   # The delivery service in the peer doesn't support dynamic update of OSN endpoints, and requires a restart in order to do that - FAB-5157   This item doesn't require special architectural change in order to address it ([~<USER> correct me if I'm wrong)    *Possible solution for 1,2,3:*   # I think that we should consider instead of joining the channel via the genesis block - joining the channel via the latest configuration block, (and have the MSP and anchor peers being configured with the latest configuration block data), with the following changes:   ## A peer that has joined the channel has the latest configuration block *i* would need to obtain blocks *0.. i, i* in order to start servicing requests, since it cannot populate the stateDB until it has a continuous ledger from *0* to *i*, and it also has to validate the raw ledger in an ascending order. I think this is actually a blessing in disguise, because currently, a peer that joins the channel can service clients even though it has old data in its stateDB, and this would prevent the peer from doing that.   ## A peer that has joined the channel and obtained the latest configuration block *i* would replicate the missing blocks *0 .. i* from other peers or from the ordering service by pulling blocks in a *descending* order ( i-1, --> i-2, --> ... 2, --> 1, --> 0) and for each block *j<i* it would verify the block by checking the previous hash on block *j+1* (which it would have at that point, since it would pull blocks in a descending order) and checking if the previous hash of block *j+1* matches the computed hash of the newly acquired block *j*. After this is done, the peer would validate the transactions from blocks *0..i* and would compute the stateDB (and the """"validated ledger""""). After this - the peer would start pulling blocks from the orderer/peers in an ascending order.   ## *We could also of course - not do backwards hash chain validation*, and instead - just make the peer use the latest config block and then pull blocks in the regular order from genesis up to the joinChannel block.   # Snapshot solution:[~<USER>: I see a few different ways to approach this:   ## *Simplest:* Whenever a peer receives a config block for a channel, it takes a copy on write style snapshot of the state database. The CSCC or other exposes some interface to retrieve the hash of this state snapshot, or the state snapshot itself. When joining a new member, the member asks """"enough"""" of the other members for the hash of the state snapshot at the most recent config block, and retrieves a copy of the state snapshot from someone. Then the peer is told to join the channel with the config block, the hash, and the state. The peer loads the state, verifies it against the hash, and is able to process further blocks based on the config block.   ## *More complex:* Same as (1), except instead of exposing the state snapshot via CSCC, allow the state snapshot to be gossiped, so that peer need only be bootstrapped with the config block and the hash of the state at that config block. This makes it much much easier to bootstrap a new peer, but, the very significant downside to this is the DoS possibilities, and the fact that it is difficult to handle the byzantine attack of a peer sending bad state. (Yes, you can detect it, but this implies retrieving an entire new copy of the world state, which might be quite large. Therefore, retrieving the state snapshot from multiple members is problematic, as any one can poison the state snapshot with bad data but only the final result is detected to be bad, forcing an entirely new copy to be retrieved).   ## *Most complex:* When a config block for a channel is received, the peers take a snapshot, compute its hash, and sign some block metadata about the hash of the world state. The peers then gossip about the hash, collecting eachother's signatures, and including them into their own block metadata. The config block could have a policy added, like """"SnapshotStateAttestations"""", which could be evaluated against the signature set to determine whether the threshold of signatures had been met. Then, to join a channel, the peer would only need the config block (with its additional gossip-ed metadata) which already contains this hash, and a way to retrieve the state (either supplied manually as in (1), or automatically as in (2).                       My gut feeling is that (1) or (3) with gossip of state would be best. IE, with (1) the process is simple to understand, simple to implement, but out of band and fairly manual.                       Or, with (3) + state gossip, the process is a bit magical, and complex, but, it would make joining a peer to 'catch up' as simple as joining one on a new channel.                       Additionally, we could relax the conditions for taking a state snapshot if this is too burdensome, for instance only taking a state snapshot when a particular config property is changed. So long as it is possible to determine if the config block should have a corresponding state snapshot, this is sufficient.         *Problem*: In both cases you have the last config block that is used to join the channel, but it may contain different root CA certs than was used in the past, and so - old blocks that the peer would get from the orderer or from other peers would contain endorsements or signatures on the block that the corresponding certificates no longer have a certificate validation path to the latest config block that was used to join channel with.   * I assume that if we could make the validation code (VSCC) in the peer use the config blocks on the chain, and not the config block that was used to call join channel with, it would be solved.   * If the state snapshot the joining peer would receive from other peers when it joins a channel would be created at the sequence of the last config block (the join channel block) - that would also solve the problem. But:  ** If this is done manually, it would only make the join channel process more complex and also - the state snapshot might be very large and hard to transfer.   ** If this isn't done manually, but via having the joining peer contact other peers and ask for a snapshot to be created, this opens a window for abuse and DOS attacks, unless the snapshot is taken on each config block  ** If a snapshot is taken on each config block, that might overload the peers of the system if a user would send several config updates (i.e - one that changes X, one that changes Y, etc.) but I think this should be considered and perhaps with guidelines to users (if you want to reconfigure a channel, do it via a single update and not via several) would be enough to solve the problem.       [~adc] [~<USER> [~<USER> [~<USER> [~<USER> [~<USER> [~<USER> [~<USER> [~<USER>""",Story,fabric
217855,"""As an exercise to understand how and where the Hyperledger Fabric code verifies and signs endorsements, transactions, blocks, etc., I conducted a basic set of tests modeled after the end-to-end CLI tests that are contained within the fabric code base and collected go profiling information.    The blockchain network consisted of two org (two peers per org) and a solo ordering service. There was no interaction with membership services, nor was TLS enabled.    peer0/org1 and peer0/org2 were targeted as gossip leaders by setting the *CORE_PEER_GOSSIP_ORGLEADER* environment variable to *true*.  This was done to determine the differences in profiling between an org leader that processes endorsements, new blocks from the ordering service, and block requests from other peers that only request blocks from the org leader.    What was surprising is that, on average, the Verify to Sign ratio was approximately 45:1 when reviewing the profiling information.  I have attached both text and graphical versions of the profiling information from all peers and the orderer.    The 30 second profiles were collected after the initial setup of the channel and chaincode were completed.  During the profile collection period, CLI commands were issued to peer0/org1 to invoke an example02 chaincode function that transfers assets between two users (*a* and *b*, where *a* transferred 1 widget to *b*).    I would like to run the same chaincode tests, but through the use of an SDK rather than the CLI to determine if the 45:1 Verify to Sign ratio is replicated.  The reasoning behind this test is that perhaps different code paths are taken (bypassing sign processing or extra verifies occur) when using the CLI and thus skewing what a real world scenario would encounter.    I will continue to investigate this high Verify to Sign ratio.""",Task,"fabric,fabric,fabric"
218255,"""The following messages are from peer.    Same INFO message appears multiple times in a second for days. It's not useful for bug diagnostic. Suggest to downgrade the message to DEBUG level. Also attaching Matt's suggestion on message logging level handle for your reference. Thank you!    ==========Suggestion from Matt===============  As an example..I would expect WARN or higher on a identity validation failure which Im sure is already there, and possibly some type of checking in message on the scale of minutes would be ok at INFO level, but every identity check from every peer, every 100-200 milliseconds at INFO level seems a bit much. We always have the option of bumping log levels to DEBUG if its necessary, which is why we made the suggestion on classification difference.   ===========================================         2017-05-18 02:18:49.007 UTC [msp] DeserializeIdentity -> INFO 18663f[0m Obtaining identity  2017-05-18 02:18:49.007 UTC [msp] DeserializeIdentity -> INFO 186640[0m Obtaining identity  2017-05-18 02:18:49.007 UTC [msp] Validate -> INFO 186641[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:49.033 UTC [msp] DeserializeIdentity -> INFO 186642[0m Obtaining identity  2017-05-18 02:18:49.033 UTC [msp] Validate -> INFO 186643[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:49.045 UTC [msp] DeserializeIdentity -> INFO 186644[0m Obtaining identity  2017-05-18 02:18:49.115 UTC [msp] DeserializeIdentity -> INFO 186645[0m Obtaining identity  2017-05-18 02:18:49.115 UTC [msp] Validate -> INFO 186646[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:49.126 UTC [msp] DeserializeIdentity -> INFO 186647[0m Obtaining identity  2017-05-18 02:18:49.126 UTC [msp] Validate -> INFO 186648[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:49.135 UTC [msp] DeserializeIdentity -> INFO 186649[0m Obtaining identity  2017-05-18 02:18:49.137 UTC [msp] DeserializeIdentity -> INFO 18664a[0m Obtaining identity  2017-05-18 02:18:49.138 UTC [msp] DeserializeIdentity -> INFO 18664b[0m Obtaining identity  2017-05-18 02:18:49.473 UTC [msp] DeserializeIdentity -> INFO 18664c[0m Obtaining identity  2017-05-18 02:18:49.474 UTC [msp] Validate -> INFO 18664d[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:49.484 UTC [msp] DeserializeIdentity -> INFO 18664e[0m Obtaining identity  2017-05-18 02:18:49.484 UTC [msp] Validate -> INFO 18664f[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:49.494 UTC [msp] DeserializeIdentity -> INFO 186650[0m Obtaining identity  2017-05-18 02:18:49.494 UTC [msp] Validate -> INFO 186651[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:49.500 UTC [msp] DeserializeIdentity -> INFO 186652[0m Obtaining identity  2017-05-18 02:18:49.500 UTC [msp] DeserializeIdentity -> INFO 186653[0m Obtaining identity  2017-05-18 02:18:49.504 UTC [msp] DeserializeIdentity -> INFO 186654[0m Obtaining identity  2017-05-18 02:18:49.504 UTC [msp] Validate -> INFO 186655[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:49.526 UTC [msp] DeserializeIdentity -> INFO 186656[0m Obtaining identity  2017-05-18 02:18:49.527 UTC [msp] Validate -> INFO 186657[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:49.539 UTC [msp] DeserializeIdentity -> INFO 186658[0m Obtaining identity  2017-05-18 02:18:49.539 UTC [msp] Validate -> INFO 186659[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:49.630 UTC [msp] DeserializeIdentity -> INFO 18665a[0m Obtaining identity  2017-05-18 02:18:49.631 UTC [msp] DeserializeIdentity -> INFO 18665b[0m Obtaining identity  2017-05-18 02:18:49.632 UTC [msp] DeserializeIdentity -> INFO 18665c[0m Obtaining identity  2017-05-18 02:18:49.632 UTC [msp] DeserializeIdentity -> INFO 18665d[0m Obtaining identity  2017-05-18 02:18:49.632 UTC [msp] Validate -> INFO 18665e[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:49.635 UTC [msp] DeserializeIdentity -> INFO 18665f[0m Obtaining identity  2017-05-18 02:18:49.635 UTC [msp] DeserializeIdentity -> INFO 186660[0m Obtaining identity  2017-05-18 02:18:49.635 UTC [msp] DeserializeIdentity -> INFO 186661[0m Obtaining identity  2017-05-18 02:18:49.641 UTC [msp] DeserializeIdentity -> INFO 186662[0m Obtaining identity  2017-05-18 02:18:49.641 UTC [msp] DeserializeIdentity -> INFO 186663[0m Obtaining identity  2017-05-18 02:18:49.641 UTC [msp] Validate -> INFO 186664[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:49.660 UTC [msp] DeserializeIdentity -> INFO 186665[0m Obtaining identity  2017-05-18 02:18:49.670 UTC [msp] DeserializeIdentity -> INFO 186666[0m Obtaining identity  2017-05-18 02:18:49.670 UTC [msp] Validate -> INFO 186667[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:49.679 UTC [msp] DeserializeIdentity -> INFO 186668[0m Obtaining identity  2017-05-18 02:18:50.055 UTC [msp] DeserializeIdentity -> INFO 186669[0m Obtaining identity  2017-05-18 02:18:50.055 UTC [msp] DeserializeIdentity -> INFO 18666a[0m Obtaining identity  2017-05-18 02:18:50.055 UTC [msp] Validate -> INFO 18666b[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:50.762 UTC [msp] DeserializeIdentity -> INFO 18666c[0m Obtaining identity  2017-05-18 02:18:50.762 UTC [msp] DeserializeIdentity -> INFO 18666d[0m Obtaining identity  2017-05-18 02:18:50.789 UTC [msp] DeserializeIdentity -> INFO 18666e[0m Obtaining identity  2017-05-18 02:18:50.790 UTC [msp] Validate -> INFO 18666f[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:50.799 UTC [msp] DeserializeIdentity -> INFO 186670[0m Obtaining identity  2017-05-18 02:18:50.799 UTC [msp] Validate -> INFO 186671[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:50.809 UTC [msp] DeserializeIdentity -> INFO 186672[0m Obtaining identity  2017-05-18 02:18:50.810 UTC [msp] Validate -> INFO 186673[0m MSP PeerOrg1 validating identity  2017-05-18 02:18:50.814 UTC [msp] DeserializeIdentity -> INFO 186674[0m Obtaining identity  2017-05-18 02:18:50.814 UTC [msp] DeserializeIdentity -> INFO 186675[0m Obtaining identity  2017-05-18 02:18:50.819 UTC [msp] DeserializeIdentity -> INFO 186676[0m Obtaining identity  2017-05-18 02:18:50.819 UTC [msp] Validate -> INFO 186677[0m MSP PeerOrg1 validating identity""",Story,fabric
218279,"""Because ChaincodeStubInterface is the central point of contact for chaincode developers, it's of utmost importance to fully specify the behavior that should be expected of the methods defined by that interface.  In particular, specifying conditions on the method arguments, and fully specifying what the return values mean and what the error conditions are.    Some methods' comments provide almost no information that isn't already conveyed by the method name itself.  For example: GetState, PutState, DelState.  In particular for these three, it's not clear what the behavior will be in the corner cases, such as if the key is not found, or if it's already present, or in what circumstances an error will be returned.  Having these all specified by the comments will facilitate chaincode developers in writing code that correctly handles all corner cases.    Some methods' comments don't provide enough in-source context for how they would be used – example usecases, etc – for example: CreateCompositeKey, SplitCompositeKey.  What is a """"composite key"""", and where do I use it?    Returning specific types from functions is a great way to self-document code, because if the return type is specific enough (e.g. GetQueryResult returning StateQueryIteratorInterface, which has its own documentation and specific example usages; or GetHistoryForKey returning HistoryQueryIteratorInterface, which is a similar concept but is fortunately represented by a distinct type), then less needs to be explicitly written in the documentation.    Contrast this with GetBinding which returns the """"transaction binding"""" as a generic ([]byte, error) value, which does not give any clear place to look for what format it is or how to use it.  Googling `hyperledger """"transaction binding""""` turned up the interfaces.go source code and the Protocol Spec doc which references code that's out of date.  At the very least, there needs to be comments to the effect of """"returns the transaction binding as a byte array in XYZ format, which can be used in functions A, B, and C to do operations P and Q (or whatever a transaction binding is used for)"""".    Generally to write good documentation it's necessary to have an accurate theory of mind for the reader – who are they, what context are they operating in, how much can they be expected to know, etc.  I would suggest that the target audience for interfaces.go should be non-expert programmers, familiar with golang, who have little DB or cryptographic knowledge/experience.  Then, give links to facilitate them to learn more about relevant topics with """"for more, see X"""" links in the comments.    Example code is also great, though for something as important as ChaincodeStubInterface, one can't and shouldn't be expected to deduce all corner cases from example code.""",Story,fabric
218805,"""PROBLEM  =======    Currently, endorsement policies are checked EXCLUSIVELY for the chaincode whose name is referenced in the ChaincodeId field of the ChaincodeHeaderExtension. However this is not the check that a chaincode developer expects, nor is it secure. The peer should instead check the endorsement policy of each ledger namespace (i.e. chaincode) that is being written to in the read-write set of the transaction. This fact would honour the invariant that     """"any write to a chaincode namespace has been verified against the endorsement policy of that chaincode""""    This it is a security vulnerability.    The attack can be perpetrated in a number of ways (I'll list only two below, more are likely to be possible):  1) from a cc with more permissive endorsement policies (cc-permissive) call a chaincode with stricter endorsement policies (cc-strict); the ensuing transaction can modify the state of cc-strict, but only the endorsement policies of cc-permissive are checked. The attacker has successfully bypassed the endorsement of cc-strict;  2) we have to expect that the default endorsement policy will be used in >0 cases (otherwise we should remove it); we also have to expect that in a channel, the default endorsement policy might exist alongside stricter endorsement policies (otherwise we should forbid it); for all those cases, with this vulnerability in place, any org can make arbitrary writes to the ledger as follows: produce a tx out of thin air with the desired rwset, endorse it, submit it as an invoke of their own cc; the attacker has successfully bypassed the endorsement policies of that channel; note that arbitrary writes include writes to system namespaces such as lscc's;    FIX  ===    The validator code should perform the following checks    if tx invokes scc {    do scc-specific validation as it is today  } else {    foreach namespace in rwset.WRITES {      if namespace is a system namespace {        tx is invalid      }          invoke lscc to extract vscc and endorsement policy for the chaincode that corresponds to that namespace;          invoke the returned vscc with the returned endorsement policy to validate the tx;      }  }  """,Bug,fabric
218813,"""The goroutine check (checks that goroutines are dead after each test) sometimes starts with the wrong number of goroutines and has a false-negative (i.e, started with 4 goroutines but has only 3 of them in the end of the test)  so it fails the entire unit test run.    I want to temporarily disable the test and investigate.""",Bug,fabric
219238,"""As a fabric tester or support personnel, it is difficult to understand why a transaction fails validation in the VSCC, even with DEBUG logs, so debugging issues is very time-consuming. At the point of failure, it would be much more helpful to see the following information displayed (maybe at the INFO level?):  * the policy being used to validate the identified transaction  * the names of the signers (peer name, and associated org name). At one point I saw a log with the hash signature of signer(s), but that is difficult for tester to understand and trace.  * the names of signers/orgs that are lacking    For example, if the policy is """"AND('OrgRedMSP.member','OrgBlueMSP.member')"""" then a TX must be signed/endorsed by any one member of OrgRed or any one member of OrgBlue. If peer00 is in OrgRed and peer00 endorses a transaction, and there are no other signatures, then the log could state something to the effect of the first part being satisfied (peer00 = OrgRedMSP.member) however the validation failed due to lacking signatures from any OrgBlueMSP.member.    And I would hope the same information could also be provided to the SDK or any process that registers for event failure notification.    Existing log:    peer2        | 2017-03-02 21:50:57.835 UTC [txvalidator] VSCCValidateTx -> ERRO 001 VSCC check failed for transaction txid=de8c2db2f9bce9be2ab2864339a08862ab9de8280c2c0b919930eb22fbaecf94, error VSCC error: policy evaluation failed, err Failed to authenticate policy  peer2        | 2017-03-02 21:50:57.839 UTC [txvalidator] Validate -> ERRO 002 VSCCValidateTx for transaction txId = de8c2db2f9bce9be2ab2864339a08862ab9de8280c2c0b919930eb22fbaecf94 returned error VSCC error: policy evaluation failed, err Failed to authenticate policy  """,Story,fabric
219440,"""Couple of points:     1) More iterative updates    2) We need to have the major headings in a different color to pop out on the index board on left    3) The transaction model topic, security model, multichannel, smart contracts, consensus topics need a overall heading- it's too messy for the overview section otherwise.    4) The overview section needs to be more about """"why"""" than just technology overview.  This is missing from a lot of the sections.  this is the feedback I got in Hyperledger hackfest in December So as an example- Yes, what is multichannel- great, but why do we need it?      In some cases, they will only need one channel- everyone will be able to see, say images.  These images allow people to determine if the details they want. So it makes sense that any company can be on the blockchain to see the data.  Others have clearly stated they need Channels- they need 3 people on a channel to do bilateral communications.  They do not even trust encryption to keep the data private and fear that because these transactions may even occur over a 10 year period, that if encryption is broken, everyone being on blockchain is not enough. As a result, multichannel helps with data partitioning.. data only goes to those who need to know.    d) With my comments in above, we need to ensure we cover  the following beyond multichain (which can exist in current sections).  Many of these or competitive advantages that we have and are ahead of the curve on.  I've added a few words here to help    §Plug-ability to enable flexibility in development and deployment   –Identity (MSP) – bring your own identity, support for multiple providers    –Consensus – no one size fits all algorithm     –Data Store – support various data store models (KV, NoSQL, RDBMS; Complex query / transaction history on an asset)--- (Clearly the importance the level db implementation makes it very difficult to perform complex queries and transaction history)    §Native SDKs: SDK specification for developers implemented in any language (Node.js, Java initial support--- people need different support given the skills they have on board     §Access control on who can query or submit transactions    §HSM (Hardware Security Module) support: Modified and unmodified PKCS11 for t-cert generation ->HSM allows for an added layer of security protection that is needed with solutions    """,Bug,fabric
219465,"""We need to describe at a high level how Fabric accomplishes work such as transactions and security.    Nick/Josh,    Here is my feedback on transaction data model.  I honestly think the section would really be called data model possibly- not transaction data model.. .There are transactions and transaction flow... and there is data model. Outlining what is the data on the immutable ledger is quite important.  So, I walked away thinking the structure and what it covers doesn't feel quite right to me for transaction data model and hard to follow the way it starts out.  If I were to lay out topics in this section, I would do the following (see in bold with comments behind each):  (I think you can use the material you have also written, but i think there is more to do here)    - what is the model of the data: You define the data as Assets and Asset Registries is the data to be stored and exchanged across the business network.. This is NOT crypto currency only.. you can trade money if you define the asset as money, if asset is pianos that move around the world,.. .  Now one can do UTXO mode which has a particular format (ie entry is spent/not spent,.. it must add up every time in a transaction-Chris Ferris should comment  here on how he likes to define this here)    - The ledger : it is immutable, it is read/write sets that are signed transactions.  Ledger can be shared across all in the network, or you can have ledgers that are only for a set of participants (relating to channel). We should probably mention here that if you want to encrypt your payload- you can and what """"encrypt this"""" means. (ie is it SHA0-256,etc,.?)    The Hyperledger-fabric data model allows for (here's where I would include bullet points in no great order at this point)  -properties (things like immutable, it's file based,.)  - all the things actually on the ledger in detail: configuration block on the ledger, read/write set,etc,.  - transactions are ordered coming from consensus  - how couchdb level db are related to it  - what goes on the ledger is validated (ie goes through VSCC, but explain in human layman terms) pointing to below section  - ask chris what else to ask here    Chaincode-Here is explanation  of chaincode related to data model-- What is put on the ledger is enforced by chaincode which has a series of transactions.  the Transaction is proposed to move asset a from bank 1 to bank 2.  Maybe defining this at a high level as it relates to the data model.    PArticipants  I think you do need to describe here who can access what data, based on being a permissioned network.    Transactions    Transactions are used to read and write to the ledger and going through the actual flow of transaction proosal, send to orderer, get an event to validate what's on the ledger or a failure of transaction going onto the block (be careful how i've identified events here)  This is where you go through the transaction flow, particularly explaining transaction ordering and transaction validation  """,Task,fabric
219474,"""As a developer, I need a Python SDK, so I develop application with NodeJS to interact with blockchain. """,Story,fabric
219475,"""As a developer, I need a Java SDK, so I develop application with NodeJS to interact with blockchain. """,Story,fabric
219476,"""As a developer, I need a Node SDK, so I develop application with NodeJS to interact with blockchain. """,Story,fabric
219479,"""As a application developer, I need to understand various SDKs, so I develop application code that interacts with Blockchain""",Epic,fabric
219709,"""As an infrastructure developer I want o be able to define and leverage multiple MSPs (or roots of trust) on the same channel   This item relates for the fabric infrastructure to support multiple MSPs within the same deployment. This includes  - ability to define / configure multiple verifier MSPs in the genesis of a channel  - ability to leverage these MSPs for defining access policies for chain/chaincode rights  - ability to re-configure verifier MSPs in a secure/modular way within a channel    The work described here relates to the design associated to multi-MSP support from an access control perspective, but also of chain reconfiguration as well as the implementation of the respective components.  """,Epic,fabric
219837,"""The GRPCServer currently has APIs to append/remove root certs but [~<USER> suggested that he'd like the orderer to maintain the list of CAs in its entirety and then simply be able to complete replace the list.  The peer can share this capability as well.    I'll leave the append / remove APIs as append might be useful in the case where we create / join a new channel for efficiency""",Sub-task,"fabric,fabric"
219848,"""h3. Introduction  The current deployment model has the following properties with regards to chaincode code (as opposed to chaincode data)  * the code is deployed on a per-channel basis  * the code is stored on the ledger for the channel    There are a few issues with the model  # the deploy transaction is large and has performance issues  # a chaincode redeployed on different channels results in different instances of the container which could put a strain on infrastructure when this duplication becomes prevalent  # by virtue of the code being on the ledger, all peers in the chain have access to the chaincode. There is no mechanism to run a chaincode on a subset of peers - for example, only on those that are endorsers of this chaincode.    While (3) may sound like a good idea, in most cases it does not make sense (in a """"Car"""" blockchain a Registration-Chaincode may belong to both DMV and Insurance but not to Dealership, while a CarAsset-Chaincode may run on all peers).    This proposal is to change the deploy semantics to _install_ on a peer instead of on a channel but at the same time _invoke_ the chaincode in the context of a channel (akin to program code being allocated once in memory once but data section for many processes that are launched for that program).    This will result in greater flexibility  * given a channel a chaincode can be deployed to a subset of the peers on the channel  * reduce load on the infrastructure to run multiple instances of the chaincode for different channels  * reduce communication overhead due to large chaincode binaries     The only downside is we will lose the ability to do """"one-shot"""" deployment of a chaincode onto all the peers in the channel. Given other benefits (and hopefully not being a common scenario) this is likely not a big detraction.    While this is a change to the """"deploy"""" model, it has implications for upgrade as well.  Rest of the proposal deals with the details of deploy and upgrade.    h3. User level changes to deploy  * a new """"install"""" command which installs the chaincode on the peer (basically puts the packaged chaincode on the file system)  and not on the channel  ** {{peer chaincode install -n <chaincode id> -v <version> -p <path to chaincode>}}  * an new instantiate command to associate the chaincode with a channel  ** {{peer chaincode instantiate -C <channel id> -n <chaincode id> -v <version> -p <path to chaincode> -c <init args>}}    +High level implementation and design details to deploy+  * chaincode code is not stored on the chain ledger but on the peer (say on the filesystem)  * The instantiate command will also update the LCCC for that chain. The command will also run the """"Init""""    function of the chaincode on a given chain.   * The LCCC entry for the chaincode on a channel will contain the hash of the chaincode in addition    to other fields such as name and location of the chaincode. This will be used to detect changes    h3. Upgrade  Notation  * channels are x, y, z  * peers are P1, P2, P3  * chaincode is mycc  * hashcode of version 1 chaincode mycc is h1-mycc    Hence, *mycc:h1-mycc/x* refers to version *h1-mycc* of chaincode *mycc* on channel *x*.    Version rule - multiple channels can run different versions of a chaincode but given a channel only one version of a chaincode can be running on it at any given time. ie,  # mycc:h1-mycc/x and mycc:h1-mycc/y - same version of a chaincode, different channels - OK  # mycc:h1-mycc/x and mycc:h2-mycc/y - 2 versions of a chaincode, different channels - OK  # mycc:h1-mycc/x and mycc:h2-mycc/x - 2 versions of a chaincode on one channel - NOT OK    Peer constraint that follows from above (as a channel encompasses many peers)  * If P1 successfully initiates upgrades to a new version on channel *x*, all other Peers have to upgrade for successful invocation of the chaincode. We will be implicitly violating 3 above.    There will be two commands related to upgrade  * {{peer chaincode install -n mycc -p <path to mycc with version hash1-mycc>}} - this will put the chaincode on the peer's filesystem.   ** this command will fail if that version exists on the filesystem  ** note that this command does not involve ledger or channel in any manner  * {{peer chaincode upgrade -C x -n mycc -p <path to mycc with version hashn-mycc> -c <init args>}}  - this will upgrade the channel  ** this command will fail if that chaincode does not exist on the filesystem  ** this command will fail if the chaincode has not been instantiated on the channel  ** this command will fail if the running version matches the upgrading version for the channel  ** on successful commit this will update LCCC for the channel to *mycc:hashn-mycc*      {code:java}  Note on the effect of upgrade on inflight transactions    The scenario - many proposals are happening on the chaincode on channel x at the time of upgrade of a chaincode. Some of them end up as transactions before the block on which upgrade turns up, others after and a few on the same block.    For consistency and determinism, enforcing the following rules will be sufficient       - invokes will be stamped not only with chainID but also version. This will thus be part of the MVCC rw-set      - at commit time, all transactions for the *mycc:--/x*  will be invalidated on the block on which there's an upgrade (they had to be on the previous version of the chaincode)      - at commit time, all transactions for the *mycc:<prev hash>/x* on previous block will follow normal processing.      - at commit time, all transactions for the *mycc:<prev hash>/x* will naturally fail MVCC validation on succeeding blocks    Note that successful commit of *mycc:hashn-mycc/x* on P1 will not bring down other *mycc:--/x* running on P1 as these may be still running on other channels. See next section on *Stop chaincode*    {code}     h3. Stop chaincode    As noted in the previous section, chaincodes may still be running and deployed even if not in use in any channels. Rather than do GC of some sort, we will provide a {{peer chaincode stop [ --destroy ]}} command to stop the chaincode and optionally remove it from the filesystem. Symmetrical to """"deploy"""" command, these do not involve channels or ledger but simply will remove them from system. A """"deploy"""" will put the chaincode back on the filesystem.    h3. User Scenarios  +I want to install a chaincode to a set of peers+  This is the standard happy path for deploying chaincode. Basically log into each peer and install the chaincode    +I want to first install a chaincode to a set of peers and then instantiate the chaincode on different channels A, B and C+ (instantiation can only be done on the peers the chaincode is deployed upon)  This is the standard happy path for instantiating a chaincode on channels    +I want to install a chaincode on another Peer 2 after it has been instantiated on the channel by Peer 1+  Install is independent of the channels. The instantiation of the chaincode on a channel will have no effect on the deployment on a Peer 2. The chaincode is inaccessible on Peer2 till it has be been installed on it.    +I want to reinstantiate a chaincode on a channel (say from a different peer which instantiated the chaincode)+  This should fail as a chaincode cannot be reinstantiated on a channel (the LCCC for the channel will have  a chaincode entry)    +I want to invoke a chaincode on a channel on Peer2 when it was instantiated on the channel by Peer1+  It will fail if it was not installed on Peer2.  It will fail if the invoke causes the chaincode to be brought up but the hash does not match what's on the LCCC.    +I want to install *mycc:hash1-mycc* on P1  It will fail if  *mycc:hash1-mycc* exists on P1    +I want to upgrade to *mycc:hash2-mycc/x* via P1  This will fail if   ** chaincode mycc is not running on x  ** *hash2-mycc* version of chaincode not deployed on P1  ** current version same as hash2-mycc    NOTE: we will add other scenarios  """,Story,fabric
220108,"""The behave tests for the Orderer Kafka service use x86 docker images for both Kafka and Zookeeper, both of which are needed to test the Kafka ordering service.      As a result the behave tests associated the the Kafka ordering service fail for z and p.    I would suggest building upon existing arch-specific docker images that are built during the fabric build process.""",Bug,fabric
220257,"""As a hyperledger fabric project representative, I would like to explain and show with visual aids and scripts to collaborators and potential customers the present architecture and behavior of fabric v1.0 (as of the end of Nov, approx sprint 6).""",Story,fabric
220274,"""As a developer, I do not want RYW semantics for `GetKey` API in KV ledger. Because, chaincode execution is a proposal to update the ledger, it does not actually update the ledger and therefore the writes are not available for query.  Also, this will not be available for couch, and therefore we need to be consistent across state databases.  For the sake of consistency and simplicity, I would like to remove RYW semantics from 'GetKey' API.""",Task,fabric
220298,"""* As a Fabric deployer, I would like to maintain data such that only its evidence is exposed to the chain, ordering service, and channel peers while the data itself is disseminated to peers based on policy, so that we can achieve finer-grained data confidentiality for transactions while still maintaining ledger consistency and still being able to leverage Fabric for both data evidence and dissemination of the data (but in a more private fashion).    *Design*     Slides attached.""",Epic,fabric
220341,"""As an owner of a peer on the BC network, I need the ability to upgrade the peer if there is a change in the protocol without loss of data.     With lifecycle chaincode, I can upgrade the chaincode. I also have the ability to apply patches/fixes if it does not apply to the protocol change itself via the SCC. Eventually I will have to accommodate this sort of change, in case there is a security vulnerability or something else that requires a major alteration. """,Story,fabric
220435,"""In the Kafka orderer, the routine sendBlocks() in client_deliver.go is implemented as a """"hot idle"""" loop. Every client that connects to the orderer for delivery service will spawn a goroutine that executes this loop, and as long as the client has """"push tokens"""", this loop will never terminate. I've attached a screenshot of """"top"""" showing that an """"idle"""" Kafka orderer - that is, one that is not actually delivering any blocks - is consuming 11.24 HW thread-equivalents on this POWER8 system when serving 100 delivery clients.    Also interesting is that 47.8% of the CPU is reported as """"waiting"""" by the OS. This is why we are not seeing even higher CPU utilization here, and is also pointing to a potential problem. The problem is that each delivery client shares a common `cd.deadChan`. When Go executes a `select`, it operates in a critical section that begins by locking every channel named in the select.  What is happening here is that every goroutine is simultaneously competing for the lock on `cd.deadChan`. Unable to immediately get the lock, the goroutines block (sleep) until it is their turn, and the OS is marking this sleep as I/O wait time. If the `cd.deadChan` case is commented out, CPU utilization can go as high as 100% of the entire machine if enough clients are connected (even though no actual work is being done).    I've attached a proposed patch idea for the first problem. Go allows `select` cases to be NIL. So instead of the 2-level scheme currently used, we can simply select on a variable that is NIL when the client has no tokens, and is set to the Kafka message channel when the client has tokens.    I have not yet looked for evidence of the global lock being an issue if this loop is event-driven. It's something to be aware of though, especially given the way that the Go runtime implements `select`.""",Story,fabric
220685,"""Ledger versioning scheme: I want to use the block/transaction height as a variable's version, instead of an incrementing version number, so that I have traceability between state data and transaction data""",Story,fabric
220866,"""As an infrastructure developer I want to be able to substitute crypto library    This requires the following tasks take place:    1. Define a modular crypto library interface that would satisfy the crypto needs of membership services, peer and client code,    2. Re-factor the code implementing crypto functionalities in HPL/fabric within a software crypto provider that implements the interface specified in (1)    3. Update the HPL/fabric crypto calls to use the specified API and use the library provided by (1) and (2).  """,Epic,fabric
221028,"""As a developer I need the ability to specify who can sign off(endorse) my transactions and how many signatures (3 out of 5, 5 out of 7, etc) are needed before submitting my transaction so that I can eliminate a single point of authority and/or failure while allowing only endorsers I trust to simulate my transactions.     * Implement basic endorsement policies. Basic endorsement policies should be very static to begin with, e.g., a fixed set of endorsers' signatures is needed to endorse a transaction (e.g., t endorsers out of n).  * Explore more robust endorsement policy specifications (but not too robust - we do not want another chaincode in the endorsement policy).""",Epic,fabric
221029,"""As a developer I need the ability to access a state cache so that we can help submitters quickly build the rw-set. This will also lead to better performance. """,Story,fabric
221032,"""As a chaincode developer I need the ability to alter or move my chaincode to a new version without having to manually copy and redeploy from start so that I can speed up my development and keep patching quickly and efficiently. """,Epic,fabric
221065,"""Story: As a developer I need more flexibility with the ledger, so that I can utilize alternate databases, have interfaces for advanced queries, and backup capabilities.    Initial external database will be CouchDB. This capability will be introduced as a beta initially, so that community feedback can be gathered.    See attachment slides for details. Broader Ledger context can be found in the Ledger slides FAB-758.""",Epic,fabric
221091,"""As application developer, I want an easy way to deploy and test my app by only having a single orderer process.    This refers to the 'Solo' ordering process.  This will be a simple to deploy, minimal code ordering process.  Because there is only one process, no 'consensus' need be reached between multiple processes, so this is largely as simple as having multiple clients write into a buffered channel, with a single thread reading it, and creating batches/blocks.    The deliver side is only slightly more complicated, as to allow for disconnection and reconnection, the orderer service must allow history to be discovered.  This requires implementing a simple in RAM ledger, and allowing individual clients to seek within it.  """,Story,fabric
221095,"""As an application developer, I want documentation for upgrade flows""",Story,fabric
221096,"""As a chaincode developer, I want use life cycle system cc to manage deployment policies for chain codes""",Story,fabric
221098,"""As an application developer, I want to provide friendly name to identify a chain code, so I can more easily manage and debug chain code.""",Story,fabric
230261,"""I understand that there's some thinking going on about possible rearrangement of the components of the prototype TAP browser UI, and in this context I heard a remark that the upper-left pane, that is, the schema-and-table selection pane, has some blank space that we could think about reclaiming.    This is to point out that at some point we'll want to facilitate joins in the UI, based on the foreign-key information in {{TAP_SCHEMA}}.    The currently empty space below the """"Tables"""" pull-down would be exactly the place for this.  I imagine that below that pull-down there would be UI elements for selecting a table to join to and the keys to use for the join.    I've started a sketch for that which I'll upload as an image.    Once the join is defined, Firefly would have to determine what the joined tables' effective schema would be, either by analyzing the schemas of the two tables and computing the combination itself, or by issuing a {{MAXREC=0}} query for the join against the TAP service.  With that in hand, the UI's table-of-columns-and-constraints (the current bottom half) could be populated with the joined schema, and then all its other features (e.g., column selection, constraint definition) would proceed as usual.  The spatial-constraint and temporal-constraint search helpers (upper right pane) should also be able to work against the joined schema.  This is all pretty well-defined and users would have very natural expectations.    Getting the joined-table schema itself defined may be the hardest part of this whole task.    *Implementing this is NOT an LSST close-out task* (sadly).  But we should not give the UI space away without at least taking this into account.""",Story,"SUIT,Firefly"
230516,"""There appears to be a github credentials problem (it is possible this was a transient problem -- I'm unsure of how the reporting works) when jenkins is trying to manage webhooks.     !image-2019-02-04-15-40-53-061.png|thumbnail! """,Bug,Developer Infrastructure
232295,"""daf_butler has a really nice, robust function for importing a symbol given a string.  I want that in afw for importing PupilFactory classes in cameraGeom, so I can replace a Python class object with a string when moving Camera to C++.""",Story,utils
232915,"""One of the topics on the near-term development list for the Firefly Python API was associated with providing Python access to the highlighting and selection of rows in a Firefly table.    (For non-Firefly-geeks reading this: """"highlighting"""" is what happens when a row in the table viewer or a corresponding data point in a plot or image overlay is clicked.  """"Selection"""" refers to the activation of the checkbox displayed at the head of a row in the table viewer; this can also be achieved by the use of the """"check mark"""" button in a plot based on the region-selection tools.  Only one row can be """"highlighted"""" at a time, whereas multiple rows may be """"selected""""/checked.  We are planning to rethink the UI mechanics of all of this at a later date.)    h1. Proposal    [~<USER> and I discussed this at length yesterday.  We propose the set of capabilities outlined below.  Initially these would be provided in {{firefly_client}}.  After exposing these to users, getting some feedback, and possibly rethinking, we would     h2. Callback on highlight    It should be possible to use the Python API to receive a callback to a Python function when the highlight is changed.    It should be possible to configure the callback to return either the entire row or a specified subset of columns.  It should be possible to request any table column by name, and it should also be possible to request the Firefly-assigned invariant row ID (the one that sticks to the original row even if the table is sorted or filtered).    It would be desirable, but perhaps not required at first, to also be able to request return the selection status (i.e., is the checkbox checked?) for the highlighted row.    The Python model for specifying the columns to be returned should be a Python *set* of column names as strings, e.g., an optional argument to the API for establishing the callback:    {code:python}      columns = { 'ra', 'decl' }  {code}    For the Firefly invariant row ID and for the checkbox status, however, I would suggest using a typed constant rather than a string, so that it would look something like:    {code:python}      columns = { 'ra', 'decl', ROW_ID, SELECTED }  {code}    [~<USER> will comment on what the most Pythonic way to accomplish that would be.  I do not want to depend on magic column names in Firefly for those attributes and make the user supply them as strings.  (This is an area where it may be difficult to figure out how to put this into {{afw.display}} later on, as these concepts may not be present in other back ends.)    We are anticipating that the primary use case for this callback will be in cases where the table was originally loaded from Python and is still available in memory in the Python process, so the most natural and efficient way to implement a Python action driven by the Firefly selection may be to allow only some single column such as the """"source ID"""" or """"object ID"""" to be specified by the user to be returned, with the callback then using that to index into the tabular data it already has.    A slightly lower-priority use case for this will be where the table was initially loaded on the Portal (Firefly) side.  In this case it may be more natural to transport all of the columns needed to implement whatever the callback action is.  (But note that in this case it may not be obvious how to connect the callback to the correct table - how will the Python user know the {{tbl_id}} to use?)    The data structure returned should be, or at least include, a Python dict paralleling the Python set used in the configuration of the callback, e.g.:    {code:python}      { 'ra': 9.32489, 'decl': 7.31042, ROW_ID: 6812, SELECTED: False }  {code}    This is the highest priority part of this ticket.    h2. Query rows based on highlighting or selection    It should be possible to ask Firefly for the content of a loaded table, and after some discussion, we decided that the capability to request only the highlighted row, or the set of selected rows, should be integrated with this.  We suggest that the API for requesting table content include optional arguments along the lines of:    {code:python}      columns = { 'ra', 'decl', ROW_ID, SELECTED }      limit = HIGHLIGHTED  # or limit = SELECTED  {code}    The requested content could be returned as a list of dicts, preserving, if possible, the displayed sort order of the requested rows.  If this is done, the dict returned for a row should have the same structure as for the one in the return to a highlight-callback function above.    However, since this is a bulk data interface, it may be more appropriate to consider using an existing Python data table model such as a Pandas DataFrame or an Astropy table.    Again, [~<USER> will think about this a bit more and comment below.    This is the second priority part of the ticket, but should still be attempted as an August deliverable.    h2. Query by row ID or by row content    It should be possible, given a suitable primary key for a table - either the Firefly-assigned invariant row ID or a primary key internal to the table content - to request the matching row, and more generally it should be possible to request the set of rows matching a column predicate.  (E.g., request all rows for which """" {{filter_band = 'u'}} """".  We should try to do this in a way that is as similar as possible to the way similar features are supported in other familiar Python APIs such as Pandas.    Note that since the internal Firefly tabular data model is a relational database table, ultimately an SQL {{WHERE}} clause could be supported for this API.    This is a substantially lower priority and can be deferred beyond August and to its own ticket.  However, it should be implemented in a way compatible with the preceding API.    h1.  Future directions    h2. Callback triggered by UI action    In addition to highlight-by-click and checkbox-selection, Firefly currently has a third way to identify data points for further action: drawing rectangles and circles, on images and/or on X-Y plots.  It is possible to turn a set of points identified in this way into a (multi-row) """"selection"""" with the """"checkbox toolbar button"""" in the plotting UI.  Firefly also makes it possible to extend the UI with buttons connected to optional actions.    It's a concern for user education that this is in fact a third selection-like mechanism in Firefly, and that's something we may want to address in future UX-improvement work.    In the mean time, it would be useful to make sure that the Firefly Python library does have good APIs for interaction with point-set identification of this nature.    """,Story,SUIT
233586,"""LSST requirement DMS-PRTL-REQ-0047 states that the Portal Aspect (i.e., Firefly) should provide a """"property sheet"""" interface to an individual row of data from a table.  {quote}*Specification:* The Portal aspect shall permit the inspection of all the data in a single row of a tabular data query result as a """"property sheet"""" for that row, taking advantage of available metadata to supply units and other semantic information for each column value.    *Discussion: Resources permitting, the property sheet may be elaborated to provide additional functionality (typically, further queries) associated with particular data items displayed.   Property sheets should, where enabled by metadata, appropriately exhibit relationships between columns, such by displaying a value and its uncertainty together.   The system must provide a generic property sheet functionality for any table for which full metadata is available. It may also provide custom property sheets for commonly-queried tables such as Object, ForcedSource, Visit, etc. that provide a more scientifically useful layout, and additional available workflows, than possible just from the metadata.  {quote}  With the related idea in DM-11154 in mind, I would like this facility to include the capability to execute the operation """"show me all the information available for this object"""". That is, if the original tabular query was for a limited set of columns, and a property sheet for a row from this query result is requested, there should be a UI element that permits requesting the full set of available columns (i.e., a """"SELECT *""""). Some design work is required for how this would handle situations where the tabular query arose from a join or other more complex query.    [~<USER> may want to assign me a subtask to write a more detailed specification for this task.          """,Improvement,"SUIT,Firefly"
234219,"""In the current irsadev version of the HiPS viewer, it appears that the FOV display is either in degrees or in arcseconds, represented by the '°' and '""""' symbols, respectively.  It does not have an intermediate display of arcminutes.  Before we go on to apply this to all image viewers, not just HiPS, we should agree on what we want.    Personally I would go for adding the intermediate layer.  I find it a bit odd to see """" 2711"""" """" as a field of view, instead of """" 45' """".  Also in other Firefly and IRSA application contexts, such as the _selection_ of a field of view, the arcminute option is presented.    The same logic should apply consistently throughout the Firefly world, I think.    Specifically: when the value is between 1 and 59 arcmin, display it as arcmin (with the ' symbol) with two digits of precision, i.e, as 1.0' through 9.9' or 10' through 59'. But if there is existing practice for something slightly different elsewhere in Firefly, it would be reasonable to re-use existing behavior.    Derives from IRSA-1606 and IRSA-1628.    Aimed at the May 2018 release.""",Story,Firefly
234820,"""This is in the test build [~<USER> is running in response to DM-13099.    I've queried two tables on PDAC: AllWISE catalog sources and WISE All-Sky (4 band) single-epoch sources.  I've identified a cluster of sources in the single-epoch data that I suspect come from a single object, and I'm trying to identify that object in the AllWISE catalog.    I drew a box around the cluster in the single-epoch data and used it as a filter.    I then tried to copy the filter specification from the """"filter panel"""" dialog from the single-epoch table:     {quote}""""ra"""" > 0.9909882575279574;""""ra""""  < 0.992205622144179;""""decl"""" > 0.0072994581062348135;""""decl""""  < 0.009400119782442063""""{quote}    to the filter panel on the AllWISE table.  That didn't work at all - a separate problem? - so instead I copied the two expressions from the column headers from the one table to the other: """"> 0.9909882575279574; < 0.992205622144179"""" for {{ra}} and """"> 0.0072994581062348135; < 0.009400119782442063"""" for {{decl}}.    That worked, and the row I wanted ended up selected in the table.  However, the selection was not successfully applied to the coverage image for the AllWISE table, only for the single-epoch table.  See screenshots.""",Bug,Firefly
234928,"""Once a tabular data result is available in Firefly, there are currently several ways to apply filters to the resulting data.  Notably:   * One can enable the column-filtering table header, and type in expressions there like """"< 3.6"""".   * One can open the column-metadata dialog in the table viewer, which displays information about each column in the data table as a row in the dialog, and includes a field in each row for applying a column filter - and displays any existing column filters applied via the table header.   * One can apply a free-form set of filter expressions in a field at the bottom of the column-metadata dialog.   * One can select regions of data in an x-y plot and then request that that selection be used as a filter.  For a rectangular region {{x_min, x_max},\{y_min, y_max}} this results in the application of two filters to each of the columns x and y.   * One can view the column filters from the """"show filters"""" button on an X-Y plot. (Not any more, 6/27/18 XW)   * In the resulting dialog, one can apply a free-form filter expression.    There are aspects of this that are not implemented uniformly across the different types of filter.  This ticket asks that these be changed to increase the uniformity and orthogonality of the user experience of filtering.    In particular:   * The dialog box for column metadata is accessed via the """"gears"""" icon on tables but via the """"funnel"""" icon on plots.  While I understand the motivations that led to this (e.g., """"gears"""" is an appropriate concept for the choose-which-columns-to-display function of the dialog that appears for tables, and is parallel to the choose-columns-to-plot concept that the plot viewer's """"gears"""" brings up), the net result is a problematic UX for """"show me the filters that are currently in place"""".  I do not have an immediate suggestion for what to do about this.   * The dialog boxes that appear are close in appearance, but still differ in ways that go beyond the fact that the one opened by the """"gears"""" icon on tables has the additional functions of column selection and control of units/filter display in the main table.  For instance, the table-""""gears"""" one can be dismissed by an """"x"""" in the upper right, while the plot-""""funnel"""" one can only be dismissed by clicking on the funnel again.  Also, only the table-""""gears"""" one has the """"Reset"""" action available.   ** Note also that the """"Reset"""" action is drawn as a link (blue text) whereas the """"Reset"""" on the plot-options dialog is a button.  But the plot-options dialog is a matter for another day.   * Plot selections via drawn rectangles allow their use for either _row selection_ or _filtering_ depending on whether the """"check"""" or """"funnel"""" action icon is selected.  Column-header (and column dialog) selections can only be used for filtering.  This is a serious break of orthogonality and is actively frustrating when doing exploratory data analysis.  It is very useful to be able to set up an x-y plot and then mark a subset of the points in the plot based on an additional selection predicate.  (This is not to be confused with the similar capability of _coloring_ points, or changing their plot symbol, based on values of additional columns.)  I don't have an immediate suggestion for the UI for switching between selection and filtering based on a column predicate, but I believe this is a very important point to address and I'll think about it more.""",Improvement,Firefly
235559,"""This ticket is to implement the final change from RFC-397. I've tweaked the wording slightly here to clarify the single/multiple inheritance point.    This is the replacement version of the section of the python style guide about {{super()}}:    {code}  `super` MAY be used with care to call inherited methods    Python provides `super()` so that each base class’s method is only called once. Using `super()` ensures a consistent Method Resolution Order, and prevents inherited methods from being called multiple times.    In Python 3, `super()` does not require naming the class that it is part of, making its use simpler and removing a maintenance issue.    The trickiest issue with the use of `super()` is that, in the presence of multiple inheritance, there is no way for a class to know for certain which inherited method will be called.  In particular, this means that the calling signature (arguments) for all versions of a method must be compatible.    As a result, there are a few argument-related caveats about the use of `super()` in multiple inheritance hierarchies:    * Only pass `super()` the exact arguments you received.  * When you use it on methods whose acceptable arguments can be altered on a subclass via addition of more optional arguments, always accept `*args`, `**kw`, and call `super()` like `super().currentmethod(alltheargsideclared, *args, **kwargs)`. If you don’t do this, forbid addition of optional arguments in subclasses.  * Never use positional arguments in `__init__` or `__new__`.  Always use keyword args, and always call them as keywords, and always pass all keywords on to `super()`.    To use `super()` with multiple inheritance, all base classes in Python's Method Resolution Order need to use `super()`; otherwise the calling chain gets interrupted.  If your class may be used in multiple inheritance, ensure that all relevant classes use `super()`, including documenting requirements for subclasses.    Following these guidelines for single inheritance hierarchies, will let them be extended to multiple inheritance with minimal difficulty.  {code}""",Story,Developer Infrastructure
236970,"""I would like the Firefly tabular-query-result support to include the following capability, if it doesn't already:    If the query result contains a column or columns that constitute a unique key, based on metadata returned from the underlying database service, flag those columns as such.    Then, for a selected row (or rows), provide convenient UI support for, effectively, performing a """"SELECT *"""" against the underlying table(s) from the original query to retrieve the full width of the available data for the selected row(s).  The results would appear as a new query result tab.    This enables a workflow for _any_ tabular query result (as long as unique-key information was retained in the original query) where a user can perform a limited-column query, browse the data, find something interesting, and then request all available details on that item.    There are complications when the tabular data results from a join, contains grouping, aggregation, etc., so in the real world this may not be trivial.  It may require support from DAX that is not already planned.    This is an improvement ticket; as usual it requires T/CAM approval to schedule any work to meet it!""",Improvement,"SUIT,Firefly"
237175,"""Firefly should provide for column metadata that marks a column as representing an inherently cyclic variable such as an angle or a fractional phase, and support the graphical display of such data over more than one cycle of the variable without requiring the duplication of rows in the associated data table.    Background: [~<USER> and I were reviewing the behavior of the light-curve viewing application and the selection of light curve data in the tri-view.  It was a requirement in the design of the interactive period-finding screen that the phase-folded light curve be displayed over two cycles (i.e., from phase 0 to phase 2).  This is well-justified scientifically, as it is helpful to be able to ensure that interesting features of a light curve are not inadvertently (and scientifically meaninglessly) cut in half at the edges of a display from phase 0 to phase 1.    When the phase-folded data table is returned to the tri-view, it is returned with a new """"phase"""" column added, and with every row duplicated (with 1 added to the phase of each duplicate), in order to allow the unmodified tri-view to display plots over phase 0-2.  However, this produces highly undesirable effects if the data are then displayed or sorted on any column other than phase, with duplicates visible both in the data table and in the image cutout pane.  The workaround also has the unexpected effect that highlighting a point in the 0-1 range of phase does not simultaneously highlight the corresponding point - and row - in the 1-2 range.     This is in most cases merely annoying (and, undoubtedly, confusing to users who do not understand the workaround we are using and its rationale). However, if the tools are now used to make a histogram, every point will be double-counted and the resulting histogram will have improper statistical properties; this could have damaging scientific consequences.    (In a separate ticket I am recommending that we no longer return duplicates upon completion of work with the phase-folding tool.)    As a long-term solution to this problem, I would like to suggest that Firefly be enhanced to understand the existence of columns representing cyclic variables.  These will commonly have ranges 0-360, 0-2pi, 0-1, or offset variants of the above.  When displaying x-y plots containing cyclic variables, if the axis range includes values outside the actual range of the variable, duplicates of the points will be shown, ideally as many times as needed to fill the display range.      Selection of a row in a table will highlight all duplicates in any display.  Selection of a point in a display, regardless of whether it is in the primary range of a cyclic variable or not, will select the primary and all duplicates, as well as the corresponding (primary-value) row in the table.    Duplicates will *not* be shown in tables.  Associated image displays will show duplicate images only when the image sort order is determined by a cyclic variable - i.e., they'll behave just like x-y plots.    Duplicates will *not* be counted in histograms accumulated over non-cyclic axes.  Histograms accumulated over cyclic axes will """"really"""" only have bins over the primary range of the variable, with appropriate duplicate bins shown (an optional feature) if the x-axis extends beyond the primary range of the variable.    Column filters on cyclic variables should operate only on the primary values (the ones actually in the table).    Data downloads will not contain duplicates.    This work is important but not time-critical.""",Improvement,"SUIT,Firefly"
237309,"""These estimates are needed as a near-term refresh of the sizing model.    I'm planning to do this initially as a spreadsheet of all planned DRP data products (including temporaries), with annotations for how they relate to user-accessible data products in the DPDD.    I'm sure I'll come up with lots of questions about how to make this information most useful to its consumers (primarily NCSA, I think), and I'll post those in the comments here.    """,Story,Design Documents
238070,"""Running the attached test file exposes a problem with using FileAppender.  {code}  $ python small.py  .File open: /home/srp/mine/legacy-ctrl_events/tests/test.log  F.  ======================================================================  FAIL: testFileDescriptorLeaks (__main__.smallMemoryTestCase)  ----------------------------------------------------------------------  Traceback (most recent call last):    File """"/scratch/srp/lsstsw/stack/Linux64/utils/13.0/python/lsst/utils/tests.py"""", line 178, in testFileDescriptorLeaks      self.fail(""""Failed to close %d file%s"""" % (len(diff), """"s"""" if len(diff) != 1 else """"""""))  AssertionError: Failed to close 1 file  {code}      -As a side note, since the file isn't flushed at any point, the test.log file ends up being empty.- edit:  this is because it was being sent to trace, not info.    The tests in lsst.log don't use the lsst.utils.tests framework so this bug wasn't exposed.  Additionally, the test that is there in {{testLog.py}} names the file """"{0}"""" - I'm not sure what this was supposed to be""",Bug,log
238372,"""In order to synthesis {{\_\_iter\_\_}} from {{\_\_getitem\_\_}} Python apparently requires *exactly* {{IndexError}} to be thrown, so that's what we should throw in these functions.    That will require rewriting the testPybind11.cc unit test in a way that has access to Python symbols (which should be done anyway).    There will likely be workaround code in afw/table/python/catalog.h that can be cleaned up after this change (I'll just catch OutOfRangeError there and re-throw as IndexError).""",Story,utils
238798,"""Three tickets: RFC-267, RFC-268 and RFC-269 seem to be duplicates of RFC-270. I don't quite understand how we have 4 identical tickets from [~<USER>.    * 267: was created 04/Jan/17 1:36 PM  * 268: 04/Jan/17 1:38 PM  * 269: 04/Jan/17 1:47 PM  * 270: 04/Jan/17 1:52 PM    Maybe they were drafts. The problem though is that I can not delete the 3 spurious RFCs as they do not have any workflow buttons for me to click on. RFC-270 does have the workflow buttons.    Please delete the 3 spurious RFCs. Weirdly, they were not visible to me last week when I was analyzing Proposed RFCs so I'm not sure why they appeared over this weekend.""",Story,JIRA
238942,"""*This story is about fixing the requested periodogram to IRSA API so we probably need to add a validator to the input field and each option should have a corresponding label.*    Lately, i've noticed that when doing a periodogram from both part of the dev version of LC, one (from the 'period finding' layout) is behaving differently than the other (from the old tab panel). The result is different.     I've found that in the 'Period finder' layout, the periodogram panel option has a 'step size' value not empty, set to 0.3 (i think this is coming from the cutout size field!) which overwrite the default empty value and in some cases it fails (if 'exponential' option is selected).    Actually, the 'step method' option is associated to different input meanings (not always 'fixed step size') and valid ranges.    Taking the mapping from NExSci, when 'step_method' is {{X}}, then label should say {{Y}}, with range valid [ ] and default value {{D}}, such as:    - {{X}}: Fixed Frequency, {{Y}}: Fixed Step Size, [ > 0.0000001 ] , {{D}}: ?  - {{X}}: Fixed Period, {{Y}}: Fixed Step Size, [ >  0.0000001 ], {{D}}: ?  - {{X}}: Exponential, {{Y}}: Oversample, [ 1, 1000 ], {{D}}: ?  - {{X}}: Plavchan, {{Y}}: Period Step Factor, [ > 0.001 ], {{D}}: ?    Default value for fixed period and frequency COULD come from a calculation on the original table that we are currently missing and would need to be specified in a different ticket because that involves an intermediate calculation on the original table that is currently missing.     For now, i would suggest to leave blank the field and no validation. Just make the labels meaningful with the step method option selected.    We need to get feedback on how to use the IRSA API about default values.  Need input at least from [~<USER>, [~<USER> and Angela Zhang <<EMAIL>> ).    For the ticket, do the following (approved by Luisa/Vandana/Harry/Steve):    1. Remove exponential and plavchan    2. Leave blank input fields and add a note to the UI panel that says """"Leave the following fields blank to use default values"""", followed by a context-sensitive help link to the documentation that contains the formulae for the defaults (DM-9164)    """,Story,Firefly
239077,"""I tried to do my first lsstsw-based installation on a new macOS Sierra computer and got this error during the afwdata build:    {code}               afwdata: Traceback (most recent call last):    File """"/Users/<USER>lsst/qa-system/lsstsw/lsst_build/bin/lsst-build"""", line 51,in <module>      args.func(args)    File """"/Users/<USER>lsst/qa-system/lsstsw/lsst_build/python/lsst/ci/prepare.py"""", line 762, in run      manifest = p.construct(args.products)    File """"/Users/<USER>lsst/qa-system/lsstsw/lsst_build/python/lsst/ci/prepare.py"""", line 725, in construct      self._add_product_tree(products, name)    File """"/Users/<USER>lsst/qa-system/lsstsw/lsst_build/python/lsst/ci/prepare.py"""", line 713, in _add_product_tree      dependencies.append(self._add_product_tree(products, dprod.name))    File """"/Users/<USER>lsst/qa-system/lsstsw/lsst_build/python/lsst/ci/prepare.py"""", line 713, in _add_product_tree      dependencies.append(self._add_product_tree(products, dprod.name))    File """"/Users/<USER>lsst/qa-system/lsstsw/lsst_build/python/lsst/ci/prepare.py"""", line 713, in _add_product_tree      dependencies.append(self._add_product_tree(products, dprod.name))    File """"/Users/<USER>lsst/qa-system/lsstsw/lsst_build/python/lsst/ci/prepare.py"""", line 713, in _add_product_tree      dependencies.append(self._add_product_tree(products, dprod.name))    File """"/Users/<USER>lsst/qa-system/lsstsw/lsst_build/python/lsst/ci/prepare.py"""", line 696, in _add_product_tree      ref, sha1 = self.product_fetcher.fetch(product_name)    File """"/Users/<USER>lsst/qa-system/lsstsw/lsst_build/python/lsst/ci/prepare.py"""", line 333, in fetch      git.checkout(""""--force"""", ref)    File """"/Users/<USER>lsst/qa-system/lsstsw/lsst_build/python/lsst/ci/git.py"""", line 49, in checkout      return self('checkout', *args, **kwargs)    File """"/Users/<USER>lsst/qa-system/lsstsw/lsst_build/python/lsst/ci/git.py"""", line 44, in __call__      raise GitError(retcode, cmd, stdout, stderr)  lsst.ci.git.GitError: Command '['git', 'checkout', '--force', 'master']' returned non-zero exit status 128.  stdout:    stderr:  Error: unknown command """"filter-process"""" for """"git-lfs""""  Run 'git-lfs --help' for usage.  fatal: The remote end hung up unexpectedly  {code}      Details:    - This is a Python 3 lsstsw installation but I also replicated the same issue with a Python 2.7 lsstsw.  - The git-lfs version I see in lsstsw is {{git-lfs/1.2.0 (GitHub; darwin amd64; go 1.6.1; git 9bd3b8e)}}.  - My computer also has a git-lfs 1.5.4 from Homebrew and I've been able to successfully clone {{afwdata}} with this git-lfs (no configuration issues).  - I've also manually cloned {{afwdata}} using the lsstsw-provided git-lfs 1.2.0 (no problem with that git-lfs in my environment).    *This leads me to think that there's some issue with how git/git-lfs is being controlled by {{lsst_build}} on macOS Sierra.*""",Story,Developer Infrastructure
240307,"""This simple script:  {code}  import lsst.afw.image as afwImage    exp = afwImage.ExposureF(1, 1)  {code}    generates the error:  {quote}  log4cxx: No appender could be found for logger (afw.image.Mask).  log4cxx: Please initialize the log4cxx system properly.  {quote}    I'd expect to be able to use LSST primitives without explicitly setting up the logging.  """,Story,log
240794,"""{{utils.TestCase}} has a number of useful methods for floating point tests, but the {{allClose}}/{{allNotClose}} methods don't have a {{msg}} kwarg, so it's not possible to specify a more detailed error message. Some of the {{lsst.afw.image.testUtils}} mixins have {{msg}}, but some don't. This can be very helpful when writing unittests where the test writer knows more about the conditions than allClose can determine on the fly.    I would suggest that whatever is passed for {{msg}} be appended to the default message in allClose, etc., not replace it.""",Improvement,utils
240899,"""The prototype could be considered as a sort of IRSAViewer for handling LC objects / Time dependent dataset, with extra feature that would enable the existing relationship between catalog and image, in particular for WISE, PTF, which could be a reference to single-epoch image (IBE referenced URL typically).    For each observation a corresponding image should be retrieved and placed in a grid (image vs time) either in the same LC object (table) or separately.   One could expect that any LC data-point (time point) would be pointing to an image (single-epoch, exposure) taken from a dataset (image axis would be a 3rd dimension or a 3rd view of LC).  LC should display LC object as following views:  * X-Y plot (typically flux vs time)  * Table  * Images (corresponding image dataset single exposure at a particular time)  The high-level requirements are:  # Easily plot light curve  # Compute Period using Lomb-Scargle   # Display folded Light Curve and Periodogram   # Link Light Curve Points with Single-Epoch Images  # Download Cutouts and Light Curves    In order to create the Light curve viewer prototype, we need to *create a skeleton app to start adding the tri-views* (Upper: plot, Bottom left: table, bottom-right: grid image): (DM-7406)  * Should handle single (one position) LC (table & plot flux or magnitud vs. time, tipically 'mjd')  ** For the prototype it will be a fixed table given  * Image of the single exposure to be displayed at least single page mode  * Should display input for computing a periodogram (the result of an API call)  ** Power spectrum (Power vs log(period) ) as table and xyplot  ** Peaks as a table (Power vs. period)  ** Period (first Period in peak table) as a field (editable), tipically in days.    In order to connect table/plot to single exposure image, be able to compute periodogram and plot based on the table focused, we need a 2 new UI components, 3 new searches and 2 new controllers (tickets will be created separately and added here):      1. UI components:    * To handle the algorithm input parameters to compute ‘periodogram’  (DM-7160)  * To handle the period input (DM-7161)    2. Processors:    whether it is in one search processor or in different one, we need to have the following task processor:    * One to deal with API to compute periodogram that will result at least in 2 tables (periodogram and peaks table, + extra output parameters fi needed) (DM-7162)  * one for getting the image cutout (single exposure) (DM-7164)  * one to build the phase folded curve based on (DM-7165)  ** Original table searched  ** Period whether it comes from the API result or any other changes from user afterward    3. Controllers:  * For image to change on row or xy plot curve clicked (DM-7166)  * For plotting chart based on the type of the table (power vs period, or LC or phase folded) (DM-7167)  """,Story,"SUIT,Firefly"
241111,"""While [~<USER> has commented that the outlines are probably good enough for planning work (and I thnk that's broadly true), the lack of text in the algorithmic components section did occasionally lead to some misunderstandings in [~<USER>'s first review pass, so I think I should flesh that out with text sooner rather than later.    In this issue, I'll stick to sections that no one else has added text for, but eventually I'll also need to work with [~<USER> and perhaps others to ensure that section has a consistent level of detail and focus.""",Story,Design Documents
241490,"""It would be helpful to have a single package that includes basically everything we support. I realize the details could be tricky, though. Products I want include everything a DM developer might work on or at least be expected not to break, including:  - obs_x for all x that we are supposed to keep working  - testdata_x  - validate_drp  - validation_data_x    We also might want variants that exclude certain packages:  - Monster CI packages that require more data or time than can reasonably be expected to be run on a laptop.    Also, I would like this package to be built by Jenkins by default, and explicitly listed as a package that will be built. As such, I think this replaces DM-6543, at least for the long term.""",Improvement,Developer Infrastructure
241749,"""I'd like to do some cleanup, which would facilitate further development. This includes:  - moving chart related code to a separate package (now it is in visualize)  - converting components created with React.createClass to es6 classes  - reorganize store and controllers to have all charts related things under 'charts'. Now we have 'charts' for charts ui, xyplot for xyplot charts, histogram for histogram charts, and tblstats for table statistics.      Fixed bugs    * missing chart mount action, when a chart is removed and then recreated on the same table    Steps to reproduce: load a table (default scatter plot created), create histogram, delete scatter, create new scatter.       The last scatter did not produce mount action, and the plot was not tracking table changes, like filter.    * undefined shows as a label when no server call is necessary    Steps to reproduce: load table (default scatter created), clear options and choose the same columns , click apply.    """"undefined"""" are shown as axis labels""",Story,SUIT
242994,"""validate_drp will test more of our code if it uses default config parameters wherever possible. To that effect I would like to ask you to eliminate all config overrides that are not essential and document the reasons for the remaining overrides.    For DECam there are no overrides that are different than the defaults, so the file can simply be emptied (for now).    For CFHT there are many overrides that are different, and an important question is whether the overrides in this package are better for CFHT data than the overrides in obs_cfht; if so, please move them to obs_cfht.    As a heads up: the default star selector is changing from """"secondMoment"""" to """"objectSize"""" in DM-4692 and I hope to allow that in validate_drp, since it works better and is better supported.    Sorry for the incorrect component, but validate_drp is not yet a supported component in JIRA (see DM-5004)""",Story,Validation
244430,"""I believe we agreed last week (Tim, Jeff, Jacek) that metric value does not have to be numeric, it can be a string. This was triggered by complex metric that I have in Qserv where we want to use a different mix of queries as a metric, so I am thinking about putting there something like """"100LV+56HV"""" (where LV - low volume queries, and hv - high volume queries).     To do that I need someone to remove the restriction on the metric value, we are currently enforcing it to be numeric.""",Improvement,Developer Infrastructure
244515,"""The current configuration of the RFC project does not allow labels to be displayed in issue views.  It is possible to set labels, but not see them outside the specific """"edit labels"""" dialog.    I'd like to use labels to help organize the issues to be addressed at AHM2015.  See the label I placed on RFC-84 as a test.    Can this property of the RFC project be changed?""",Story,"Developer Infrastructure,JIRA"
245710,"""Please merge lsstsw:repos_option branch into master.   As a rule of thumb, while branches sometimes need to be deployed for testing, I'd like to see this is a transient process and either backed out or merged into master in ~48 hrous. """,Bug,Developer Infrastructure
247133,"""Russell writes:  {quote} I think our system for getting code reviewed using JIRA needs some improvements. It seems that people don't always know that they have been assigned to review a ticket. Also, even if I know I have been assigned to review a ticket, I find it hard to find on JIRA.  More concretely, I would like to see these improvements: - Much clearer notification that one has been assigned as a reviewer. Presently the email is quite generic and easy to miss. In fact I find that most JIRA notifications are rather hard to read -- it's not always easy to see what has changed and thus why I should care. The signal to noise ratio is poor.  - By default a user should see which issues they have been assigned as reviewer when they log into JIRA. (If there is a way to reconfigure the dashboard for this, I'd like to know about it, but it really should be the default). One way to fix this, of course, is to reassig the ticket when putting it into review, but we have good reasons to avoid that.  -- Russell {quote}  and I added:  {quote} In fact, you don't know that the ticket has passed into review unless you scroll all the way to the bottom of the comment.  If the comment associated with the change in status is long and you don't scroll all the way down, then you may not know that you were assigned to review.  With Trac, the important information was at the top of the e-mail. {quote}""",Story,Developer Infrastructure
247290,"""From HipChat/Data Management:  [12:09] <USER> @<USER>@KTL @KSK Could you double-check if any of you got an e-mail from Jira on Saturday (Apr 12th) re issue DM-78 (I made you reviewers, but it looks like you weren't notified)? [12:10] K-T Lim: I don't recall and can't determine now; it would have been deleted (irrevocably). [12:10] <USER> I did get an email. [12:10] <USER> @<USER> ah, it appears that I actually did.  The fact that I was a reviewer was just buried, and I didn't notice it. [12:10] <USER> I must have missed that I was a reviewer. [12:10] <USER> OK, thanks!   That gives me not one, but two useful data points (#1 -- emails work, #2 -- they're useless :) ). [12:12] <USER> I'm not sure why they are useless.  The emails from trac were a very important part of my workflow as far as being notified of review responsibility goes.   Maybe it's just the volume from Jira. [12:14] <USER> Yeah, same here.  Though the volume from JIRA hasn't been so bad, so I don't think that's it.  Maybe my brain just has to get used to the new email format. [12:14] K-T Lim: (In my case, I'm mostly paying attention to the RSS feed although the mailbox serves as a backup.) [12:22] <USER> One of the things that made gnats a good bug tracker was that the emails contained the right amount of information (I did have source code...), and trac was pretty good too when we tuned it;  bugzilla always used to be awful.  I bet we can fiddle with Jira to make its mail more useful;  I don't just mean filtering what it sends, but making sure that each email is self contained, but not too long""",Story,Developer Infrastructure
247651,"""As an administrator, I'd like to be able to export a .csv file that includes the name, username, email address, and role of all active users of accounts I administer. """,Story,UI
248177,"""From the last All Staff Retreat, I know this is a """"known"""" issue. But, I wanted to track the scenarios that I've discovered thus far.  The Bulk Upload Applet responds in the following ways when loaded in the following environments: * Mac OSX (10.6.8) + Firefox 11.0 - Initially, saw a """"permanent"""" hang. I had to kill Firefox to continue. However, after restarting Firefox, on second attempt it loaded almost immediately. * Mac OSX + Chrome 18.0 - Temporary hang (1-2 mins), eventually loads properly. * Mac OSX + Safari 5.1.5 - Loads almost immediately. * Windows 7 + Chrome 18.0 - Loads in about 20-30 seconds * Windows 7 + Firefox 12.0 - Applet never seems to load & I always see a """"blank"""" pop-up window (just includes Duracloud header/footer)  UPDATE: Actually the issue was that the Java Plugin was disabled. See first comment below.  Just as a note, Internet Explorer & Opera also likely should be tested once DURACLOUD-694 issue is resolved.""",Bug,UI
248878,"""Use case: as a data analyst, I'm finding some data anomalies during the debugging of my aggregation pipelines.    When I expand subdocuments in the preview area, switch to another tab (opened against the same collection) to review the contents of the documents, and switch back to the aggregation pipeline tab, Compass folds back the expanded subobjects of the aggregation pipeline, which is confusing the end user.""",Bug,Aggregation pipeline
248984,"""h3. User Story    As an ADL user,   I want to see that I am connected to ADL in Compass   So that I am reassured I connected to the right place    h3. Acceptance criteria  - When I am connected to ADL, the sidebar should says so.  - Edition should be Atlas Data Lake     !image-2020-03-30-11-22-18-595.png|thumbnail! """,Story,Connectivity
249185,"""I have a favorite connection to my atlas free tier. For some reason, it includes the password as """"undefined"""" which results in the connection timing out (rather than failing or prompting for passwd). I've attached a screenshot.    Here's the URL that was auto-populated.    mongodb://dan:<EMAIL>:27017/admin?authSource=admin&replicaSet=surflog&readPreference=primary&appname=MongoDB%20Compass%20Beta&ssl=true    If I go to the """"Fill in connection fields individually"""" screen it will render as """"******""""    If I fill in the password correctly, then toggle back to the shortcut screen, the password is correct.     """,Bug,Connectivity
249208,"""h3. User story  As a Compass user  I want the buttons to update when an import completes  So that I don't accidentally import duplicate data into my collection    h3. Acceptance Criteria  -     h3. Notes  - From Slack:  -- max: the import button should be disabled, or maybe become “Done”  -- joy: so to clarify.. if it reaches 100%  what if we 1) remove “cancel” 2) replace “import” with “done” until the progress is complete, “import” is disabled  """,Story,Import/Export
249355,"""As a customer, I would like the ability to click to draw a polygon, in the same way as I can currently drag out a circle, to create a GeoWithin Query.""",New Feature,Schema
249370,"""I would set to set the default font size of the document window. It is currently the smallest font on screen in the UI and when you use zoom-in every font increases in proportion. It should be an option the use should be able to set and remain set as an option. """,New Feature,UI / UX
249396,"""h3. User story    As a Compass user   I want to create multiple documents in extend JSON   So that I can easily copy paste from my code editor or from an API response    h3. Acceptance criteria   - There is a new JSON mode in the """"new document"""" dialog   - It's possible to paste an array of documents in extended JSON into the code editor with syntax checking and highlighting   - When an array is inserted into the code editor, the """"field by field"""" mode is disabled and when the user switches to it, an informational message is displayed.  - It's possible to insert a new document only if the JSON is valid""",Story,Documents
249397,"""h3. User story    As a Compass user   I want to create a new document in extend JSON   So that I can easily copy paste from my code editor or from an API response    h3. Acceptance criteria   - There is a new JSON mode in the """"new document"""" dialog   - It's possible to paste extended JSON in a code editor with syntax checking and highlighting   - It's possible to switch between JSON and """"field by field"""" mode and the changes made in one mode are preserved and visible in the other mode   - It's possible to insert a new document only if the JSON is valid""",Story,Documents
249398,"""h3. User story    As a Compass user   I want to see and edit documents in extended JSON   So that I have a way to work in an environment that is similar to my code editor where I feel productive    h3. Acceptance criteria   - There is a new JSON mode alongside the existing default (list) and table modes.   - In JSON mode, the extended JSON of the documents will be displayed   - In JSON mode, it is possible to edit documents in a code editor with syntax highlighting for (extended) JSON. Copy-paste should work   - In JSON mode, like with the other modes, only one document at a time can be edited.    h3. Notes   - If we display encrypted fields as {{*******}} also in JSON mode, can we prevent the user from updating them? Sounds like this is a yes, as ACE editor allows us to set lines as read-only.""",Story,Documents
249513,"""h3. User story    As a Compass user   I want a keyboard shortcut for Disconnect   So that I can quickly disconnect from a cluster    h3. Acceptance Criteria   - When I am connected to a cluster, I want to disconnect with a keyboard shortcut   - Disconnecting with the keyboard has the same effect as doing Connect > Disconnect in the system menu""",Story,Connectivity
249524,"""h3. User story  As a Compass user  I want to duplicate an existing View  So that I don't need to start from scratch when I want to create a new, similar one    h3. Acceptance criteria  - In the sidebar, I have an entry point to duplicate a View  - When I click on the entry point to duplicate a View, I can specify the name for the new View and then the new View is created with the same aggregation pipeline that is behind the original View  - When the duplicate View is created, a new tab is opened pointing to the Documents view of the new View  - When the server returns errors (e.g., the user doesn't have the necessary permissions to create a new view), Compass needs to handle the errors nicely""",Story,"Aggregation pipeline,Views"
249525,"""h3. User story  As a Compass user  I want to edit a View (as in """"edit the aggregation pipeline behind the View"""")  So that I can go and adjust the details if the documents I am seeing are not in the shape I expected them to be    h3. Acceptance criteria  - In the sidebar, I have an entry point to go and edit the aggregation behind each View  - In the main screen, when I am looking at a View, I have entry points to go and edit the aggregation behind the View  - When I go and edit the aggregation behind a View, I am editing it in the aggregation pipeline builder of the Collection or View the View I am editing is based on  - When I am done with editing the aggregation pipeline, I can update the View with the new pipeline  - When the server returns errors (e.g., the user doesn't have the necessary permissions to modify a View), Compass needs to handle the errors nicely    h3. Questions  - When a View has been updated, can we jump back to wherever the user was when they clicked on an entry point to editing the View?""",Story,"Aggregation pipeline,Views"
249527,"""h3. User story  As a Compass user  I want to create a View from an aggregation pipeline  So that I can make sure my data is in the right shape to be used in my application or to create a chart from it.    h3. Acceptance criteria  - When I am in the aggregation pipeline builder and I have written a valid aggregation, I want to be able to save it as a View and give it a name  - When the server is older than 3.4, Save as a View is hidden  - When an aggregation is saved as a View, it immediately appears in the sidebar  - When an aggregation is saved as a View and I open the Database screen, I can see the newly created View  - It is possible to create a View on top of another View  - When I am trying to create a new View with the same name of an existing View, I want to see a meaningful error message  - When the server returns errors (e.g., the user doesn't have the necessary permissions to create a view), Compass needs to handle the errors nicely  - When the View has been created, Compass will open the new View in the main screen (in a new tab, as soon as tabs are available)""",Story,"Aggregation pipeline,Views"
249720,"""h3. User story  As a Compass user  I want more real estate for my stage editor  So that I can see everything I type into the editor without a lot of scrolling, even when my stage contains a lot of code.    h3. Acceptance Criteria  - When I am working on a stage, I can make the stage editor full-screen  - When the stage editor is full-screen, I can see:      -- The stage selector      -- The code editor      -- A way to reach the documentation/examples for the current stage      -- A way to delete the stage      -- A way to disable the stage      -- A sample of the data entering the stage      -- A sample of the data outputted by the stage      -- A way to access the agg. pipeline settings, in case I need to adjust for instance the timeout if the current stage is too slow  - When I close the full-screen editor, the changes I made are reflected in the “small editor” and the results in the preview are updated  """,Story,Aggregation pipeline
249723,"""h3. User story  As a Compass user  I want a quick and discoverable access to “export to language”  So that I can easily find it and I can take advantage of it to speed up my application development.    h3. Acceptance Criteria  - When I look at the Agg. Pipeline builder, I want to know it is possible to export my pipeline to different languages  """,Story,Aggregation pipeline
249724,"""h3. User story  As a Compass user  I want to set sample size, number of docs in preview and maxTimeMS  So that I have better control over how Compass is executing and displaying my pipeline.    h3. Acceptance Criteria  - There is a place in the UI where I can set sample size, number of docs in preview and maxTimeMS  - By default, these settings are set to the default that is in Compass today  - When I change one of these values, the new configuration is preserved across different Compass sessions    h3. Notes  - Should the setting be per collection, per pipeline, or global? How does that resonate with tabs (COMPASS-3289)?""",Story,Aggregation pipeline
249947,"""Feature request: As a user who has documents with a large[-]ish (100) number of keys, I want to be able to cmd+F to search for the key Im looking for to check its value without having to scroll through reading over each key.""",Story,Documents
249983,"""As a developer, when I am modifying data from a script or new piece of code I would like to see how my changes affected the database through Compass.     At the moment I have to refresh and reconnect compass to see the updated data.    My server is run on Node.js and is hosted locally through Docker compose. """,New Feature,Feature Request
249996,"""I'm entirely sure that this should be reported as a bug vs. a feature, however:    As a user connected to some deployment using Compass:  - I can click the Aggregations tab  - I have created at least one stage in my pipeline  - If my stage results preview contains nested documents and I have expanded one or more of these nested documents ...  - Making any keystrokes (including whitespace in an effort to increase the height of my expression input and thus the results preview area for demo purposes) within the expressions input will re-trigger the pipeline and my previously expanded documents will collapse    It might be useful to be able to expand nested documents and have their expanded state persist.    Screenshot:  !Jun-18-2018 22-17-43.gif!    cc [~<USER>""",Bug,Aggregation pipeline
250052,"""h3. User story    As a Compass user  I want to adjust the width of the column where I author my pipelines   So that I can adjust the editor interface according to my screen and my needs  h3. Acceptance Criteria   - It's possible to change the width of the editor column in the aggregation pipeline builder   - The width is not expected to be changeable for single stages but rather for the whole screen   - Width configuration is persisted across sessions and application restarts   - Resizing the column should have reasonable performance and feel smooth. If it's not the case, then we can park this for now and do COMPASS-3295 instead.""",Story,Aggregation pipeline
250076,"""This may not be supported but I'm raising it anyway, perhaps as a feature request.       h5. Affected Version    1.14.0-beta.1  h5. Actual Behavior    When adding comments in the editor for aggregation stages, it doesn't parse the objects anymore and instead displays an error expecting a [ or AggStage.    That's confusing and it took me a while that the comment caused this issue.    See attached screenshots.  h5. Expected behavior    The comments are silently ignored.""",Bug,Aggregation pipeline
250158,"""As a user I would like to view my schema analysis data in a tabular format.""",Task,Schema
250177,"""As a Compass user I would like to export and import my favorite list so that other users may access the same hosts without having to type anything""",Story,Connectivity
250178,"""As a Compass user I would like to persist my list of favorites after a Compass version upgrade so that it is quicker for me to access hosts that I have saved before    After trying to upgrade from 1.11 til 12.2, and afterwards to 1.33.0 beta 7, it seems this feature is not in the product. My favorites turn out blank after upgrade.""",Story,Connectivity
250264,"""As a new user, I want to generate sample data using a GUI instead of editing a JSON Schema document, in order to start using my currently empty database.""",Story,Sample Data
250265,"""As a new user, I want to generate sample data from a JSON schema, in order to start using my database.""",Story,Sample Data
250294,"""As a user I would like to duplicate a favorite so that it is quicker for me to use existing information as a starting point when creating a new favorite""",Story,Connectivity
250307,"""As a user of schema analysis with a large collection (150M records) I want a larger schema sample than 1000 so I can get a more accurate/meaningful analysis.""",New Feature,Schema
250311,"""As a product manager I want telemetry in the agg pipeline builder so I can see how our customers use the interface. Interesting questions to ask:    - How long was spent building the pipeline?  - How many stages was the pipeline?  - What are the most frequently used stages?  - What are some common mistakes made?  - What is the size of the sample set by users?  - How often does someone get stuck writing a pipeline / fail to create a valid pipeline?""",Story,Aggregation pipeline
250320,"""As a user I would like a dark theme so that working with Compass is easier on my eyes.""",New Feature,UI / UX
250325,"""As a developer I want to write queries in the same way as they would be written in the shell so that I can context switch between shell and UI with ease.""",New Feature,Query
250326,"""As a developer I want to bulk update documents so I can correct multiple documents I know to be incorrect at the same time.    Use case from interview with Jamie Weldon and Andrew Skinner""",New Feature,Bulk Operations
250327,"""As a developer I want to bulk delete documents so I can remove documents from my collection that I know to be incorrect from testing the application during testing.    I don't want to drop the whole collection because I don't want to have to reinitialize it (through code).    Use case from an interview with Jamie Weldon""",New Feature,Bulk Operations
250339,"""As a user I would like to see the time it took to execute a query when the documents view loads.""",Story,"CRUD,Documents"
250340,"""As a user I would like to have multiple connections in a single Compass window so that I don't have to have multiple windows open when working across many deployments.""",New Feature,Connectivity
250341,"""As a user with large documents I would like to have my documents collapsed to their ObjectID's by default and then click to expand to see the whole document so that I can make the most of my screen real estate.""",Story,"CRUD,Documents"
250342,"""As a user with large documents I want to provide a filter or project before the documents view populates for speed and performance.""",Story,"CRUD,Documents"
250343,"""As a user I want to click column names to sort them because it's a familiar paradigm to me and so I don't have to type in the query bar to sort the field.""",New Feature,"CRUD,Documents"
250344,"""As an SA I would like to remove individual entries from my recent connections list so that I can remove previous customer hostnames when demoing at other customers.""",Story,Connectivity
250345,"""As a developer I want to copy a document to clipboard as json so I can share the document with other members on my team and/or compare it with the output from my application.""",Story,"CRUD,Documents"
250347,"""As a user I would like visual feedback when executing queries so that I can tell that the operation was sent to the server and is currently in progress (remove uncertainty).     From Joe:    In Compass (all versions up to 1.10.1.0) long running queries give no feedback when running or when completed. As a result you have to spot the number of results changing to determine if a query is complete. I have attached a screen capture demonstrating this. Sorry about the background noise on the video. I was in an airport lounge :-)    """,Story,Query
250348,"""As a developer I want to insert a new document as JSON.""",Story,"CRUD,Documents"
250349,"""As a developer I want to view my documents in JSON because:  * I prefer to see my data in the same way that my application sees it (helpful for debugging)  * I am familiar with it  * I find it easier to view nested structures""",New Feature,"CRUD,Documents"
250350,"""As a user I want to be able to write a note or description on a field name in the schema view so I can quickly understand and reference what it means later on.""",Story,Schema
250351,"""As a user I would like to be able to """"search"""" for the existence of a field whilst in schema view so I can save time and don't have to scroll through to find what I'm looking for when looking at my schema analysis results.    Ctrl + f (find in Window) would be an acceptable solution here.""",New Feature,Schema
250357,"""As a new user I want a quick way to seed a new mongodb database so I can play around with test data.""",Story,Import/Export
250358,"""As a developer I want to copy data from production or staging to my local development environment so I can debug an application issue.""",New Feature,Import/Export
250359,"""As a developer I want to take a restore a dump to my local machine so I can develop against real data.""",New Feature,Import/Export
250360,"""As a new developer I want to restore a dump to my local machine so I can set up a new development environment.   """,New Feature,Import/Export
250364,"""As a user I want to provide dns entries to whitelist in the Compass Content Security Policy, so I may access data from locations not currently allowed in Compass.""",Story,Plugins
250366,"""As a brand new user I would like to get started with an Atlas instance as quickly as possible.""",Story,Connectivity
250370,"""As a developer I want to understand what each pipeline operator does so I don’t have to google for an explanation.""",Story,Aggregation pipeline
250372,"""As a user, I want Compass to log 3rd party plugin plugin actions that may be a risk to security, in order to audit what plugins are doing and potentially disable them.""",Story,Plugins
250373,"""As a user I want to be informed about what plugins are trying to do and either authorize or deny those actions, in order to control the security of my Compass instance.    h5. Acceptance Criteria    User is prompted to authorize a plugin to:  - Access the database  - Access the network via any protocol  - Access the filesystem (read-only)  - Access connection details  If the user clicks """"no"""", then the plugin will not be loaded.  If the user clicks """"yes"""", then the plugin will be allowed to access those items in the list.""",Story,Plugins
250374,"""As a user I want to opt-in to the use of 3rd party plugins so that I am awareness (and consenting) when I install a 3rd party plugin from other vendors and so I can be sure that the default Compass plugins are secure and provided by MongoDB.""",Story,Plugins
250379,"""As a user I want to resize columns in table view so that I can make better use of my screen real estate (make columns narrower for the things Im not interested in) or widen a column to view more of a value.""",New Feature,Documents
250396,"""As a Compass user I want to install Compass in a certain directory so that I abide by my organisation's security policy.""",Story,Packaging
250397,"""As an Ops team member I want to script installs of Compass across many machines so I can deploy Compass on laptops of new employees (install widely across my organization).    *Product Acceptance Criteria*  - A working MSI installer (for the same platforms that we have .exe's for)  - MSI installer is signed by MongoDB as the publisher  - MSI installer should allow user to choose installation directory for Compass  - MSI installer should allow user to choose an installation directory for preferences and favorites  - Must support a """"quiet"""" installation and output the log to a specified location.""",Story,Packaging
250429,"""As a DBA, I would like to see the sharding status of existing sharded collection in my cluster, and also do administrative tasks like shard a new collection via Compass.""",Task,Feature Request
250480,"""*User Story*  * As a user, I want to see which document that I'm actively focused on. Also, I want to be able to move my focus up and down the list of documents by using either hotkeys or mouse click. """,Task,CRUD
251044,"""We have a strongly-typed options::create_collection class that doesn't accept flexible BSON options. It has to deprecate fields whenever the server drops them (such as """"autoIndexId"""") and add them whenever the server does (such as """"validationLevel""""). This also leads to excessive additions to the API; for example, create_view was added in CXX-999 to create non-materialized views, whereas other drivers simply allowed users to pass """"viewOn"""" the same as any other BSON option.    Deprecate and remove all the strongly-typed fields in options::create_collection and allow an arbitrary document of arguments passed to the createCollection command. The actual options::create_collection class may be deleted or just shrunk, I'm not sure. Deprecate and remove database::create_view, there's no need for a separate member function for that rare use case. (Additionally, options::create_view includes a write concern, documented as """"Write concern that will be used when computing the view."""" I think the server ignores this write concern.)""",Improvement,API
251435,"""Backport CXX-1043.    {quote}  Doxygen generates broken links to mongocxx and bsoncxx namespace documentation pages because those namespaces aren't documented with the """"namespace"""" tag.    They also don't show up on the """"TODO"""" list (CXX-852), so I've created this ticket as a reminder.  {quote}""",Bug,Documentation
251566,"""Doxygen generates broken links to mongocxx and bsoncxx namespace documentation pages because those namespaces aren't documented with the """"namespace"""" tag.    They also don't show up on the """"TODO"""" list (CXX-852), so I've created this ticket as a reminder.""",Bug,Documentation
251696,"""In the same vein as CXX-879.  Given the chrono facility available in any environment targeted by this driver, manually converting a human-readable duration to seconds expressed as an int32_t seems a bit quaint, so I'm curious if there would be value in using a chrono::duration to express index TTL values.""",Task,API
252615,"""I created a pull request, so copying the text from that:        If we have an interface like:    {code}  public interface SampleInterface {   int getFirst();   String getSecond();  }  {code}    which is implemented as:    {code}  public abstract class SampleImplementor implements SampleInterface {   public abstract boolean isThird();  }  {code}    and has a concrete implementation that's called `SampleImplementorImpl`.   When `PojoBuilderHelper` goes to create the property models, it only checks  for methods on the current class and super classes - not interfaces. In  the above example, this means the property model will only have """"third"""" entry -  no """"first"""" or """"second"""" property model. Today, you can manually get around that by  creating a @BsonCreator by hand:    {code}  public abstract class SampleImplementor implements SampleInterface {   @BsonCreator   public static SampleImplementor newInstance(   @BsonProperty(""""first"""") int first,   @BsonProperty(""""second"""") String second,   @BsonProperty(""""third"""") boolean third) {   return new SampleImplementorImpl(first, second, third);   }    public abstract boolean isThird();  }  {code}    The presence of the `@BsonProperty` on the `@BsonCreator` method will  create the property models. Conversely though, if you want to leverage a  `Convention` implementation to dynamically create a `InstanceCreator`  that knows how to find `SampleImplementorImpl` above, you won't be able  to. `InstanceCreator` is only provided properties for which  `PropertyModel` exists, so above, since `PojoBuilderHelper` didn't  discover the interface fields, and there is no exposed API to add  property models, your `InstanceCreator` will never be provided the  `first` and `second` fields present on the interface.    Simply put, if you provide the pojo codec a class that is not concrete,  extends an interface for methods, and does not have a `@BsonCreator`  annotation, there is no way to implement a `InstanceCreator`  implementation that works for the non concrete class. You get stuck in  a place where the class can serialize since the concrete implementation  `SampleImplementorImpl` is provided at runtime, but then you have no way  to deserialize it since usages in the code only reference `SampleImplementor`.    We've worked around this problem for years and created 700+ hand written   `@BsonCreator` annotations, so at this point I want to fix actual the problem.    This fix is relatively straight forward: Update `PojoBuilderHelper` to  scan implementing classes and interfaces, which provides a fully  populated property model.""",Improvement,POJO
252723,"""I've recently experienced intermittent DNS resolution issues while working on a project and these issues eventually resulted all future queries failing with """"Timeout waiting for a pooled item ..."""". Restarting the program helped but the problem would resurface after enough resolution failures.    Now this is technically a Scala application using the Scala driver but I was able to pinpoint the problem. Invoking {color:#0747a6}{{getAsync(SingleResultCallback<InternalConnection>)}}{color} on {color:#0747a6}{{DefaultConnectionPool}}{color} will invoke {color:#0747a6}{{openAsync}}{color} to open the pooled connection if it isn't already open. And when the connection is backed by a {color:#0747a6}{{AsynchronousSocketChannelStream}}{color} or {color:#0747a6}{{NettyStream}}{color} that invokes their {color:#0747a6}{{openAsync(AsyncCompletionHandler<Void>)}}{color} which executes {color:#0747a6}{{serverAddress.getSocketAddresses()}}{color}. It appears that {color:#0747a6}{{openAsync}}{color} in {color:#0747a6}{{DefaultConnectionPool}}{color} is only expecting exceptions via it's callback. But exception thrown form {color:#0747a6}{{serverAddress.getSocketAddresses()}}{color} are propagated all the way back to it.  Now at least in the Scala driver the exception is caught by an {color:#0747a6}{{ErrorHandlingResultCallback}}{color} eventually, which stops the exception. *But nothing releases the connection back to the pool.* This eventually exhausts the pool and makes it unusable.    After enabling trace logs I noticed that the connection that was being opened right before  {color:#0747a6}{{ErrorHandlingResultCallback}}{color} logged an error was always lost. Even after 3 hours it was never checked back into the pool or referenced in any other log message.    I believe that the {color:#0747a6}{{openAsync}}{color} methods in {color:#0747a6}{{AsynchronousSocketChannelStream}}{color} and {color:#0747a6}{{NettyStream}}{color} should capture throwables and use them to fail the {color:#0747a6}{{AsyncCompletionHandler}}{color}. I'm not sure if that could break any existing use cases. Though based on the history of these two files the execution of {color:#0747a6}{{serverAddress.getSocketAddresses()}}{color} used to be inside a try block, but was moved out of when JAVA-2700 added support for connecting to all IPs and part of the method was made recursive.""",Bug,"Connection Management,Async"
252804,"""Hi,    I'm getting a NPE at DecoderContext.decodeWithChildContext, but I'm not able to reproduce it at will..  The query works for some time, and suddenly starts to fail erratically (some times it fails some times it doestn't).  I tried to find if it was a concurrency type of issue, tried to syncronize access but didn't mattered.    Also happens with many version of the server (3.6.x, 4.0.x).  Tried changing the driver version (now running 3.11.0-rc0) but also didn't mattered.    Anything else you think I can try to corner it?    Regards,    Juan.  {noformat}  Caused by: java.lang.NullPointerException   at org.bson.codecs.DecoderContext.decodeWithChildContext(DecoderContext.java:93) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.pojo.PojoCodecImpl.decodePropertyModel(PojoCodecImpl.java:213) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.pojo.PojoCodecImpl.decodeProperties(PojoCodecImpl.java:197) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.pojo.PojoCodecImpl.decode(PojoCodecImpl.java:121) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.pojo.PojoCodecImpl.decode(PojoCodecImpl.java:125) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.pojo.MapPropertyCodecProvider$MapCodec.decode(MapPropertyCodecProvider.java:92) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.pojo.MapPropertyCodecProvider$MapCodec.decode(MapPropertyCodecProvider.java:60) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.DecoderContext.decodeWithChildContext(DecoderContext.java:93) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.pojo.PojoCodecImpl.decodePropertyModel(PojoCodecImpl.java:213) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.pojo.PojoCodecImpl.decodeProperties(PojoCodecImpl.java:197) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.pojo.PojoCodecImpl.decode(PojoCodecImpl.java:121) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.pojo.PojoCodecImpl.decode(PojoCodecImpl.java:125) ~[bson-3.11.0-rc0.jar:?]   at com.mongodb.operation.CommandResultArrayCodec.decode(CommandResultArrayCodec.java:52) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.operation.CommandResultDocumentCodec.readValue(CommandResultDocumentCodec.java:60) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at org.bson.codecs.BsonDocumentCodec.decode(BsonDocumentCodec.java:84) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.BsonDocumentCodec.decode(BsonDocumentCodec.java:41) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.configuration.LazyCodec.decode(LazyCodec.java:47) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.BsonDocumentCodec.readValue(BsonDocumentCodec.java:101) ~[bson-3.11.0-rc0.jar:?]   at com.mongodb.operation.CommandResultDocumentCodec.readValue(CommandResultDocumentCodec.java:63) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at org.bson.codecs.BsonDocumentCodec.decode(BsonDocumentCodec.java:84) ~[bson-3.11.0-rc0.jar:?]   at org.bson.codecs.BsonDocumentCodec.decode(BsonDocumentCodec.java:41) ~[bson-3.11.0-rc0.jar:?]   at com.mongodb.internal.connection.ReplyMessage.<init>(ReplyMessage.java:51) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.internal.connection.InternalStreamConnection.getCommandResult(InternalStreamConnection.java:413) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:309) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:259) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:450) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:72) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:218) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:269) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:131) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:123) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.operation.CommandOperationHelper.executeCommand(CommandOperationHelper.java:341) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.operation.CommandOperationHelper.executeCommand(CommandOperationHelper.java:332) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.operation.CommandOperationHelper.executeCommandWithConnection(CommandOperationHelper.java:218) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.operation.FindOperation$1.call(FindOperation.java:730) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.operation.FindOperation$1.call(FindOperation.java:724) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.operation.OperationHelper.withConnectionSource(OperationHelper.java:463) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.operation.FindOperation.execute(FindOperation.java:724) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.operation.FindOperation.execute(FindOperation.java:88) ~[mongodb-driver-core-3.11.0-rc0.jar:?]   at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:189) ~[mongodb-driver-3.11.0-rc0.jar:?]   at com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:143) ~[mongodb-driver-3.11.0-rc0.jar:?]   at com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:92) ~[mongodb-driver-3.11.0-rc0.jar:?]   at com.mongodb.client.internal.MongoIterableImpl.forEach(MongoIterableImpl.java:121) ~[mongodb-driver-3.11.0-rc0.jar:?]   at com.mongodb.client.internal.MongoIterableImpl.into(MongoIterableImpl.java:133) ~[mongodb-driver-3.11.0-rc0.jar:?]   at [my code]{noformat}""",Bug,POJO
252903,"""I have a class that implements Codec<T>. In the `encode` method there was a NPE thrown but there was nothing in the logs. It took me a few hours to figure it out. That should be fixed so other people are not hit by this like me.    I'm using the latest version of the async Java driver.""",Bug,Async
253133,"""The javadocs for both the MongoCollection #updateOne and #findOneAndUpdate methods both state for the 'update' param:       {code:java}  @param update  a document describing the update, which may not be null. The update to apply must include only update operators.  {code}  Perhaps this should also include information about passing in a replacement document instead of an just an update document that contains update operators.    I could not find this document anywhere, but if I pass in an empty document to the update parameter, mongo will replace the existing document with that empty document. I assume that because the empty document contains no update operators it is treated a name/value paired object? I would have thought that passing an empty document would result in no changes the existing document, as it does actually not contain any fields.    Also, when dynamically constructing a Bson update object using the Updates.combine method, if at least one update operator is not added it will be considered an invalid update because it does not contain any update operators. Is it possible to treat the empty array that would be created by the Updates.combine method as a valid update operator that essentially does nothing instead of throwing an error?    As it stands, the update parameter cannot be null (which seems somewhat inconsistent with the other Bson params that are nullable), and we cannot pass an empty array of updates either.          """,Improvement,"Documentation,Query Operations"
253407,"""I would like to leverage from ChangeStream usage to organize versioning/audit in my application. ChangeStreamDocuments are to be intercepted after any of insert*/save*/delete* operations is called using Spring Data/Mongo Repository.    The following code snippet is used to iterate over collection's ChangeStreamDocuments.  {code:java}try (MongoCursor<Document> cursor = collection.watch().iterator()) {      while (cursor.hasNext()) {          store(cursor.next());      }  }  {code}  The issue is that this code sticks after the first operation (new entity insertion) if the DB is empty. An infinite loop is run inside _cursor.hasNext()_.    I could only come up with an asynchronous solution which opens a persistent cursor against the DB and applies a Block function on each new incoming event. The biggest drawback of this solution is that each persistent cursor consumes a connection from the pool.  {code:java}Block<ChangeStreamDocument<Document>> persistBlock = changeStreamDocument -> store(changeStreamDocument);    collection.watch().forEach(persistBlock);       {code}  Please, advise what could be done in this case and whether my code should work as expected, but has a bug.""",Task,Query Operations
253421,"""Remove direct support for JUL in the 4.x driver, making SLF4J the only way for the driver to log.  Applications requiring JUL logging must use the SFL4J-JUL bridge library.    If SLF4J is not detected, the driver will log a single message to JUL indicating that logging is disabled.  h4. Original Description    I am working on an OSGI DS project that implements an OSGI fragment with ch.qos.logback.classic to attach to slf4j. Because I am using slf4j, the driver uses it for its default logging unless I override it by setting the logger level to OFF in the code base. However, even with this, the driver continues to use com.mongodb.diagnostics.logging.JULLogger, java.util.logging.Logger, as a fallback. The driver should not force an application to output its logging but rather some configuration or programmatic setting should be added to prevent any logging to the console.    I would think at least a public interface to the JULLoger or diagnostics.""",New Feature,Monitoring
253582,"""We started implementing the ORM that came with version 3.x.    We decorate our Pojos with a custom hash. We now use your ORM to convert the object to BsonDocuments, we then edit it, and then we use it with your MongoCollection.    I've got it all to work, but i'm disappointed that some methods do not accept 'raw' bson as a parameter (eg: insertOne and replaceOne).    I've got 2 work-a-rounds for this:  - Emulate these methods around with updateOne.  - Generic the MongoCollection to BsonDocument instead.    I much prefer if MongoCollection supports these method with Bson as a value, as update does.    For your reference, what I suggest is adding:  * MongoCollection<D>.insertOne(Bson document)  * MongoCollection<D>.replaceOne(Bson filter, Bson replacement)  etc.  Similar to updateOne(Bson filter, Bson update).     Looking through the code of MongoCollectionImpl, it seems internally the Pojo is rather quickly converted to Bson.""",New Feature,API
253598,"""Hi,    I'm  trying to decode a BSON written by other system using the BSON library and I'm unable to decode correctly. I'm just reading values from mongodb (this app read-only, data stored is legacy).    The bson has an {{id}} property (instead of the classical {{_id}}) embeded inside other object, and its never set the java property.    The first patch show the decoding issue for a simpler doc:  {noformat}  From 8e00ddd5b65177bacf87facf55fb2680797bfca0 Mon Sep 17 00:00:00 2001  From: """"<USER>"" <<EMAIL>>  Date: Thu, 1 Feb 2018 16:22:11 -0300  Subject: [PATCH] Test that shows id properties aren't read from bson    ---   .../unit/org/bson/codecs/pojo/PojoCustomTest.java  | 26 ++++++++++++++++++++++   1 file changed, 26 insertions(+)    diff --git a/bson/src/test/unit/org/bson/codecs/pojo/PojoCustomTest.java b/bson/src/test/unit/org/bson/codecs/pojo/PojoCustomTest.java  index d47dd959f..e66a07f5b 100644  --- a/bson/src/test/unit/org/bson/codecs/pojo/PojoCustomTest.java  +++ b/bson/src/test/unit/org/bson/codecs/pojo/PojoCustomTest.java  @@ -265,6 +265,32 @@ public final class PojoCustomTest extends PojoTestCase {                   fromCodecs(new SimpleEnumCodec()));           roundTrip(registry, new SimpleEnumModel(SimpleEnum.BRAVO), """"{ 'myEnum': 1 }"""");       }  +      +    static class Foo {  +        private ObjectId id;  +          +        public Foo() {  +              +        }  +        public ObjectId getId() {  +            return id;  +        }  +  +        public void setId(final ObjectId id) {  +            this.id = id;  +        }  +    }  +      +    @Test  +    public void testIdWithoutUndescore() {  +        final CodecRegistry registry = getCodecRegistry(getPojoCodecProviderBuilder(Foo.class, SimpleEnumModel.class));  +        final Codec<Foo> codec = registry.get(Foo.class);  +        final org.bson.io.OutputBuffer encoded = encode(DOCUMENT_CODEC, org.bson.BsonDocument.parse(""""{ 'id' : { '$oid' : '59a79beb24345d0004fe1a44' } }""""));  +        final Foo result = decode(codec, encoded);  +        org.junit.Assert.assertNotNull(result);  +        // fails  +        org.junit.Assert.assertEquals(new ObjectId(""""59a79beb24345d0004fe1a44""""), result.getId());  +    }          @Test       @SuppressWarnings(""""unchecked"""")  --   2.14.3    {noformat}    You might say that the field should be named like _id, and I might agree.  Removing the following lines of codes make the tests succeed (and breaks others):    {noformat}  diff --git a/bson/src/main/org/bson/codecs/pojo/ClassModelBuilder.java b/bson/src/main/org/bson/codecs/pojo/ClassModelBuilder.java  index cb579cb76..21d8f871c 100644  --- a/bson/src/main/org/bson/codecs/pojo/ClassModelBuilder.java  +++ b/bson/src/main/org/bson/codecs/pojo/ClassModelBuilder.java  @@ -260,10 +260,6 @@ public class ClassModelBuilder<T> {              for (PropertyModelBuilder<?> propertyModelBuilder : propertyModelBuilders) {               boolean isIdProperty = propertyModelBuilder.getName().equals(idPropertyName);  -            if (isIdProperty) {  -                propertyModelBuilder.readName(ID_PROPERTY_NAME).writeName(ID_PROPERTY_NAME);  -            }  -               PropertyModel<?> model = propertyModelBuilder.build();               propertyModels.add(model);               if (isIdProperty) {  {noformat}    My _real-life_ case happens when decoding an embed object (that I might argue that in doesn't need to follow  the {{_id}} convention):    {noformat}  From df327b122bf407701dc442fc2ac497d5403890ca Mon Sep 17 00:00:00 2001  From: """"<USER>"" <<EMAIL>>  Date: Thu, 1 Feb 2018 16:22:11 -0300  Subject: [PATCH] Test that shows id properties aren't read from bson    ---   .../unit/org/bson/codecs/pojo/PojoCustomTest.java  | 76 ++++++++++++++++++++++   1 file changed, 76 insertions(+)    diff --git a/bson/src/test/unit/org/bson/codecs/pojo/PojoCustomTest.java b/bson/src/test/unit/org/bson/codecs/pojo/PojoCustomTest.java  index d47dd959f..e8238cdcd 100644  --- a/bson/src/test/unit/org/bson/codecs/pojo/PojoCustomTest.java  +++ b/bson/src/test/unit/org/bson/codecs/pojo/PojoCustomTest.java  @@ -265,6 +265,82 @@ public final class PojoCustomTest extends PojoTestCase {                   fromCodecs(new SimpleEnumCodec()));           roundTrip(registry, new SimpleEnumModel(SimpleEnum.BRAVO), """"{ 'myEnum': 1 }"""");       }  +      +    static class Foo {  +        private ObjectId id;  +        private Bar bar;  +  +        public Foo() {  +              +        }  +          +        public ObjectId getId() {  +            return id;  +        }  +  +        public void setId(final ObjectId id) {  +            this.id = id;  +        }  +  +        public Bar getBar() {  +            return bar;  +        }  +        public void setBar(final Bar bar) {  +            this.bar = bar;  +        }  +    }  +      +    static class Bar {  +        private ObjectId id;  +        private String name;  +          +        public Bar() {  +              +        }  +  +        public ObjectId getId() {  +            return id;  +        }  +  +        public void setId(final ObjectId id) {  +            this.id = id;  +        }  +  +        public String getName() {  +            return name;  +        }  +  +        public void setName(String name) {  +            this.name = name;  +        }  +    }  +      +      +    @Test  +    public void testIdWithoutUndescore() {  +        final CodecRegistry registry =getCodecRegistry(getPojoCodecProviderBuilder(Foo.class, Bar.class));  +        final ObjectId a  = new ObjectId(""""59a79beb24345d0004fe1a44"""");  +        final ObjectId b  = new ObjectId(""""59a79beb24345d0004fe1a45"""");  +        final String name = """"bar"""";  +          +        final org.bson.io.OutputBuffer encoded = encode(DOCUMENT_CODEC, org.bson.BsonDocument.parse(  +                """"{ '_id' : { '$oid' : '59a79beb24345d0004fe1a44' }, """"  +              +    """"'bar' : { 'id' : { '$oid' : '59a79beb24345d0004fe1a45' }, 'name' : 'bar' } }""""));  +        final Codec<Foo> codec = registry.get(Foo.class);  +        final Foo result = decode(codec, encoded);  +        org.junit.Assert.assertEquals(a,    result.getId());  +        org.junit.Assert.assertEquals(name, result.getBar().getName());  +        // will fail as value is null  +        org.junit.Assert.assertEquals(b,    result.getBar().getId());  +    }          @Test       @SuppressWarnings(""""unchecked"""")  --   2.14.3  {noformat}    I don't have a proposed solution, just the report.    Regards,    Juan.""",Bug,POJO
253758,"""This 3.4.5 db has a number of collections (about 10). When I query a collection by    {code:java}  mycollection.find(Filters.regex(""""field"""",""""anything matching""""))   {code}    against one collection, I generally get valid records from the WRONG COLLECTION if """"field"""" is not actually a field of the documents in the chosen collection. If """"field"""" is a valid field, or if I use eq instead of regex, this does not happen.    I would expect to get no matches if """"field"""" is not present in the documents in the collection referred to by """"mycollection"""".    Short of adding a lot of code to verify that only """"field"""" names are used that are actually in the documents, I don't see any way to fix this problem. Please help!        """,Bug,Query Operations
253817,"""An issue pretty much like JAVA-1931.    After a few operations on the database, idle time in between, I receive the following exception:  {noformat}  2017-06-09 10:37:34 com.mongodb.diagnostics.logging.JULLogger log [WARNING] Got socket exception on connection [connectionId{localValue:5, serverValue:366843}] to clusterxxx.mongodb.net:27017. All connections to clusterxxx.mongodb.net:27017 will be closed.    2017-06-09 10:37:34 com.mongodb.diagnostics.logging.JULLogger log [INFO] Closed connection [connectionId{localValue:5, serverValue:366843}] to clusterxxx.mongodb.net:27017 because there was a socket exception raised on another connection from this pool.    com.mongodb.MongoSocketWriteException: Exception sending message   at com.mongodb.connection.InternalStreamConnection.translateWriteException(InternalStreamConnection.java:465)   at com.mongodb.connection.InternalStreamConnection.access$1000(InternalStreamConnection.java:66)   at com.mongodb.connection.InternalStreamConnection$2.failed(InternalStreamConnection.java:323)   at com.mongodb.connection.netty.NettyStream$3.operationComplete(NettyStream.java:188)   at com.mongodb.connection.netty.NettyStream$3.operationComplete(NettyStream.java:184)   at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)   at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)   at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)   at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122)   at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:906)   at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:791)   at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1291)   at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:739)   at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:731)   at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38)   at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1090)   at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1137)   at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1079)   at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)   at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)   at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:445)   at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)   at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)   at java.lang.Thread.run(Thread.java:745)  Caused by: java.nio.channels.ClosedChannelException   at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)  2017-06-09 10:37:34 com.mongodb.diagnostics.logging.JULLogger log [INFO] Discovered replica set primary clusterxxx.mongodb.net:27017    {noformat}    As in JAVA-1931, I'm calling System.setProperty(""""org.mongodb.async.type"""", """"netty"""")  I'm using Vertx3 (Netty 4.1.8.Final) to connect to a MongoDBAtlas v3.4.4 and tried with different mongo-driver-async/core/bson versions: 3.4.1, 3.4.2 and 3.5.0 without any success.""",Task,Connection Management
253944,"""As a user I want to have an API for easy creating IndexOptions from Bson objects.    Given Bson object containing all the options I need - I have to individually map them into IndexOptions.""",New Feature,API
254266,"""When executing an aggregate pipeline, the Java driver gives me a NullPointerException without further explanation.    While debugging I found that it's happening while trying to unbox {{AggregateIterableImpl.batchSize}} of type *java.lang.Integer* to be written to field {{FindIterableImpl.findOptions.batchSize}} of type *int*.    When I specify a {{batchSize()}} at the {{Aggregate}} other than {{null}}, e.g. explicitly setting it to """"0"""" (the documented default value), everything is fine.    So, instead of just using {code}List<Document> result = collection.aggregate(pipeline).into(new LinkedList<Document>);{code} I need to write {code}List<Document> result = collection.aggregate(pipeline).batchSize(0).into(new LinkedList<Document>);{code} which is rather unusual and smells a bit bad ;-).    I think it would help to change the type of the batchSize attribute of {{AggregateIterableImpl}} from {{java.lang.Integer}} to {{int}}, but I cannot tell whether it has any side-effects...""",Bug,Query Operations
254724,"""Currently, the only thing that tests aggregation via the new API is CollectionAcceptance test. We should:   - Work out which tests, if any, that use the old API need porting to use the new API  - Find holes in the coverage and add tests.  In addition, there was an Ignored test in JavaClientOldTest:      public void testAggregateOnSecondary()  As it was ignored, this was not being run. But it's not clear why it was ignored or what it was trying to test.  If this is an important test, we need to implement it using the new API.""",Task,Test
254778,"""There are unused methods on the GridFS API, which suggests they are not tested:   - public DBCursor getFileList(final DBObject query)  - public DBCursor getFileList(final DBObject query, final DBObject sort)  - public GridFSInputFile createFile(final InputStream in)  In addition, the GridFSFile class is referenced in any test classes at all - granted it is an abstract class, but I would expect it to be tested as a side effect of testing the GridFS functionality.  There are also untested methods on GridFSInputFile:   - public int saveChunks()""",Task,Test
255400,"""Hi, I have an object inside my mongodb like this  { """"_id"""" : ObjectId(""""5129d8a4da06f4571f45d1a2""""),  """"User_Id"""" : {  """"Profile"""" : { """"Naveen_009"""": } } }. Now i want to check the profile value is Naveen_009,if result is true i want to insert data like this  """"Twitter"""" : { """"08 Feb 2013"""" : [ 11, 10, 0, 1 ], """"09 Feb 2013"""" : [ 890, 546, 35, 309 ], """"10 Feb 2013"""" : [ 728, 458, 30, 240 ] }. To do this am using following logic. BasicDBObject query =new BasicDBObject().append(""""009.Profile"""",""""Naveen_009"""");  BasicDBObject newDocument3 = new BasicDBObject().append(""""$set"""", new BasicDBObject(key,arrayObj)); collection.findAndModify(query, newDocument3); But as a result am getting result in some unwanted manner like this { """"009"""" :  { """"Profile"""" :  {""""Naveen_009"""": },  """"08 Feb 2013"""" : [ 1253, 857, 55, 341 ],  """"09 Feb 2013"""" : [ 890, 546, 35, 309 ],  """"10 Feb 2013"""" : [ 728, 458, 30, 240 ] }, """"_id"""" : ObjectId(""""5129bf1dda06f4709d2b98ee"""") } Help me to get required result if u suggest some books it il be help full too.""",Bug,API
255401,"""Hi,  I have an object inside my mongodb like this  { """"_id"""" : ObjectId(""""5129d8a4da06f4571f45d1a2""""),  """"User_Id"""" : {  """"Profile"""" : { """"Naveen_009"""": } } }.  Now i want to check the profile value is Glamour_009,if result is true i want to insert data like this  """"Twitter"""" : {  """"08 Feb 2013"""" : [ 11, 10, 0, 1 ], """"09 Feb 2013"""" : [ 890, 546, 35, 309 ],  """"10 Feb 2013"""" : [ 728, 458, 30, 240 ] }.  To do this am using following logic. BasicDBObject query =new BasicDBObject().append(""""009.Profile"""",""""Glamour_009"""");   BasicDBObject newDocument3 = new BasicDBObject().append(""""$set"""", new BasicDBObject(key,arrayObj)); collection.findAndModify(query, newDocument3);  But as a result am getting result in some unneeded manner like this { """"009"""" :  { """"Profile"""" :  {""""Glamour_009"""": },  """"08 Feb 2013"""" : [ 1253, 857, 55, 341 ],  """"09 Feb 2013"""" : [ 890, 546, 35, 309 ],  """"10 Feb 2013"""" : [ 728, 458, 30, 240 ] }, """"_id"""" : ObjectId(""""5129bf1dda06f4709d2b98ee"""") }  Help me to get required result if u suggest some books it il be help full too.      """,Bug,API
255418,"""We got the exception below. I think it's caused by a socket timeout, but from the exception message below, it's apparently not possible to know for sure. I would like the MongoException message to be different if the IOException is a SocketTimeoutException. Something like """"socket timeout trying to call something"""" rather than """"can't call something"""".  Here is the exception. I am not asking for help troubleshooting this exception, I am including it as an example.   com.mongodb.MongoException$Network: can't call something : (host info removed)         at com.mongodb.DBTCPConnector.call(DBTCPConnector.java:226)         at com.mongodb.DBApiLayer$MyCollection.__find(DBApiLayer.java:313)         at com.mongodb.DB.command(DB.java:174)         at com.mongodb.DB.command(DB.java:158)         at com.mongodb.DB.command(DB.java:198)         at com.mongodb.DBCollection.getCount(DBCollection.java:899)         at com.mongodb.DBCollection.getCount(DBCollection.java:870)         at com.mongodb.DBCollection.getCount(DBCollection.java:858)         at com.google.code.morphia.query.QueryImpl.countAll(QueryImpl.java:174) """,Improvement,Monitoring
255591,"""When dumping a database or collection with dates \-at least on Mac OS X and from .NET gererated Output\- all date-data-object are encodes as timestamps in the form:  {code} { when : { """"$date"""" : 1234567890000 } } // an unqouted Long value {code}  when I try to parse such an String with JSON.parse(), I either get an java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.String, if this timestamp is an unquoted Long or silently a null, if it was an quoted Long as in '""""1234567890""""'. Requesting such object leads e.g. to:   {code} { """"when"""" : null } {code}  I'm quite astonished that nobody found nor fixed this issue so far, since it is even an internally used format.  I fixed this in JSONCallback.java by assuming first that the value is a parsable Long which can be turned into a Date-object. In case of an NumberFormatException ISODate is will be tried by applying defined SimpleDateFormat (Which is also no correct ISODate, since incomplete format. But this is another issue.)  See attached patch. """,Bug,API
255900,"""The API should have a way to configure a minimum response time a slave must have before it can be used for queries (using slaveOK).  I have 5 mongod instances, 2 capable of being primary. 3 as read only slaves (priority 0). One of the 3 read only slaves is performing poorly (a switch is dying, latency is spiking > 900ms). Because the current algorithm has an element of randomness in it, some of the requests are being served by this poorly performing server. This is due to ReplicaSetStatus.getASecondary() calling """"if ( diff > 15 || ( ( badBeforeBest - mybad ) / ( _all.size() - 1 ) ) > _random.nextDouble() )"""".   The driver should support a minimum cut off. getASecondary() should enforce a latency ceiling in choosing random slaves. I've submitted a patch that does this, using a default of 200ms as a cut off, configurable in MongoOptions. """,Improvement,Performance
255902,"""mapReduce() has a DBObject signature which makes it easy to pass in a Command object in order to provide all the optional parameters to mapReduce. I'd like to see this same signature added for group()  Thanks for considering""",Improvement,API
258496,"""In the old UI, I was able to visually distinguish regular patches from commit queue tasks, since I could see the task titles:   !Screen Shot 2020-06-05 at 2.51.26 PM.png|thumbnail!   I am not able to make this visual distinction in the my patches view in the new UI.     Also, commit queue tasks are filtered by default in the old UI, but not the new UI.    I rarely need to review commit queue tasks when looking at my patches, so having them filtered by default is helpful. If I do need to review them, I would like to be able to know which are the commit queue tasks in the my patches page so I can more easily navigate to them.""",Improvement,ui
258873,"""As a DAG engineer,  I'd like to have a wireframe for the perfomance baron page in spruce, such that, I can use the wireframe to guide implementation and discuss the UI with other engineers.    AC:  * Wireframe exists""",Task,ui
258953,"""As a DAG engineer,  I'd like to add a coordinator service that is used by the expanded metrics table, such that, users can specify multiple sorts and have them ordered properly before being passed through to the service fetching these lists.    AC:  * Coordinator orders user input chronologically and passes it to the service properly""",New Feature,ui
317160,"""We are hosting a rather big Moodle instance and are seeing a lot of trafic these days due to COVID and schooling from home. Apart from legit traffic, there are also various attacks on the service, which in some cases, can introduce downtime / slowness of the service for most of the users.    Our site allows access to some parts of moodle without login (not even guest mode is needed) due to historic reasons and when we get an attack, this renders the service unavailable due to apache/php-fpm workers exhaustion as all requests are waiting for session locks (they are in session_lock() function most of the time. Some get processes and some get timed out).    !image-2020-10-22-09-58-16-048.png!    Would it make sense to use read-only sessions in cases where users are not logged in and not guests, something in the line of:  {code:java}  lib/classes/session/manager.php            if (!isloggedin() || isguestuser()) {              $requireslock = !READ_ONLY_SESSION;          } else if (defined('READ_ONLY_SESSION') && !empty($CFG->enable_read_only_sessions)) {              $requireslock = !READ_ONLY_SESSION;          } else {              $requireslock = true; // For backwards compatibility, we default to assuming that a lock is needed.          }  {code}  Would this work or would this explode in some way? I'm not too familiar with Moodle code and web app development, so I'm probably overlooking things.     """,Improvement,Performance
317164,"""If a very large PDF is uploaded (or created by a document converter), assign edit PDF will convert all the pages, one at a time, before any pages can be displayed. This is happens either when a teacher tries to grade the entry, or cron will attempt it for them.    For us, unoconv made a 40k page PDF out of a jpeg, when then kept the assign PDF cron task busy for a bit over 7 days. But the time it finished, there was a huge backlog of documents waiting to be converted, and resulted in a whole... thing. We saw this with a number of PDFs.     We applied a patch that if the page count is over 1000 pages, it only converts the first page. Conveniently, if the converted page count doesn't equal the PDF page count, the UI already displays a message saying that the PDF could not be displayed due to corruption or some other problem. We just updated that message a bit to include 'over 1000 pages'.     While I'm not saying our particular solution was best (minimal core code change was a criteria), I do think something needs to be done to this part of code, to prevent very larges PDFs from monopolizing resources. Internally we felt that it was impractical to think somebody was going to be using that markup interface for such a document anyways.""",Bug,Assignment
317203,"""We should only be accepting tokens sent to an account's current email address, so when a user's email account is updated, we should invalidate tokens used in password reset (forgotten password) links sent to the old address. (It would be worth also having a look for  any other similar tokens that may need similar treatment.)    Given the (default) short life of such tokens and the fact that they are single use, this is a fairly edge case, but is a valid fix to comply with security best practice. Taking that into account along with the low risk of being an exploitable issue, I've marked this as a security benefit, and not assigned a security level.         _Written up by_ [~<USER>_, based on reports by_ [~vivnat] and [~saurabhmhatre]""",Bug,Authentication
317216,"""I enrolled in the Plugin Development Basics today to check it out, and was instantly reminded at how jarring as a student the '<USER>as complete' is.    It is a way more natural flow to click into a page, read it, scrolling to the bottom as you go, and then when you get to the bottom you get a '<USER>as complete' button right there on the page. This is not only more natural but also gives a slightly better chance that the student has actually clicked into the page, and has actually read through the page. Every other LMS I've used Totara, Coursera etc, even courses in wordpress I've done work this way and I think for genuinly better UX reasons.    So proposing:    a) new activity competition setting for pages so say whether the completion button is on the course page, or inside the page, or both    b) possibly a site wide setting to set the default, and or lock it down so it's consistent everywhere    This also has the side benefit of when you have finished the page and go back to the course page you not only get the correct completion status but you might also get the next activities unlocked and visible, so it side steps the slowness due to another page reload when ticking the box (see MDL-41165)                    """,Improvement,"UX,Usability"
317218,"""I think this has been the case for a while but I only noticed due to a new visual look in 3.9 that adds a more prominent border.  {code:java}badge badge-circle calendar_event_user{code}  Those classes are used to add a little circle beside calendar events on the main calendar.    I'm not sure if these predate the addition of Bootstrap, but bootstrap has it's own .badge thing, and as a result these seem to pick up some styles that they probably shouldn't.    Notably, the padding makes them very slightly not circular.    !MDL-69945.png|width=525px!     """,Bug,HTML
317297,"""Hello,   I'm Ge5iveK. I'm reporting _Sessions-Based Insecure Direct Object Reference (IDOR) allowing unauthenticated user profile access_ in latest Moodle version 3.9.2 and possibly earlier versions. I have previously sent an email to moodle security, but there hasn't been any confirmation since a week, so I'm trying to report again using the tracker.    *1.0 INTRODUCTION*   Insecure direct object references (IDOR) are a type of access control vulnerability that arises when an application uses user-supplied input to access objects directly.    *2.0 ISSUE*   I recently tested for IDOR in Moodle and found that the proof of concept I'm going to give you was exploitable in all the institutions that I tested, that used Moodle.    I wanted to access all the online users (students) profile with my student account. So, I intercepted the GET request made for getting a student's profile. Surprisingly, I was able to just change the ID paramater and I could access an online student's profile. I tested this again after locally installing moodle and testing in a safer reportable manner. I found the same results. I tested using different devices for simulating real world students, and found the same result.    *2.1 PROOF OF CONCEPT (POC) / STEPS TO REPRODUCE*         Video PoC already sent to security@moodle & uploaded in tracker attachment below.    1. Open 2 student accounts, and add them both to the same course.   2. Log in using both the student accounts on different containers (machines), to simulate real world login.   3. Intercept the requests when the students click on 'profile' drop-down menu item which expands when we click the profile icon on the top right corner.   4. Assume that student 1 is the attacker and student 2 is the victim.   5. Send the attacker's GET request to repeater (i'm using burpsuite).   6. Copy the ID parameter in the GET request of the victim's request and replace the attacker's ID parameter with that of the victims.   7. The attacker can now access any logged-in user's profile without moodle session change and authentication.    Usually this should be protected against in the default moodle installation.    *3.0 IMPACT*   The security impact for organizations using moodle is very high because there is confidential information like students' personal email, sessions time, account access location, private files upload etc., on a student's main profile page. If this IDOR can be mixed with an intruder attack, then mass access to student profiles can be made by an attacker.    *4.0 CONCLUSION*   Thank you for reading this responsibly disclosed report. I would kindly request the moodle team to add me in the Hall of Fame (HoF) and declare this as a Common Vulnerability Exposure (CVE) report. Thanyou again for your time reading this.    ~ GeFiveK    :)     """,Bug,"User management,Authentication"
317306,"""I originally thought this was Atto-related, but it turns out the same thing happens when you set your HTML editor preference to be the basic text editor.    I also thought it only applied to the user policy editor, (where this effect can be seen both in the policy and the summary) but later found other places that it affects.    It seems that any mildly complex HTML in combination with the multi-lang filter will cause problems, but not in every location where you can enter text.  {code:java}<span lang=""""en"""" class=""""multilang"""">  <p>english</p></span>  <span lang=""""fr"""" class=""""multilang"""">  <p>french</p></span>{code}  When the above is saved something, I'm not sure what, is rewriting that so that it looks like this:  {code:java}<span lang=""""en"""" class=""""multilang"""">  </span><p>english</p>  <span lang=""""fr"""" class=""""multilang"""">  </span><p>french</p>{code}  and therefore breaking the multilang behaviour, as the spans no longer wrap the relevant text.    Without any html tags like <p> or <div> inside the span, it works as expected.    Also, the same thing appears to work fine in some places, e.g. a label or a a desciption for a forum, but then not in a forum post, where the same behaviour is seen.    Possibly this is something to do with security, and some areas are put through more extensive checks than others for XSS etc?    Worth noting that technically, divs, p tags etc. aren't allowed inside spans, but I believe that's what you need to use for this multilang filter. (I think there is a 3rd party one that supports using {} instead of html tags).          """,Bug,"Language,HTML"
317321,"""Sometimes you have the need of having a Moodle course which is just informal and not really a 'course'.  * A collection of material for your faculty to share (that could also be a shared question bank or a content bank).  * A course with all students enrolled for the scope of notifying them through announcements.    These courses should be treated in a special way: typically you don't set an end date. This means you don't require enrolled users to regularly access.    Those courses shouldn't trigger insights (learning analytics) notifications. The models 'Students at risk of dropping out', 'Upcoming activities due' and 'No teaching' do not send out actions therein.    Another possibility could be to respect such courses differently in regards to other reporting: Actions in those courses woudn't weight in in (site?) stats or analytics.    User stories:    As an administrator, I want to avoid situations where my teachers come to me and complain about """"Students who have not accessed the course recently"""" notifications in courses which are 'eternal' and therefore unrelated.    As a teacher, I want not to receive """"Students at risk of dropping out"""" messages for my 'eternal' course in which I provide general study materials not related to a specific lecture. Therefore I want to tick a """"eternal"""" checkbox in the course settings or setting the correct option in a choice """"timed course""""/""""eternal course"""".""",Improvement,Course
317382,"""1. Choose a course and navigate to Course Administration -> Reports -> Course participation.    2. Choose any option that will display at least 1 user.    3. Click the checkbox to select all users.    4. With selected users...send a message.    You will notice that the """"Send message to # people"""" button text displays one additional user. Sending the message will fail with no obvious error – the """"Message sent to # people"""" confirmation dialog does not appear, and the messages will not be sent out. I've tested this on 3.9 and 4.0 (Moodle QA Testing Site).    Upon closer inspection, you will see that the """"select all"""" checkbox is being passed as an argument along with the actual users that were selected, causing the entire process to fail.    The problem lies in report/participation/amd/src/participants.js. """"[data-toggle='slave']"""" needs to be added to the """"Selectors"""" constant as follows:    const Selectors =\{ bulkActionSelect: """"#formactionid"""", bulkUserSelectedCheckBoxes: """"input[data-togglegroup^='participants-table'][data-toggle='slave']:checked"""", participantsForm: '#participantsform', };    When this addition is made, bulk messaging works as expected. In fact, """"[data-toggle='slave']"""" is already included in user/amd/src/participants.js, where bulk messaging also works as expected.    This is my first bug report, so any guidance on how to properly address issues like these would be greatly appreciated. Thanks, everyone!""",Bug,Reports
317401,"""I'd like to create a front page (ie page arrived at after login) that is customised to different user groups. This will allow me to provide content that is more tailored to the needs of each of these groups (- these needs can be +very+ different).    However, current options for the front page appear to be limited to either the Dashboard or Site Home, and neither of these pages appear to allow limiting access to blocks, activities or resources - except by role.    To get around these limitations I'm planning to register a domain name just so I can point it to a course page which I am setting up for this purpose as a 'Start Page'. I'll then publicise this domain name as the address for our Moodle site rather than the standard url.    So my suggestion/request would be that it would be much more useful if a further option was added to Dashboard and Site Home, allowing admins to specify a default course page that users would be taken to after successful login. This would allow the full flexibility of course pages to be used, eg restrict access via synced cohorts, user profile fields etc.    If you wanted to extend this idea, you could also provide users with the ability to specify which page they would like to land on after login. I'd suggest this would be a true 'user preference' rather than the current choice available for the defaulthomepage field.    (I was hoping this suggestion would already be covered by MDL-57208 however it appears this isn't the case.)    Thanks,    <USER>",Improvement,"UX,Usability"
317419,"""An issue that I'm sure we all struggle with on a regular basis is the time limitation imposed by features all over Moodle.    Thankfully we usually make good use of out time constants, including {{MINSECS}}, {{DAYSECS}}, {{WEEKSECS}}, {{YEARSECS}}, {{DAYMINS}}, and {{HOURMINS}}. As a result we should be able to increase these values and reduce the time pressure that all users experience.    Whilst this may be a break from standard times as typically defined, I believe that these modest increases will be well received and lead to a significant increase in productivity. With enough momentum we should be able to convince other organisations to follow suit.    We still need to raise an RFC with the IETF to open this for comment, and provide a migration plan for those on the old system, but this proposal puts Moodle at the heart of a Working Candidate for approval.""",Improvement,Admin
317516,"""In Moodle 3.7, course custom fields were introduced and there is something I have wondered about. Why have you formatted dates as human readable strings it makes no sence. A webservice API is an integration tool and now you are dictating how other systems should display / use the date.    We have a problem using the dates, since we would like to see if the date has passed or not and now we have to parse the date first. This could be an issue, since the date also takes into account the users preferred language and parses from that setting. This means it is quite messy determining the correct format, parse it and then comparing it to the actual date.    Why is not just an unix timestamp as the rest of moodle dates?         I would almost categorize this as a bug, since it is almost useless if you use it with an external system.""",Improvement,"Course,Web Services"
317570,"""When resetting a course and deleting all submissions of an assignment with PDF annotation enabled, we've realized that the submissions are deleted but the assignfeedback_editpdf files linked to them aren't. That leaves orphaned files on the MoodleData, and they can make storage troubles in the long term.    I don't know if the approach should be to delete them when deleting the submissions or to create a scheduled task looking for and deleting orphaned assignment feedback files.    Replication steps:   # Create a course, enrol a student and create an assignment for file submission with PDF annotation enabled  # As a student, submit a file to the assignment (not a PDF, to force the conversion)  # As a teacher, go to the """"Grade"""" page of the student's submission (the submission should be converted to PDF when doing it if it wasn't already converted by the scheduled task)  # Reset the course checking """"Delete all submissions"""" on the assignment section  # As an admin, make a backup with user data and download it  # Change the extension from MBZ to ZIP and browse the content. The PDF version of the submission is correctly deleted, but in the files.xml you should find the file again  # The file itself should be inside the files folder. On this example is this: files/83/8351695938a60ce9b524ea2e53f29e168a26cf16  # I've seen on the database that on the files table most of these orphaned files have these filearea values : readonlypages, pages and combined.""",Bug,Assignment
317718,"""Due to what I would call a 'non-normalization' of the Analysis interval, users can get an _Upcoming activities due_ email after the activity was due.    Let me give a specific example.    We have our analytics set to run every night at 6am (default is once a day, at a random hour).    There is a user that has an Analysis interval that looks like this:    Tuesday, July 28, 2020, 3:10 PM to Tuesday, August 4, 2020, 3:10 PM    So what happens for this user, is that each Wednesday at 6am, it checks for assignments due from the Tuesday prior at 3:10PM, until the Tuesday after at 3:10PM. Anything due after Tuesday at 3:10PM will show up in the *next* weeks email, after that thing was already done.    In this particular case, the user had something due Tuesday Aug 4th at 11:59PM, but that was included in the August 5th processing (ie. late), not in the July 29th processing.    Basically all of our users fall in these non-standard time slices, and I'm not clear how it happened - or for that matter how the slice is decided the first time a new user is analyzed.     """,Bug,Analytics
317768,"""The function toggleInlineSelectionClass in styling.js was updated when rangy deprecated createCssClassApplier and replaced with createClassApplier (MDL-51661).    However, rangy Version 1.3 resulted in a change from using the HTML elements """"className"""" property (which accepts a list of space separated values) to using """"classList.Add"""", which takes a comma separated list.    The only impact I'm aware of is to the """"styles"""" plugin for which an issue was logged (MDL-53338). Someone then tried to fix the styles plugin by changing the code to iterate through the classes and only apply one at a time - unfortunately not recorded in (MDL-53338). Perhaps this worked in some version of Moodle, however, it fails now (at least) because it only applies one class even if multiple are specified. This is possibly because the code looses focus / the selection after applying the first style. I've run out of time to chase this up and I only picked this up by reading through the code (see next para). So someone also needs to double check the details - should be straightforward(?).    Being new here I'm not sure of the proper processes for a fix and I don't have a dev environment set up (I just read through the code). I'm happy to be advised about both and contribute code. I've previously worked as a programmer (C++, VB, C# .net etc.)... I've currently picked up javascript (due to using Moodle) and had a quick look at PHP etc. All pretty straightforward to get across.""",Bug,"HTML,Libraries"
317821,"""h1. Steps to reproduce  h2. As a teacher:   # Create new assignment with:   # Submission types = File submissions   # Require students to click the submit button = Yes    h2. As a student:   # On the assignment, Edit submission:   # Upload a file and save changes   # Open the Assignment page (with the Submit assignment button) _in two browser tabs_   # In one of the tabs, click and confirm the """"Remove submission"""" button (the submission now has no files)   # In the other tab, click Submit assignment and confirm.    h1. Expected results    Moodle rejects the submission, due to checking and finding that it had no files.  h1. Actual results    Moodle accepts the submission.  h1. Notes    Obviously this is a slightly pathological case, as it must take some malevolence or considerable confusion for a student to take those steps and create an empty submission - but I've had a student that appears to have managed just that!""",Bug,Assignment
317854,"""First, I would like to say thank you so much for what you do!  I teach chemistry and your team has been instrumental in helping me to create engaging and interactive content online.  My goal is to include symbols and equations in H5P content.         1. Before upgrading to Moodle 3.9+, only <USER>hvp was available.  I uploaded the MathDisplay library 1.0.5 and was able to inlcude symbols and equations through LaTeX in Course Presentation text, multiple choice, drag and drop, etc (I have not been able to use mhchem as I am in other parts of Moodle, however)  After upgrading to Moodle 3.9+, both <USER>hvp and <USER>h5pactivity (through the Content Bank) are available.  The <USER>hvp works as before, however, when I try LaTeX in <USER>h5pactivity, it does not work/display.  I've tried this in the course and site Content banks with no luck.  Thank you in advance for your help.      2. Moodle version: Moodle 3.9+ (Build: 20200618)    3. Form: Desktop    4. Browser: Firefox 78.0.2    5. H5P versions: <USER>h5pactivity (2020061500) and <USER>hvp (2020020500)    6. Course Presentation 1.21.7    7. Any browser errors?: No browswer console errors    8. Any PHP error?: No PHP errors    9. Any screenshots to include?: No screenshots to include    10. Any changes to Moodle?: upgraded to Moodle 3.9+ which made Content bank and <USER>h5pactivity available for the first time    11. Any changes to the browser?: No updates to the browser that I am using (Firefox)    H5P framework v1.24 (h5plib_v124)    I checked filters and they are set to display LaTeX""",New Feature,H5P
317896,"""Lets say I have 4 tabs open with 4 different moodle pages.    My session times out, eg the next day I pick things back up. If I reload all 4 tabs (which can also happen automatically with a browser session resumes) then all 4 of them redirect to the login page, and in the process each sets in the session state the wants url and so the last one to write to it wins.    Symptom 1: Then I log in again in one of them at random, and I might go to the page I wanted but 75% of the time I'll go to the page from one of the other tabs.    Symptom 2: now on the other 3 tabs I'm technically logged in but they are still on the login page and don't know that yet. If I reload them then I get a dumb error: """"You are already logged in as Admin User, you need to log out before logging in as different user.""""    So proposing:    1) the wantsurl state is additionally stored, where possible, in the url of the login page as a query param    2) the param is kept on subsequent login page submits, ie if you get your password wrong    3) if the wantsurl is in the param and the session the param takes precedence    4) If you load the login page, and you have a wants url, and you are logged in, then just redirect back and don't show the red herring error message    This should fully solve the issue for non-sso auth plugins. The sso ones need to fix it themselves.    There is also a 3 third half overlapping bug which is around the hash anchor being lost after a login redirect, which I think can be solved as part of this or might get split out, eg:    /admin/search.php#linkappearance     """,Bug,"Authentication,Usability"
318095,"""[~<USER> has reported a major performance regression with 39 over 38 for administrators.    I've tracked it down to the Badges v2 API.    When logged in as an administrator, or any user to manage badges, every inclusion of the {{admin/settings/badges.php}} file (i.e. when building the site admin tree) triggers a call to {{badges_verify_site_backpack()}} which attempts to authenticate against badgr.io.    Ideally we should only do this periodically, and cache the result (success/fail).    Not only are we killing our performance, we're also DOSing badgr.io.    This is the current output:  {code}  stdClass Object  (      [expires] => 3600      [error_description] => Too many login attempts. Please wait and try again.      [error] => login attempts throttled  )  {code}    In addition we're trying to authenticate without providing a username or password _anyway_ so it will _never_ succeed.  The site backpack does not even allow me to enter any issuer credentials except for password.    I think that we should not even be making this call at all at all.""",Bug,"Performance,Badges"
318138,"""For zip files we support the option to turn them into a SCORM activity, but we don't appear to do so for H5P.    I've tried this several times expecting it to work, and it does _appear_ to work because the h5p file is created as a File resource and gets the H5P Activity icon.    Surely we can support creating an H5P activity from drag/drop upload?""",Bug,H5P
318164,"""As mentioned by Jun during his review of MDL-67883 we add a way to allow site admins to decide what will be displayed in the activity chooser footer with the default set to MoodleNet.    This will help in the case another plugin implements the required callback and wins the race against MoodleNet.    See below for context:  h5. \core_course_external::get_activity_chooser_footer()   # I'm not sure whether we should be limiting the execution of other plugins implementing the """"{{_custom_chooser_footer}}"""" hook. If another plugin implements this function, but we still want to display the link to MoodleNet, if the call to """"{{\get_plugins_with_function()}}"""" runs the other plugin's implementation first, then the MoodleNet link won't be shown anymore. Perhaps we should remove the breaks in the loop and either do the following:   ** Allow the activity chooser footer to be able to display multiple activity_chooser_footer items from plugins.   ** Add an admin setting that allows the admin to select which `activity_chooser_footer` item should be displayed in the activity chooser footer. This would provide reliability on the item will be displayed in the activity chooser footer as opposed to the first-processed, first-displayed nature of the current implementation.   Let me know your thoughts. In any case, this can be done as a separate issue/follow up.""",Improvement,Course
318219,"""I'm getting an """"Invalid response value detected"""" error when calling the WS <USER>h5pactivity_get_results. More details:    {{attempts => Invalid response value detected: duration => Invalid response value detected: Invalid external api response: the value is \""""4 seconds\"""" of PHP type \""""string\"""", the server was expecting \""""int\"""" type}}    It seems the WS is returning the duration already formatted, but it should be returned as an integer.    EDIT: I tried setting the duration as a hardcoded integer and then another error is thrown:    {{attempts => Invalid response value detected: results => Invalid response value detected: options => Invalid response value detected: Error in response - Missing following required key in a single structure: correctanswer""""}}}    I'm having these errors with a drag&drop type, I attached it to this issue.""",Bug,"H5P,Web Services"
318291,"""I really am not sure if this is a bug since I am sure that someone else should have found it, but I am not doing anything that I wouldn't normally do in Moodle.    I created a course and enrolled myself under another name etc. into the course.    I also created an assignment which allows the student to enter free text as a submission type.    I also set that the student must click submit, and must also click on accepting the submission statement.    As a teacher, there is also an event monitoring rule that checks that the statement has been checked.    So as a student I went in and completed my assignment. Checked all the boxes and left the course.    Then as a teacher I went in to grade the work. As a teacher I see the grade button, but when I click on it it returns an error message, see below. I can activate quick grading, and I can grade using quick grading, but I do not get to see the submission preview window, which is odd.""",Bug,Assignment
318383,"""I've been trying to solve an issue where I was previously listening for a native event and have had to update my code to listen for an Event generated by {{core/custom_interaction_events::trigger()}}.    I have hit a snag because the two APIs are not compatible and lead to different results.    These are my findings so far:    h2. JavaScript Events    h3. jQuery:  The following trigger:  {code}  element.trigger('someEvent', 'Hello', 'Marvin');  {code}    # only triggers to jQuery listeners - i.e. those added via a call like:  {code}  $.on('someEvent', handler});  {code}  # provides any provided parameters ( e.g. {{'Hello'}}, {{'Marvin'}}, etc) as arguments _after_ the {{Event}}, i.e.  {code}  $.on('someEvent', function(eventObject, saluation, name) {  });  {code}    h3. Vanilla JS:  The folloiwng trigger:  {code}  element.dispatchEvent(new CustomEvent('someEVent', {      bubbles: true,      detail: [          'Hello',          'Marvin',      ],  });  {code}    # Trigger both Vanilla listeners, and jQuery listeners. i.e. both of the following will be triggered:  {code}  $.on('someEvent', handler});  document.addEventListener('someEvent', handler);  {code}  # Passes the additional parameters in the {{detail}} element into the {{detail}} element of the eventual handler. e.g.  {code}  $.on('someEvent', function(e) {      window.console.log(e.detail); // Will return `['Hello', 'Marvin']`  }});  document.addEventListener('someEvent', e => {      window.console.log(e.detail); // Will return `['Hello', 'Marvin']`  });  {code}    h2. Question  h3. How do we provide a migration path from jQuery events to vanilla events?  With difficulty.    Issues:  # They have a different API (detail vs. params)  # jQuery listens to vanilla events but does not trigger vanilla events    Possible solutions:    h4. Multiple triggers  When we call {{element.trigger()}} we also generate a new CustomEvent with a different name    Example:  {code}    // In core/custom_interaction_events::triggerEvent():  $(e.target).trigger(eventName, [{originalEvent: e}]);  e.target.dispatchEvent(new CustomEvent(getNativeEventName(eventName), {      bubbles: true,      detail: {          originalEvent: e,      },  }));    // Existing (b/c way):  $(someDiv).on(CustomEvents.someEvent, function(e, data) {      window.console.log(data.originalEvent);  });    // Listen to new event via jQuery:  $(someDiv).on(CustomEvents.getNativeEventName(CustomEvents.someEvent), function(e) {      window.console.log(e.detail.originalEvent);  });    // Listen to new event via native APIs:  someDiv.addEventListener(CustomEvents.someEvent, function(e) {      window.console.log(e.detail.originalEvent);  });    {code}    This works for the most part but is cumbersome:  # We need to generate an Event name for both  # For Vanilla listeners we need to listen for a different event name to allow for b/c  # Possible issues with {{stopImmediatePropagation}} on whichever event type is called first""",Bug,JavaScript
318514,"""As a student/teacher, I want to easily be able to add emojis, links and videos to my entries so that I can create posts and content that will be more enjoyable for my audience.     Currently the below 4 filters are automatically enabled:   * Display H5P   * MathJax   * Activity names auto-linking   * Multimedia plugins    This means that emoticons aren't automatically converted to images, URLs not converted to links and youtube links aren't automatically converted to videos, in places like forum quick replies. This is what people are used to and expect nowadays.    Update the filters that are enabled by default to:   * Display H5P   * MathJax   * Activity names auto-linking   * Display emoticons as images (moodle auto-format, html and markdown ON)   * Convert URLs into links and images (moodle auto-format, html and markdown ON)   * Multimedia plugins    Also update the atto toolbar to include the emoji picker by default (highlighted in *bold* below):    collapse = collapse   style1 = title, bold, italic   list = unorderedlist, orderedlist   links = link   files = *emojipicker*, image, media, recordrtc, managefiles, h5p   style2 = underline, strike, subscript, superscript   align = align   indent = indent   insert = equation, charmap, table, clear   undo = undo   accessibility = accessibilitychecker, accessibilityhelper   other = html""",Improvement,Admin
318676,"""*As a teacher*   *I want* to be able to select from the file picker the H5P content stored in the folders of the Content bank   *So that* I can reuse it easily everywhere (activities, resources...).  h3. Acceptance criteria:   * Be able to select content stored in the folders of the content bank through the repository   * Be able to browse the folders of the Content bank in the repository.   * Be able to differentiate the different types of folders in the repository:   ** Folders of the Content bank   ** Folder to represent System, Categories and Courses.   ** See the attachments to see how to show the folders.    h3. List of tasks:   * Implement needed methods or functions to show the folders in the content bank repository.   * Design and implement folders view.""",Improvement,"H5P,Repository"
318766,"""I wanted to insert a user as a student in all Moodle courses, so I marked the type of context system on the student role. At Assign system roles, I choosed the user as a student. But when I loged in as the user, I can't access the courses that he should be enrolled in automatically.""",Bug,"Roles / Access,User management"
318903,"""h2. Default Settings for Completion Criteria for Activities and Resources    Add a setting page at the system level to manage all the default completion criteria for each activity and resource    At the system level we have the activation of completion tracking.   Just like competencies, the activation should add a specific settings page for general completion criteria.    This page will be able to use the content of the """"Default activity completion"""" tab accessible from the course.    This is because it can be set at the course level, but not at the site level.  h2.      Summary    Project size: medium   Audience: primary schools, universities, work places   Target users: teachers, administrators       h2. User Stories    Today, as an admin I can:    enable system level completion tracking from Enable completion tracking   choose the default completion tracking   by using the default activity setting   by not displaying the completion status    Tomorrow, as admin, I'd like to:    enable system-level completion tracking from Enable completion tracking   choose the default setting for each activity/resource independently of each other    Today, as an admin, at site level, I don't have any presets available for completion criteria.   Everything is at the course level.    Tomorrow, as an admin, I would like to have the possibility to manage the default completion criteria for the whole site for each activity/resource.    As a teacher, I should only be able to change the criteria I wish to change.    For example, when the settings are at site level on """"default activity setting"""", the label has a manual default criterion   The teacher must change this criterion each time a label is added or think about doing it in a bulk action in each course.    If the admin can set at system level that for the label, by default no expected completion, the teacher will not have to do anything else.       h2. Requirements   * Before completion tracking enable      !proj_visu01.png!   * Completion tracking activation      !proj_visu02.png!   * After completion tracking enable      !proj_visu03.png!   * Course completion criteria panel      !proj_visu04.png|width=1104,height=687!   * In the course :    !visu04.png|width=1120,height=651!""",Improvement,Admin
318916,"""As a Moodle provider I would like Moodle to be highly available in the face of hardware failures or nodes going away.    Cockroach DB is for better or worse postgres SQL compatible and requires some code tweaks to get working.""",New Feature,Database SQL/XMLDB
318958,"""A little bit of history of the validation of profile fields shortnames:   - Originally only {{PARAM_ALPHANUM}} was allowed.   - In MDL-63068 we switched {{PARAM_TEXT}} plus some extra validation in {{define_validate_common()}} to ensure that only {{[a-zA-Z0-9_]}} are allowed.   - No matter the clear note, there are some issues (not critical for now, because they are readers/getters that are adhering to the {{PARAM_TEXT}} trend. Last case is MDL-67641.    So, this issue just proposes to change that {{PARAM_TEXT}} to {{PARAM_ALPHANUMEXT}}, knowing that the later also allows """"-"""" (and {{define_validate_common()}} will continue controlling it). But, at very least, will prevent the proliferation of {{PARAM_TEXT}} that are way too much for this case.    Ciao :)    PS: BTW, I've already seen a couple of cases (recently) where we want to just enable {{{{[a-zA-Z0-9_]}} }} maybe we should consider adding some new type or adding some """"extra"""" param to allow the regular expressions to be dynamically expanded when some more char is allowed.... note this is a side idea, it doesn't belongs strictly to this issue.""",Improvement,User management
319059,"""Steps to reproduce:   # Login as admin.   # Create a course. Enable completion tracking for this course.   # Add an activity to the course (let's say, an assignment).   # Create another activity to the course (for instance, another assignment). During the activity creation, add a restriction with """"Activity completion"""" and select the previous activity.    Unexpected behaviour:    A label-warning is displayed next to the delete button x (a screenshot is attached to display it).         I've been able to reproduce it only in master (in 3.8 is working as expected).""",Bug,Themes
319097,"""*USER STORIES*   As a user who is +not logged in+, I want to be able to open multiple tabs while browsing and I want Moodle to remember my page locations in all tabs and maintain this for the duration of my session.    As a user who +logs in after+ browsing and opens pages in multiple tabs I want Moodle to remember my page locations in all tabs and maintain this for the duration of my session.    As a +logged in+ user, I want to be able to leave my session and come back to it and when prompted to login again be able to continue my session without losing my page locations in all tabs.        *CURRENT BEHAVIOUR*    User +logs in after+ browsing & opening multiple tabs   * All tabs will redirect to login page and ask for username/password.   * Once user has +logged in+ from any one tab, the other tabs will still show the login page so user is forced to refresh OR try to hit login button.   * When they do either, since the system recognises you are already logged in it will reload the page with a confirmation prompt:   alerting you to the fact that you're already logged in as [NAME] and whether you want to log in as a dfferent user and provides only two button options:  _Logout_ & _Cancel._   * If you select either Logout or Cancel, it will redirect to the homepage and loses your page location on all tabs.    User +is logged in+, has multiple tabs open and the session expires   * The session timeout warning dialog box will appear: 'Your session has expired please login again.'   * There is no link to login page or a login button or a continue session option   * It doesn't update the not logged in/login option at top right - so it still shows your username as if you're logged in   * To log back in, you have to refresh the page, which will prompt you for a login, then when you login it will take you back to the page you were on   * This only works for the first tab, all other tabs will not remember the page location.        *EXPECTED BEHAVIOUR*   When you're either already logged in or login in after browsing,    _If presented with the login page:_   * User can login from any tab   * All tabs will then know that the user has logged in   * All open tabs will remember page location (previous to login)    _If presented with confirmation prompt:_   * It should provide a 'Continue as current user' button instead of 'Cancel' button to allow the user to continue where they left off (applies to all tabs)   * Once they hit the Continue button, it should redirect the user to the page they were on or the homepage if there isn't a session history yet.""",Improvement,Authentication
319229,"""h3. Summary    Integrate the H5P editor with all its functionality in core. Based on the H5P Editor PHP library and the <USER>hvp plugin.  h3. User stories    _As an instructional designer_   _I want to modify an existing H5P content_   _So I edit the content in the H5P editor_    _As an instructional designer_   _I want to create a new H5P content_   _So I create the content in the H5P editor_  h3. Tasks   - Add H5P editor library in core (lib/h5p/editor)   - Moodle-ish the implementation of the interfaces (following Moodle coding standards)   - Display inside a standalone form (lib/h5p/editor.php)   - Integrate translations with Moodle (if possible)""",New Feature,H5P
319230,"""*As a teacher*  *I want* to be able to search in the content bank repository by content's name  *So that* I can find the content I need to select in the file picker  h3. Acceptance criteria:   * Be able to search content by name in the content bank repository   * Reset search criteria to see all content in the content bank repository    h3. List of tasks:   * Implement needed methods or functions to search in the content bank repository.   * Design and implement search results view.   * Reset the search criteria in order to go back to 'see all' view.""",Improvement,"H5P,Repository"
319231,"""*As a teacher*   *I want* to select from the file picker the H5P content created in the Content bank   *So that* I can reuse it easily everywhere (activities, resources...).  h3. Acceptance criteria:   * Create a new repository for users to be able to select and reuse content uploaded/created in the Content bank.   * Show the content in the content bank inside the file picker.   * Select content from this repository and attach it everywhere where the file picker is displayed.    h3. List of tasks:   * Create new repository_contentbank plugin skeleton   * Adapt or create DB tables as needed   * Create capabilities   * Implement the Privacy API or implement a null privacy provider and create a new issue   * Create a view for the content bank to be shown inside the repository_contentbank""",New Feature,"H5P,Repository"
319232,"""*As an instructional designer*  *I want* to edit existing H5P in the content bank  *So that* I can improve, update or fix existing H5P content in the site  h3. Acceptance criteria:   * Be able to access H5P editor via content bank to change existing H5P content.    h3. List of tasks:   * Define information wokflow between content bank and H5P content editor to fill the editor with existing content.   * Implement needed classes, method and functions to edit existing H5P content.   * Add new PHP Unit tests   * Add behat tests or create a new issue to add them""",Improvement,H5P
319242,"""*As a manager*   *I want* to log users' activity   *So that* I can check when H5P content in the content bank has been created, updated, deleted or viewed.  h3. Acceptance criteria:   * Log when content in the H5P content bank has been created.   * Log when content in the H5P content bank has been updated.   * Log when content in the H5P content bank has been deleted.   * Log when content in the H5P content bank has been viewed.    h3. List of tasks:   * Create contentbank_h5p viewed event.   * Create contentbank_h5p created event.   * Create contentbank_h5p updated event.   * Create contentbank_h5p deleted event.""",Improvement,H5P
319244,"""*As a user*  *I want* H5P content bank implements the right privacy provider based on the user data stored  *So that* my privacy rights are protected  h3. Acceptance criteria:   * All user content information should be managed by the H5P content bank privacy provider.     h3. List of tasks:   * Implement provider class for contentbank_h5p""",Improvement,H5P
319245,"""*As a user*  *I want* content bank implements the right privacy provider based on the user data stored  *So that* my privacy rights are protected  h3. Acceptance criteria:   * All user content information should be managed by the content bank privacy provider.    * Export data for all the contexts allowed (System, Course Category and Course).    h3. List of tasks:   * Implement provider class for contentbank""",Improvement,H5P
319246,"""*As an instructional designer*   *I want* to be able to search in the content bank by content's name   *So that* I can find the content I need  h3. List of tasks:   * Implement needed methods or functions to search in the content bank.     """,Improvement,H5P
319247,"""*As an instructional designer*  *I want* to be able to remove empty folders the content bank  *So that* I can order content in a better way  h3. Acceptance criteria:   * Be able to delete empty folders.    h3. List of tasks:   * Implement needed methods or functions to delete folder.   * Implement needed capabilities to delete folders.""",Improvement,H5P
319248,"""*As an instructional designer*  *I want* to be able to remove existing content from the content bank  *So that* I can delete wrong or old content  h3. Acceptance criteria:   * Be able to delete existing content.   * The user would see a warning message because we can't know if that content is in use, or where is used. The user could confirm or cancel deletion.    h3. List of tasks:   * Implement needed methods or functions to delete content.   * Implement needed capabilities to delete own/other users' content.   * Implement the warning informing the user.   * Adapt H5P player to be sure there's no error if the file to render doesn't exist.""",Improvement,H5P
319250,"""*As an instructional designer*  *I want* to be able to move content from one folder to another  *So that* I can order content  h3. Acceptance criteria:   * Be able to move a content from the root to a folder.   * Be able to move a content from a folder to another.    h3. List of tasks:   * Implement needed methods or functions.   * Implement UI to move content from one folder to another. In this initial phase we would need menu options to move content and drag&drop won't work.""",Improvement,H5P
319253,"""*As an instructional designer*   *I want* to have the option to change the name of the content   *So that* I can correct mistakes, typos, order content in a better way or make the content easier to find.  h3. Acceptance criteria:   * Be able to change existing content's name.    h3. List of tasks:   * Implement functions needed to change the name of the content.   * Create or use capabilities to update content.   * Implement the UI to rename content.""",Improvement,H5P
319256,"""*As an instructional designer*  *I want* to have the option to create folders into content bank  *So that* I can order content  h3. Acceptance criteria:   * Be able to create folders in the content bank that other users can see.   * Be able to upload H5P content into a folder.    h3. List of tasks:   * Implement the way folders are created and stored in the content bank   * Implement the way to create folders into existing folders.   * Create generic capabilities for the Content Bank   * Adapt content bank view page to show folders   * Add new PHP Unit tests   * Add behat tests or create a new issue to add them""",Improvement,H5P
319257,"""*As an instructional designer*   *I want* to have a shared content bank   *So that* I can share content with other users in the site    *As an instructional designer*   *I want* to be able to upload H5P fils to the content bank   *So that* I can share content with other users in the site  h3.  Acceptance criteria:   * Be able to upload some H5P content that everybody with proper permission can see.   * Be able to implement plugins that would manage some other content type and share with other users.    h3. List of tasks:   * Create new 'contentbank' plugin type   * Create DB tables as needed for the content bank   * Create generic capabilities for the Content Bank   * Implement a null privacy provider for now   * Create a new view page   * Create a new H5P 'contentbank' plugin   * Adapt or create DB tables as needed for the plugin   * Create capabilities for the H5P plugin.   * Implement a null privacy provider for the plugin   * Implement the option to upload H5P content to content bank   * Add new PHP Unit tests   * Add behat tests or create a new issue to add them""",New Feature,H5P
319270,"""When using the Classic theme, blocks cannot be placed in the Right region of an activity module page, i.e. assignments, forums, etc.       *Steps to replicate*   * Login as admin   * Create a new course   * Create an assignment   * Add the activities block to the course page   * Configure the activities block to appear in the Right region on any page   * Click Assignments from the Activities block on the course page        *Expected result*: Activities block appears on the right-side of the Assignments page   *Actual result*: Activities block appears on the left-side of the Assignments page       I've reproduced this behaviour in the Classic theme on 3.6.2, 3.7, 3.7.1 and 3.8.1. Note that the issue is not present using Boost theme on the same versions, nor is it present using Clean theme on 3.6.2; please see attached screenshots for reference.       Re-configuring the block region from the activity module page has no impact. Adjusting the weight of the block while configured to appear in the Right region simply moves the block vertically along the left side of the page relative to any other blocks also configured to appear in the Right region.""",Bug,Themes
319280,"""Hello,    I'm one of the company administrators on our MoodleCloud website and I am unable to access the site this morning. See the screenshot of the error below.          I was advised by our IT department that this is a host error. Could you please advise when this will be fixed?     Please note that I can't access Moodle to check to see which version we are running, but I think it would be the most recent.""",Bug,Accessibility
319303,"""I cannot restore a course that I backed up using the standard Moodle interface,  The course just sits there with """"Course restoration in progress"""" as the title, and shows under """"Available courses"""" on the site home when I'm logged in as admin (but not when logged out - I've turned visibility off).    I cannot delete the course using the standard Moodle interface - deleting it does nothing.  I also cannot delete the category that contains the course - deleting it does nothing.  I tried to delete it via a php script, and that also failed.           Disabling the recycle bin finally allowed me to delete the course.""",Bug,"Course,Backup"
319325,"""*As a teacher*  *I want* to log users' activity  *So that* I can when H5P activity has been created, updated, deleted or viewed.  h3. Acceptance criteria:   * Log when a H5P activity has been created.   * Log when a H5P activity has been updated.   * Log when a H5P activity has been deleted.   * Log when a H5P activity has been viewed.    h3. List of tasks:   * Create <USER>h5p viewed event.   * Create <USER>h5p created event.   * Create <USER>h5p updated event.   * Create <USER>h5p deleted event.""",Improvement,H5P
319326,"""*As a manager*   *I want* to able to include H5P activity in course backup and restore process   *So that* I can backup and restore courses with all their activities  h3. Acceptance criteria:   * H5P activities should be included in the course backup file.   * H5P activities should be restored from course backup files.     h3. List of tasks:   * Implement backup classes for <USER>h5p   * Implement restore classes for <USER>h5p""",Improvement,H5P
319328,"""*As a user*  *I want* H5P activity implements the right privacy provider based on the user data stored  *So that* my privacy rights are protected  h3. Acceptance criteria:   * All user content information should be managed by the H5P module privacy provider.     h3. List of tasks:   * Implement provider class for <USER>h5p""",Improvement,H5P
319332,"""*As a teacher*   *I want* to create an H5P content activity   *So that* I can use the H5P activities as other Moodle activities  h3. Acceptance criteria:   * Create a new activity type for teachers to be able to upload an H5P file and render it in the course.   * Be able to define activities standard options such as Availability, Restrict access and other common module settings.    h3. List of tasks:   * Create new <USER>h5p plugin skeleton   * Adapt or create DB tables as needed   * Create capabilities   * Implement a null privacy provider for now   * Create a new edit page and its form   * Create a new view page   * Implement Moodle's basic activity completion not using H5P completion   * Add behat tests or create a new issue to add them""",New Feature,H5P
319388,"""This is more a placeholder to collect more ideas on a more holistic and performant approach following on from MDL-67486, MDL-67211, MDL-67483 and MDL-67363.    Things are now much better but at high scale, but with very unequal sized adhoc tasks you can still end up with some tasks hogging cron abd blocking processing of other things. MDL-64610 will help a lot here, but in an ideal world the task manager would dynamically adjust the priorities of tasks based on as much info as it has and not need manual tuning by either the developer or the admin.    Some example scenarios:    1) A queue of very slow tasks, eg async backups that take 10 mins each, is followed by some small tasks like sending emails which generally we want to be done fairly fast. Even with QoS the slow tasks end up pegging all of the available cron runners, because QoS is only considering what should start next based on what is in the queue, not what is already running.    2) You have say 2 or 3 types of heavy task and nothing else. We end up splitting the load 50/50 and cron is pegged on heavy tasks. There are no 'spare' runners to start on a random new type of task which comes along.         The concept I'm thinking about is roughly:    1) after MDL-67211 lands we have metadata on what is running and for how long total, grouped by type    2) when we look at what should be picked up next we weight the priorities by the totals above, already running tasks get progressively lower and lower priorities    3) We tune this so that if there is say 10 runners, then no one task can ever hog more than say 2/3 of the runners so we always have something spare to start on new tasks, but we don't have an explicit limit on any one type of task    4) If new types of task appears in the queue then we want to balance the runners across all of them. So if there are 5 types of tasks and 10 runners then each should get roughly 2 processes each regardless of how long each specific task takes.    5) The current QoS layer slows down at scale, try to rebuild it in sql as much as possible          """,Improvement,Tasks
319438,"""When doing *grunt ignorefiles* on Windows systems, this can lead to ignorefiles containing the following path (as an example):    *auth\saml2\extlib\saml2/*    Worse, this could also lead to stuff like    *auth\saml2\extlib\saml2\/*    To prevent this we changed                {{var lib = path.join(dirname, node.toString());}}    to               {{var lib = path.posix.join(dirname, node.toString());}}    I'm not sure if this is really a bug or if this would be massaged away by the linting processes.  I just want to check because this is a hack I'd like to lose if it isn't necessary.""",Bug,JavaScript
319455,"""I'm using the <USER>of Moodle to create and update thousands of courses. At the end, I need to know which courses have been created, updated or when nothing happened.    Unfortunately the webservice *core_course_update_courses* gives only warnings when something goes wrong, but I can't find any way to know if some data actually changed.    Would it be possible to receive success feedback (""""updated"""" + """"unchanged"""") as well as the warnings ?    The problem is the same for *core_user_update_users* which returns no feedback at all.    I also didn't find anything about it in the documentation.  I think it affects all versions.    Does MoodleHQ see this as a good potential improvement ?    Thanks a lot.""",Improvement,Web Services
319462,"""I wrote some questions using some images from my """"private files"""" folder. The questions are shown well, and the quiz also worked well.  Now, if I go into """"Manage private files"""" from my Dashboard, I expected to view all the resources in which I used each image (as a link).  A list box appears: There are x alias/shortcut files that use this file as their source.  But that box only shows me references to """"page"""" resources.  If I imported an image (as a link) into a question, the """"Manage private files"""" box only shows the word """"Undisclosed"""".    So, my question is: can I use private files (as links) in quizzes?  I wouldn't want it to cause inconsistency in the question bank or in the quizzes. I'm scared by the word """"Undisclosed""""...  """,Improvement,Files API
319568,"""When we run `grunt amd` it is _very_ slow. It should not be. There's a bug:  Every time we call the eslint or dist commands we are generating a list of files to work on. That list uses globstar ({{**}}) patterns like {{**/amd/src/**/*.j}}.    Ultimately these are slow and non-performant. We already have a list of all subsystems, and plugintypes, and we can easily turn that into a list of components (subsystems + plugins + subplugins). From there we can create a smaller list whcih, while still using the globstar format, has a much smaller surface area.    Whilst I discovered this with babel:dist, I also discovered that it applies to several other places, and there was some confusing behaviours too:  * eslint:amd suffered the same/similar issue  * eslint:yui suffered the same/similar issue  * stylelint:css suffered the same/similar issue  * stylelint:scss suffered the same/similar issue  * in a directory [path/to/component] the grunt command had different behaviours:  ** {{grunt}} builds the whole of Moodle  ** {{grunt watch}} watches just the current directory  * in a directory [path/to/component/amd] the grunt command had different behaviours:  ** {{grunt}} builds only the current directory  ** {{grunt watch}} claims to watch just the current directory but does nothing    Therefore I've taken the liberty of fixing these issues too. They all had a similar root cause and similar fix.""",Bug,JavaScript
319645,"""*DESCRIPTION*  """"Nothing is Submitted"""" Error Occurs when a Student Submits Risky Content while """"moodle/site:trustcontent"""" is Enabled    On Moodle 3.6 and Moodle 3.7, it appears that in at least two instances, the setting """"moodle/site:trustcontent"""" is being ignored. When its enabled, students are still unable to submit iframes, or embedded content as a response to their assignments.    I haven't been able to replicate this issue on Moodle 3.8, however, upgrading isn't an option.     *CLIENT IMPACT*  medium. They are trying to leverage an embed feature we've included but it requires students to submit iframes and embed code into their responses.    *REPLICATION STEPS*  # As an administrator:  ## Enable trusted content from Site Policies  ## Enable trusted content from user policies  # As a student:  ## Go into a class  ## Go to respond to an assignments  ## click on the html button  ## Paste an iframe-based response    Expected Results: Since trusted content is enabled, the response is submitted successfully  Observed Results: The submission errors with """"Nothing has been Submitted"""".     *TROUBLESHOOTING*  - Confirmed that trustcontent is enabled for students at both the GUI and Database.    !trust-content-1.png|thumbnail!      !trust-content-student.png|thumbnail!     {code}  mysql> select roleid, capability,permission from <USER>role_capabilities where roleid = (select id from <USER>role where shortname = 'student') and capability like '%trust%';  +--------+--------------------------+------------+  | roleid | capability               | permission |  +--------+--------------------------+------------+  |      5 | moodle/site:trustcontent |          1 |  +--------+--------------------------+------------+  {code}    {code}  mysql> select * from <USER>config where name like '%trust%';  +-----+-----------------+-------+  | id | name | value |  +-----+-----------------+-------+  | 220 | enabletrusttext | 1 |  +-----+-----------------+-------+  {code}    - GUI:   - Iframe content isn't specific and all content is impacted  - No errors in the webconsole when the submission is attempted     *RELEVANT INFORMATION*  - Issue:   !issue-1.png|thumbnail!     *RECENT CHANGES*  - Trusted Content enabled at system and user level    *ANALYSIS/RECOMMENDATION*  Based on the fact that trustcontent is enabled, and iframes aren't being submitted by a student, I want to believe that this is a bug. I'm hoping that somebody might be able to point out a setting I missed so we can get back on track though.  """,Bug,Assignment
319736,"""Could we please have the string """"Post to forum"""" (<USER>forum,posttoforum) for posting to forums in general?    If I want to answer a forum post in Moodle 3.7 I must """"Submit"""" my reply to the forum unless I use the Advanced option, in which case I must """"Post to forum"""".   In Moodle 3.8 with """"Use experimental nested discussion view"""" set to Yes, I must """"Post"""" my reply to the forum, while I """"Post to forum"""" as usual in advanced mode.    Now the translation is a little tricky, while it is not possible to see the origin of language strings when replying to a forum post.   So I tried unsuccessfully to find the string """"Post"""" on my own. The """"new"""" option, """"Post"""" is not taken from forum.php, and luckily it is not the core string """"post"""", which is translated as a noun in the Danish lang pack.""",Bug,Language
319741,"""When I make an assignment submission using an editable PDF filled with data, Moodle stores the original file and I can download it without issues. The problem comes in when I want to grade the assignment. The FPDI plugin strips the text from the PDF and gives me blank PDF without information like the student submitted a blank document.     Copy from the linked issue:  Steps to reproduce the problem:  # Make a course with an activity that accepts submissions in pdf format.  # Enrol the course, set yourself both to Teacher and Student.  # Get a pdf file that's not protected so you can write on it in any pdf viewer (I tried Foxit 9.7.1 and the latest Acrobat DC).  # Type any text on the pdf and save it.  # Upload it as a submission to your activity.  # Go to grading, find your submission, click Points.  # Moodle converts the uploaded pdf into an other file and presents a view. The student-made annotations are missing from the view.  # Still on the Grading page there is a link to the uploaded pdf file. Click on it.  # Your browser downloads the pdf file. Open it in any external pdf viewer. The annotations will be there.""",Bug,"Assignment,Grading methods"
319910,"""Our file system makes content unique based on its contenthash.  In H5P we store this contenthash.    We should be able to do some <USER>where, when we unpack a package for the first time, we check for a matching contenthash in the h5p table.  If we find it but the pathnamehash is wrong then we should be able to simply _copy_ the existing record and update the configuration of the pathnamehash and content.    What I would suggest is that for the _first_ time a package is updated - i.e. there is _no_ contenthash match we unpack it etc. and then place it the core_h5p component in a masters filearea. That acts as a _master_ and means that the first time a copy of that file is viewed, if it is viewed by a student, it gets the _default_ settings.""",Improvement,H5P
320091,"""User Story:  As an admin, I want to make use of $CFG->curlsecurityblockedhosts to prevent that evil Moodle users scan my internal networks for any resources.  However, if I have a subnet larger than /24 for IPv4 or /112 for IPv6 to protect and to allow a small number of hosts within this subnet at the same time, I have a hard time to specify all the blacklist rules. This is because range notation is only allowed for the last address element.    As an admin, I would like to have a setting $CFG->curlsecurityallowedhosts and a setting $CFG->curlsecurityallowbeforeblock which will accompany $CFG->curlsecurityblockedhosts in this way:    * Hosts and subnets from $CFG->curlsecurityblockedhosts will be blocked as they have always been.  * Hosts and subnets from $CFG->curlsecurityallowedhosts will be allowed.  * If $CFG->curlsecurityallowbeforeblock is enabled, hosts which are found in $CFG->curlsecurityblockedhosts and $CFG->curlsecurityallowedhosts at the same time are not blocked.  * If $CFG->curlsecurityallowbeforeblock is disabled, hosts which are found in $CFG->curlsecurityblockedhosts and $CFG->curlsecurityallowedhosts at the same time are blocked.""",Improvement,Admin
320567,"""As a teacher, I use Assignments to control access to problem statements, due dates, grades, etc. But the students actually submit their work elsewhere online. I would like to make it impossible for students to upload their work to Moodle, so I unclicked """"File submissions"""" under Submission Types for the Assignment.    Unfortunately, the feedback the student sees when """"File submissions"""" is disallowed says,  """"Submission status    This assignment does not require you to submit anything online.""""   Which is simply not true.     Can we get this changed to be """"anything here"""" or """"anything in Moodle"""" or something that would be correct?""",Improvement,Assignment
320682,"""There is a whole class of 'config edits' which are strictly speaking not config edits because they are not in <USER>config or <USER>config_plugins, but most admin's would think of them in the same way. Essentially anything in an admin externalpage.    There is also a large class of these 'other config edits' which currently don't emit a moodle log entry either, such as disabling or editing a scheduled task (see MDL-66287) so there is currently missing audit trail. The more you look the more you realize how much isn't logged like enabling / disabling plugins, changing the order of auth plugins, crud'ing OAuth services. I'm kinda scared to keep looking.         The 'config changes' report is good, but I think conceptually should show all of these changes:     /report/configlog/index.php    Also I think it could be improved a lot, eg obvious things like filters, but it would be great down the track to layer on the ability to revert a config change - which means it needs to remain highly structured so it knows how to reverse a changed. Right now that is trivial but I'm thinking of reverting the delete of something complex like an OAuth2 service with lots of parts to it.         I'm not really sure the best approach though, either we:         a) change the schema of <USER>config_log / add_to_config_log() so that it can accept more general classes of config changes    Pros:   * ?    Cons:   * lots of code touch points to push the data into it         b) the config changes report needs to pull stuff out of the moodle log.    Pros:   * config changing code just emits an event    Cons:   * The moodle log doesn't have the structure required, all the   * will be extremely slow and /or impossible to query if the logs are truncated or an external log store is used etc         c) or maybe something a bit larger building on (b):    Calling add_to_config_log() also emits a moodle log entry. Weirdly these log records reference a 'create' on the <USER>config_log table which seems a bit off, wouldn't it be more accurate to report an 'updated' (or create/delete) on the real <USER>config_plugins table instead? We would certainly not 'undo' something on the <USER>config_log table.         It seems to me that in hind site this is the wrong way around, it would be better if:    a) there was a base 'config change' event class which things could extend and the config changing code just emits the event. This base class would define the semantics of how to show before and after values and how to restore the state in its log event    b) The config changes report can then have a log observer and filter out just the config events out and stash them as needed for faster and persistent reporting.    c) deprecate add_to_config_log() as it's redundant     """,Improvement,Admin
320835,"""Steps to reproduce:  * Login as admin  * Set $CFG->notifyloginfailures to your admin account  * Leave $CFG->notifyloginthreshold set to 10 which is the default  * Create 11 user accounts in Moodle  * Get 11 devices which are talking to Moodle through a NAT router  * On each of these devices, do one login attempt with one of the user accounts _with wrong credentials_ in a way that no user account is attempted twice.    Expected result:  * You will not be logged into Moodle with any of the user accounts  * As admin, you will not be notified about these single login failures    Actual result:  * You will not be logged into Moodle with any of the user accounts  * As admin, you will get an email about the fact that there were more than 10 login failures    Interpretation:  * As admin, I want to be informed if there is a high amount of login failures. Currently, Moole informs me if the configured login failure threshold is exceeded for the same user or the same source IP.  * For login failures from the same user, 10 is a good value which should trigger a notice to the admin.  *  At the same time, for login failures from the same IP (as it is the case with users connecting over NAT), 10 is really low and will trigger too many false alarm emails. If one of the users behind the NAT router runs a brute force attack on more than one account, I want to be notified not before a larger number of login failures like 100.""",Improvement,User management
320836,"""As an admin, when going to """"Users > Accounts > Browse list of users"""", I only get a """"User full name"""" search box. To be able to search for other common user attributes like username or email adress, I always have to click """"Show more"""" at the moment.     To help me with my daily searches for users, I want to be able to configure which user filters are shown by default and which user filters are still hidden behind the """"Show more"""" link.""",Improvement,User management
320862,"""I've done a bit of research, and I believe that bundling all compiled JS in first.js is a false economy with significant drawbacks.    At the moment, the first time a page is loaded we load _all_ AMD JS in Moodle.    This currently accounts for approximately 900KB and growing.    The TTFB is also high at about 80ms as we combine all of the various files and then echo them out at the end. This therefore also has an impact on memory as all of this content is stored in memory until it is echoed out (peak memory usage just before we finish compiling is about 1.3MB).    If any single JS file has an issue then all JS on the site will fail.    Additionally when we debug we change things around and send the unminified sources instead (about 9.5MB).    All of this means negative things for scale, for user experience, reliability, for browser memory usage, and for debugging.    I believe that, once MDL-62497 lands, we will be able to switch to one AMD module per request and leverage browser caching far more effectively.   With the introduction of source maps in 62497, we can make these always available for files which have one, to facilitate easier debugging of production systems.    From what I can see, whilst the first load will require more files to be fetched, these will be fetched faster, and cached more efficiently. We will also be more reliable, and less browser memory.""",New Feature,JavaScript
320863,"""The ability to randomise element IDs was introduced, as an optional config for forms, in MDL-65217. That issue didn't make all required changes, resulting in JS errors when trying to validate a simple form with randomisation of element ids enabled. The result is that the form posts, when it should not.    I'll include a test script demonstrating the problem, which can also be used for testing the fix.    To replicate the bug:    1. Place 'testform.php' in your webroot and access it  2. Open browser dev tools -> console and make sure 'persist logs' is enabled (or equivalent). Leave the console open.  3. Submit the form empty    Expected: The form validation will catch the empty field and display a form error message without submitting anything (the field is set to use client validation after all)  Actual: Notice a JS error and that the page posts the form""",Bug,Forms Library
320961,"""+Repro Steps+   # Login as an admin and set the capability """"category:viewcourselist"""" to """"Not set"""" for all the roles.   # Enable the guest access enrolment plugin.   # Choose a course and enable guest access to it via the """"Enrolment methods"""" screen in the course administration area.   # Verify in the course settings that the course is visible.   # Login as a non-admin user which is not enrolled in the relevant course.   # Try to access the course         +-Expected- Actual Results+    The course can't be accessed and a message says that the course is not open for students.    +-Actual- Expected Results+    The course should be open to non-enrolled users, since guest access is active.    +Problem+    The capability category:viewcourselist affects the guest access to a course, even it should not, in my opinion. Sorry for the lame metaphor, but the fact that I don't want someone to see the entire route to a place, doesn't mean that I wouldn't want to give him the option to get there blindfolded :)    We don't want our users (student and teachers) to see regular courses that they are not enrolled in, but we still want to let them access support courses that are supposed to be open for all the users in the system.""",Bug,Course
320999,"""*What is happening*  We had a few users in our database that got created with the same username as another user. We fixed it with the extension merge users and that suspended the old user. Once we upgrade to 3.5.6 we got an error on login that says <USER>login() multiple found. Then returns an error as the $user variable. Since it returns an error Moodle thinks there is no user and for every login moodle will create a new user. We has some users that got up to 15 users created in 3 days.     *Expected Outcome*  When a username is stored in the database and one is suspended I would expect moodle to grab the unsuspended user and log the user in on the active user.     When a username is stored twice in the database and none are suspended I would expect moodle to grab one of the users, preferably the lowest ID number and log the user in on that user. Before the bug in 3.5.6 the user would be logged in, but sometimes it was inconsistent on which user they would be logged in as.     *Work Around*  Delete all duplicate users. You can not have the user Suspended, they must be deleted from the database. """,Bug,"User management,Authentication"
321053,"""At the university where I'm working, we have made sure that teachers can not use files that exceed all size restrictions (moodle / course: ignorefilesizelimits). We have a 200mo restriction by default.    On the other hand, the teacher has the possibility of granting this permission for his students in his class. We wondered if it was normal for the teacher to grant this permission when he does not have it himself.    I did several tests by granting me, as a student, this permission and I was able to file a 6go file without any problems.    My team and I want the teacher to no longer be able to give this permission *moodle/course permission:ignorefilesizelimits* to his students.              To reproduce the bug, here the steps:   # Make sure that the teacher role does not have the moodle / course permission: ignorefilesizelimits in _Site administration > Users > Permissions >Define roles_ and select _Teacher._ In the filter type *ignore* to be sure that the teacher role does not have permission moodle / course: ignorefilesizelimits.(See Step 1 image)   # After making sure the teacher role does not have permission, create a homework assignment with the default 200mo limit in your Moodle space.   # After creating the assignment, go to the list of your participants. Click _User Administration> Permissions_. (See step 3 and 4 image)   # In _Advanced role substitute_ select the _Student_ role. Write *ignore* to simplify your search. Click on the *+* and add this permission to your students. (See step 3 and 4 image)   # After granting this permission to your students. Log in as a student and try to hand over a homework that exceeds the 200mo limit. You should be able to do it.    *We use the 3.4 version of Moodle""",Bug,Roles / Access
321279,"""This is a natural follow on from MDL-60043    To set the scene this is an the symptoms we are seeing in a large site running on postgres and tracing it back to the root cause    1) get_role_definitions_uncached takes around 500ms as a best case. When things are bad we've seen it spike up to double digit seconds. There are 33000 records for some role definitions.         2) We are seeing the _uncached function regularly being run by different seemingly unrelated and read-only requests for random requests. Often it is various ajax calls but this is purely because they are is called very often and are just the victims here, eg:    /lib/ajax/service.php?sesskey=xxxxx&info=core_calendar_get_action_events_by_timesort     /<USER>quiz/autosave.ajax.php         3) When this happens, we see a cluster of requests together racing to rebuild the roledef cache and then it comes good again. All these requests which would normally be ~100ms are now ~600ms         4) The roledef cache item is being purged which in this particular case was traced back to an academic who was editing their course. Over the course of a couple hours they made many small changes. Lots of ad hoc tasks created 'course_delete_modules'  which eventually calls accesslib_reset_role_cache(). But there will be a bunch of code paths to that cache reset.         5) In debugging this I also found that each of the roledefs cache items were roughly 1mb when uncompressed, so every request looks at 1 but often 2 roles and so it is loading a crazy amount of io from muc / redis.         So I think just about every step in the chain could be improved here:    a) get_role_definitions_uncached the sql could possibly be further tuned / indexed         b) In various places instead of simply resetting the roledef cache we should rebuild it once. Especially in the places like the ad hoc tasks. This solves 3)         c) But the real root cause here seems to be the design the of this cache key which holds information on capabilities and access across the entire system. So when any change at all happens anywhere then the whole thing is throw away. I'm thinking a better approach might be to split this into smaller cache items, eg 1 for the main trunk of the tree starting at the site context and then smaller cache items for each course. When we need it we would load both and graft just the parts of the context tree that we need. When we invalidate something we can just purge the cache for a single course. This solves 4) an 5)         d) To support c) get_role_definitions_uncached can have a second optional $course or $context argument so we can query just the part of the sub tree we need instead of the whole site. Maybe passing the context path is the best way to filter.         e) Possibly making the role cache localizable. Maybe this is redundant if c) & d) are implemented.""",Improvement,"Performance,Roles / Access"
321323,"""I'd like to suggest an improvement to the LTI integration with Echo360 ALP.  (Perhaps this could be part of consideration of MDL-51012 for instance.)    Currently links to existing Echo360 ALP videos break when a course is rolled over. This means that in each time a new version of a course with these links is restored, the links need to be recreated to the same videos on Echo's systems.     In preparation for year start, this means manually relinking hundreds of videos in rolled over courses, with the prospect of the number of these videos growing each year as use of our Echo platform increases. This manual relinking also slows down finishing prep of any single ad hoc course with Echo links that needs to be backed up and restored.    I understand that the reason that these links break is that a new resourceid is created for each Echo link on restore. (I've contacted Echo support to ask if this can be confirmed.)    As a possible solution to this, I'm wondering whether it might be worthwhile adding a field to mbz files that records the resourceid of the originally created link (eg 'originalechoresourceid'). This field could then be linked to a checkbox shown on the restore screen which would allow the user to select whether they wanted the original Echo links restored or fresh, unlinked copies of the Echo activities added to the page instead.    I also understand Echo might be working on an update to fix this but there's no ETA.    Thanks""",Improvement,Backup
321350,"""h2. Intro  A number of the missing params come from the following scenario:    # I am on a page which has a file manager  # There are no query params in the current URL because the current page was reached via a POST  # I open the file manager    The act of _opening_ the file manager causes the current page to be fetched for some reason. I haven't worked out why yet - the code is difficult to follow and the inspector claims that it has come from setting the innerHTML of some part of the file manager, however I'm unable to work out which one as none of them contain the offending URLs.    h2. Replication instructions:    h3. Problem D: Badges  # I am a teacher  # I am on a course page  # I click on """"Badges""""  # I click on """"Add a new badge""""  # I click on """"Choose a file...""""    h3. Problem G: Folder  # I am a teacher  # I am on a course page  # I turn editing on  # I add a new folder setting only the name  # I view the new folder  # I click on """"Edit""""  # I click on the """"Add..."""" icon in the file manager    h3. Problem  J: Backup  # I am a teacher  # I am on a course  # I open the settings menu  # I choose """"Restore""""  # I click on """"Manage backup file""""  # I click on the """"Add..."""" icon in the file manager    h3. Problem E: Lesson  Note: There may be another way to reach this issue  # I am a teacher  # I am on a course with editing turned on  # I create a new lesson  # I click on """"Import questions""""  # I click the """"Import"""" button without entering a file  # I click on the """"Choose a file..."""" button  Note: I haven't worked out how to reproduce this issue on the editpage.php URL""",Sub-task,JavaScript
321365,"""I propose two new insights for teachers:  * Students who have not accessed the course recently  ** Periodic insights (e.g. weekly, monthly...)  * Students who have not started the course since its start date  ** Just 1 insight generated after X days (e.g. 15 days, 1 month...)    The model can extend from the course_enrolment base and reuse an existing read_actions indicator as any course view generates a 'read' event. The suggested actions would include the usual Send message + View details + Acknowledge + Not useful. I would not include the """"Outline report"""" here though.    These models have great value for the resources we have to dedicate to them.""",New Feature,Analytics
321495,"""When the session lock is getting hammered for whatever bad reasons, it's generally a poor user experience to sit and wait around for the session to unlock. The root causes are many and being tackled elsewhere ie MDL-58018, MDL-64449.    But if the session is locked, and the request is idempotent, ie a simple GET request which wouldn't change any critical state on the server, then it would be a much better UX to just abort the request and say 'hey you are doing things in another tab or the site is super busy' rather than waiting around forever.    This also happens when thing have gone south during massive scaling events, which makes things worse as all these requests queue up and sit there waiting and polling the session store. It is also worse because if a user reloads the page and / or aborts the request, the front ends can't detect this and kill the process until *after* they've got a session lock, and then do stuff and then try to write to the socket, which will then fail and abort the process. It's way too late in the process.    So I'm proposing to have two session lock timeouts, one for basic idempotent / reads which could be configured to something very short like 10-20 seconds., and one for things which change state which could be a couple minutes so the user is less likely to lose data.    The heuristics I have in mind are:   * must be GET   * must not have a sesskey in the url   * must not have NO_OUTPUT_BUFFERING set (these long running ones should unlock the session anyway)    Also I am thinking of refactoring as part of this and introducing the two generic timeouts as    $CFG->session_lock_timeout    $CFG->session_lock_idempotent_timeout    and making all the core session stores honor these new settings as well as their respective existing store specific settings.     """,Improvement,Performance
322013,"""I'm using Postgresql. The insert_record_raw() function has a $bulk parameter.  In  the code """"@param bool $bulk true means repeated inserts expected"""".  But in /lib/dml/pgsql_native_moodle_database.php the $bulk parameter is never used in the function at all.  It seems like no bulk insert logic was ever added to pgsql's insert_record_raw().  My issue is I can't use insert_records() because I need to define my own id field values.     Were there ever overlooked plans to make an """"insert_record*s*_raw() """" instead of a $bulk parameter on insert_record_raw()?    I'm not sure if this is a bug or an incomplete feature that got scrapped.     """,Bug,Database SQL/XMLDB
322118,"""Every now and then we have to purge caches on our production server to flush out something. We try not to purge caches during business hours but will when it's absolutely necessary, such as an emergency fix. The other day one of our admins purged the caches in an attempt to try and fix an audio issue for students writing tests in our Test Centre (not sure what the audio issue was, that's an issue for another day). When he purged the caches (using """"Purge All Caches"""" in Site Administration) everyone got slammed with these mustache """"Failed to write cache file"""" error's. I was notified and hit the page and saw the same error. I then purged our Memcached, Moodle Cache and OPCache. Hit refresh a few times, still saw the error and after about the 5th refresh it went away.    I can now replicate this issue.    We typically have an average load of 200 up to 500+ users. So I ran a load test on our test stack, which is identical to our prod stack, I was simulating 200 users that login via CAS, write 1 of 3 quizzes, post to a forum and view random files. This test was done with JMeter for 10 minutes and within that time I had hit """"purge all caches"""" once with the UI button and then twice via CLI, 2-3 minutes apart. I had added various extra error_log statements so I could try to track the order of events. When I purged the Moodle caches, while running the test some interesting things were revealed.    Here's my setup:   - Moodle Version 3.5.3   - Load balanced to 2 Debian 9.6 VM's   - PHP 7.0.33-0+deb9u1 fpm-fcgi Linux with Opcache   - Both head ends point to one Dell Compellent NFS file system   - Memcached in place using Twemproxy pointing to a Couchbase server with multiple buckets.    The extra logging was added to the following functions:    lib/mustache/src/Mustache/Cache/FilesystemCache.php      - buildDirectoryForFilename()      - writeFile()    lib/moodlelib.php      - purge_all_caches()    lib/outputrenderers.php      - get_mustache()    What seems to happen here is as soon as I hit """"Purge All Caches"""" the get_mustache() function immediately creates a new cache directory based on the $themerev number changing. My initial $themerev is 1549486915 and it then creates a new folder, 1549556523. In that same function immediately right after the new folder is created it then tries to perform a cleanup and delete all older folders, in this case the 1549486915 folder. With other users hitting templated pages the log's show in the FileSystemCache.php -> writeFile() function that files are now being created in both folders and it continues back and forth several times, creating and deleting. Eventually the 1549486915 get's purged and both app server's show the Error:    """"Default exception handler: Exception - Failed to write cache file""""    This is all before the purge_all_caches() function is finally triggered which then deletes everything......as it should.....followed by one more:    """"Default exception handler: Exception - Failed to write cache file"""" error.    Then I see my log message: moodlelib.php -> purge_all_caches() -> localcachedir has been created, all mustache cache files are now being created in the new 1549556523 folder.    Right now I'm thinking is it critical to check and remove any old localcache/mustache/[$themerev] folder for every single mustache request or as soon as a new $themerev folder is created? During those few seconds users may be hitting the new or old localcache/mustache/[themerev] folder......so leave it there. Obviously in this case the purge_all_caches() function obliterated all folders, including the newly created 1549556523 folder (which was already in the process of populating the cached files). There are various ways to tackle this, maybe have a scheduled task to remove the old localcache/mustache/[themerev] folders every night or x days or whatever.    Thanks,    <USER>""",Bug,Caching
322122,"""I will try and explain this as best as I can:         I have 3 moodle instances on the same server running the same version. Since I upgraded to 3.6.2, 1 of them is having issues converting student submission (docx) documents to pdf's for annotated grading.         My tests that I've run produce non-consistent results:    I will refer to instances as instance a, b, and c – where c is our most active    Test 1:    Newly created Docx files in Word 2013    a) Instance a - Existing course/assignment - edited submission, uploaded new doc, went into grading and pdf was generated.    b) Instance b - Existing course/assignment - edited submission, uploaded new doc, went into grading and pdf was generated.    c) Instance c - Existing course/assignment - edited submission, uploaded new doc, went into grading and pdf was generated.         Test 2:    Downloaded submitted doc from student submission on instance c that is not generating pdf    a) Instance a - Existing course/assignment - edited submission, uploaded new doc, went into grading and pdf was generated.    b) Instance b - Existing course/assignment - edited submission, uploaded new doc, went into grading and pdf was generated.    c) Instance c - Existing course/assignment - edited submission, uploaded new doc, went into grading and pdf was blank         Test 3:    Used same doc from test 2, but open and saved as a new document with a new filename    Stopped testing a and b as it doesn't seem to be an issue    Instance c - Existing course/assignment - edited submission, uploaded new doc, went into grading and *pdf generated as expected*         Test 4:    Went into an existing course/assignment where issues was reported to me.  First assignment - grade - went through all assignments and pdf was generated from docx submissions for all students.    2nd assignment - no submitted documents are being generated.         Once in awhile this error appears in my log:    PHP Warning: touch(): Utime failed: Permission denied in ../moodle/files/converter/unoconv/classes/converter.php on line 139    Does anyone have any ideas as to what the issue might be?""",Bug,Assignment
322182,"""MDL-60944 is scheduled for 3.7, it will allow people to create as many prediction models as they want, reusing the same target.    A new """"Restrict model to the following contexts"""" option would allow people to set up separate predictive models for different course categories or courses. A model restricted to a set of contexts (just CONTEXT_COURSE and CONTEXT_COURSECAT) would limit the analysable elements used by the model to analysable elements whose context is equal to, or a child of, any of the contexts specified in """"Restrict to the following contexts"""".    The required changes to core could be summarized here:  # A new field to 'models' db table (e.g. contexts)  # A new field in the edit model form (e.g. an autocomplete field with a list of courses and course categories)  # A new \core_analytics\local\analyser\base::supports_context_filtering() method to show/hide the new field (the point right above this one)  # Modifications to the analytics API to pre-fill options['filter'] with the list of contexts  # Modifications to core's get_analysables_iterator() implementations (core_analytics\local\analyser\by_course and core\analytics\analyser\users) to receive $this->options['filter'] as a parameter and take it into account   # A new helper method somewhere in core_analytics to return an SQL that joins with the context table and uses the ctx.path value of the provided context to filter out analysables that do not fit into the restricted list of contexts for the model. This method will be called by get_analysables() implementations where supports_context_filtering == true  # A modification to the current evaluation process, which supports a filter by course option. I would vote to just replace this filter by course for the filter by contextid/s  # A review of the analytics caching system to ensure we don't break anything   # Unit tests should be added to test everything. Specially helpful to detect possible issues in the point right above this one     I may be missing extra stuff, but those are the most critical points we need to cover.""",New Feature,Analytics
322183,"""# Create a course   # Create an assignment   # Go to Gradebook setup   # Set a grade hidden until date in the gradebook (screenshot 1)   # Go to gradebook overview and turn editing on   # Check the gears next to a students grade   # Notice there's no 'hidden until' date set    As an extra remark, I'd like to point out that it is weird you can set a date without marking 'hidden' first. What does the 'hidden' checkbox do if you already use a 'hidden until' date?    It seems to be malfunctioning and not logical.""",Bug,Assignment
322327,"""System:   * GPL Ghostscript 9.26 on Linux CentOS 7.5.1804   * 8 cores CPU   * Moodle 3.5.2+ (Build: 20181031)    How to reproduce:   * In Moodle settings:   ** Set GhostScript path in _ghostscript_   ** Enable _assignfeedback_editpdf_   * In a Moodle course:   ** As a teacher, create a new activity """"assign""""   ** As a student, post the high resolution PDF (see attached) in the activity and submit it to teacher    Problem:    GhostScript's process on this PDF will never ending (I waited 96 hours). GhostScript takes 100% of one CPU core and stay stuck on this special PDF. The others PDF files in queue are never processed. I can see the PNG growing very slowly in the data/temp folder but it's like it will never stop growing despite the rendering is stuck at a certain point.    When I run the same GhostScript's shell command manualy on the PDF file, it takes some minutes but finally success. Here is the shell command:    /usr/bin/gs -q -sDEVICE=png16m -dSAFER -dBATCH -dNOPAUSE -r100 -dFirstPage=1 -dLastPage=1 -dDOINTERPOLATE -dGraphicsAlphaBits=4 -dTextAlphaBits=4 -sOutputFile=[OUTPUT_FILE_PATH] [INPUT_FILE_PATH]    I'm not sure if it's a bug or if it's a normal behaviour regarding the resolution of the PDF as nothing appears in the logs. Even so it impacts performances on whole Moodle site and prevent other PDF to be rendered. As there is no control on which PDF students can submit, a limit on what GS will process based on file resolution or execution time could be an improvement (if it's not a bug).     """,Bug,Assignment
322437,"""We log a \core\event\webservice_function_called event every time a ws is called, the more JS-based UIs we have the more events we have, same for requests from the mobile app. Those events already represent a 2-digits percent of the standard log store table in some cases.    This event was added in MDL-21127 (2010) and it does not even seem related to the issue's main purpose. People might be using it (incorrectly) to observe calls to ws functions. I don't know if the events2 API would allow events to be observed but not stored by the log stores layer. If it does not I would vote for just removing the event.    [~<USER> proposed to add a setting to opt-out. A possible problem of the opt-out approach is that a sys admin may not know if any of the 3rd party plugins they installed is observing this event, so they would not know if they should keep the setting enabled. I may be ignoring other uses of the event.    In any case we should ask the community about this.""",Improvement,Performance
322460,"""After upgrading to Moodle 3.5.3 from Moodle 3.1.3 there was an adhoc task added """"\core\task\build_installed_themes_task"""" which is the same as running this CLI command """"/admin/cli/build_theme_css.php"""".    This task and CLI script are failing with the following errors:  {code:java}  Execute adhoc task: core\task\build_installed_themes_task  ... started 13:38:14. Current memory use 48.4MB.  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  ... used 4 dbqueries  ... used 29.244378089905 seconds  Adhoc task failed: core\task\build_installed_themes_task,Unexpected end of document [line no: 43155]{code}  When adding some extra error logging to see which themes are failing and running the script with verbosity, the below is the output.       {code:java}  root@SERVER:~# sudo -u www-data php /var/www/moodle/admin/cli/build_theme_css.php -v  == Build theme css ==  No themes specified. Finding all installed themes.  Checking that each theme is correctly installed...  Building CSS for themes: boost, bootstrapbase, clean, essential, more  parsing theme boost  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  parsing theme boost  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  parsing theme bootstrapbase  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  parsing theme clean  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  PHP Notice: Undefined offset: 0 in /var/www/moodle/lib/php-css-parser/Parser.php on line 418  Default exception handler: Exception - Unexpected end of document [line no: 43155] Debug:  Error code: generalexceptionmessage  * line 113 of /lib/php-css-parser/Parser.php: Sabberworm\CSS\Parsing\SourceException thrown  * line 197 of /lib/php-css-parser/Parser.php: call to Sabberworm\CSS\Parser->parseList()  * line 119 of /lib/php-css-parser/Parser.php: call to Sabberworm\CSS\Parser->parseAtRule()  * line 96 of /lib/php-css-parser/Parser.php: call to Sabberworm\CSS\Parser->parseListItem()  * line 87 of /lib/php-css-parser/Parser.php: call to Sabberworm\CSS\Parser->parseList()  * line 82 of /lib/php-css-parser/Parser.php: call to Sabberworm\CSS\Parser->parseDocument()  * line 1904 of /lib/outputlib.php: call to Sabberworm\CSS\Parser->parse()  * line 1123 of /lib/outputlib.php: call to theme_config->post_process()  * line 208 of /lib/outputlib.php: call to theme_config->get_css_content()  * line 117 of /admin/cli/build_theme_css.php: call to theme_build_css_for_themes()  !!! Exception - Unexpected end of document [line no: 43155] !!!  {code}       I've tried to investigate where it might be happening but I'm unsure. Our current theme """"essential"""" is working correctly though.""",Bug,Themes
322472,"""It has been very difficult to gather full data sets from institutions with which to develop, train, and test learning analytics models. We hope that more models will be developed in partnership with institutions, or independently by institutions using Moodle. To help facilitate this, we could provide a service that automates sharing models and model training data sets. This service might become part of MoodleNet, but would require work within Moodle to facilitate ease of use.    Moodle would need the following features in Core:   # Opt-in trigger in site registration to submit data on what models are in use on the system, their accuracy, and their usage rate (i.e. how many users are receiving insights and what proportion of those insights are being viewed and/or acted on).  Some metadata about the site should also be included, i.e. from MDL-57900. Note that some of this data is not currently collected within Moodle sites, but there are proposals to do so, e.g. MDL-62192, MDL-62302.   # Administration tool to submit a model definition in a reusable form (e.g. PMML) to the Moodle-supplied service - see MDL-60944   # Administration tool to submit non-identifying model data in csv form to the Moodle-supplied service, identified by a hash code or other mechanisim to tie the data to the model used to generate it - this exists as a manual export capability now, but submission needs to be very easy (even with an automatic option) to encourage sites to submit.   # Administration tool to select, download and import a model definition, as generated in #2, based on information provided about the model accuracy, testing status, and usage (number of sites, number of users, type of site, as collected in #1)   # Administration tool to select, download and import model training data, as generated in #3, based on information provided about the accuracy of the model when trained on that data set across multiple sites, types of sites, etc.    I'm sure this proposal needs to be refined further.""",New Feature,Analytics
322578,"""In our institution we do not want our 'DPO' staff to be able to make delete requests.     Delete requests are dangerous as obviously they delete the user's data meaning that our system stops working for the user. Although there are plenty of steps in the process, we and they are still worried that it might happen by accident, so we have been requested to remove the option.     (Some background: our process is that we will never apply student deletion requests on our main system - we can use various other aspects of GDPR law to justify keeping the data for all students for the retention period.)    Currently if you give somebody the tool/dataprivacy:managedatarequests capability, they by default become a DPO, or you can configure the role list in admin settings... this capability is weird... Anyway my point is there is an existing tool/dataprivacy:managedatarequests capability, it is complicated and I don't want to change it.    h2. Proposal    Add a new capability    *tool/dataprivacy:requestdeleteforotheruser*    1. When visiting the form for making a new request for another user, admin/tool/dataprivacy/createdatarequest.php?manage=1, if you do *not* have this new capability then the 'Type' dropdown would be fixed to the single value 'Export all of my personal data'. (The form field will be frozen so it doesn't appear as a dropdown any more.) You will not be able to select the other option 'Delete all of my personal data'.    2. If making a request for yourself (the same form but without the manage=1) then there is no change and both options will be available regardless of capabilities. (Note: We could consider also adding a second new capability tool/dataprivacy:requestdelete, that works a similar way but for individuals - we haven't got that requirement here but I'm happy to get it implemented as part of this if people think it would be beneficial/more consistent.)    3. The new capability would initially be made to default to the value of tool/dataprivacy:managedatarequests (so maintaining existing behaviour).    4. The development should include a simple Behat scenario to check the new feature (i.e. set up a role that does/doesn't have this capability, go to the form and confirm the option is/isn't there).""",Improvement,Privacy
322775,"""Related to the users' full name format, as a teacher, when I access a course:   * I see $CFG->fullnamedisplay from the participants' page.   * I see $CFG->alternativefullnameformat from the enrolments' page.    In both pages I would expect to see the content defined in $CFG->alternativefullnameformat because the teacher has the viewfullnames capability.""",Bug,User management
322807,"""Overview    A medium sized project to improve quicklist functionality in the Moodle assignment activity. The primary audience for this development will be graders at any level from primary through to HE.         Goals   # Greater consistency in the grading process between departments/markers/schools because markers draw their comments from an agreed list   # Improve the efficiency of the grading process which will mean that students can receive their feedback in a more timely way.         User Stories    {color:#55595c}As a marker, I would like to have a pre-populated list of comments for my moodle courses that are ordered in categories so that I can make best use of my time when giving feedback. {color}    {color:#55595c}As a marker I would like to be able to add personal coments to the quick list and add them to existing or new categories, and be able to select some or all of my comments and share them with some or all of my colleagues{color}    {color:#005000}As an admin I should be able to organise quicklist items into different categories for different courses and different contexts. It would be helpful if this worked in a similar way to questionbank with lists configured at different context levels. For instance, a course, category and site level.{color}         We could not find any existing tracker items directly related to this particular development               """,Improvement,Assignment
323157,"""We're experiencing an issue with Global Search when a user has access to a large number of courses and resources. It seems to be a similar issue to MDL-60826 but for search rather than calendar.    Much like that calendar bug, it looks like the search engine builds a list of contexts the user can access and filters the Solr results so users only see things they have access to. This section of code appears to be failing on users with access to a large amount of material.    If a user tries to use the search box, and they have been given Manager access either at site level or to a category with lots of subcategories and courses, the page will stall for several minutes until it reaches the PHP memory limit, at which point it dies.    Site administrators aren't affected, because the context gathering code is skipped and the function just returns 'everything'.    Ordinary teachers and students aren't affected, because they don't have access to a ridiculous number of contexts.    I'm away from my work computer right now, but I've had a quick look and get_areas_user_accesses() in /search/clases/manager.php is my prime suspect.""",Bug,Performance
323241,"""Given a course with an enrolled teacher who doesn't have permission to restore a course from a backup ('restore:restorecourse') but who is entitled to backup the course ('backup:*' permissions).     When performing a backup, after completing the backup the teacher is redirected to 'backup/restorefile.php'. Due to the missing permission 'restore:restorecourse' the user gets a permission error. The user cannot download the created backup.     Expected is a separation of permissions, so either:    * It is possible for the teacher to access 'backup/restorefile.php' without being granted 'restore:restorecourse' and he/she sees only the relevant sections/actions   * There is a separate view for downloading backups    This issue has already been mentioned but not solved in MDL-35429:    @<USER>  {quote}I have a teacher, with the usual teacher role which includes moodle/backup:downloadfile set to """"allow"""", who is backing up a course to try taking to another institution's moodle server. She has permissions to make backups but not permissions to restore, because we don't give teachers access to that for fear they'll blow away whole courses. I know not every school sets permissions that way, but I know a fair number of us do, for this reason. We're on 2.8.    She can run the backup, and as soon as the backup completes she gets the """"Sorry, but you do not currently have permissions to do that (Restore courses)"""" error, when it should otherwise display her the page where she can look at recent backup files created and download the one she just made. When I go in to do the same backup, I can see multiple backup copies have already been made of this course, as she tried and tried to do it; the backups were completing, but it was giving her an error message because without permission to Restore she can't view the page where she can retrieve the backup she had permission to make and should have access to as well. I see <USER>reporting that you can get to them in """"recent files"""" but that's very unintuitive.    I just went ahead and made the backup for her and shared it with her via google drive, and I know I could give her restore permissions temporarily so she can get to the file herself, but I'd really like moodle/backup:downloadfile to actually allow the thing that you would think, from the name of the permission, that it allows. It would keep workload lower on our liaisons working with faculty, if making backup files can be fully self-service.  {quote}  @<USER>  {quote}This issue won't address the problem (mentioned in comments) in which a teacher backs up a course is then hit with an error based on the fact that they can't restore courses. The whole 'restorefile.php' page is currently controlled by the 'restore:restorecourse' capability. Improving this page so that users can see and download their backups irrespective of the restore capability is another issue.  {quote}   """,Bug,"Backup,Roles / Access"
323484,"""This issue covers three main issues:  # users are currently expired when they _finish_ a course, not when the courses that they are in _expire_. This currently means that some data is removed before it should be  # it is not possible to expire a user if they are enrolled in any open-ended courses  # any user with a user block is not deletable due to a mis-understanding in the API as to how to handle blocks -- it treats them all as belonging to a course    This issue solves these by:  # switch to respecting the _course_ expiry as a dependency of _user_ expiry.  # provide a system setting to ignore open-ended courses when calculating the user expiry (rely purely on last login time)  # rewrite the dataprivacy expiry manager to not separate out course contexts from user contexts using context level, but instead do so by actual context.    The biggest change in this patchset is for the third item.  The original code assumed that any block was always a course block, and ignored the potential for user blocks. Because of this user expiry was essentially not possible if the user had any blocks on their Dashboard.   I felt that the best way to handle this was:  # to merge the user and course context expiry managers. The existing code separated them out based purely on their context level, which is not an accurate gauge of their type  # to change the behaviour such that any child context of a user is bundled in with the user context. It does not make sense not to do so as we do not allow any data registry configuration (purpose/category) for a specific user or for children of these anyway.    In order to complete this work I also moved the location of capability checks from the API class to the endpoints which call them. These tests do not belong in the API, and we have found this in other related issues (user bulk deletion, {{is_site_dpo}} changes, etc).    This work also necessitated that we finally stop deleting the user context, and instead just delete the related content.  As far as I'm aware, the only time we delete other contexts is when the record itself is removed. We do not remove the user record at any time, therefore we should not remove the context. The context has many things stored against it and, although some items are removed during the context deletion, this only applies to removal of _core_ items (and not even all of them). It does not support deletion of user context data in plugins at all. Again, this same change has been identified as a necessity in other related issues.""",Bug,Privacy
323543,"""I've been trying to override \core_user_renderer::unified_filter(), which itself calls another method in the same renderer: handle_missing_applied_filters(). Unfortunately this is defined as a private function in the core renderer so it cannot be called directly from the overridden renderer -- only duplicated by it -- which seems unnecessarily messy.    Unless there is a specific reason why this method is private, could its visibility be changed to protected to facilitate renderer overrides?""",Bug,Themes
323688,"""Hi,    We are trying to move to a CAS server for all our aplications. So, we are testing with Moodle CAS authentication plugin in Moodle version 3.4.         If I log in from the CAS main page and access Moodle, it works perfectly.  If I log out from MOODLE, the CAS session is closed and I can't access to any aplication. So, it works perfectly.  But, If I'm logged in Moodle (with CAS) and I log out from CAS server, the user is still authenticated in Moodle. This is the problem.    Does Moodle support CAS Single Sign Out? Or maybe I'm missing some configuration parameter?    Any help appreciated.    Regards,    Umapathy""",Bug,Authentication
323803,"""I would like to suggest an improvement to how the Participants screen is viewed...    It would be great if there was an option to indicate more quickly the enrolment method being used for individual users on the Participants screen.    This could be via a different colour for each enrolment method, or perhaps via a symbol.  This would make it clear how a user has gained each role in a course. For example, a user might be directly/manually enrolled, but also enrolled via cohort sync, and inherit access at a category level / as an 'Other user'.  There could be one colour for manual enrolments, and a different colour for cohort sync, etc.    (Under versions prior to 3.5x, I have been using separate roles so I can tell whether users arrive on the Participants page via cohort or direct enrolment.)    Many thanks""",Improvement,User management
323860,"""It is demanded to have a feature that the user has to agree to the terms and conditions statement first prior going through the rest of policies acceptance workflow. This requirement follows on from the initial mock-up:     !screenshot-1.png|thumbnail!     The basic use case comes from uni's (or other organisations) wanting a user to first sign off on the site terms and conditions, as a contract of use for the site.  If the user doesn't want to sign off on this then they won't be allowed access to the site (no matter what), and there is no point at all in proceeding to show them privacy statements (why waste their time?).    So, in more 'user story-like' language:    As an admin I need my users to agree to the site T&Cs as a contract and their agreement of use, before proceeding with any other sign on functionality as without their agreement to the T&Cs they will not be allowed on the site.    This is also a way to more formally separate the T&Cs from privacy statements - they have different purposes.""",Task,Privacy
323906,"""Steps to reproduce:   * Create a test course with two or more sections using the topics format.   * Create an activity in each topic.   * Hide one of the topics.   * Create a CSV file with shortname, fullname, category, format and templatecourse fields. Set the templatecourse value to the shortname of the test course, format to topics and everything else to some suitable value.   * Import that CSV file using the """"Courses > Upload Courses"""" function.   * Go to the newly created course.   * The topic that was hidden in the original template course will now be visible, although the activities it contains will still be hidden.         The root cause of this problem seems to be a minor bug in backup/moodle/restore_stepslib.php, so I've tagged this as a backup problem. It might affect earlier versions of Moodle, but this is the one where a customer reported it to me. The attached patch fixed this for me, but I can't say if it has other knock on implications.""",Bug,Backup
323927,"""When creating a new pix_icon instance, you are only able to pass a pix path (e.g. i/calendar) to it. Later, when the pix_icon is rendered in a theme  * if the theme is using icon_system_standard, this pix is output directly as an img tag  * if the theme is using icon_system_fontawesome, this pix mapped to a FontAwesome icon and the pix is output as FontAwesome icon tag.    This approach is fine for backwards-compatibility, but it has three main downsides for developments which only target FontAwesome-capable themes like Boost and Boost Child themes:  1. When creating a new pix_icon instance, you always have to pass a pix path first and have the create an additional FontAwesome mapping which is a code overhead.  2. The pix file is expected to exist on disk, even if it is never rendered in the theme, which is also an overhead.  3. The pix file should cohere visually with the mapped FontAwesome icon which isn't always possible without creating new icon files (which will never be rendered as mentioned in 2.)    That's why I would like to propose to add native FontAwesome support to the pix_icon class to simplify the work with FontAwesome icons in themes like Boost.    This is especially relevant in code areas where I must use pix_icons (e.g. when adding an icon to a navigation_node instance) and cannot use a custom icon rendering mechanism.    ----    TL,DR:  As a developer working on code for a FontAwesome-capable theme like Boost and Boost Child Themes, I want to be able to use FontAwesome identifiers like """"fa-check"""" when creating a pix_icon instance without having to add any pix fallbacks which will never be rendered in my theme. """,Improvement,"Themes,HTML"
323965,"""*DESCRIPTION*  It is possible for a student to submit their lone assignment attempt but for the submission to not be marked as the student's latest attempt.  In the database, <USER>assign_submission.latest is 0 rather than 1.  When this occurs the submission is not counted in the assignment summary and when viewing the page of all submissions as a teacher the student is listed with a status of """"no submission"""".  However, if the teacher happens to select the student while in the assignment grading interface the submission will be available to grade.      This situation has been observed on three separate occasions.  Unfortunately, I have been unsuccessful in replicating the outcome.    In the absence of knowing how this situation arises, I would propose an extra validation step during the submission process to prevent this outcome.    During final submission of an assignment attempt conduct the following two checks:1. Is there only one <USER>assign_submission record for the user for this assignment?  2. If yes, is the latest column set to 1 for that record?    If the first check passes but the second check fails, update the <USER>assign_submission record setting latest = 1.  The logic is that if the attempt being submitted is the student's only attempt on this assignment it should be marked as the latest attempt.    *STEPS TO REPLICATE*  I have been unable to replicate from scratch.  While I have tested with a variety of assignment settings, these are the submission settings in use on the assignments where the issue has been observed.  Require students click submit button: No  Require that students accept the submission statement: No  Attempts reopened: Never    *REPLICATION LOCATIONS*  Conducted several tests in Moodle 3.3.5 but unable to replicate.  Incidents in live courses have occurred in 3.3.3 and 3.3.5.    *USER IMPACT*  Because the student is not indicated as having a submission on the view all submissions page, a teacher may easily overlook that the students has a submission available for grading.  """,Improvement,Assignment
324083,"""When doing a Moodle update / upgrade via CLI (/admin/cli/upgrade.php), there is an output which informs about the affected components / plugins and about the progress. Example:    {code}  -->Sprachpakete verwalten: de  ++ Das Sprachpaket ‘de' ist aktuell. ++  -->Kernsystem  ++ Fortschritt (3,02 Sekunden) ++  --><USER>choicegroup  ++ Fortschritt (0,12 Sekunden) ++  -->block_quickmail  ++ Fortschritt (0,08 Sekunden) ++  -->local_boostnavigation  ++ Fortschritt (0,09 Sekunden) ++  Die Aktualisierung wurde erfolgreich beendet.  {code}  (sorry for the example being in german, but you have hopefully all seen this before in english)    Now, when there are new settings in core or in a plugin which are installed during an update / upgrade, the admin can see and modify them within the manual upgrade wizard in the browser.    When using the CLI upgrade, the admin does not have any idea if and which new settings are installed. They are simply installed silently with their default values.    I would propose to add an output for each newly installed setting to inform the admin about this fact and to give him the chance to review the setting later manually. Example:    {code}  -->Sprachpakete verwalten: de  ++ Das Sprachpaket ‘de' ist aktuell. ++  -->Kernsystem  ++ Fortschritt (3,02 Sekunden) ++  --><USER>choicegroup  ++ Fortschritt (0,12 Sekunden) ++  -->block_quickmail  ++ Fortschritt (0,08 Sekunden) ++  -->local_boostnavigation  ++ New setting """"local_boostnavigation | removeprivatefilesnode"""" installed with default value: 0  ++ Fortschritt (0,09 Sekunden) ++  Die Aktualisierung wurde erfolgreich beendet.  {code}    ----    PS: The proposed patch by [~<USER> in MDL-41047 which unfortunately got stuck might serve as a boilerplate for this ticket.""",Improvement,Admin
324158,"""Today, I tried using the """"Show All xx students"""" link on the course """"Participants"""" web page.  It didn't work.  As I looked at the code behind both the """"Show All"""" link and the code behind the buttons, I noticed that the button's URL ended with """"page=1"""".    I tried copying the code for the """"Show All"""", put this into my browser, and added """"page=1"""" to the end, AND THIS WORKED!    So, I thought that I would report this as a bug.    I am using Moodle 3.5+ (Build: 20180531).  I don't know how far back this problem might go.     """,Bug,Admin
324304,"""In Moodle 3.5, I really like how the NavDrawer highlights the course and upper components. I have discovered what might be a problem.    When I pick a course, it gets highlighted in both the Courses area and the upper area.  Great! (graphic #1)    When I pick Participants within a course, Participants gets highlighted at the top, but the course becomes unhighlighted. Well, maybe okay.  But shouldn't the course remain highlight to designate that I am looking at Participants within a course? (graphic #2)    Then, when I click on Grades, it does not get highlighted but the Course does remain highlighted.  Hmmm, this is inconsistent with what clicking Participants does. (graphic #3)    I suggest that the Course within My Courses always remains highlighted, and if Participants or Grades are selected, that they become highlighted in addition to highlighting the Course in My Courses.    This is a minor problem, but I thought that I would report it.  I am also reporting it as a """"bug"""" instead of a """"feature request"""" because of the inconsistency.""",Bug,"Themes,Usability"
324355,"""The way Font Awesome is currently integrated into Moodle makes it too difficult for users to install and use Font Awesome Pro (or a customised version of Font Awesome).    It should be possible to begin using a different version of Font Awesome by simply unpacking the desired Font Awesome package into a specified location (which would not overwrite files present in core).    There are several problems with the current setup:   * If I integrate Font Awesome Pro into our system, I cannot then share my Moodle repository publically   * Making use of Font Awesome Pro unnecessarily complex (i.e. requiring modifications to css and/or scss, or splitting the package contents into multiple different locations) is poor UX for our users   * Making use of Font Awesome Pro unnecessarily complex (i.e. requiring modifications to css and/or scss, or splitting the package contents into multiple different locations) is rude - if we are to take advantage of FortAwesome's work (the free version of FA), we should not leave barriers to our users' use of the paid version in place.   * Theme inheritance does not work well with FA - it is too difficult to modify behaviour in a child theme when the parent uses scss.   * FA support leaves too much to the theme.    It seems to me that if moodle core is going to use FA, then it should to be supported such that:   * theme support is not required, but can override defaults   * vanilla FA free version is unpacked $somewhere in core, and used directly from there   * custom/pro FA can be unpacked next to $somewhere without overwriting any core files, and can/will be used directly from there if present (& perhaps configured)    Currently, we're using a customised version of Adaptable as theme. In this case, the nearest I am able to get to the desired state is:   * remove use of font-awesome css in bootstrapbase   * unpack fontawesome 5 into e.g. /fontawesome or /fontawesome-pro   * page->requires->js on a file containing FA config (if present) to set searchPseudoElements: true   * check for presence of /fontawesome-pro/svg-with-js/js/fontawesome-all.js and page->requires->js on /fontawesome-pro/svg-with-js/js/fontawesome-all.js if present   * otherwise, check for presence of /fontawesome/svg-with-js/js/fontawesome-all.js and page->requires->js on /fontawesome/svg-with-js/js/fontawesome-all.js if present    This kinda sorta works, but would give poor UX (all FA elements are replaced with <svg> tags, which makes it a pain to style, especially when the original element was a pseudoelement).    Plain CSS + webfonts can't be used unless the font paths are preprocessed.    SCSS + webfonts could be used if $fa-font-path could be set to something that would behave like a simple directory - font.php requiring specific argument format breaks this at present. [[font:...]] is no good at present as it can only specify path to a single font.    Another thought was that perhaps this could be done via plugins, but at the moment their CSS capabilities are too limited.    I'm happy to work on this, if consensus can be found both that I'm making sense and around how it should work. Any thoughts?""",Bug,"Themes,HTML"
324391,"""At the moment it is not possible for a theme to dynamically change the images in use via image_url (at least, not without using $CFG->moodlepageclass to override page class, and having the new page class instantiate a different theme_class).    There are several other methods besides resolve_image_location that could usefully be overridden, but since that would involve rather more work, I'd suggest for now allowing resolve_image_location to be controlled in a similar manner to csspostprocess.    Adaptable, for example, has a setting to control use of theme or base icons, but is unable to actually control it fully and requires moving the theme's pix_plugins and pix_core/f directories out of the way as well. This is horrible UX.""",Bug,Themes
324523,"""While preparing the unit tests for MDL-62240 I've realised that JSON is not enabled by default. That means that, when exporting data, the extension is lost if the file exceeds the long limit, because JSON is not in the mime types list (so it's not treated as an known extension).    We should add JSON to the default mime types list now that we're using for the privacy export (to make sure all the files preserve the .json extension).""",Bug,Privacy
324616,"""Hi,  the settings of the assignment can be combined in almost any combination.  If the setting """"Require the students to accept the submission statement"""" is enabled, there's only one submission statement for all possible combinations right now.    We think there should be three statements:  * For submissions as a single user  For example:  """"This assignment is my own work, except where I have acknowledged the use of the works of other people and cited them correctly.""""  * For submissions in group mode  For example:  """"I'm submitting this assignment on behalf of my entire group and I confirm for me and my group members on behalf that this is our own work. If we have acknowledged the use of the works of other people, we cited them correctly.""""  * For submissions in group mode with the settings """"Require students to click submit button"""" and """"Require all group members submit"""" enabled  For example:  """"I'm submitting this assignment as a group member and I confirm that this is our own work. If we have acknowledged the use of the works of other people, we cited them correctly.""""    Best, Kathrin""",Improvement,Assignment
324768,"""core_tag\privacy\provider::export_item_tags() exports too much information about the tags themselves.  Just because user A have tagged something with tag X, it does not mean that he anyhow owns any other information about this tag except for its name. In fact, some fields like 'tagcollid', 'isstandard' and 'flagged' may not be even available for him.    I would only export """"rawname"""" of each tag and nothing else. The field 'name' is just lowercased 'rawname'     At the same time core_tag does not export tags """"belonging"""" to the user and does not remove userid from tags on user deletion request. The name of the person who first created the tag is shown on """"Manage tags"""" page. It does not have much value. We can anonymise such tags when processing user deletion request. They will be removed automatically by cron if the tag is no longer used by anybody.     Examples:    1. User added """"birdwatching"""" as his interest. The tag was automatically created and tag.userid points to this user. Nobody else used this tag. User has requested to be forgotten. The tag """"birdwatching"""" is modified to set userid=0. The user is deleted. Next cron will remove records in tag_instance table that point to interests of deleted users. When there are no tag_instance records linking to the tag , the tag """"birdwatching"""" will also be deleted.    2. User added """"computers"""" as his interest. The tag was automatically created and tag.userid points to this user. Another user also added """"computers"""" as his interest. The record in 'tag' table is not changed, new record in tag_instance table was added. The first user has requested to be forgotten, we set userid=0 for the tag """"computers"""", however the tag is not removed because it is used elsewhere. When manager views """"Manage tags"""" page the column """"User name"""" is empty for this tag.  """,Bug,Privacy
324796,"""If a user has the +moodle/user:viewdetails+ permission as not set or prevent, the user grade report no longer shows the student's name/profile picture as the page header, and shows generic title in the format course title: View: User report.    This is inconsistent with the rest of the grader report, which still shows names and profile pictures, even if the teacher has moodle/user:viewdetails set to prevent.    The name of the permission suggests this should just prevent you from viewing the user's profile, so I would expect the user report to still show the user's name and profile picture.    Steps to reproduce:   # Create a user with the teacher role, with the permission +moodle/user:viewdetails+ set to prevent.   # Create a course with the teacher user from step 1, and at least one student user enrolled.   # Add a grazeable activity such as a quiz.   # Try to view the user grade report of the student user. The page title of this page incorrectly displays as Course Title: View: User Report, instead of showing the user's profile photo and name (see screenshots).""",Bug,"User management,Reports"
324845,"""As a developer (of the CodeRunner question type) I am finding the following code in lib/outputrequirementslib.php a bit troublesome:    {{ if ($CFG->debugdeveloper) \{}}  {{    $toomanyparamslimit = 1024;}}  {{    if (strlen($strparams) > $toomanyparamslimit) \{}}  {{        debugging('Too many params passed to js_call_amd(""""' . $fullmodule . '"""", """"' . $func . '"""")',       DEBUG_DEVELOPER);}}  {{    }}}  {{ }}}    Is there a good reason for this check?  The error message itself is confusing as it's not the number of parameters that's the issue, but the lengths of the JSON-encoded parameter list. I am passing an array of selected language strings to my AMD modules and often triggering this error. Since it only affects developers, it's not a major problem and I've simply commented it out but I would like to know if there is a potential problem with passing an array of strings to AMD modules.     """,Bug,Libraries
324893,"""Hi, I posted a similar message in the General Help section (without the codes), but did not received a response. I'm not sure if this is the right place to post this.    The issue is that an user can use javascript in the browser developer tool console to change the score they received in the SCORM module. I'm not sure if Moodle is aware of this issue or this is something beyond Moodle's control. Essentially, I'm saying with some knowledge of javascripts, a student can change their grades in SCORM report! I haven't tried it in other activities.    I did not purposely try to look for a way to change SCORM scores via javascript. I was looking for a better way to track progress for SCORM objects (for e.g. created in Articulate Storyline) that doesn't rely on quiz results.    These are the codes below. You have to login as a student for a course with a SCORM object. Then go to Chrome browser Developer's tool > consoles > choose the SCORM object > cut and paste the codes > You will score 100% in the report without having to complete the SCORM.         function findLMSAPI(win) \{   // look in this window   if (win.hasOwnProperty(""""GetStudentID"""")) return win;    // all done if no parent   else if (win.parent == win) return null;    // climb up to parent window & look there   else return findLMSAPI(win.parent);  }    function setStudentProgressViaScore(setScore) \{   var currentScore = lmsAPI.GetScore();   if (currentScore < setScore)    lmsAPI.SetScore(setScore,100,0);  }    var player = GetPlayer();  var lmsAPI = findLMSAPI(this);    setStudentProgressViaScore(100);    lmsAPI.SetReachedEnd();  lmsAPI.SetPassed();     """,Bug,SCORM
325005,"""Build the UI part of the deletion workflow for expired contexts:    ||User Story||Acceptance Criteria||  |As a privacy officer, I want to be able to view a report listing data which should no longer be held, so that I can select which data to remove to comply with the principle of """"Privacy by Design"""".|* Moodle will provide a report listing all types of data which has passed its retention period, so that the Privacy Officer can review and choose which data areas are to be erased.   * This report will highlight data areas (courses, plugins, etc) and will not include the actual data itself.  * Choosing to delete all selected data should queue the selected data for deletion.  |""",Improvement,Privacy
325034,"""The phpunit provider testcase has a bug in its {{export_file}} function - it exports them to the customfiles instead.    Essentially this means that you can't fetch the files out for the purpose of Unit testing.  I'm writing unit tests which make use of this functionality in MDL-61309 for <USER>forum.""",Bug,Privacy
325049,"""In MDL-59720 we introduced a scheduled task that iterates through expired contexts (expired according its retention period) At the moment we just delete them but ideally we would provide a report for admins so they can confirm which contexts should be deleted.    ||User Story||Acceptance Criteria||  |As a privacy officer, I want to be able to view a report listing data which should no longer be held, so that I can select which data to remove to comply with the principle of """"Privacy by Design"""".|* Moodle will provide a report listing all types of data which has passed its retention period, so that the Privacy Officer can review and choose which data areas are to be erased.   * This report will highlight data areas (courses, plugins, etc) and will not include the actual data itself.  * Choosing to delete all selected data should queue the selected data for deletion.  * Scheduled task will run to look for queued jobs and will remove the relevant data (that which the Privacy Officer selected).  * Admin account and any associated data must be considered exempt from the deletion process (see next point).|""",Improvement,Privacy
325071,"""Dear Moodle community,    I have an error when I want import a course into another with """"Include blocks enabled"""".    Moodle throw an exception :    """"Can not find data record in database block_instance"""".    Debug info : SELECT id, parentcontextid FROM \{block_instances} WHERE id = ?    [array (0 => 0,)]    The context :    I have a big Moodle instance with 5000+ courses. We use it as a courses catalog with 4 years of courses. Every year, the entire catalog is duplicated (with the API).This system started with Moodle 2.6 and we have the latest Moodle now. I have done many upgrades of the same database.      Maybe I have orphaned blocks but I do not manage to find a query to find them... If I create a fresh new course, import is done without problem. I have the error if I use the duplicated course (potentially duplicated for 4 years) as destination of the import.      Do you have an idea ? Sorry for the light bug report, I do not have many clues and i have not found similar problem with block_instance in forums...    Thanks a lot for help""",Bug,Course
325114,"""The quick grading feature in Assignment does not work properly with team submissions. The effect is almost invisible, but results in students in team submission assignments getting notified every time any grades are updated with quick grade, not just their own. If a teacher was adding grades one page at a time, in a large assignment, this could result in dozens or hundreds of notifications. There may be other side effects I haven't discovered.  h3. Quick grade's big complicated SQL    Quick grade uses a subquery in its very big complex SQL query to determine the current attempt number for each user. The subquery looks like this:  {code:java}  LEFT JOIN (SELECT s.userid, s.attemptnumber AS maxattempt  FROM {assign_submission} s  WHERE s.assignment = :assignid1 AND s.latest = 1) gmx ON u.id = gmx.userid  {code}  However, for a team submission, assign_submission.userid is equal to zero, and thus the join condition is never met.    This means that as far as process_save_quick_grades is concerned, for any team submission assignment, no-one has any grades. Which means that *every non-null grade is considered to be modified*.    The result of this is almost invisible; the grades are updated to what they already were, so no harm no foul save a few wasted database hits. But there is a reason that process_save_quick_grades checks whether the grades were modified – because grades being modified generates events, and by default, notifies all the students whose grades were modified.  h3. Steps to reproduce   # Create an assignment with team submission enabled, in a course with at least two groups of students.   # Update the preferences of a student in one of the groups so that they receive notifications from Assignment even when offline.   # As a teacher in the course, go to the assignment   # Click on view all submissions   # Enable quick grading   # <USER>the student who you enabled notifications for   # Wait for the cron job, or run it manually   # Log in as the student from step 2 and clear the notification   # Log back in as a teacher, and go back to quick grading the assignment   # Grade another student in a different group   # Wait for the cron job again   # Log in as the student from step 2 again    h3. Expected Results    The user's notification queue should be empty.  h3. Actual Results    The user has been notified of the updated grades, despite the fact that their own grades remain unchanged.  h3. Some thoughts about potential fixes    I currently have a fix that works for this bug. In fact, I have two. But I'm not sure either of them are very good, and I think that the better fix might be to just get process_save_quick_grades to repeatedly call get_user_grade, compare it to the value in the form data, and then call  save_grade if necessary. That might be more database hits, but (and I'm completely guessing here) I don't think the performance would be significantly worse, and may even be better due to the lack of complex joins and subqueries. And the significant code de-duplication would prevent more problems of this nature arising in the future.    The two implementations I have both involve calling get_submission_group for every participant, and then passing that information to the big complex SQL query somehow. The first method I tried passes them as a constant table expression in a VALUES() statement, and the second which I found to be neater was to create a temporary table and feed the values in that way.    I think the first solution might run into problems with very large queries, though I don't know for sure; the second will definitely work all the time but I've noticed it's very rare in Moodle to use temporary tables to do anything.""",Bug,Assignment
325139,"""Looking at the portfolio (and this may be the case for some other areas too), I see a situation where a given component calls the portfolio subsystem, which links to the plugin types (portfolio). The plugin types (Mahara for example) link externally.    The issue I see here is that only the component (let's use forum as an example), knows what data it passes to the subsystem, and which subsequently gets exported externally. The subsystem and the plugins just pass it along, and don't know this. To me, it sounds like we need a way to say something like """"Link subsystem with this data"""" - which we currently don't have with the link subsystem class. Right now, we don't / can't describe metadata about the fields sent externally through this flow: forum -> portfolio subsystem -> mahara plugin.    I'd think one way to address this would be to allow passing in fields to link_subsystem.""",Improvement,Privacy
325223,"""I've encountered this problem while working on MDL-61423. While attempting to create core templates in auth, I've received a warning:    Warning: include(/moodle/auth/templates/version.php): failed to open stream: No such file or directory in /moodle/lib/classes/component.php on line _1169_    Moodle seems to consider the templates folder as a plugin type.""",Bug,Authentication
325229,"""When you perform a restore of a course, which fully exhausts the character limit for the course name of 254 characters, the restore process will concat 'copy' together with a counter to the course name, as long as the course name already exists.    However, the length of the course name is never checked for sanity according to the database limit of 254 characters. Neither when the user can set the course name within the restore process, nor when in retore_dbops.class.php:1686 the function calculate_course_names actually calculates the name of the new course.    The result for the user is that a error message 'Error writing to database' is displayed, without helpful information about what just happened and why and how to fix it.    Steps to reproduce (from comments below):  {quote}   # As a manager (or admin), create a backup of a course   # As a manager (or admin), restore the course backup as a new course (you need to be manager or admin for that)   # specify the name of the course, with something very long. I've used this : """"A very long title of a marvelous Moodle course about possible bugs in latests versions with very long names of backup filenames (of course, this don't occure very often, but can occure, and it's really a bad thing). So, when it's more than 254 characters long, it generate an error.""""   # do the restore, and you'll see the error  {quote}""",Bug,Backup
325267,"""I accidentally managed to make my moodle installation crash; I received a 503 error and the notification 'No top level course found'. The only way to repair it was to change a record in the database. It appeared to be a bug, so here's what I did:    Logged in as admin I went to Site administration > plugins > plugin overview > click the gear next to Activity modules > click the '1' in the Activities column on the row Forum > Search results shows the site name and description > click button Manage courses > Select the box in front of site name > Move selected courses to... and click a course category. And then it gives the error mentioned above.    (probably a fast way to get there is going to /course/search.php?modulelist=forum).    How to fix it is to change the course category id of the site name record in the database back to 0.    Background info: we don't use the forum function and I wanted to disable forum but I didn't find an option to do this, I ended up 'breaking' the website instead.""",Bug,"Admin,Database SQL/XMLDB"
325291,"""Hi,  I'll quote part of the description from MDL-47051:  {quote}  To help the grading process, we would like to request a 'rotate' button that could help solve the problem of students submitting homework in landscape mode, or upside down.  {quote}    This issue was closed as a duplicate of MDL-43462.  Unfortunately, this was not a duplicate, because MDL-43462 dealt about PDFs to be cut off and we cannot reopen MDL-47051 again. So I'll open this issue.    This is about adding a possibility to rotate the generated PDF within the editor.  This is useful, especially if users do their solutions handwritten, take a photo (smartphone in landscape orientation) of this and upload the jpg. This has two effects:  * The jpg is reduced in size to fit to the plain PDF background which orientation is in portrait  * And it is placed in it's landscape mode     !landscape_image.png|thumbnail!     Without the possibility to rotate this representation, it's hard to read and correct the uploaded solution!    Best, Kathrin""",Improvement,Assignment
325318,"""The site has recently been upgraded from version 2.6.11 to 3.3.4 (Moodle 2.6.11 > 2.7.20 > 3.0.10 > 3.3.4)    Now when viewing the SCORM reports (Basic report, Interactions report and Objective reports) no report results are displayed. With debugging enabled the following error is displayed. The page loads near instantly so it doesn't appear to be any sort of PHP memory issue or timeout.    *Fatal error: Cannot use object of type stdClass as array in path/to/moodle/code/lib/tablelib.php on line 493*    Prior to the upgrade, the SCORM reports loaded as expected.    The 'Graph report' is displaying as expected.    I've switched to Boost / clean theme and I get the same results.    All courses are affected by this problem, along with courses created after the upgrade.    My only thought as to why this might be happening is due to the size of one of the SCORM tables in the database which is large - <USER>scorm_scoes_track - 3.3 GB ~ ~8,965,557 entries - but it worked in version 2.6?          """,Bug,SCORM
325324,"""Provide a way to set the purpose and retention without review.    Related user story    || User Stories || Acceptance Criteria ||  | As a Privacy Officer, I want to be able to set purpose for any content I create without the need for review. | * Moodle should provide a capability to be used when creating instances of content:  ** Manage_purpose  * If the user doesn’t have the capability (or suggest), then the form element should be read only / hidden. |""",Improvement,Libraries
325325,"""To help set purposes and retention information effectively we would like a way to set this across a context level i.e. I want all courses to have the same purpose and retention period.    Related user story    || User Stories || Acceptance Criteria ||  | As a privacy officer, I want to be able to apply the default value for context to all existing instances at that context only. E.g. Apply a default of ‘Education’ to all course categories. | * Data Privacy tool to provide a new page allowing application of default values to existing data within that context.  * Page should allow the PO to choose the context on which to act, the purpose to apply to items, and an option to override existing values (defaulted to not set). |""",Improvement,Libraries
325326,"""We need a page where we can set the defaults for purpose and retention for each context.    Besides site, each of the settings would have 'inherit' in the select options.    These defaults would then be what is selected by default when a new item is created after this point in time (existing items would not be affected).    Related user story  || User Stories || Acceptance Criteria ||  | As a privacy officer, I want to be able to set defaults for all the contexts in Moodle, so that when creating new instances (course, <USER> etc), the default purpose can be used. | * The Data Privacy tool should provide a setting for each of the contexts, allowing a default purpose to be set for that context.  * Values should include:  ** Inherit  ** Others from the list created by the PO.  * System context must exclude ‘Inherit’ from its list. |   """,Improvement,Libraries
325338,"""We require a registry to list the context / component tree and allow a purpose and retention period (among other settings) to be applied to each level of the tree.   The most granular item would be an instance of an activity (module). *Need to confirm that this doesn't go down to sub-plugin.*    It should be possible to set the purpose and retention period for a context and and have it be applied to all of its children down the tree. The form should have a checkbox for applying the setting to the child elements that by default is unchecked. Checking this box will apply the setting to the child elements but will put up a warning saying that the Privacy Officer (PO) needs to review every section to demonstrate compliance.   The purpose select element should start with two options   Options.   * Inherit from parent (unset)   * Inherit from parent    By default """"Inherit from parent (unset)"""" allows for each element in the tree to be flagged, requiring attention from the PO.   If """"Inherit from parent"""" is set then the attention flag is removed.    The tree on the left should show all context / component items, but have a toggle to only show areas that need attention.    When setting the purpose / retention period settings, the purpose should be selectable and the retention period should only be shown (not selectable). To set a different retention period a new purpose will have to be created.    Related user stories  ||User Stories||Acceptance Criteria||  |As a Privacy officer, I want to be able to quickly see which areas of Moodle I have not set the purpose and retention periods for user data.| * Moodle should provide a report that shows the current system and which areas have not had the purpose and retention periods specified in relation to user data.   * Updating the purpose and retention period will mean that this area will no longer show as requiring attention.|  |As a privacy officer, I want to be able to efficiently set purposes and retention periods for existing content in my Moodle system.| * At each level of the context tree (System, Course category, course, activity, etc.) it should be possible to set the purpose and retention period for user data and have this setting be applied to all child areas. (e.g. I go to a course and set the purpose and retention period and then have it apply it to all activities in that course).   * The default purpose for all existing items in a site should be set to ‘Not set (currently inheriting)’ meaning fewer changes will be needed for compliance.|""",Improvement,Libraries
325423,"""I've found this issue while testing MDL-61133 (questions tags)     I added a question without tags as a teacher and later pressed the 'tag' button to add tags to it; the suggestions box is empty and I think that may be the key to reproduce this issue; I just need to press the 'up' or 'down' key twice and the JS error below is triggered:    {code}  Uncaught TypeError: Cannot read property 'top' of undefined      at activateItem (first.js:7783)      at activateNextItem (first.js:7807)      at HTMLInputElement.<anonymous> (first.js:8140)      at HTMLInputElement.dispatch (jquery-3.2.1.min.js:3)      at HTMLInputElement.q.handle (jquery-3.2.1.min.js:3)  activateItem @ first.js:7783  activateNextItem @ first.js:7807  (anonymous) @ first.js:8140  dispatch @ jquery-3.2.1.min.js:3  q.handle @ jquery-3.2.1.min.js:3  {code}""",Bug,"JavaScript,Forms Library"
325460,"""From the user stories:  1.  As a Privacy Officer I want to be able to add and edit the categories of personal data in my institution.    Moodle should provide a report listing categories of personal data, with options to edit existing items, or create new items.  Only users with relevant capability should be allowed to add/edit.    2.  As a Privacy Officer, I want to be able add and edit the purposes for collection of personal data in my institution.    Moodle should provide a report listing purposes for capturing personal data, with options to edit existing items, or create new items.  Only users with relevant capability should be allowed to add/edit.  I should be able to provide a default value for retention period during this add/edit process.    When creating a purpose there should also be a checkbox or something similar for stopping user deletion requests from being granted. Some purposes such as keeping grades has a higher standing for retention above the right to be forgotten. This flag would ensure that the areas that use this purpose do not have user data deleted from user's making such requests.  """,New Feature,Admin
325545,"""{color:#333333}I want to use the Webservice Function for Upload a File for different user.{color}I tried this with this example, Rest API and the simpleserver.php   Authentication with wsusername and wspassword.  {code:java}$fileinfo = array(      'contextid' => 0,       'component' => 'user',       'filearea' => 'draft',       'itemid' => 0,       'filepath' => '/',      'filename' => 'sample.txt',      'filecontent' => base64_encode(""""Hello World!""""),       'contextlevel' => 'user',      'instanceid' => $userid,  );{code}  {color:#333333}The Upload on files/externallib.php works and the file is on a temporary directory, but they exit (_*throw new moodle_exception('nofile');*_ {color}) - method upload    // Move file to filepool. if ($dir = $browser->get_file_info($context, $component, $filearea, $itemid, $filepath, '.'))   $dir is NULL         {color:#333333}My output from the variables context, component, filearea, itemid and filepath{color}    *Context:* object(context_user)#236 (5) \{    [""""_id"""":protected]=> int(5)    [""""_contextlevel"""":protected]=> int(30)   [""""_instanceid"""":protected]=> string(1) """"2""""   [""""_path"""":protected]=> string(4) """"/1/5""""   [""""_depth"""":protected]=> string(1) """"2"""" }    *Component:* string(4) """"user""""    *Filearea:* string(5) """"draft""""    *Itemid:* int(176260806)    *Filepath:* string(1) """"/""""  ----  *I found a Problem* at the file lib/filebrowser/file_info_context_user.php    There is a check on Line 183  {code:java}        // access control: only my files          if ($this->user->id != $USER->id) {              return null;          } {code}  $this->user->id is the ID from the specified User (5) and $USER->id is the Webservice User.  ----  {color:#333333}I hope someone can fix it.   There are a lot of Forum Posts about this Error, but no one has a solution.   All Examples used the webservice/upload.php with a Token - and this is not possible for our project.{color}""",Bug,"Web Services,Files API"
325548,"""||*User Story*||*Acceptance Criteria*||  |As a Moodle administrator I need to have a consent form which summarises the privacy policy in simple, easy to understand language.| * The consent form should show a summary of the privacy policy.   * The text of the summary supports multi lang filters   * The versioning of this content should be linked to the privacy policy page.   * When the consent page is changed, users are notified and must accept the new version of the site policy before they can access the site.|  |As a Moodle administrator I need the consent page to contain a list of third party sections that the user will have to agree to, to use the site.| * The administrator and privacy officer should have the ability to make alterations   * The versioning of this content should be linked to the privacy policy page.   * When the consent page is changed, users are notified and must accept the new version of the site policy before they can access the site.|     """,Task,Admin
325574,"""I've been trying to solve an issue and can't find a solution anywhere. I've tried role permissions, activity group restrictions, separate groups ...    I a course there are groups containing one teacher and several students. Some students are members of several groups and therefore have access to activities that are restricted to members of any of their groups.     If the teacher of group 1 goes into the assignment submissions, for example, for an activity restricted to group 2, that teacher will see submissions from members of group 1 if they are members of both groups. Members of group 1 did submit assignments but they got access as members of group 2. The teacher of group 1 should not see submissions for this activity at all, or receive notifications which is also the case.         So is it possible to allow students not just to be granted access to activities based on groups but to do the activities as a member of that group?         Then, if an assignment is restricted to members of group 2 the report should show submissions only from members of group 2 and none from group 1 even if some students are members of both groups.""",Improvement,Assignment
325581,"""*Example of issue:*    If *<USER>Bloggs* has a UserInfoField of Department with the value: Computing    and the external DB changes the department to Music it updates correctly.         However if <USER>Bloggs was created with an empty Department field.    When the external DB changes the department to Music it does not update.         *Code at fault:*   {code:php}if (isset($user->{$key}) and $user->{$key} != $value) {  {code}  Should not have the isset part as if the value is not set yet the new value needs to be set.              *Proposed solution (two options)*   {code:php}if ($user->{$key} != $value) {{code}  OR:  {code:php}if ((isset($user->{$key}) AND $user->{$key} != $value) OR (!isset($user->{$key}) AND isset($value)) {  {code}   For Simplicity I would recommend the first option, but either would work.""",Bug,Authentication
325593,"""h2. *Overview*    Improve assignment settings / behavior so that students can submit multiple drafts up to an assignment deadline but not re-submit after the deadline. Moodle currently attempts to overcome this with the Require Submit Button option:    Requiring Submit = ON problematic, students either click too early and cannot subsequently cannot update their assignment or forget to click all together meaning their assignments are marked late. It is not a good process or user experience.    Require Submit = OFF off means students can submit as many times as they want before the deadline which works well. The problem is, if they become confused and submit again after the deadline their assignment is marked late. This is not a good process or user experience.    What would work better is:   * Students can submit and re-submit as many times as they want up until the deadline   * After the deadline arrives students who have already submitted would NOT be able to re-submit (marking their assignments late)   * After the deadline students who have not submitted an assignment at all would still be able to (marking it late)    For this to work Moodle would need to determine whether the deadline had passed when a student viewed their assignment and would prevent them re-submitting if they had already submitted a previous version / draft  h2. Summary   * *Project size*: small   * *Audience*: primary schools, universities, work places   * *Target users*: teachers, students, administrators       h2. User Stories     As a teacher, I should be able to enable a feature to """"prevent editing / updating of assignments after deadline""""    As a student, I should be able to edit / re-submit my assignment as many times as I need to until the deadline. Once the deadline passes I should be prevented from accidentally editing my assignment again which would result in it being marked late.    As an administrator I should be able to set the default value for this setting +*and lock it*+ in line with my orgainisations policies on assignment handling.     """,Improvement,Assignment
325625,"""+*Summary*+    Improve scope for integrating and reporting on formerly graded assignments (as opposed to formative work).   * Project Size: Small   * Audience: Any organisation wishing to improve grade export to other systems or provide students with a clearer view of summative assignments   * Target Users: Allow administrators to better integrate assignments, allow teachers to flag formal assessment and students to filter by assignment types (if reports are subsequently created using these fields)    +User Stories+    As a student I would like to be able to see only my formerly assessed / graded assignments as opposed to the optional draft assignments, formative assignments and other clutter currently sprayed across my dashboard.    As and Administrator / Integrator I would like to be able to better integrate / link assessemnt in my student records system with corresponding assessments in Moodle.    +*Description of Changes*+    Two additional fields would be useful in the Assignemnt settings:    +*1) ID Number*+    Integrating / Linking assignemnts with other systems would be helped by the addition of an """"ID Number"""" field as exists for courses. Its purpose would be the same as that of the Course ID Number:    """"The ID number of an *Assignment* is only used when matching the *Assignment* against external systems and is not displayed anywhere on the site. If the course has an official code name it may be entered, otherwise the field can be left blank.""""    +*2) Purpose (drop down / select list)*+    The assignment activity can be used in many different ways, for example:   * Draft submissions   * Formative work   * Summative / graded assessment    Currently there is no way to differentiate between these differing user cases. This results in a convuluted dashboard / assignment list for students. By adding a field that could be used to separate formative / draft assignments from those that are formerly graded / affect a students final qualification it would be possible to allow filtering on the dashboard and other reports to give students a concise list of formal assessments as opposed to simply listing every assignment activity regardless of the purpose for which it is used.     """,Improvement,Assignment
325745,"""If a scheduled task throws an exception, its next run will be delayed (to prevent too many retries when something is broken). The 'fail delay' amount increases until it reaches 86,400 seconds (24 hours).    Supposing that a system operator then corrects the problem. Because of the fail delay, the task will still wait up to 24 hours before retrying. It would be nice if the task starts running again according to its normal schedule, but there is no way to clear the fail delay except for manually editing the database table.    I propose a link to clear the fail delay, from the scheduled tasks screen. The task will then run according to its normal schedule.    (This is a small issue that annoyed me twice from an operational rather than development perspective recently, so I thought I would contribute an enhancement.)    Note: As a workaround, you can use the 'Run now' feature to run the task immediately, and assuming it succeeds this will also clear the fail delay. However, in our setup, some tasks do not work well if run from the web browser like this. The CLI version of 'run now' also works and removes the web browser limitation, but again in our setup we don't have shell access to production servers... Basically I don't think this feature is in any way essential as there are workarounds (for us, updating it manually in the database and then editing the schedule settings is the easiest), it's just a nice-to-have.""",Improvement,"Admin,Tasks"
325784,"""When a file is added as an alias in a folder module, the downloading of the folder create a zip archive with an empty text file of 0ko having the name of the original (consistent) file.    I've seen in MDL-44066 that a similar problem happend for the restoration of a course where the SCORMs were added using an alias.     I've done a bit of research and it seams that the problem is eiter   * the use of _$fs->get_file(...)_ in _mod/folder/download_folder.php_ versus _$fs->get_area_files(...)_ in _mod/resource/view.php_  * the function _get_directory_files(...)_ declared in _lib/filestorage/file_storage.php_ and called in _lib/filestorage/zip_packer.php_ in the function _archive_stored(...)_    I've tried to get the right file using the <USER>files.referencefileid field (looking in the <USER>file_reference table for matching id) but I didn't see a right way to do it with the moodle file API.    I feel like the best way to fix that bug is to change the function _get_directory_files()_ and make it return also the original content of an alias file but I am not aware of all the consequences that this modification can create.    Could you, dear moodler, explain your point of view on the proposed solutions or give another one ?    Have a nice day,  Guillaume""",Bug,Files API
325802,"""We want core backends to be as reusable as possible and to hardcode the learning rate, or the numbers of hidden layers makes them less adaptable to the different input datasets that can be generated by different models.    I would propose to:  * Perform this cross validation process as part of the evaluation process (the original proposal using sklearn was already doing this) because at that point we have a single dataset generated from all the database contents. Not all database contents are available during the first training execution.   * Models based on data can just be enabled without running the evaluation process so we still need defaults  * Stuff that, ideally, should change depending on the model: learning rate, regularisation, activation function, neural network layers setup... up to the machine learning technique used by the backend    About the technical solution for this:  * Expand machine learning backends API to include a new interface for hyperparameter tuning so it is a desirable feature mlbackends have, but not required  * We could store the best hyperparameters setup returned by mlbackends evaluation processes in a new mlbackend_hyperparameters table  * Related with this point above, we need to define the """"key"""" as it is not only the model id; evaluation 'xyz' could use indicators 1, 2 and 3; evaluation 'asd' could use indicators 2, 3 and 4 and so on. I would propose a simple key based on the list of classes used by the model (i.e. classes exported by MDL-60944) People can even be changing PHP code between evaluation runs but we should be fine if we store just the latest hyperparameters setup for each of those keys  * We can provide a helper class to ease hyperparameters retrieval so mlbackend plugins don't have to DB->get_record  * As the stuff that can be tuned depends on the backend I would not whitelist a set of hyperparams that we allow backends to tune and just add a flexible tunable::store_hyperparameters (just an example)""",New Feature,Analytics
325807,"""Dear all    While working on our plugin and testing the backup/restore process of it we encountered following error when Importing an older backup. Unfortunately I'm not so familiar with the Event API but it seems this is something little that got forgotten.  h3. Steps to reproduce    1. You need a backup of a course with tool_recyclebin active. It must have event logs (e.g. course_bin_item_deleted event)  2. Restore into your Moodle 3.4 stable with developer mode on  3. During Import you see following messages:    In order to restore course logs accurately the event """"tool_recyclebin\event\course_bin_item_deleted"""" must define the function get_objectid_mapping().  line 550 of /lib/classes/event/base.php: call to debugging()  line 100 of /admin/tool/log/backup/moodle2/restore_tool_log_logstore_subplugin.class.php: call to core\event\base::get_objectid_mapping()  line 63 of /admin/tool/log/store/standard/backup/moodle2/restore_logstore_standard_subplugin.class.php: call to restore_tool_log_logstore_subplugin->process_log()  line 137 of /backup/util/plan/restore_structure_step.class.php: call to restore_logstore_standard_subplugin->process_logstore_standard_log()  line 112 of /backup/util/helper/restore_structure_parser_processor.class.php: call to restore_structure_step->process()  line 178 of /backup/util/xml/parser/processors/grouped_parser_processor.class.php: call to restore_structure_parser_processor->dispatch_chunk()  line 100 of /backup/util/helper/restore_structure_parser_processor.class.php: call to grouped_parser_processor->postprocess_chunk()  line 148 of /backup/util/xml/parser/processors/simplified_parser_processor.class.php: call to restore_structure_parser_processor->postprocess_chunk()  line 92 of /backup/util/xml/parser/processors/progressive_parser_processor.class.php: call to simplified_parser_processor->process_chunk()  line 190 of /backup/util/xml/parser/progressive_parser.class.php: call to progressive_parser_processor->receive_chunk()  line 278 of /backup/util/xml/parser/progressive_parser.class.php: call to progressive_parser->publish()  line ? of unknownfile: call to progressive_parser->end_tag()  line 179 of /backup/util/xml/parser/progressive_parser.class.php: call to xml_parse()  line 158 of /backup/util/xml/parser/progressive_parser.class.php: call to progressive_parser->parse()  line 110 of /backup/util/plan/restore_structure_step.class.php: call to progressive_parser->process()  line 181 of /backup/util/plan/base_task.class.php: call to restore_structure_step->execute()  line 178 of /backup/util/plan/base_plan.class.php: call to base_task->execute()  line 167 of /backup/util/plan/restore_plan.class.php: call to base_plan->execute()  line 339 of /backup/controller/restore_controller.class.php: call to restore_plan->execute()  line 224 of /backup/util/ui/restore_ui.class.php: call to restore_controller->execute_plan()  line 135 of /backup/restore.php: call to restore_ui->execute()  h3. Expected Result:    No such warnings  h3. Additional Info:    it was a few hours old Moodle 3.4 stable installation (using mdk)""",Bug,Backup
325809,"""Dear all     While working on our plugin and testing the backup/restore process of it we encountered following error when Importing an older backup. Unfortunately I'm not so familiar with the Event API but it seems this is something little that got forgotten.  h3. Steps to reproduce     1. You need a backup of a course with <USER>folder active. It must have event logs (e.g. all_files_downloaded event)  2. Restore into your Moodle 3.4 stable with developer mode on  3. During Import you see following messages:     In order to restore course logs accurately the event """"<USER>folder\event\all_files_downloaded"""" must define the function get_objectid_mapping(). * line 550 of /lib/classes/event/base.php: call to debugging()   * line 100 of /admin/tool/log/backup/moodle2/restore_tool_log_logstore_subplugin.class.php: call to core\event\base::get_objectid_mapping()   * line 63 of /admin/tool/log/store/standard/backup/moodle2/restore_logstore_standard_subplugin.class.php: call to restore_tool_log_logstore_subplugin->process_log()   * line 137 of /backup/util/plan/restore_structure_step.class.php: call to restore_logstore_standard_subplugin->process_logstore_standard_log()   * line 112 of /backup/util/helper/restore_structure_parser_processor.class.php: call to restore_structure_step->process()   * line 178 of /backup/util/xml/parser/processors/grouped_parser_processor.class.php: call to restore_structure_parser_processor->dispatch_chunk()   * line 100 of /backup/util/helper/restore_structure_parser_processor.class.php: call to grouped_parser_processor->postprocess_chunk()   * line 148 of /backup/util/xml/parser/processors/simplified_parser_processor.class.php: call to restore_structure_parser_processor->postprocess_chunk()   * line 92 of /backup/util/xml/parser/processors/progressive_parser_processor.class.php: call to simplified_parser_processor->process_chunk()   * line 190 of /backup/util/xml/parser/progressive_parser.class.php: call to progressive_parser_processor->receive_chunk()   * line 278 of /backup/util/xml/parser/progressive_parser.class.php: call to progressive_parser->publish()   * line ? of unknownfile: call to progressive_parser->end_tag()   * line 179 of /backup/util/xml/parser/progressive_parser.class.php: call to xml_parse()   * line 158 of /backup/util/xml/parser/progressive_parser.class.php: call to progressive_parser->parse()   * line 110 of /backup/util/plan/restore_structure_step.class.php: call to progressive_parser->process()   * line 181 of /backup/util/plan/base_task.class.php: call to restore_structure_step->execute()   * line 210 of /backup/moodle2/restore_activity_task.class.php: call to base_task->execute()   * line 178 of /backup/util/plan/base_plan.class.php: call to restore_activity_task->execute()   * line 167 of /backup/util/plan/restore_plan.class.php: call to base_plan->execute()   * line 339 of /backup/controller/restore_controller.class.php: call to restore_plan->execute()   * line 224 of /backup/util/ui/restore_ui.class.php: call to restore_controller->execute_plan()   * line 135 of /backup/restore.php: call to restore_ui->execute()    h3. Expected Result:    No such warnings  h3. Additional Info:    it was a few hours old Moodle 3.4 stable installation (using mdk)""",Bug,Backup
325825,"""If you set up an assignment as a group assignment, it is only possible to revert the assignment, if you find out who of the group submitted the submission. It is not possible to select an other member of the group and revert the group submission to the draft status.  It would be also great, if it were possible in the overview to sort by group or select the whole group at once do to an action.    Test instructions:    As a teacher:  - Create an assignment A1.  - Set the setting """"Students submit in groups"""" to yes.  - Create a group and assign two students to this group. For instance studentA and studentB.    As a studentA  - For this assignment submit a submission for your group.    As a teacher  - You can see, that there is a subission for the group.  - Try to revert the submission to the draft status for studentB.  - As you can see, the status of the assignemnt for the group is still """"submitted for grading"""" instead of """"Draft (not submitted)"""".  - As a teacher I would expect, that the group submission would be set to draft status for the whole group.    - Try to revert the submission to the draft status for studentA.  - You can see, that the status of the assignment changed correctly to """"Draft (not submitted)"""".    In conclusion, it is not possible to see, which person of the group the teacher have to select, to revert the submission.""",Bug,Assignment
326080,"""{color:#ff0000}Please close this issue, I give many retries and it seems to be OK now, sorry my bad.{color}         The web service's method """"*core_user_update_users*"""" suddenly stops to work since weekly release Moodle 3.3.2+ (Build: 20171027). (Maybe related to -MDL-51945-?)    The method returns null as usual, but nothing happenings in my Moodle database (no data change at all).    Here is a sample of the POST query I sent to Moodle:  {code:java}wstoken=[token here]&moodlewsrestformat=json&wsfunction=core_user_update_users&users[0][customfields][0][type]=EstEtudiant&users[0][customfields][0][value]=0&users[0][id]=670&users[0][username]=XXXXXX&users[0][firstname]=XXXXXXX&users[0][lastname]=XXXX&users[0][email]=XXXX%40XXXX.be&users[0][auth]=shibboleth{code}  The problem happens in our production server so I've tested my query on the developpement server which was not up-to-date yet. It was a success as usual. Then I updated my developpement server to the last weekly release and the bug appears exactly as in my production server.    Nothing in the logs, nothing in the response (debug mode ON), no change in the database. I also checked the permissions and the token. Other methods like *core_user_create_users* still works like a charm.""",Bug,Web Services
326081,"""Add a site policy settings page with some minimal features.   # Separate site policy page (separate to privacy settings)   # Versioned (every edit makes a new version)   # Require all users to accept latest version of site policy before they can access the site       ||User Story||Acceptance Criteria||  |As an Moodle administrator I need all users on my site to read and accept the latest version of my site policy| * There is a site policy page for site settings containing all site policy related settings   * There is a site administration setting to enable / disable site site policy   * There is a site administration setting to force all site users to accept the latest version of the site policy before they can access the site   * There is a rich-text field allowing site administrators to write the text of the site policy   * The text of the site policy supports multi lang filters   * Each change to the site policy is saved as a separate version and it is possible to see all the previous versions   * By default any policy change will require users to re-consent.  Users are notified and must accept the new version of the site policy before they can access the site   * The requirement for all users to re-consent can be disabled on a case-by-case basis.   * If there exists both a site policy and a privacy policy, the users must actively consent to both before they can access the site|""",Task,Admin
326120,"""I'm currently writing a plugin in which I extend the class flexible_table. The base class has an option to display two """"initials bars"""" that can be used for filtering by the initial of the first name or surname. You can either display both initials bars or none.    However, I want to be able to let my users filter only by the initial of the surname, not the first name. I want to display only one of the two initials bars. This is possible by deriving from flexible_table and overriding the _print_initials_bar_ method. But then I need to use _get_initial_last_ to get the currently selected initial. However, that method is deprecated since Moodle 3.3. I can't access _$prefs_ from the base class directly because it's private.    I would like to have some non-deprecated way of achieving this without having too much redundant code in the end. I don't mind having to override one method. My suggestion would be to either:   * make _get_initial_last_ (and _get_inital_first_) protected and non-deprecated (reduces visibility of these methods; may break something)   * make $prefs protected   * change the class _flexible_table_ itself to allow both bars to be enabled individually (changes the public interface; may break something)    What would be the """"best"""" solution? Does anyone have another suggestion?""",Improvement,Libraries
326189,"""As a Module designer/Exam administrator, I want to be able to set the deadline of an Assignment to 23:59 so that students have 'until midnight' to submit.     While 00:00 the next day is also possible, this confuses students so we avoid doing it.   """,Improvement,Assignment
326245,"""Copying comments from MDL-59039 comments:    *<USER>Mo:*  Today we got a random failure related to this issue:  {code}  1) search_manager_testcase::test_partial_indexing  Failed asserting that actual size 0 matches expected size 1.    /Users/Shared/Jenkins/Home/git_repositories/master/search/tests/manager_test.php:243  /Users/Shared/Jenkins/Home/git_repositories/master/lib/phpunit/classes/advanced_testcase.php:80    To re-run:   vendor/bin/phpunit search_manager_testcase search/tests/manager_test.php  {code}    It is the first time it fails, I haven't checked how it all works but:  1. the label is created  2. we wait for the next second (waitForSecond)  3. we index contents for 1 second  4. ERROR 0 docs indexed  the only possible cause I can think of is that 2 full seconds pass from the second the label is created and the second we index, I imagine that, although not likely, it could be possible, any ideas?    *<USER>reply:*  Sorry I was away on holiday but I've now had a look at this and I think I've figured it out. It doesn't matter how long time passes after the label being created (as long as it is at least the next second).  When we index contents with 1 second time limit, this part is supposed to test that the forum is done last (because it took longer last time). It is supposed to work as follows:  1 Index all the other search areas apart from forum  2 Most of these areas have no new data it should take less than 1 second to check all of them (ASSUMPTION!)  3 When it hits label, it will 'index' the label, and also do the fake 1.2 second delay that the mock code adds in to simulate indexing taking time  4 Because that took 1.2 seconds, indexing will now stop  It checks the time limit after processing each search area (even if no documents were added there), and after adding each document within a search area.  I think what is happening is that it's taking more than 1 second to check some other search area or areas (which contain no new documents but it still has to do the query) before it even gets to label. Therefore it exits before doing a single document. In other words the assumption is not always true.  I can think of three approaches to fix this:  1 In the test, increase all the times (throughout the test) a bit to reduce the chance of a failure. I tried to get the times fairly tight so this doesn't add ages to phpunit run time but we could make them a bit bigger...  or  2 In the real code, change the timeout logic so that it won't stop indexing before it indexes at least one document (i.e. if you set a stupid small limit and it takes that long to just look at all the search areas with nothing new in).  or  3 Change the real code so that there is a way to override the current time value, for use in unit tests, so that the timeout part is independent of real time (i.e. it would only 'advance' time when the mock search engine indexes a document, and nothing would really have to wait any more). This will probably mean using the PHPUNIT_TEST define and a global variable for the current fake time. Could be a little complicated and makes the tests fractionally less 'realistic', but maybe the most comprehensive solution.  Obviously there is also option 4 to do nothing if the failures are sufficiently rare.  I'm OK to code a fix if you want, but not certain which approach is best?""",Bug,Unit tests
326278,"""A single machine learning backend is not the best choice for all models. It depends on the model.     I would propose to:  * Use the current predictions processor setting as a default predictions processor for all models  * Add a new field to analytics_models table that would overwrite the default      PD: My previous issue description got lost because I lost the network connection; I may extend this description in the future but not now, I am grrring""",New Feature,Analytics
326335,"""Hi          Is possible restore courses, SCORMs, assignments, and other elements ?    I try restore courses in Moodle from Canvas and I need import SCORMs, assignments and other elements but I can not restore those elements.     My version of moodle is 3.2.2.         Regards.""",Task,"Assignment,Course,SCORM"
326538,"""Hi,    I would like to be able to give a default grade to all students who have submitted an assignment. Would be great if a filter was added to perform bulk insert only on those who submitted.    Thanks a lot.    <USER>",Improvement,Assignment
326648,"""As requested in the parent ticket by [~<USER>'s peer review, this subtask was split to focus onto the necessary changes to blockquote editing in atto.    Our original idea was:    {quote}  * The Atto plugin will have to handle the styling of the quote and the management of replies which are inserted into the quote like it's possible with every today's email client (handling cases like to split the div at the point when the 'enter' key was pressed, adding a non-styled section in between).  * Based on that functionality, it would make sense if the Atto plugin can also be used detached from the use case at hand as a quote-styling button (especially as MDL-48905 and MDL-45662 to implement blockquotes in Atto seem not to come to an end).  {quote}    <USER>answered this in his peer review:  {quote}  We should really use the standard HTML blockquote when creating the quoted text, so I don't know exactly what changes would be made in Atto and I think this is likely to be the area of most critique in this issue. You'd need to go into a bit more detail on how you'd anticipate this working, especially for those adding multi-line block quotes. I guess you're looking for something like Jira when dealing with markdown - e.g. if you are in a list, and press return, you continue in the last. If you press return again, it exits the list.  {quote}    I would agree that we can use the blockquote tag instead of a custom div for the quote paragraphs. And you are completely right that this part needs some more elaboration.     However, we can already say that there are some basic things which will have to be changed to have Atto """"officially"""" support blockquotes:  * A user should be able to set an existing paragraph as quote (see especially MDL-45662 which is blocked by MDL-48905 and where nothing has happened since ages) in all Atto instances, independently from Atto inside forums.  * Moodle as a whole and the Atto preview has to display a blockquote tag as quote (currently blockquotes are simply not distinguishable from normal paragraphs)  * Atto has to support multiline blockquotes in a way like users expect it from today's mail systems and especially in a way to let the user split a blockquote easily into to parts and insert a normal paragraph in between.""",Sub-task,HTML
326743,"""I'm marking this as a bug, but it could be argued it is an improvement.    Plugins can define their own CSS in styles.css in the root directory of the plugin. Moodle picks up and complies these CSS files even when a plugin is disabled. A disabled plugin should not effect the operation or appearance of Moodle.    By not including plugin CSS files for disabled plugins, there will be less chance of collisions with other plugins and will lessen the amount of compiled CSS     """,Bug,Themes
326781,"""When grading an assignment submission that has been configured to accept both a file submission and an online text submission, the file fails to render and appears blank. I would expect that the submitted file would render. Note that any online text submitted does appear as expected. Steps to reproduce:   * As a Teacher in a course, create an Assignment   * Within the *Submission types* section, make sure both *Online text* and *File submissions* are checked   * As a Student, submit a file to the Assignment (whether you also enter text or not in the *Online text* field is irrelevant)   * As a Teacher, grade the submissions; any submitted file should fail to render    This is, admittedly, mostly an edge case. I suspect that most Assignments are configured to allow either file or online text submissions (with most, I imagine, accepting files by default), and rarely both. I caught this only by chance when a new faculty unfamiliar with Moodle accidentally checked both. I can, however, imagine certain use cases where an instructor might want to have both available, such as, e.g., if the submitted file is an Excel document or perhaps a PowerPoint (which will still render), and in addition the user needs to include some explanatory text or whatever within the *Online text* field.""",Bug,Assignment
326808,"""As per MDL-52996 it's possible to pass options for the toolbar to Atto via an option of atto:toolbar, but it only works if you're calling the editor directly.    If you build a form using moodleform and addElement, there is no way to pass in atto:toolbar, because use_editor explicitly requires it be part of the options array, and MoodleQuickForm_editor never handles that item - it only accepts the items defined in _options inside MoodleQuickForm_editor.    Seems to me that this needs to be defined as an allowed option in MoodleQuickForm_editor so that moodleform instances can customise what options are shown on the toolbar. Not something that would be useful in the core, but would be useful for plugins that need to handle this.    I needed to solve this in a plugin (and do so in 3.1), and created a class called MoodleQuickForm_simpleeditor that simply extended the existing MoodleQuickForm_editor, set the relevant item in $this->_options in its constructor and then deferred to the parent constructor. Registering this as a new element type and calling it with addElement gave the desired effect. This suggests to me that there wouldn't be side effects to doing this, any more than the existing atto:toolbar support would have.""",Bug,Forms Library
326989,"""This is related to a problem I ran in to today where trying to create stub accounts in Moodle for our users in AD via the Moodle web services API resulted in Moodle overwriting passwords in AD unexpectedly and very much against my wishes or intentions!    I had tried to log a bug, but the system would not let me, so below is what I had intended to log earlier:  ----  When creating accounts with *auth='ldap'*, Moodle overwrites the password that exists for the user in LDAP, locking them out of their account. This happens even when the *auth_ldap | stdchangepassword* setting is set to *No.* The description for that setting says clearly states there should be no password write-back when it is set to *No*.     What I was attempting to do was to create a stub account in Moodle for each of the users in our AD so they can be enrolled in courses before their first login. In the past we used Moosh for this, which allows us to create an account in Moodle with *auth='ldap'* without triggering either an email to the user, or, affecting the password in AD in any way.     This is what I was  trying to replicate with the web services API when I discovered that Moodle is writing back to AD when an account is created with *auth='ldap'*, hence, locking the user out of their AD account.  Because I absolutely did not want a new password written to AD, my first attempt was to invoke the *core_user_create_users* function via a *POST* request with arguments of the following form:  {code:java}users: [{    username: 'test1',    firstname: 'Test',    lastname: 'User1',    email: '<EMAIL>',    auth: 'ldap',    idnumber: 'test1'  }]{code}     This results in the following message from Moodle:  {quote}'Invalid password: you must provide a password, or set createpassword.'  {quote}  Based on that error I next tried:     {code:java}users: [{    username: 'test1',    createpassword: 0,    firstname: 'Test',    lastname: 'User1',    email: '<EMAIL>',    auth: 'ldap',    idnumber: 'test1'  }]{code}     This results in the same error     Since setting *createpassword=1* would result in a random password being generated, and that password being emailed to the user, I absolutely did not want to do that!     Next I tried using a blank password:     {code:java}users: [{    username: 'test1',    createpassword: 0,    password: '',    firstname: 'Test',    lastname: 'User1',    email: '<EMAIL>',    auth: 'ldap',    idnumber: 'test1'  }]{code}     Again, the same error.     This left no choice but to pass a dummy password.     Since this is an external auth method, and since I have *auth_ldap | stdchangepassword* set to *No*, I expected this dummy password to pass validation and then be ignored, or, perhaps written to the Moodle DB where it would then be ignored at login when the auth request was went to AD. I also have the Moodle setting *auth_ldap | auth_user_create* set to *no*, and nothing ticked anywhere in the LDAP settings to enable password write-back of any kind (I even searched the LDAP settings page for the word 'password' to be sure I had not missed anything), so I had no reason to assume Moodle would try to reach back to AD and effectively hijack the user's account, but that is exactly what happened.     Our logs show Moodle writing a new password to the user's AD account using the bind credentials supplied on the Moodle LDAP plugin settings page.     My proposed fix would be that when *auth='ldap'*, the *password* field not be required by the API, allowing for the creation of stub accounts within Moodle without locking the user out of AD. Further, the web service function needs to respect the *auth_ldap | stdchangepassword* setting and not try a write-back unless it is set to *Yes*.  ----   I've since found a workaround - create the account with *auth='manual'* with a dummy password, then *POST* the *core_user_update_users* function with arguments of the following form (using the ID returned by the initial call to *core_user_create_users* to create the manual account):  {code:java}users: [{    id: 12345,    auth: 'ldap'  }]{code}""",Bug,"Authentication,Web Services"
327036,"""As an instructor  I want to hide my course during a final exam  So that students do not cheat and access content they should not be able to during that time period.    As an instructor  I want to automatically hide my course after a term ends  So that students do not have access to its contents    As an instructor  I want to automatically reveal my courses to students  So that I can prepare course content without students seeing a """"work in progress"""" and without me having to remember to unhide the course once the term begins.    This patch adds in the ability for instructors to set when their courses will become hidden or unhidden via a cron task.""",New Feature,Course
327064,"""Create a simple form to send a request to the site data protection officer.    By creating the form in Moodle, we can verify the identity of the user making the request.    The record of the request should be stored in Moodle and an email sent directly to the data protection officer.    We can then describe the process of contacting the data protection officer on the privacy policy page.  ||User Stories||Acceptance Criteria||  |As a Moodle administrator I want moodle to provide a form for contacting the data protection officer for my moodle site.| * There is a site admin setting """"contactdataprotectionofficerthroughmoodle""""   * If enabled, users will be provided links from the site privacy policy, as well as from their own profile page to contact the sites data protection officer.   * The link will go to a new form with simple instructions and a text box for providing more details.   * When submitted - a record of the request will be stored and an email will be sent to the data protection officer.   * The language of these automated links will respect the users current language.|  |As a Moodle administrator with an alternate process for contacting my data protection officer, I do not want these requests to go through moodle.| * I can disable the new setting and no automatic links will be shown in the privacy policy or in users profiles.   * I can write manual instructions for contacting the data protection officer in my privacy policy.|""",Improvement,Admin
327065,"""A data processor is a third party plugin, integration or tool that Moodle transmits or receives data about an identifiable real person.    For each data processor we need fields to record:   * The personal data that is sent   * The purpose of sharing this data   * The data protection officer for the third party   * Any binding agreements in place to protect the data   * The privacy policy of the third party         These third parties must all be listed in the site privacy policy and changes to any of these fields will require re-acceptance of the new version of the privacy policy by all site users.       ||User Stories||Acceptance Criteria||  |As a Moodle administrator, I want to list all the third parties that my Moodle site will share personal data with in order to inform users about how and where their personal data is processed and the protections in place.| * In the Moodle site administration privacy settings, I can add a list of third party sites / services with which personal data is shared.   * For each third party I can list the type of personal data (profile fields, activity data etc).   * For each third party I can list the purpose of sharing this data   * For each third party I can list the data protection officer including contact information   * For each third party I can link to binding agreements that protect the privacy of the data (“data processing agreements”)   * For each third party I can link to a privacy policy   * Each of these fields containing free text must support multi-lang filters.|  |As a Moodle user, if a new third party plugin, integrated system or other add-on is added to the privacy policy, I need to review and accept the new list of third party tools before Moodle can send my personal data to the third party in respect to my privacy.| * Each change to the list of third parties tools should generate a new version of the privacy policy   * Every time a new version of the privacy policy is created all site users must consent to the new version before they can access the site|  |As a Moodle administrator I need the list of 3rd party plugins, integrations or tools to be displayed on the consent page during the onboarding process| * The consent record page can display a list of 3rd party tools, with a link to their privacy policy   * Each tool should have a checkbox next to it to record the user has purposefully consented to their data being shared with this 3rd party   * The user will need to consent to all 3rd party plugins to be able to use Moodle|          """,Task,Admin
327066,"""Add fields to record a Moodle site data protection officer and automatically include this information in the sites privacy policy. Any changes to these fields should trigger a new version of the privacy policy that must be re-accepted by all site users.       ||User Story||Acceptance Criteria||  |As a Moodle administrator I need to display the data protection officer for my site as part of my privacy policy in order to comply with the GDPR| * When these settings have been set, the information is automatically included in the sites privacy policy.   * Any changes to these settings will generate a new version of the sites privacy policy and require re-consent from all users.|  |As a Moodle administrator I need to be able to assign the Data Protection officer role to a user| * A new role can be created called Data Protection officer that can be assigned to a user (as a collection of capabilities)   * This role can create, view and execute subject access requests and requests from subjects invoking their right to be forgotten   * The activities of this role will be logged to create an audit trail|  |As a Moodle administrator I need to be able to assign the Data Privacy officer role to a user| * A new role can be created called Data Privacy officer that can be assigned to a user (as a collection of capabilities)   * This role can:      - Create and update entries in the data registry, including         defining purpose and retention time for collected data      - Update the sites privacy policy and determine whether           users need to re-consent   * The activities of this role will be logged to create an audit trail|""",Task,Admin
327067,"""Minors (< 16 years but can vary by EU member state) must have parental / guardian consent to accept a privacy policy.    We need a site setting to allow/disallow minors and if allowed we need a process for obtaining parental/guardian consent.  ||User Story||Acceptance Criteria||  |As a Moodle administrator, I need a way to prevent access to my site from minors in order to guarantee the protection of their privacy as well as a way to provide access to minors that complies with GDPR requirements of obtaining parental/guardian consent.| * A new user signing up for Moodle should be asked for their country of residence, before any user profile data is recorded   * There should be a separately maintainable list of countries with ages of digital consent, e.g. a listing of all EU countries (and any other relevant countries)   * If a new user’s country is one of those on the list, they will be asked whether their age is over the age of digital consent that is recorded in the list.   * If the new user is over the age of digital consent the sign-up process continues as normal.  If they are below the age of digital consent a page is displayed asking the user to contact the site admin as they are considered a minor and parental consent is required.   * If the site privacy policy is changed, the minor’s account will be frozen until the admin has received renewed consent from the parent / guardian.   * The record of accepting the privacy policy needs to include the guardian details that accepted the privacy policy, the minor (user) details, the version of the privacy policy and the date/time.   * Administrators will have to create a parent role (as described in Moodle docs) to be associated with any minor account created.   * It should be possible for admins to accept the privacy policy on behalf of users, after they have received parental / guardian consent (through means outside of Moodle).|  |As a parent/guardian I need to prevent my child from accessing a Moodle site until I give my consent to the terms of service and the privacy policy in order to protect their privacy.| * As a parent / guardian, if a new account is created for my child and they are a minor, their account will not be created until the site admin has been alerted and has contacted me through other means to obtain my consent.   * If the privacy policy is updated, access for my minor children should be frozen until the site admin has obtained my consent for the updated policy.|  |As a Moodle administrator I need a way to allow everyone access to my site regardless of age.| * A Moodle administrator should be able to not set the minimum age for accessing the site.   * This should make the age and country field optional for each user|""",Task,Admin
327068,"""Add a site level privacy settings page with some minimal features.   # Separate privacy policy page (separate to terms of service/site policy)   # Versioned (every edit makes a new version)   # Require all users to accept latest version of privacy policy before they can access the site       ||User Story||Acceptance Criteria||  |As an Moodle administrator I need all users on my site to read and accept the latest version of my privacy policy before I can collect, store and process that users personal data so that I don't face heavy fines. To do this I need settings to enable and manage policies.| * There is a privacy page for site settings containing all privacy related settings   * There is a site administration setting to enable / disable site privacy policy|  |As a Moodle administrator I need a page where I can enter in a privacy policy. This privacy policy should keep the previous versions that I have created that were accepted.|There is privacy policy page where:   * There is a site administration setting to force all site users to accept the latest version of the privacy policy before they can access the site   * There is a rich-text field allowing site administrators to write the text of the privacy policy   * The text of the privacy policy supports multi lang filters   * Each change to the privacy policy is saved as a separate version and it is possible to see all the previous versions   * By default all users will have to re-consent for any policy change on their first login following the change. This can be disabled on a case by case basis for minor changes to the policy that do not alter the meaning of its content.  If disabled, the users would not have to re-consent on their next login.|""",Task,Admin
327159,"""Custom profile fields currently have 3 view permissions that control who can see the user information on the user's profile:   * Not visible - For private data only viewable by administrators   * Visible to user - For private data only viewable by the user and by administrators   * Visible to everyone    It would be useful to have another two permissions 'Visible to teachers' and 'Visible to parents' that allows Moodle course teachers and parents to see a particular field on a student's profile.    For example, if a student has declared a disability, a field could contain the reasonable adjustments that staff are expected to make for that student. Anyone on a course with edit rights could then see that profile field, as well as parents of that student, but other students could not.    This is not similar to MDL-45242, which is instead asking for custom profile fields to be used as the user identity under User Policies. This ticket is about who can see the field, not which is set as the user identity. My last ticket MDL-59587 was closed as a duplicate, when the ticket is unrelated, so I'm re-raising this, so it can be considered for development.     Thank you.""",Improvement,"Admin,User management"
327202,"""As a student (testing user) I submitted a file so teachers can confirm rubric and marking in the live setting.    Once completed I want to remove the test student submission so it does not get mixed with the real ones.    So set submission back to braft.    Login as student, edit submission, delete file.    Now prevented from saving changes (removing file) as Error is returned """"{color:#b94a48}Nothing was submitted{color}""""    Needs to Save anyway.  If there are no files and Moodle saves 'No files' what does that matter?    Thanks     """,Bug,Assignment
327217,"""The version.php file has a way to declare which first major stable version is supported via:    {{$plugin->requires = '2014010101';}}    There are two problems with this:    1) There is no possible way to declare that a particular stable is not supported. In particular when  API's are deprecated, in the plugin we need to have multiple branches which each declare exactly what major stables they support., and don't support.    2) it's opaque and difficult to often match the date back to a stable version at a glance (admittedly a minor issue)    What I'm suggesting is a new way of declaring support like this:    {{$plugin->supports = array('2.9', '3.0', '3.1');}}    The intent of this is to declare what is known to work, which may not necessarily be exhaustive and could be out of date when new sables arrive but still actually work.    There could be a second array which explicitly declares what is known to not be supported too:    {{$plugin->unsupported = array('3.2', '3.3');}}    This data would be leveraged in a couple ways:   * If 'supported' is provided AND the current version is not in the list, then at install / upgrade time a warning would be produced.   * If 'unsupported' is provided AND the current version is in the list, or a lower value is, then this should be a hard install / upgrade failure. We can safely assume that if say 3.3 is not supported then all future versions after 3.4+ are also not supported. This value could just be the first unsupported version string, and not an array, as they are functionally identical.   * In the Moodle plugin directory this info can be parsed out and use to automatically populate the plugin metadata. The plugin directory could be configured to ingest multiple git branches from github which each declare different support levels.    The latter point is the more important part as I believe it is the last remaining piece of the puzzle which is blocking full automation of plugin publishing (MDLSITE-4781).    Fully automating the plugin publishing into the plugins directory is a big deal. Maintaining the directory metadata is onerous and error prone and results in a lower level of trust in people trying to install stuff from there.     """,Improvement,Admin
327349,"""If you make an ad-hoc cache using cache::make_from_params, this is not reset by cache_helper:purge_all. As a result, these caches can persist between PHPunit test runs.    I think cache_helper:purge_all should delete the adhoc caches. I'll see if it is easy to code this.""",Bug,"Caching,Unit tests"
327362,"""I would like to request for a new improvement made on the assignment module.    Presently, the assignment has a rubric and it allow users to copy rubrics from template. At the moment, it only allow 1 rubric to be copied.    I would like to request for assignment to copy multiple rubrics from a master template and being merge into 1 rubric so as administrator, I can setup all common rubrics (preset by the local authorities), then teachers can just select which rubrics to be use, system will combine them into a new rubric for the assignment. Teachers then can edit the assignment rubrics to add more evaluation criteria.""",Improvement,Grading methods
327398,"""Making settings for groups and groupings in Assigment is quite confusing for the user, but it is also a bug(-like?) risk of making contradicting settings.    When Restrict Access is activated there are three different areas where Gropus and Groupings are managed in Assignment:   # Group submission settings   # Common module settings   # Restrict access    It is very easy to choose the wrong settings, but it is also very easy to make contradicting settings, as A and B in the attached image.    The three blocks of settings being closed as default doesn't make the overview better.    This needs to be cleaned up. As I see it the whole """"Common module settings"""" block could be removed, but maybe I'm wrong. In any case, something should be done to make this more intuitive and fail-safe. """,Bug,Assignment
327399,"""In MDL-58957 we are trying to implement block searching. In the Boost theme, linking to blocks looks weird. For example:   # Create a new course with default settings.   # Add a block, such as the Calendar block.   # Using browser developer tools or another method, determine the instance id of the block, for example inst52.   # Type #inst52 at the end of the URL    See the two screenshots attached - one is 'normal' view, second is 'link to calendar' with #inst52. The Calendar block has a nice grey bar above it in the normal view, but appears to run directly into the content above when you apply #inst52 (there is still a faint 1 pixel line which is the block border but that's all).    This is caused by these rules:    :target \{       padding-top: 80px !important;       margin-top: -50px !important;   }    These are designed to make the target appear just below the fixed header, rather than directly at the top of the page underneath the header.    With blocks they make it look a bit odd - the block is white with a grey background between blocks. The 80px added padding is white, and the 50px negative margin removes the grey space between blocks (replaced by the last 30px of the padding). So the result is a block with more white space above it (intended although still weird) but also no grey border separating it from the previous block.    I investigated some block-specific styling; this might be an option in the sense of making it look a bit better, but I had some trouble with borders, and I was concerned that the changes I would have to make to get it to look nice might well not work well on modified children of Boost theme, thus causing problems for themers.    I think the best solution for this might be to have the Boost theme remove this CSS rule and instead use JavaScript to update the scroll position (for all cases not just blocks as it surely can make things look weird on other items too) - however this could have negative consequences such as making the page jump around on load, so it may not be simple to implement...""",Bug,Themes
327623,"""As a theme designer I want a fast change test iterate loop in order to dev things quickly""",Task,Themes
327624,"""As a user, I want to interact with the page, before CSS is fully ready:  After upgrades  After theme setting changes  After cache purges  """,Task,Themes
327625,"""As a hosting provider, I want to reset local cache without causing CPU spikes, in order to scale up and down""",Task,Themes
327691,"""When restoring a course backed up by a previous version of Moodle, Moodle 3.3 throws exceptions in the file restore_format_weeks_plugin.class.php due to the absence of a course end date in the backup file. The exceptions aren't displayed in the web page but appear in the apache error log file.    Steps to repeat:   # In Moodle 3.3, as an administrator, restore a course backup created in a previous version of Moodle (3.0 or 3.1 are known to fail - I'm not sure about 3.2).   # Check the apache error log.    The full error traceback is the following (as generated by running the phpunit tests in the CodeRunner plugin, qtype_coderunner_restore_testcase::test_restore)    Undefined index: enddate    /var/www/html/moodle/course/format/weeks/backup/moodle2/restore_format_weeks_plugin.class.php:166  /var/www/html/moodle/backup/moodle2/restore_plugin.class.php:101  /var/www/html/moodle/backup/util/plan/restore_structure_step.class.php:446  /var/www/html/moodle/backup/util/plan/restore_task.class.php:106  /var/www/html/moodle/backup/util/plan/restore_plan.class.php:204  /var/www/html/moodle/backup/moodle2/restore_final_task.class.php:121  /var/www/html/moodle/backup/moodle2/restore_stepslib.php:821  /var/www/html/moodle/backup/util/plan/restore_execution_step.class.php:34  /var/www/html/moodle/backup/util/plan/base_task.class.php:181  /var/www/html/moodle/backup/util/plan/base_plan.class.php:178  /var/www/html/moodle/backup/util/plan/restore_plan.class.php:167  /var/www/html/moodle/backup/controller/restore_controller.class.php:339  /var/www/html/moodle/question/type/coderunner/tests/restore_test.php:78  /var/www/html/moodle/question/type/coderunner/tests/restore_test.php:105  /var/www/html/moodle/lib/phpunit/classes/advanced_testcase.php:80""",Bug,Backup
327774,"""Hello,    When you open students information as a teacher on a course from navigation-block -> participants -> select a student -> Under report-title you choose Grade -> this shows students all marks/grades on the course but if you then click f.ex. assingment-activity from this page it doesn't take you straight to _this students_ submission, it just opens the general Assignment grading summary page where you can select """"view all submissions"""" or """"grade"""".     I would like to see that it would take the teacher straight to the selected students submission in the selected activity.""",Improvement,Reports
327870,"""I originally started to log in via OAuth2 to our Google account about a week or two ago  At the time, I logged into my Google account, and was logged straight into Moodle. As a result, I didn't pay much attention to the e-mail confirmation that was sent with it.    Now I've tried to log in again, I seem to be stuck in a no-go zone:  # Trying to log in using OAuth2 (Google) gives me: """"{{This external account is already linked to an account on this site}}""""  # Trying to click the link in my e-mail gives me """"{{Invalid confirmation data}}""""  # Logging in using the old-fashioned way, and navigating to the Linked logins gives me """"{{Can not find data record in database table oauth2_issuer.}}""""    """,Bug,Authentication
328032,"""This is neither a bug nor an improvement strictly speaking but I think it would be wise to have a proper think about it.    On a database of any significant size the conversion to full unicode support is impractically slow. My test site has around 30,000 users, 8,000 courses and several years of data. Experiments (on a reasonably well specified MariaDB server) indicate that this may well take the order of several days to complete.     I'm still looking into this but it looks like the conversion may use vast amounts of disk space too. It's probably for temporary tables but if your server doesn't have it...    It's a big ask for large sites to go offline for this period of time.     I know it's optional just now but these things have a habit of being required down the line.      """,Bug,Database SQL/XMLDB
328166,"""We (and others) have noticed that when using scripts that create backups (so many are made during one session), each trip through the process the backup gets slower and slower.    I've tracked it down to sql_generator->getNameForObject(), it uses a tracking array to keep track of index names it has already issued, in an inefficient way.""",Bug,"Performance,Backup,Database SQL/XMLDB"
328186,"""I noticed this while testing MDL-57858 and I think it should be either confirmed as a feature, or addressed as a regression of the new Dashboard.    h3. Reproduce    # On 3.2 site, create an Assignment with the default Due date enabled  # As a student, submit that assignment  # As a teacher, check that the assignment is reported at your Dashboard - it requires your attention because there is a submission to be graded (do not grade it)  # Upgrade the site to 3.3  # Expected result: I would expect the teachers continue to see these reminders  # Actual result: Nothing is displayed on the timeline  """,Bug,Assignment
328197,"""You can currently set a sitepolicy as a url people need to read when they first login and agree too. This is ok but it is:    a) very simple with no scope for more complex rules like how to version the policy and under what conditions to invalidate the agreement (eg every year regardless of wether it's changed)    b) the use of an external url inside an iframe isn't great. There are other trackers like MDL-46553 to improve on this    c) we've implemented a couple plugins for different clients who have replaced this with something much more nuanced, like needing to complete a course or an activity, or they need to fill out some other extra form or profile fields but we don't want that form up front on the self enrollment page.    Some sitepolocy logic is also tightly coupled with the email authentication plugin which should be untangled.    This seems like another good candidate for a new family of related callbacks so that we can write plugins to intercept this whole flow. It should also be possible to chain several of them together, either because some will only fire under certain conditions or we want the student to go through a couple unrelated stages.    So I haven't completely worked through what the cleanest api would be, but we'd want some sort of callback probably inside require_login which returns a temporary alternate wants url where they go do something and then come back. And a second callback inside user_not_fully_set_up() which also gets passed the strict flag.    Then finally the existing sitepolicy code can be cleaned up and refactored into a standalone plugin, probably an admin tool.""",Improvement,User management
328282,"""Accept xAPI and/or Caliper statements, so we can allow them to be used as indicators in learning analytics. This would enable the system to take demographic or grade history into account.    We may need to provide a recommended taxonomy/vocabulary for xAPI or Caliper statements, which can be almost anything. I would suggest starting with some expected indicators like SES (socio-economic status), cumulative GPA, and basic categories of prior test scores or qualifications (e.g. mathematics, language of instruction), with a way for the system administrator to specify which statements from external LRS systems should align with each category. Institutions with existing LRS and analytics systems will be more interested in capturing statements from Moodle Inspire to include in their own systems, but for many institutions, Inspire is likely to be their first analytics system. An alternative to accepting LRS statements would be the ability to select custom profile fields for these data elements, as many institutions have some mechanism for importing SIS data to student profiles, or even use Moodle as a simple SIS.""",New Feature,Analytics
328425,"""Wanted to float this as an idea for core. It's a change we've been working with since the adhoc tasks system was introduced and it has proved very reliable for us.    This patch introduces little change to the way adhoc tasks are run, but allows plugin developers to build new queue interfaces (for example, we use Redis, and originally used beanstalk).    The reason for this change is that we quickly grew beyond a cron queue. We currently generate about 16 tasks per second during high load and having cron go through 960 tasks every minute is very impractical. We also have a need for improved response times for tasks such as Turnitin submissions where we don't want to wait for an available cron process to get round to processing the submission.    In the core integration I've taken the approach of pulling it in the same way alternative file systems was implemented as I felt, after much thought, that one would never want multiple queues due to locking issues between them. You only want one queue in the same way you want one database driver, or one file system.""",New Feature,Tasks
328549,"""I have a single Moodle course, into which I'm uploading a series of SCORM packages to test.    They were all created in Articulate.    The first couple worked fine.    But now I can't upload any zip files and unzip them. Hence why I've escalated this as a blocker.    When I try to upload them (either as a SCORM package, or as a file for Moodle to unzip into a folder) I get the following error message:    Error: Incorrect pool file content     Debug info:   Error code: hashpoolproblem    ×Stack trace:  line 2047 of \lib\filestorage\file_storage.php: file_pool_content_exception thrown  line 1601 of \lib\filestorage\file_storage.php: call to file_storage->add_string_to_pool()  line 484 of \lib\filestorage\zip_packer.php: call to file_storage->create_file_from_string()  line 547 of \lib\filestorage\stored_file.php: call to zip_packer->extract_to_storage()  line 269 of \<USER>scorm\locallib.php: call to stored_file->extract_to_storage()  line 165 of \<USER>scorm\lib.php: call to scorm_parse()  line 121 of \course\modlib.php: call to scorm_add_instance()  line 161 of \course\modedit.php: call to add_moduleinfo()    ×Output buffer: Warning: mkdir(): File exists in D:\home\site\wwwroot\lib\filestorage\file_storage.php on line 2044    I have asked our sysadmins to check for disk space issues. But why is this occurring?    I thought it was fixed: MDL-27137 and MDL-36959""",Bug,SCORM
328585,"""When work is downloaded in bulk from an assignment using """"Download all submissions"""" or its variants, the folder hierarchy is too """"deep"""" and creates issues when grading student work submitted as a zip of a hierarchical folder structure and requires extra work to """"unzip"""" at multiple levels and filepath-length issues.  Previous versions of moodle did not suffer from this error, since the documentation is outdated, even for version 3.2, it is unclear when this change was made, but it has made grading harder for my programming-heavy courses.    Since my students submit their Visual Studio or Unity projects as deep folder hierarchies, here is the hierarchy that I get when I download using """"Download all submissions"""":  - Zipped assignment file  -- Folder with student's file submission [NEW LEVEL]  --- Zipped folder as file submission  ---- Original project folder (THIS IS WHAT NEEDS TO BE GRADED!)  -- Folder with student's text submission  --- HTML page of student's text submission (SOMETIMES GRADE THIS)    The previous version of the hierarchy was significantly better:  - Zipped assignment file  -- Zipped folder as file submission  --- Original project folder (THIS IS WHAT NEEDS TO BE GRADED!)  -- HTML page of student's text submission (SOMETIMES GRADE THIS)    The problem with the new folder hierarchy is that I must open the folder marked [NEW LEVEL] for EACH student, then unzip their projects individually.  Previously, I could simply highlight ALL of the students zip files (after sorting the zip from the HTML pages, which is a single click in Windows Explorer) and unzip them at once.    While that is a hassle, more importantly, adding the extra folder levels in the hierarchy has created grading problems with Visual Studio projects as VS has a file-path limit of 256 characters and with the addition of these extra levels, my students' projects frequently exceed this limit, forcing me to move all of their projects to C:\ or similar before grading and then move them back to store copies after.  I'm ignoring the filenaming/foldernaming issues as there is another issue that has already been raised for this, but it is related as the length of the filenames and foldernames created increase the filepath length.  Most of the extra text added is irrelevant (I use a batch renaming tool to chop off everything but the student name anyway).  Unfortunately, this renaming tool is worthless if I have to manually add EACH file or folder (previous versions I could just drag all of the zip files over, rename them, and then extract them).    I believe this started around version 3.0, but the documentation does not reflect this update, so I'm not certain.    The ideal solution would be to allow the admin/teacher to customize the configuration of their downloads both by configuring the downloaded folder structure and configuring the filename of the downloads.  Ideally, the folder structure needs to at least allow the admin/teacher to return to the previous version and the folders and filenames needs to be completely customizable with username, id, etc.""",Improvement,Assignment
328597,"""This is a followup of MDL-48228, where support of utf8mb4 was added to mysql/mariadb.    One of the differences is that, after the patch, new sites being installed from scratch get the dbcollation written to config.php, while that was not happening before (when everybody was utf8 and done). It has both ADVantages and PROblems:    1) ADV: 1-less query per request. Having the collation in config.php makes moodle trust it 100% and a minimum of one query (against slow information schema) is saved on each request.  2) PRO: In case of the dbcollation being changed (using our mysql_collation.php CLI, or any other strategy)... it's possible that the config.php becomes outdated and easily forget about t.    So this is about:    1) reconsider if we want to move new installations back to the initial proposal - aka, don't write the dbcollation to config.php ever, knowing it's 1-Q slower. That's exactly <USER>s patch. And it's how Moodle has been working since the born of the times.    2) if we decide to continue with dbcollation written in config.php.. then we need some check (enviromental or health) detecting if file and real collations diverge at any point.    And that's it. Would be interesting to have the outcome of this decided ASAP, so I'm raising its importance.    Also, [~<USER>, would you please drag your patch from MDL-48228 here so everybody knows what we are talking about (getting rid of collation in config.php). TIA!    Ciao :-)""",Bug,Database SQL/XMLDB
328640,"""A school is backing up courses and restoring into new courses in order to transfer content from the current term to courses for the upcoming term.  These restore attempts often fail repeatedly or take up to 30 minutes to complete if they are successful.    Using MONyog to monitor the database during the course restores consistently reveals locking on the <USER>question table caused by this query:    UPDATE <USER>question SET questiontext = '0' WHERE qtype = 'random' AND questiontext = '' AND id IN (SELECT bi.newitemid FROM <USER>backup_ids_temp bi WHERE bi.backupid = '4f244f111cef3c2340215c87131f0be2' AND bi.itemname = 'question_created')    This query was added as part of MDL-45763 to ensure the proper setting of questiontext on random questions.  My reading of that issue is that the fix addressed the question creation process to ensure questiontext is set correctly on new random questions and also added the above query to the upgrade code.  Thus any random question created since 2.7.1 will be properly configured, and any site that has been upgraded from a release prior to 2.7.1 would have had all random questions updated to the appropriate configuration.  Based on this understanding the need to execute this query during a course restore should only occur when restoring a backup created against a site on a version earlier than 2.7.1.  Therefore, I would propose that the course restore process check the moodle_backup.xml file to identify the version of the Moodle site from which the course backup was created and only execute the update query on the <USER>question table when the backup originated from prior to 2.7.1.       *STEPS TO REPLICATE*  1. Create a backup of a course containing numerous random questions.  2. Go to Site administration > Courses > Manage courses and categories.  3. Locate a course and go to Restore.  4. Upload the backup file created in step 1 and click Restore.  5. Select Restore into this course > Merge the backup course into this course.  6. Proceed through the rest of the restore wizard using the default settings and perform the restore.    OBSERVED BEHAVIOR:  Restore takes several minutes to complete or does not complete at all.  Monitoring software shows locking occurring on the <USER>question table caused by the query UPDATE <USER>question SET questiontext = '0' WHERE qtype = 'random' AND questiontext = '' AND id IN (SELECT bi.newitemid FROM <USER>backup_ids_temp bi WHERE bi.backupid = '4f244f111cef3c2340215c87131f0be2' AND bi.itemname = 'question_created')     EXPECTED BEHAVIOR:  Course restores complete consistently and in a timely fashion by not execute unnecessary database updates.  """,Bug,Course
328647,"""I have an issue with editing the front page of our school Moodle site which is effectively our school website. There don't seem to be any other issues editing any other page only the front page.    On order to edit the page you have to log in and then Turn on Editing. Then I usually scroll down to the pencil icon which is usually in the bottom left hand corner of the page but seems to be stuck onto the text on the right. This means I am unable to open the page to edit it. I am aware that I am using an older version of Moodle but I've never had any problems in the past and I use it as our website also.    I have tried to speak to my local LEA ICT support (Bridgend County Borough Council)  but I am having great difficulty in getting any information from them on how to resolve it. I have tried using a wide variety of search engines to see if it makes a difference but unfortunately to no effect. Firefox has always been the most effective in editing the Moodle site in the past.    My version is 1.9.5 and doesn't appear below so I have picked any one in order to create this Tracker. Apologies for any confusion caused.""",Bug,"Admin,Usability"
328682,"""I want all my users to be able to see their colleague's profiles, including their name and email address (if the user has allowed it).    I have set Authenticated User capabilities:  moodle/user:viewdetails: Allow  moodle/user:viewalldetails - Allow    Expected behaviour:  Users should be able to view their own profile and see their firstname, lastname and email  Users should be able to view other's profiles and see their firstname, lastname and email (if that person has allowed it)    Actual behaviour:  The profile is visible. I can see Country, City/town, Interests - but not name or email.  This is true for my own and others' profiles.    """,Bug,User management
328696,"""How to reproduce:    # Create a small course using development tools  # Edit one of the assignments and enable submission (text for example)  # View the gradebook (ex: /<USER>assign/view.php?id=XXX&action=grading)  # Order by 'Last modified (submission)' descending (triangle pointing down), it is ok as we have no submissions yet.  # Login as a user and send a submission.  # Go back to view gradebook, last page, it should be there the submission as expected.  # Login as yet another user, go to the submission page BUT DO NOT SUBMIT.  # Go back to view gradebook, last page, the user will be shown there as if it has submitted something. It happens because when he/she visits the submit page it creates a record in the {assign_submission} table with status = 'new' and a timemodified.    I would expect it to only consider if status='submitted' as implied by the column name """"last modified (submission)"""".""",Bug,Assignment
328788,"""I am aware that Moodle has a minimum time based activity completion constraint option for Lessons now, but I need that exact same capability for SCORM.      I do volunteer work for an organization that has to meet government requirements.  They require us to enforce a minimum time spent on each SCORM module, before it can be marked complete.  I know it's controversial and there are ways to defeat the intent, but we only need to be able to require this minimum time spent on the module, regardless of opinions.    What I expect-------------  Exactly the same functionality current existing with Activity Completion """"Minimum Time Required"""", except that it should work on SCORM Activities as well.    I would like to offer a donation to Moodle directly, particularly if this can be accomplished no later than 14 Feb 2017.  I am new to Moodle but I am a big fan of open source software so I would really like to contribute to get this feature, particularly if our initial effort can turn into something commercially viable into our niche environment.  My apologies if this comes about the wrong way... I am unfamiliar with the monetary side of Moodle.  I just needed this feature in short amount of time.""",Improvement,SCORM
328843,"""The Atto editor, mostly when used in IE11, becomes sluggish as text is typed. The more text that is typed the slower it appears on screen in the Atto text editor until eventually IE freezes the page because it has become unresponsive and the browser crashes. I have experienced this issue in Firefox 51 also.  The issue as it was originally reported is replicated below:    ----------  We have an issue where users are experiencing the page not responding while replying to a forum post, this can result in text being lost when refreshing or the page hanging and never being recovered and is causing much frustration for many users. (Similar experiences are reported when clicking the post button, sometimes it fails to post or succeeds but doesn't show that it has succeeded.)  When the issue occurs, while typing, the text displayed slows right down and doesn't keep up with what has been typed, this eventually results in the page not responding. The end end result is the same if you don't type anything at all. It seems the longer you have the page open, the greater the chance of the page not responding.  This happens somewhat consistently in IE, the issue seems to happen less frequently in Firefox and Chrome, although it does still happen sometimes.    Users are reporting that this issue occurs at our premises and also at other locations, predominantly while using IE11 on Windows 7 as this is their standard setup. IE11 only reports that cem.ac.uk is not responding and asks to recover the webpage, Yes/No.  Some users have what appears to be the same issue in Mozilla Firefox. the error message is more useful and contains:  Header:  Warning: Unresponsive script  Message:  A script on this page may be busy, or it may have stopped responding. You can stop the script now, open the script in the debugger, or let the script continue.   The above error occurred in a newer version of Windows on an unrelated PC and network to the others.    I speculate as to whether this is related to the autosave feature?    This doesn't seem to have been experienced before the 3.1.4 security update but users have been experiencing it since 16 January, possibly a little longer.    Is it possible that this issue could be instigated or exacerbated by a poor connection?    Are you aware of other clients experiencing a similar issue?  ----------    Steps to follow to replicate the issue:  1. Start IE11  2. Log into Moodle (v3.1)  3. Enter any forum  4. Start a forum post  5. Type for an extended period of time  6. The appearance of the text should become sluggish as you type and eventually stop appearing.  7. The page will eventually become unresponsive  8. Recovering/refreshing the page does not guarantee all of the text typed has autosaved    I would expect the the the editor to handle normal typing speed when text is entered, but it does not.""",Bug,HTML
329049,"""We have an azure database that we want to use as a source of our users.    The sqlsrv driver by microsoft is now available for both linux and windows so it's no longer only windows servers that can connect with azure or sql server through this driver.    I noticed the external database auth plugin uses adodb in the background, adodb already has support for pdo_sqlsrv so it's mostly a matter of changing the settings form to allow you to select this driver.    I've tested it out myself with moodle 3.2.1 by hardcoding some changes in /auth/db/auth.php    change line 159 to: $authdb = ADONewConnection('PDO');  change line 164 to: $authdb->Connect('sqlsrv:server=SERVERNAME;database=DATABASENAME', $this->config->user, $this->config->pass);    Everything seemed to work without problems with this hardcoded version. I suppose the biggest issue is the fact that the connection string through PDO looks very different from the currently supported database types. Which makes it more work to add this then simply expanding the options list with one extra option.""",Improvement,Authentication
329191,"""Dear devs,  i would like to request a feature to set a cut off date for each attempt of an assignment.  Say i have set 2 attempts i want the first done by next Monday and the second by Tuesday.  For a Java class we currently do it as follows:   The cutoff date is set to Wednesday. The teacher then grades every submission and allows for the students for a second chance by manually overriding the cutoff date for each student. If a student has not submitted a first attempt until the original date, he will not be able to get a second attempt.  Besides the need to manually set the new cutoff date for each student, we can not easily compare the first and second attempt.  We figured that setting maximum attempts to 2 would do the <USER>except that we can not set a different cutoff date for different attempts.    thanks for your time and help   merry x-mas and a happy new year    yours <USER>""",Improvement,Assignment
329246,"""Hi,    I detected this because, since some days ago, my email client (Thunderbird) that strongly relies on the """"Message-ID"""" and their counterparts (""""In-Reply-To"""" & """"References"""") are not matching anymore, so I'm getting here all the forum messages without proper threading.    Looking to old mails, they were coming with these type of """"Message-ID"""" headers:    {{<<EMAIL>>}}  (that are the usual ones, calculated by us in moodle, see {{generate_email_messageid()}})    But, since some days ago, those """"Message-ID"""" headers look like:    {{<<EMAIL>>}}    (completely different from the ones generated by Moodle and obviously, """"In-Reply-To"""" & """"References"""" - also generated by us - do not match anymore).    -I've been looking for code changes but have found nothing justifyng that change. So I think this is something related with some moodle.org configuration change (within moodle, or php.ini mail setting, or mail server itself, or gmail...). But it seems clear that now our (moodle-generated) headers are not being sent anymore, but replaced by another one, completely different.-    MDL-56000 has been identified as a potential change affecting this.    So matching does not happen anymore and threads are destroyed for some email clients.    Ciao :-)""",Bug,Libraries
329432,"""When an AJAX call fails due to a network problem or server issue (i.e. not when Moodle raises an exception) the fail() handler on the promises returned is never called, even though there appears to be a handler for it in the code.    h2. STEPS TO REPRODUCE:  _Note: if you don't want to bother copying all this across I've included a zip of local/ajaxfail_ [^ajaxfail.zip]   1. Create a local plugin called """"ajaxfail"""". (It will need a version.php, lang/en/local_ajaxfail.php etc.)  2. Create file called ajaxfail/classes/external.php  3. Add the following code to that file:    {code:php}  <?php    namespace local_ajaxfail;    use external_api;  use external_function_parameters;    class external extends external_api {        public static function fail_parameters() {          return new external_function_parameters([]);      }        public static function fail() {          // Simulate a server failure          die();      }        public static function fail_returns() {          return null;      }    }  {code}    4. Add the following entry in /local/ajaxfail/db/services.php    {code:php}  <?php    $functions = [        'local_ajaxfail_fail' => [          'classname' => 'local_ajaxfail\\external',          'methodname' => 'fail',          'type' => 'read',          'ajax' => true      ]        ];  {code}    5. Add the following mustache template to /local/ajaxfail/templates/fail.mustache    {code:mustache}  <h1>This will fail.</h1>    <p>Clicking this button should generate an error.</p>  <button id=""""{{uniqid}}-button"""">Fail</button>  {{#js}}  require(['jquery', 'core/ajax', 'core/notification'], function($, Ajax, Notification) {           $('#{{uniqid}}-button').click(function() {          var promises = Ajax.call([{              methodname: 'local_ajaxfail_fail',              args: {}          }]);            promises[0].done(function() {              alert(""""The call succeeded"""");          })            promises[0].fail(function(error) {              alert(""""The call failed"""");              Notification.exception(error);          })      });    });  {{/js}}  {code}    6. Add the following code to /local/ajaxfail/fail.php    {code:php}  <?php    require_once('../../config.php');     require_login();     $PAGE->set_context(context_system::instance());  $PAGE->set_url(new moodle_url('/local/ajaxfail/fail.php'));    echo $OUTPUT->header();    echo $OUTPUT->render_from_template('local_ajaxfail/fail', []);    echo $OUTPUT->footer();  {code}    7. Navigate to /local/ajaxfail/fail.php  8. Click the button marked """"Fail""""    h2. EXPECTED RESULTS    An error should be presented to the user.    h2. ACTUAL RESULTS    Nothing happens. At all. There's no error log in the server logs, there's no error in the JavaScript console, the fail handler isn't called. This is practically the definition of a silent failure.    h2. NOTES    The culprit here is what looks like some old, non-functioning code that somehow made it all the way through to the present day.    {code:javascript}      var requestFail = function(jqXHR, textStatus) {          // Reject all the promises.          var requests = this;            var i = 0;          for (i = 0; i < requests.length; i++) {              var request = requests[i];                if (typeof request.fail != """"undefined"""") { // this is the problem line                  request.deferred.reject(textStatus);              }          }      };  {code}    The designated way to accept a failure from AJAX is to use the fail() handler of the returned promises, but there was a legacy method of setting the 'fail' property on the request itself. Even when using that legacy behaviour, the error delivered to the fail handler is utterly unlike the other failures delivered to that handler, as it is a bare string which is of little use to anyone.    The comment at the top of the function clearly states """"*Reject all the promises*"""" – yet unless the promise was created with the legacy method of setting a 'fail' property, _the promise is not rejected_.    Of all the calls to Ajax.call in core, not one uses this old syntax, which means that *not one handles request failure*. They all use the modern Promise-based syntax, which means that {{request.deferred.reject(textStatus)}} in the above function is _never called_ in all of core Moodle. I strongly doubt it's called by any third-party plugins, either. Server failure is simply unhandled by Moodle's AJAX library.    Thankfully, there's an incredibly simple 2-line patch to fix it.    {code:javascript}      var requestFail = function(jqXHR, textStatus, errorThrown) {          // Reject all the promises.          var requests = this;            var i = 0;          for (i = 0; i < requests.length; i++) {              var request = requests[i];              request.deferred.reject(errorThrown);          }      };  {code}    This works as expected and is handled properly by Notification.exception.  """,Bug,"JavaScript,Web Services"
329516,"""If you have a PDF with lots of pages (thousands), the conversion interface becomes crippled, and the user is basically unable to grade.    This was a less common problem before unoconv, but now if students upload an excel (or some other file) that has lots of rows to it, unoconv converts it to PDF, and then Moodle converts those files to images for the PDF markup.    I'm attaching some example PDFs. In my case, if I add the 3 files, it will timeout while trying to load them.    A few problems we have observed:  # The fragment/widget doesn't ask for an increased timelimit, meaning if the setup takes longer than 30s (this includes unoconv conversion, combining into a single PDF, and determining page count), a timeout happens. This timeout is 'silent' to the user, the grading interface just shows spinning loading icons, and the users session is held. See document_services::page_number_for_attempt().  # Same goes for document_services::generate_page_images_for_attempt() - while converting combined pdf to images, it doesn't ask for more time (this timeout is less likely, because exec() doesn't count against the timelimit).  # The file conversion (unoconv), generating combined PDF, and generating images all hold the user's session open. The last one is particularly problematic because the interface has loaded, with a progress bar, but autosave throws errors (because the session is held), and the user can't move to another user, or save the grade/comment because of the session being held open. IMO editpdf/ajax.php -> loadallimages could/should release the user's session.  # editpdf/ajax.php \-> loadallimages returns no data until complete, and a separate ajax poll is done to update the on-screen progress bar. This will run into load balancer timeout ($CFG->maxtimelimit). A better scenario would be to stream back using the progressbar class interface. It would reduce the number of connections and meet the requirements of data flowing through the connection to keep it open.  # If conversion to page images is interrupted (timeout, user navigates away, etc), when returning to grade that submission again, rather than resuming the conversion, it displays the number of pages that were finished, acting as if there are no more (opened MDL-57200 for this)    It also dawned on me that it seems silly that we are generating and loading all the page PNGs up front, rather than doing it lazily (opened MDL-57201). Changing that would potentially fix #2, 3, and 4 above.""",Bug,Assignment
329825,"""We have had reports from some of our users about how slow Moodle was. Looking in my mysql slow query log I found a query was appearing time again which was taking in excess of 6 seconds to run.    An example of this query is attached, along with an explain which shows that it is doing a full table scan instead of using the available indexes.    I have traced this query to {{/lib/accesslib.php}} and the function {{get_user_access_sitewide()}}:    {code:php}  foreach ($raparents as $roleid=>$ras) {          $cp++;          list($sqlcids, $cids) = $DB->get_in_or_equal($ras, SQL_PARAMS_NAMED, 'c'.$cp.'_');          $params = array_merge($params, $cids);          $params['r'.$cp] = $roleid;          $sqls[] = """"(SELECT ctx.path, rc.roleid, rc.capability, rc.permission                       FROM {role_capabilities} rc                       JOIN {context} ctx                            ON (ctx.id = rc.contextid)                       JOIN {context} pctx                            ON (pctx.id $sqlcids                                AND (ctx.id = pctx.id                                     OR ctx.path LIKE """".$DB->sql_concat('pctx.path',""""'/%'"""").""""                                     OR pctx.path LIKE """".$DB->sql_concat('ctx.path',""""'/%'"""").""""))                  LEFT JOIN {block_instances} bi                            ON (ctx.contextlevel = """".CONTEXT_BLOCK."""" AND bi.id = ctx.instanceid)                  LEFT JOIN {context} bpctx                            ON (bpctx.id = bi.parentcontextid)                      WHERE rc.roleid = :r{$cp}                            AND (ctx.contextlevel <= """".CONTEXT_COURSE."""" OR bpctx.contextlevel < """".CONTEXT_COURSE."""")                     )"""";      }        // fixed capability order is necessary for rdef dedupe      $rs = $DB->get_recordset_sql(implode(""""\nUNION\n"""", $sqls). """"ORDER BY capability"""", $params);  {code}    Looking in the tracker I have found an issue that touches on this, but in the end was rejected (MDL-49398).    I'd really appreciate bigger brains than mine looking at this one, as I'm not sure how this part of Moodle hangs together.    I should perhaps add that we make extensive use of meta course enrolments which might be affecting this. I've tried to slim these down, but we still have a lot, and this number will continue to grow because we add new courses for each academic year.    I've also boosted my join_buffers to help ameliorate this situation, but I think the full table scan is killing any benefit here.""",Bug,"Performance,Roles / Access"
330147,"""While reviewing MDL-56139 I've noticed that simplevalues is being used as a cache definition attribute, I think it is a confusion and simpledata should be used instead.""",Bug,Caching
330239,"""MDL-56328 continuation, feel free to ignore if you think we shouldn't fix this in boost.    To resume .<USER>indent-outer with editing mode off don't need a 32px padding, in MDL-56328 the padding was moved to editing mode on, so the move activity icon is not overlapped (that was the original purpose of that padding for what I understand) Not sure if the same solution using px would be the best option in boost case.    Marking it as a bug because MDL-56328 is marked as a bug, I would say this is an improvement.""",Bug,Themes
330522,"""While I was working on some minor changes on our moodle site it seems that I touched something because all of sudden the website-administration and course administration block completely vanished from everywhere altough I am logged in as an administrator. There is nothing that I can do to make them appear again.    I had that problem already once today but fortunately at least at the start page the administration blocks were still visible and could make them reapppear on every site. But now they are gone.    Necessary steps:    # I need urgent help to resolve this problem  # It should not be possible to get into a problem like this, there has to be a control system preventing such problems or a workaround for Administrators that they still find acces to the website adminstration    """,Bug,Admin
330525,"""This came from MDL-29941 and i've always been quite nervous about it. But I think it's time to go, because:    # It's not really as relevant in our scss/less world - it doesn't apply to any core themes  # We can do things like this with stylelint (we could have a grunt cssoptimise command which gives more aggressive optimisation recommendations for example). Then its controlled by the developer and not the potential to introduce bugs (e.g. MDL-33504) especially with new css features  # It was always an experimental 'feature'  # It's a point of confusion with the other css libraries we've got for scss processing and so on  # It's fitting with other big changes happening in 3.2  #  367 additions and 5,881 deletions.""",Task,Themes
330527,"""This summer we upgraded to version 3.1. In 2.9 and previous, when you downloaded all assignment submissions it placed them in one folder with the student's info appended to the file name. We now see (see attached file) that downloading all submissions is creating individual folders within the zip folder for all submissions, appending the student info to that folder instead of the file names. I'm not sure if this was a change in the coding or a problem with how users may be setting up the assignments.""",Bug,Assignment
330573,"""Created an assignment with a grade type of *None*, but I selected *Marking Guide* before. So I'm able to give grades to students, but the system makes it seemed like I never graded them. It still says """"Not graded"""".    # Create an assignment with default param  # In settings, make sure to select *Marking guide*, then select *None* in grade type  # Save  # Grade a student (should be able to grade them with marking guide)  # Save the grade  # Assignment will still say that the student was not graded (which is not true)  # Connect as the graded student  # Check if you're able to see grades (shouldn't be the case)    If a teacher wanted to create a formative assignment, having a marking guide without any grades would make sense, but Moodle doesn't seem to accept this kind of approach...    I was able to reproduce this in 3.1.1 and 3.2 DEV.  """,Bug,"Assignment,Grading methods"
330577,"""If you create future events that display in the calendar (e.g. assignment or quiz deadlines) then disable the <USER>that they relate to, then calls to the calendar function calendar_get_events will still return those events, even though in other parts of the code they will not be shown to users (for example, the activity itself won't be shown on the course page).    If you have a calendar block on the page or visit the main calendar then it'll throw a notice similar to:     PHP Notice:  info_module::is_user_visible called with invalid cmid 6919    Various other bits of code seem to expect only active modules events to be returned though they often throw errors rather than actually display any info.    I've marked this as a minor security issue, as there is the possibility for information leaking unexpectedly, though it's probably quite difficult to stumble across this in normal usage and unlikely to leak anything critical.""",Bug,Web Services
330636,"""Currently fragment callbacks have to reside in lib.php and you could say this is justified as it is technically an api method.  However, since a fragment callback is only something that will be accessed via AJAX it does not make sense for it to be in lib.php as it will be included unnecessarily on every page load.  Instead it would be better if the callback was a public static method in a fragments class - i.e    classes/fragments.php    So if I have a 'get_branding' callback for my component theme_supertheme, the component_callback function in moodlelib.php would call    \theme_supertheme\fragments::get_branding    For backwards compatibility it could also look in lib.php if the method doesn't exist in the namespaced class.    I'm happy to code this improvement up and contribute it if people agree that this is indeed an improvement.   """,Improvement,Libraries
330644,"""During the testing of the MDL-55916 I've got the following error after enabling the maintenance mode and trying to access the log report as student:  {noformat}  Coding problem: $PAGE->context was not set. You may have forgotten to call require_login() or $PAGE->set_context(). The page may not display correctly as a result  line 458 of /lib/pagelib.php: call to debugging()  line 773 of /lib/pagelib.php: call to moodle_page-><USER>get_context()  line 1418 of /lib/weblib.php: call to moodle_page->__get()  line 1164 of /lib/pagelib.php: call to format_string()  line 2943 of /lib/weblib.php: call to moodle_page->set_title()  line 2692 of /lib/moodlelib.php: call to print_maintenance_message()  line 108 of /report/log/index.php: call to require_login()  {noformat}""",Bug,Reports
330672,"""While MDL-52489 can be seen by some as an improvement (since it doesn't rename the files and places each students work in a separate folder) it does significantly increase the amount of work a teacher needs to do to grade work offline when using Moodle.    Prior to MDL-52489 (In version 3.0 or prior) a teacher could download all student work and open it for grading only requiring a 4 clicks:  1. Unpack the ZIP  2. Open the folder  3. Select all files  4. Select File-> Open    Now that each student submission is contained in a separate folder, the number of clicks increases by 2* number of students. (So if you have to open and grade work from a class of 25 students, it now requires 52 clicks instead of the previous 4, a class of 50 would require 102 clicks and so on)    I've received several complaints from my faculty that this """"improvement"""" is creating a significant amount of additional work for them. (Since they each have approximately 100 students, split between 5 groups this change requires 200+ clicks to grade an assignment.)  Is there any chance we can make the old behavior available to teachers either through an assignment setting or a user preference?  """,Bug,Assignment
330703,"""I haven't been able to track down the exact cause, but My PHP error logs are full of entries like this:    PHP Notice:  Undefined offset: 640 in C:\Web\sites\moodle\completion\classes\external.php on line 173    It seems to be specific to courses where the user is accessing it using the official mobile app.  Unfortunately neither the Moodle nor Apache logs are helpful in tracking down the cause beyond this.    From what I've been able to determine about this error (Such as the one posted above) the userid that is triggering the error is 640, and that user is enrolled in 2 courses.  One course has activity completion enabled (Using ONLY manual completion on file resources and labels), The other has activity completion disabled.""",Bug,Web Services
330727,"""It exists an overflow in the text, on Course and category management page when you select a course.     Steps to reproduce:    * Login as admin  * Go to Site administration  * Go to Courses  * Go to Manage courses and categories  * Select a course (to make sure the 3 columns are shown)  * Change your site language to Danish (this causes some of the words to exceed the width of their container and not wrap, as shown in the attached image).    When there are 3 columns (Course categories - Category - Course), the error occurs when the screen is too big (1920x1080), I'm attaching some pictures to see the problem.    Expecting to see the titles at the course (view) not to overflow when the titles are longer.  """,Bug,Themes
330769,"""This is a followup up of MDL-49026, where web-service tokens started being deleted on password reset (as a security measure, specially for mobile users).    There, it was detected that the behavior was sub-optimal for non-mobile services, because they would stop working without notice.    So this is to consider which could be be the best solution for those non-mobile services and implement it. Here there are various alternatives, not mutually exclusive, extracted from the comments in the original issue:    A)  Only reset mobile services.  B)  Notify on change password about the invalidated tokens/services.  C)  Regenerate the token under some conditions.  D) Put tokens on hold instead of deleting them.    Also, related, it's needed to verify what happens with """"user_private_keys"""", if they should also be invalidated, or no... and, of course, the tokens UI needs some love (MDL-53400,  MDL-55003 ...) to be able to manage them better.    That's it, ciao :-)""",Improvement,"Authentication,Web Services"
330780,"""This is a variant( or another part)  of the issue reported in MDL-46721. Although students are now getting the right participant number in email notifications the same is not true for teachers.     When feedback is given to a student for an assignment that has blind marking enabled, the email receipt sent out  to students say something like """" Participant xx has given feedback...""""  where xx is the same number given to the student as part of the assignment. Hence it looks like the student has given themselves feedback which is not true.  To recreate the issue :  Create an assignment with blind marking turned on  As a student, add a file to the assignment  Student should get an email receipt saying they have submitted the assignment  As a teacher who cannot see the name of the student, <USER>the assignment and give the student feedback  - An email is sent out to the student saying that the """"Participant number (which is their number) has given feedback....""""    *I'd appreciate if this issue was not closed as duplicate without prior discussion in the comments section as I strongly feel this isn't a duplicate *.    Thanks  Hittesh""",Bug,Assignment
331135,"""In 3.1, the File API added the ability to convert files, but local unoconv was a requirement.    There are various web services and different applications that can do this conversion work.    I would be really good if the file conversion was handled by a plugin interface, so sites can easily install different conversion services.""",New Feature,"Assignment,Files API"
331149,"""We have a Debian infrastructure with multiple web servers and a dedicated cron server where we are running unoconv as a service. The unoconv test in the admin menu failed consistently, which eventually turned out to be due to the file_exists() call in create_converted_document() returning false when the converted file had actually been created successfully. We worked out that 'touching' the file (and then checking its size rather than its existence) gives a much more reliable result in our case, although I can't offer an explanation as to why this is. However, this may conceivably be the case for some other people's setups too.    -Whilst debugging this we also found a couple of other bugs, including one which results in the test file only needing to be successfully created once. After this Moodle just accesses the stored file and reports success every time (which makes debugging fun :D).- (Fixed in MDL-54647).    Another bug just results in the temp directory never being cleaned up after the testing process completes.    I've attached a patch containing a commit for each fix.""",Bug,Files API
331426,"""I'm not sure why I marked as an improvement MDL-54955 since is clearly a bug affecting existing users, see MOBILE-1692""",Bug,Web Services
331638,"""As an administrator, I cannot see web service tokens that are created by others, including other administrator users.     The admin settings page """"webservicetokens"""" {{$CFG->wwwroot/admin/settings.php?section=webservicetokens}} allows managing tokens that authorise access to <USER> I can create tokens for other users, but I will always see only those tokens that I have created.    I would like to be able to revoke tokens created by others, e.g. in case I detect or get notified of abuse of tokens. In our particular case, however, a former colleague has created tokens for technical users, which I am now unable to change :)""",Bug,Web Services
331908,"""Whilst reviewing MDL-53598, I've found multiple issues with the random glossary block.    These are largely performance-related issues, but some other issues are also present.    At the moment I am combining these into a single issue as I suspect that they will be best solved by a refactor.    h5. Initial setup  # Install another language (e.g. German)  # Enable the multilang filter at all levels  # Create a new glossary  # Add an entry which makes use of multilang:  {code}    <span lang=""""en"""" class=""""multilang"""">A small(ish) horse</span>    <span lang=""""de"""" class=""""multilang"""">ein großes Kaninchen</span>  {code}  # Add an instance of the block to the page and configure it to use this glossary - all other settings default.    h5. Observations  # With the block on the page there are now 5 new DB reads, and 2 new DB writes:  {code}  // In specialization function:  SELECT COUNT('x') FROM <USER>glossary_entries WHERE glossaryid = $1 AND approved = $2 [array ( 0 => '1', 1 => 1, )]  SELECT * FROM <USER>glossary WHERE id = $1 [array ( 0 => '1', )]  SELECT cm.*, m.name, md.name AS modname FROM <USER>course_modules cm JOIN <USER>modules md ON md.id = cm.module JOIN <USER>glossary m ON m.id = cm.instance WHERE m.id = $1 AND md.name = $2 AND cm.course = $3 [array ( 0 => '1', 1 => 'glossary', 2 => '7', )]  SELECT id, concept, definition, definitionformat, definitiontrust FROM <USER>glossary_entries WHERE glossaryid = $1 AND approved = 1 ORDER BY timemodified ASC LIMIT 1 OFFSET 0 [array ( 0 => '1', )]  UPDATE <USER>block_instances SET configdata = $2 WHERE id = $1 [array ( 0 => '45', 1 => 'Tzo4OiJzdGRDbGFzcyI6MTM6e3M6NToidGl0bGUiO3M6MjE6IlJhbmRvbSBnbG9zc2FyeSBlbnRyeSI7czo4OiJnbG9zc2FyeSI7czoxOiIxIjtzOjc6InJlZnJlc2giO2k6MDtzOjQ6InR5cGUiO3M6MToiMCI7czoxMToic2hvd2NvbmNlcHQiO3M6MToiMSI7czo4OiJhZGRlbnRyeSI7czoxNToiQWRkIGEgbmV3IGVudHJ5IjtzOjEyOiJ2aWV3Z2xvc3NhcnkiO3M6MTY6IlZpZXcgYWxsIGVudHJpZXMiO3M6OToiaW52aXNpYmxlIjtzOjE3OiIodG8gYmUgY29udGludWVkKSI7czo4OiJuZXh0dGltZSI7aToxNDY0MTA1NjAwO3M6NToiY2FjaGUiO3M6MTc1OiI8aDM+RG9nSMO8bmQ8L2gzPjxkaXYgY2xhc3M9Im5vLW92ZXJmbG93Ij48c3BhbiBsYW5nPSJlbiIgY2xhc3M9Im11bHRpbGFuZyI+QSBzbWFsbChpc2gpIGhvcnNlPC9zcGFuPg0KDQo8c3BhbiBsYW5nPSJkZSIgY2xhc3M9Im11bHRpbGFuZyI+RWluZSBncsO2c3NlIGthbmluc2NoZW48L3NwYW4+PC9kaXY+IjtzOjE0OiJnbG9iYWxnbG9zc2FyeSI7czoxOiIwIjtzOjg6ImNvdXJzZWlkIjtzOjE6IjciO3M6ODoicHJldmlvdXMiO2k6MTt9', )]    // In get_content function:  SELECT * FROM <USER>glossary WHERE id = $1 [array ( 0 => '1', )]  UPDATE <USER>block_instances SET configdata = $2 WHERE id = $1 [array ( 0 => '45', 1 => 'Tzo4OiJzdGRDbGFzcyI6MTM6e3M6NToidGl0bGUiO3M6MjE6IlJhbmRvbSBnbG9zc2FyeSBlbnRyeSI7czo4OiJnbG9zc2FyeSI7czoxOiIxIjtzOjc6InJlZnJlc2giO2k6MDtzOjQ6InR5cGUiO3M6MToiMCI7czoxMToic2hvd2NvbmNlcHQiO3M6MToiMSI7czo4OiJhZGRlbnRyeSI7czoxNToiQWRkIGEgbmV3IGVudHJ5IjtzOjEyOiJ2aWV3Z2xvc3NhcnkiO3M6MTY6IlZpZXcgYWxsIGVudHJpZXMiO3M6OToiaW52aXNpYmxlIjtzOjE3OiIodG8gYmUgY29udGludWVkKSI7czo4OiJuZXh0dGltZSI7aToxNDY0MTA1NjAwO3M6NToiY2FjaGUiO3M6MTc1OiI8aDM+RG9nSMO8bmQ8L2gzPjxkaXYgY2xhc3M9Im5vLW92ZXJmbG93Ij48c3BhbiBsYW5nPSJlbiIgY2xhc3M9Im11bHRpbGFuZyI+QSBzbWFsbChpc2gpIGhvcnNlPC9zcGFuPg0KDQo8c3BhbiBsYW5nPSJkZSIgY2xhc3M9Im11bHRpbGFuZyI+RWluZSBncsO2c3NlIGthbmluc2NoZW48L3NwYW4+PC9kaXY+IjtzOjE0OiJnbG9iYWxnbG9zc2FyeSI7czoxOiIwIjtzOjg6ImNvdXJzZWlkIjtzOjE6IjciO3M6ODoicHJldmlvdXMiO2k6MTt9', )]  {code}  The specialization function fetches the glossary record from the table, but does not cache it internally, even though it is almost always fetched within the same request.  # We write block configuration whenever the block is viewed. This usually happens twice (once in specialization and once in get_content). This also happens regardless of whether the configuration has changed    h5. Subsequent stuff  # Modify the block configuraiton to set the value of {{Days before a new entry is chosen}} to {{1}}.  # View the page with the block again  # Note the displayed entry  # Open the glossary and modify that entry (e.g. correct a typo)  # Refresh the page with the block again  ## *The original content is still shown*  # Delete the entry altogether  ## *The original content is still shown*  # Change page language to German and Refresh the page with the block again  ## *The original content is still shown in English*  # Modify the configuration and reset the Days value to {{0}}  # Refresh the page with the block again  ## *The original content is still shown in English*    h5. Recommendations  From what I can see, the {{specialization}} function is being misused to pre-fetch some information.  As a result some values are fetched in the {{specialization}} function, and also in {{get_content}}.    A good first step would be to standardise the fetching of configuration and content into a single place and then to remove the specialization function if it is not required.  We should also remove the excessive config writes (modify to write changes only on data change), and switch from {{get_coursemodule_from_instance}} to {{get_fast_modinfo}}.  Following on from this we need to fix the issues with the caching of entries in their filtered and formatted state - the best way of doing this is probably by caching the entry id rather than the end result. It should not be a cache but a current record holder. If caching is desired, MUC should be used to support invalidation.""",Bug,Performance
331986,"""Our current csslint configuration is very permissive - more or less things it detects are real errors like incorrect combinations of styles, invalid properties or real errors.    I would like to see us start enforcing lint-free css in the grunt builds, as a precussor to that we need to fix the lint issues in the codebase""",Epic,Themes
332020,"""After using the built-in WS Test Client in Moodle for testing an issue today, I'd like to propose its removal from core in Moodle 3.2 onwards.    It has served us well, but there are other tools now which are more suited to the task. In addition it only has access to three services now (all the rest were deprecated in 3.1).""",Task,Web Services
332031,"""There are plenty of cases where the PDF annotation tool has no reason to be used, yet with the new grader there is no way to disable it - it always takes a considerable amount of screen real estate, increases loading time, and increases server load.    I'm teaching a course where the students are submitting zipped code projects and we use a marking guide. The new interface takes up such a huge amount of space that we don't use, it makes it hard to use the marking guide. Even if there was a session sticky way to hide it (an X to click, or a way to collapse it), that would be a big usability win for workflows that don't conform to the narrow pattern this seems to be designed for.""",Bug,Assignment
332146,"""When a student views their dashboard, it displays assigned activities for each class with a message that there are quizzes or lessons due, there are scorms that need attention, etc.  If they complete these activities, it still says it.  If you click the message, you can see that you've done it but many (most) students don't click the message.  They just get really upset that it says they're not done (even if that isn't what it says).  It's confusing.    And, to add to the unintuitiveness...it only does this for six activities:  Assignment (fixed), Chat, Forum, Lesson, Quiz and SCORM.  If I'm required to complete a Feedback or Choice activity, it doesn't remind me.  If I'm required to do something in a Glossary, Workshop, Survey or Wiki, it doesn't say anything.     So, that's what it does.  What I would like it to do is drop the message once the requirement is met.  All these activities have the ability to assess whether or not the work is considered done (grade to pass, has a grade, has submitted the requisite # of forum posts, has completed, etc).  It seems reasonable to me that when you have met the expectation, it should stop hounding you.  And, if the site tracks course completions...stop listing the course if they have completed.  Not block them from accessing, just stop displaying it on the dashboard.    It would be great if whatever triggers these messages would recognize third party plugin activities, too, but that's probably pie-in-the-sky.    A recent tracker resolved the issue for Assignment but nothing else in 2.9 and up.  I'd assumed there must be something in the tracker for all the other activities, as I've been seeing people complain about this in the forums for...well, 10 years, but I don't see anything.  """,Improvement,SCORM
332291,"""I came across this issue indirectly because a plugin (Datatables.net) wanted column headings for every table column.    If I use $table = new html_table();  to create a table we can get things started with:  $table->head = array('Unit ID', 'Unit Name', 'Enrolments', 'Participants');  $table->headspan = array(1, 1, 1, 1);    Now I can only have one(1) array of headings for a table, not two(2) even though headspan will allow me to do this:  $table->headspan = array(1, 1, 1, 2);  and allow the fourth and fifth columns combine as per a html colspan attribute.    If in that fourth and fifth column if I wanted 'Participants' as listed there now, I might want under that header ('First Name', 'Surname') as a sub heading as the table sub-column headings.    Currently html_table() doesn't allow that.  It seems funny to allow headspan but not multiple row'd headings.    If possible, it would be good to either:  $table->head = array('Unit ID', 'Unit Name', 'Enrolments', array('Participants','First Name','Surname'));  $table->headspan = array(1, 1, 1, 2);  which fails right now because it's expecting a string, not an array; or Plan B a 2D array (much cleaner and exponentially expandable)  $table->head[0] = array('Unit ID', 'Unit Name', 'Enrolments', 'Participants');  $table->head[1] = array('First Name', 'Surname');  which delivers:  <tr>    <th>Unit ID</th>    <th>Unit Name</th>    <th>Enrolments</th>    <th colspan=""""2"""">Participants</th>  </tr>  <tr>    <th>First Name</th>    <th>Surname</th>  </tr>    Thanks,    Karl.  Moodle v2.7.13  Postgres DB""",Improvement,HTML
332339,"""I get this error:    {code:java}    An error occurred while loading the CSV file: The CSV file is empty    More information about this error  Debug info:  Error code: csvloaderror  Stack trace:        line 488 of /lib/setuplib.php: moodle_exception thrown      line 130 of /admin/tool/uploaduser/index.php: call to print_error()  {code}    Moodle v3.0.3 from Git, On a Debian Stable server, so PHP 5.6.17-0+deb8u1    File /lib/csvlib.class.php    Function load_csv_content has    {code:java}           fseek($fp, 0);  {code}    I replaced this with    {code:java}          fclose($fp);          $fp = fopen($tempfile, 'r');  {code}    And it worked.    Proper fix:    {code:java}  diff --git a/lib/csvlib.class.php b/lib/csvlib.class.php  index f159d5a..ec16372 100644  --- a/lib/csvlib.class.php  +++ b/lib/csvlib.class.php  @@ -121,7 +121,8 @@ class csv_import_reader {               return false;           }           fwrite($fp, $content);  -        fseek($fp, 0);  +        fclose($fp);  +        $fp = fopen($tempfile, 'r');           // Create an array to store the imported data for error checking.           $columns = array();           // str_getcsv doesn't iterate through the csv data properly. It has  {code}    I don't know what other side effects this will have - at least have to check for error on fopen I guess. So I'm not properly recommending this as a patch. Just wanted to comment on possible bug?""",Bug,User management
332378,"""We are currently experiencing issues with uploads (including assignments) on our Moodle and Digital Campus site, this issue has been happening since Tuesday. When users attempt to add a document either by drag and drop of using the file selector the 'wheel' continually spins and it doesn't move beyond this. We have tried troubleshooting the issue and have purged the caches, we have also tried restarting the server. We are using the Essential theme (version 2.8.7+), so i think our next step is to try changing the theme to see if this helps, ideally we would like to avoid doing so as all users are used to the layout. !attachment-name.jpg|thumbnail!    The error we are getting is:  ('moodle-core-notification-ajaxexception' ,function(){return new  M.core.ajaxException(data)}) ;this.fpnode.one('.fp-content').setContent('');  return}else{if(data.msg)scope.print_msg(data.msg,'info');if  Unable to get property 'msg' of undefined or null reference  (args.action!='upload'&&data.allowcaching)scope.cashed_responses[params]  =data;args.callback(id,data,p)}}},arguements:{scope:scope},headers:  {'Content-Type':'application/x-www-form-urlencoded; charset=UTF-8},      What is working:    The dev servers are fine. Was trying to work out if there are any differences...  You can search and grab stuff from some repositories:- youtube, google drive (but only images, I might need to change a setting in my google drive though)  I've just tried swapping the theme on the DC dev and it doesn't look like anything major imploded so I might try a theme switch. I don't think it will help though.  You can still grab things from your recent files.  You can add a label - highlight and add a weblink but you can't use this method to upload a file.     I've tried extending the file size allowed, and the timeout. Obviously no help.   Someone else has posted that there file picker has stopped working. I'm in the middle of replying to her to see what we have in common.     It's like Moodle has stopped talking to the server.   """,Bug,"Themes,Assignment,JavaScript,Repository"
332454,"""I need to give the permission the auto inscription to de course when the user has a particular department on his profile. Or may be another user profile field.  How can i do that. It is possible ? """,Task,Accessibility
332480,"""Since cut-off date was added to the assignment settings (2.4), you can set a date to close the submissions different to the due date. That's great, but my concern is when that cut-off date is disabled, because (many people think it means that submissions are closed on the due date but) I've realized that it actually keeps it opened forever and ever, and the help string doesn't warn about it (""""If set, the assignment will not accept submissions after this date without an extension."""").    It makes no sense to me to keep the submission opened forever (the teacher will have to grade it at some point), so I think that the behaviour should be changed to this:    - When cut-off date is disabled, the cut-off date is the same as the due date (if enabled).    If this change is not accepted I think the help string should be changed to something like this: """"If set, the assignment will not accept submissions after this date without an extension. If disabled submissions will be accepted always and forever"""", and I'd suggest to make it enabled by default (to make responsible the one who disables it).    """,Improvement,Assignment
332535,"""As an administrator I want to be able to enabled assignment quick grading for all of my users by default and let them choose to disable it in order to reduce the time my teacher have to take to enable the feature.""",Improvement,Assignment
332633,"""On the View/Grade all submissions page of assignments. Several clients have reported that specific teachers don't see all their students on that page. What I've seen is that they haven't selected """"All"""" for First name and Last name near the top of the page. Clicking """"All"""" for both (depending on other filters at the bottom), usually fixes the problem.    It seems that if a teacher has filtered on a students' name in one course, the filter is saved as a preference and it will remain that way until the teacher selects to view all. The preference is saved even after the teacher has logged off the site.     """,Bug,Assignment
332637,"""Currently you cannot tab to the block show/hide buttons using keyboard; you can tab to the 'dock' button but not the show/hide one.    The code for this was implemented in MDL-27197 but broken in MDL-49144 due to a case mistake in a property name (tabindex instead of tabIndex).    As an additional bug, after fixing this part, the focus does not work properly - when you activate the button, focus disappears. It should focus the corresponding button i.e. if you selected 'hide' then the focus should be left on the 'show' button rather than on the now-disappeared hide button.    I'll submit a patch that fixes these issues.""",Bug,Accessibility
332749,"""There are a few less than optimal things about the way the less compiled css files are handled with git. I don't want to open a debate about whether the derived files should even be in git at all because that ship has already sailed, I want to just make dealing with that reality better.    Some issues with the current set up:  * If you are making a patch and checkin both less and css then the diffs are horrible because the combined and minified css or js is all on a single line    Solution:  * <USER>the various derived files as not to be diffed in .gitattributes   """,Improvement,Themes
332797,"""On /user/index.php?id=<COURSE-ID>, there's the participants page for a course. By default, this page does only show the first 20 users and has page switchers. There is also a """"Show all"""" link.    Usability tests with our teachers have shown that the positions of the page switchers (above and below the user list) and the show all link (at the very end of the page) do not correspond with each other and that the """"show all"""" link is not recognized as a feature to control the user list at first sight.    While this page somehow needs a complete overhaul, as a quick solution for the time being I would like to see the """"Show all"""" link moved directly beneath the page switchers (see screenshot).""",Improvement,"Course,User management,Usability"
332868,"""I would like there to be some means in the admin UI for deciding to disable SSL validation in IMAP features, related to both IMAP as authentication and for incoming mail.  This will allow for the use of otherwise untrusted certificates (self-signed, etc.)""",Improvement,"Admin,Authentication"
332889,"""Our students keep complaining that they are not informed centrally when new materials become available in their Moodle courses. They are missing an information on their course overview block on MyMoodle.    So we investigated this:    The standard course overview block provides news from certain course modules to the student.  These news are fetched with the <<USER>_print_overview() function which has to be implemented by the module plugin. These modules currently provide this function:  {code}  ./<USER>chat/lib.php:function chat_print_overview($courses, &$htmlarray) {  ./<USER>assign/lib.php:function assign_print_overview($courses, &$htmlarray) {  ./<USER>quiz/lib.php:function quiz_print_overview($courses, &$htmlarray) {  ./<USER>checklist/lib.php:function checklist_print_overview($courses, &$htmlarray) {  ./<USER>forum/lib.php:function forum_print_overview($courses,&$htmlarray) {  ./<USER>scorm/lib.php:function scorm_print_overview($courses, &$htmlarray) {  ./<USER>lesson/lib.php:function lesson_print_overview($courses, &$htmlarray) {  {code}    There is also the recent activity block which can be placed within a course. This block uses a similar mechanism to fetch its news, it looks for the <<USER>_print_recent_activity function within the modules. These modules currently provide this function:  {code}  ./<USER>chat/lib.php:function chat_print_recent_activity($course, $viewfullnames, $timestart) {  ./<USER>assign/lib.php:function assign_print_recent_activity($course, $viewfullnames, $timestart) {  ./<USER>wiki/lib.php:function wiki_print_recent_activity($course, $viewfullnames, $timestart) {  ./<USER>book/lib.php:function book_print_recent_activity($course, $viewfullnames, $timestart) {  ./<USER>survey/lib.php:function survey_print_recent_activity($course, $viewfullnames, $timestart) {  ./<USER>forum/lib.php:function forum_print_recent_activity($course, $viewfullnames, $timestart) {  ./<USER>lti/lib.php:function lti_print_recent_activity($course, $isteacher, $timestart) {  ./<USER>glossary/lib.php:function glossary_print_recent_activity($course, $viewfullnames, $timestart) {  ./<USER>workshop/lib.php:function workshop_print_recent_activity($course, $viewfullnames, $timestart) {  {code}    These are almost the same modules as the ones with the _print_overview function, but _print_recent_activity seems to be slightly better supported.    Furthermore, block_recent_activity also prints informations if a resource or folder has been added / removed from the course. This is not done with a _print_recent_activity function, but with the get_structural_changes() function within the block which gets the information about all course structure changes directly from the DB / the Logs.     Unfortunately, block_course_overview does not leverage get_structural_changes() or anything similar. So, our students can currently only see if a file or folder was added / removed if they look directly at the recent_activity block within the course.    Now, my thought was: Let's add a _print_overview function to <USER>resource and <USER>folder to have new / changed resources and folders shown in the course overview block. There is already a similar functionality for <USER>forum which shows new forum posts in the course overview block until the user visits the course for the next time.    So, we propose to:  1. Add resource_print_overview() function to <USER>resource:   -> The function should return a news item if a resource was added to the course, was updated or has become visible (due to a teacher changing visibility status or due to a conditional activity)  -> This function could re-use the mechanisms which are already there with get_structural_changes()  -> The news item should be shown until the user visits the course for the next time  -> The function should provide a direct link to the resource within the news message    2. Add folder_print_overview() function to <USER>folder:   -> The function should return a news item if a folder was added to the course, was updated or has become visible (due to a teacher changing visibility status or due to a conditional activity)  -> This function could re-use the mechanisms which are already there with get_structural_changes()  -> The news item should be shown until the user visits the course for the next time  -> The function should provide a direct link to the folder within the news message    3. Optionally, we could add two settings for administrators:  -> A setting which would let administrators enable / disable this new course news source (to prevent course overview clutter in installations which don't need this feature)  -> A setting which would let administrators define a threshold (for example, 30 days) to hide the course news item even if the user did not visit the course in the meantime (to prevent overly long course overview news)    [~<USER>:  I would be happy to hear your opinion about this proposal. If it is likely to be accepted for Moodle core, we will specify the implementation details here before starting with the implementation.    Thanks,  <USER>",New Feature,Course
333008,"""The assignment file feedback component has an option that allows an uploaded feedback file to be applied automatically to all other members of the same group when group (team) submission is enabled. Currently this option is only available for individual but not batch uploaded feedback.    This patch extends the existing functionality to allow batch uploaded (zipped) feedback to be applied to all members of a group, in the same way.    I'm submitting it here in case anyone else finds it useful.""",Improvement,Assignment
333098,"""There are some problems, and inflexible bits of the new fragments javascript API.    1. It does not notify the filters about inserted content  2. It does not return a promise so there is no way to chain actions after the fragment is loaded  3. It does not allow any control over how the dom nodes are injected (fade in etc)  4. It is not using the safe YUI instance from core/yui  5. The javascriptnodeidentifier argument is unnessesary  6. It does not need to remove event listeners from script nodes.   7. It does not pass the context to the callback so it can be verified.  8. The arguments are passed from javascript as a named array - but through php as a flat array which means the order is undetermined.     A better API would be to change fragmentAppend to loadFragment which returns a promise. The promise will be resolved with the html and js variables and the caller is responsible for adding them to the page. They can re-use the templates.replaceNodeContents function to add the nodes - which will correctly notify the filters etc.     There is some code in there to cleanup YUI event listeners - I'll check the YUI source to see if that is truely needed - and if so - we can add it to the templates.replaceNode and templates.replaceNodeContents functions.     """,Improvement,JavaScript
333133,"""I know I can upload a file, but I want to add this file as a standard feedback file (blueprint) and it will save a lot of time to have this uploaded in the system when I produse the assignment.""",Improvement,Assignment
333260,"""Within a course, completion tracking is turned on. A resource in the zero topic block is set for manual completion. All sections within the course are set to become available once the resource is marked complete. This is set up in the course and works as expected.      When the course is used as an import (even when everything is selected) we have inconsistent results; some of the time the access restrictions are imported over and some of the time they are not.  (I would say a majority of the times they are not.) We have seen better success pulling them over on a backup/restore, but this too is still inconsistent.     After scouring the tracker, documentation and forums it's been difficult to identify the expected behavior. Could someone verify the conditions where the restrict access settings should import consistently? """,Bug,Backup
333446,"""If a collapsible region contains one or more Atto editors, collapsing the region and expanding it again will result in the content being truncated.  This is caused by a combination of the collapsible region code only checking the height of the content when initialised (as opposed to each time it's triggered), and the initialisation happening before Atto has loaded; the Atto editor is taller than the textarea which it replaces (at least in some cases - I'm not sure if editors with specific heights will match the textarea height), but the collapse/expand function doesn't account for that change.    Setting the """"$ondomready"""" parameter of the $PAGE->requires->js_init_call() call in print_collapsible_region_start() to true results in the collapsible regions being initialised late enough that Atto has had a chance to initialise, and therefore seems to be a quick/simple way to work around this - though perhaps if/when the collapsible region code is rewritten as an AMD module, this issue can be avoided in a different way.""",Bug,JavaScript
333485,"""in auth/<USER>auth.php Moodle tries to refer $this->config->language to phpCAS without checking to see if the variable is defined first.    We don't configure a language setting as the English default works for us.    Sometimes (and I'm not sure why only sometimes) phpCAS complains with an error message indicating that it is not appropriate to declare a language of null.    I fixed this by adding an if isset block around this declaration.    so in my install the appropriate block in auth.php looks like:    {code:title=auth/<USER>auth.php:173}          // Force CAS authentication (if needed).          if (!phpCAS::isAuthenticated()) {              if (isset($this->config->language)) {                  phpCAS::setLang($this->config->language);              }              phpCAS::forceAuthentication();          }  {code}    I run several moodle sites and I've only had two sites break, but I've since deployed this change to all of them so that it doesn't become a problem in the future.    I checked and this seem to be a problem in all moodle versions.""",Bug,Authentication
333495,"""Using the blog menu block inside of an activity (such as assignment) will generate a debugging notice when any user tries to create a blog associated with that activity.    Notice: Coding problem: unsupported modification of PAGE->context from 30 to 70        line 971 of /lib/pagelib.php: call to debugging()      line 1016 of /lib/pagelib.php: call to moodle_page->set_context()      line 683 of /blog/lib.php: call to moodle_page->set_cm()      line 93 of /blog/edit.php: call to blog_get_headers()    in /Applications/MAMP/htdocs/moodle29/lib/weblib.php on line 2945    I'll attach a behat test that will replicate the issue.""",Bug,Blog
333542,"""EXPECTED RESULT  Editing his profile, an user could add/update/remove tags in the section """"Interests"""" using any language pack of his choice.    ACTUAL RESULT  When using others language packs then English (en) or Spanish (es), the div.fcontainer within fieldset.id_moodle_interests lacks the YUI id used to trigger the field visibility when the section is expanded.    STEPS TO REPRODUCE  1. Change your profile language to English (en) or Español (es)  2. Go to Dashboard > Preferences > User account > Edit profile  3. Expand the section """"Interests"""" and you are able to insert a tag.  4. Remove all existing interests and update your profile  *5. You are able to edit your interests in this language pack*  6. Change your profile language to Português - Brasil (pt_br)  7. Perform steps 2 and 3  *8. You are NOT able to edit your interests in this language pack*    OTHER  I've found that when using Português - Brasil (pt_br) the """"div.fitemtitle label"""" has a incorrect """"for"""" attribute; and """"div.fcontainer.clearfix"""" completely lacks the YUI id.""",Bug,Forms Library
333782,"""I had a report of problem loading the instance configuration page for the Progress Bar block when there are a high number of activities in a course (CONTRIB-5958).    The configuration page includes a few form elements for each activity in the course and there are dependencies between the elements. When there are several hundred activities, the page generation can take up to minute and rendering the page can take several minutes, that's if the page loads at all.    I was able to narrow the efficiency problem to the date selector form elements. The culprit seems to be the JS (YUI2) pop-up calendar that is accessed from the date selector icon. When this icon/pop-up is removed, the block instance configuration page for a course with ~500 activities loads in a reasonable amount of time.    The ideal solution would be to track down the inneffiency within the JS and make this better, but I would settle for an option on the date selector that allows control over whether this icon/pop-up is displayed.""",Bug,Forms Library
333796,"""I'm seeing mixed messages about whether we're intending to backport PHP7 fixes to stable branches or not.     I think we all can agree that PHP7 has a different sort of motivation than a usual php release, just look at the fact multiple of our partners have already posted marketing articles about it.    I would propose the following:    * We try to fix all compatibility issues with the most simple and straight forward fixes possible and discourage major refactoring simply for the goal of achieving PHP7 compatibility. The intent would be to backport to supported branches  * There might be some changes which developers/integrators do not deem as safe to make on stable branches - thats OK. There is still benefit in having all the rest of the simple fixes backported - partners (or HQ for our projects) might decide to take that risk and do those risky backports, and we might do it in a backporkport request later.    What I don't really want to see us doing is avoiding straight forward fixes because of policy or encouraging people to let loose with the refactoring which will make the solution to the problems take a different approach.    Please post the outcome of this on the general developer forum.""",Bug,Policy
333979,"""Teachers can't type in Atto Editor accents marks characters like á, é, í, when they are grading and previously they download the submitted file from the student. It doesn't happens with TinyMCE.    Steps to reproduce:    * The editor must be """"atto""""  * As a teacher create an assignment with file submission.  * As a student send a pdf file to be graded  * As a teacher entre on """"View/grade all submissions"""" and click pencil to grade the student  * As a techaer verify you can type á in the Feedback Comment  * As a teacher click to download the file submission  * As a teacher verify now YOU CAN NOT type á in the Feedback Comment    The teacher must reload the page and now he can type diacritic marks.    The problem does NOT HAPPEN in:    Ubuntu 14.04 with Firefox and TinyMCE editor  Ubuntu 14.04 with Chromium 45 and Atto Editor  Windows XP with Firefox 40.03 (same version in Ubuntu not working)    Maybe the problem is in Ubuntu or in Firefox for linux, but I would need some feedback before reporting them this BUG.""",Bug,HTML
334041,"""If a student submits a file before the due date it is indicated as """"submitted early"""". If the student then renames the file and saves the submission after the due date and before the close date the submission is indicated as being """"past due"""".    I agree the current behavior is correct because the file name is part of the file repository hashing system in Moodle. Moodle simply considers the renamed file as a new file hence the submission has been changed after the due date. So it is """"past due"""", but from a student's point of view I understand the idea that doing something like renaming a file isn't really changing the files content.  The term """"past due"""" is an upsetting one to students. So I understand why they may be upset after an """"on time"""" assignment is suddenly marked as """"past due"""" after you do something like rename the file. Especially if the Teacher asked the student to rename the file to follow some kind of convention after is was submitted on time.    That is why I'd like to propose a warning to the student when they save an edit to a submission after the due date and before the close date.""",Improvement,Assignment
334269,"""I've been fixing some bugs recently where the singlebutton was not coded for RTL languages. This lead me to find a whole plethora of CSS/LESS in odd places such as admin.less, that were adding a bottom margin to the 'singlebutton', which already has a bottom margin set in buttons.less    Why are developers doing this?  It's crazy...  """,Bug,Themes
334288,"""10-20 UPDATE:  I checked to make sure that our db has both the """"backup logs"""" and """"task scheduled"""" tables that cron says it cannot find in the error logged below, and we do have both tables; named """"mood_backup_logs"""" and """"mood_task_scheduled"""" respectively.    Also I've added a zip file of all manual cron logs I ran since the day this error occurred.      ----      10-19 UPDATE:  Problem was not fixed - error reappeared again today when again running cron manually on same course OM2112. Removing the scorm file again fixed the problem, but again re-uploading the same file with a new name appeared to have no affect on cron (which was run again manually after upload), but this is the same thing I did on 10-15 when the error seemed to be fixed, and then error returned today. Only difference between 10-15 and today, was that the scorm file was viewed and had completion tracking on for activity and course. Course completion status auto updates on our site appear to take upwards of 48hrs on non-quiz scorm files, so for this reason I'm wondering if it is a problem with completion tracking on this scorm file. Scorm file was made in Captivate 8, and includes a short video that had video compression flv used on it. Any advice would be appreciated. If you need to see the scorm file, please contact me because due to company's privacy I cannot post it online.    Here is the cron log for 10-19 with fatal error reappearing:    Backing up OM2112...... used 2801 dbqueries... used 69.833654880524 secondsScheduled task failed: Automated backups,Table """"backup_logs"""" does not existBacktrace:* line 72 of /backup/util/loggers/database_logger.class.php: call to mysqli_native_moodle_database->insert_record()* line 63 of /backup/util/loggers/database_logger.class.php: call to database_logger->insert_log_record()* line 94 of /backup/util/loggers/base_logger.class.php: call to database_logger->action()* line 100 of /backup/util/loggers/base_logger.class.php: call to base_logger->process()* line 100 of /backup/util/loggers/base_logger.class.php: call to base_logger->process()* line 197 of /backup/util/helper/backup_helper.class.php: call to base_logger->process()* line 83 of /backup/controller/base_controller.class.php: call to backup_helper::log()* line 414 of /backup/util/helper/backup_cron_helper.class.php: call to base_controller->log()* line 199 of /backup/util/helper/backup_cron_helper.class.php: call to backup_cron_automated_helper::launch_automated_backup()* line 50 of /lib/classes/task/automated_backup_task.php: call to backup_cron_automated_helper::run_automated_backup()* line 74 of /lib/cronlib.php: call to core\task\automated_backup_task->execute()* line 81 of /admin/cron.php: call to cron_run()!!! Table """"task_scheduled"""" does not exist !!!Debug info: Error code: ddltablenotexistStack trace: * line 585 of /lib/dml/moodle_database.php: dml_exception thrown* line 1428 of /lib/dml/moodle_database.php: call to moodle_database->where_clause()* line 590 of /lib/classes/task/manager.php: call to moodle_database->get_record()* line 102 of /lib/cronlib.php: call to core\task\manager::scheduled_task_failed()* line 81 of /admin/cron.php: call to cron_run()Fatal error: Uncaught exception 'dml_read_exception' with message 'Error reading from database' in /public_html/lib/dml/moodle_database.php:443Stack trace:#0 /public_html/lib/dml/mysqli_native_moodle_database.php(526): moodle_database->query_end(false)#1 /public_html/lib/ddl/sql_generator.php(230): mysqli_native_moodle_database->get_tables()#2 /public_html/lib/ddl/database_manager.php(99): sql_generator->table_exists(Object(xmldb_table))#3 /public_html/lib/ddl/database_manager.php(318): database_manager->table_exists(Object(xmldb_table))#4 /public_html/lib/dml/moodle_temptables.php(140): database_manager->drop_table(Object(xmldb_table))#5 /public_html/lib/dml/moodle_database.php(370): moodle_temptables->dispose()#6 /public_html/lib/dml/mysqli_native_moodle_database.php(481): moodle_database->dispose()#7 /public_html/lib/dml/moodle_database.php(152): mysqli_native in /public_html/lib/dml/moodle_database.php on line 443  ----    10-15 UPDATE:  I was able to fix the cron error myself I think. For some reason, deleting the scorm file that had been recently added to the course fixed the problem. The only difference between that scorm file and all the other ones we use is that it had video compression used on a short intro video. If you know of a common problem with video compression in scorms, please let me know so I can avoid it.     ----    I ran cron manually through browser on our development site on 10-9 manually, no errors, I ran it again today 10-15 manually and got the fatal error below and cron stopped running. No manual edits have been made to the db or the files mentioned. (Note: OM2122 is a course name that recently had 1 Scorm file added to it, nothing else. Also I removed from this error log everything before """"/public_html"""" in the paths just for privacy). I attached all files mentioned in the stack trace and error. Hope someone can help some with this. Let me know if you need more info.    This was the cron error:    Backing up OM2112...  ... used 2784 dbqueries  ... used 68.280972957611 seconds  Scheduled task failed: Automated backups,Table """"backup_logs"""" does not exist  !!! Table """"task_scheduled"""" does not exist !!!    Fatal error: Uncaught exception 'dml_read_exception' with message 'Error reading from database' in /public_html/lib/dml/moodle_database.php:443  Stack trace:  #0 /public_html/lib/dml/mysqli_native_moodle_database.php(526): moodle_database->query_end(false)  #1 /public_html/lib/ddl/sql_generator.php(230): mysqli_native_moodle_database->get_tables()  #2 /public_html/lib/ddl/database_manager.php(99): sql_generator->table_exists(Object(xmldb_table))  #3 /public_html/lib/ddl/database_manager.php(318): database_manager->table_exists(Object(xmldb_table))  #4 /public_html/lib/dml/moodle_temptables.php(140): database_manager->drop_table(Object(xmldb_table))  #5 /public_html/lib/dml/moodle_database.php(370): moodle_temptables->dispose()  #6 /public_html/lib/dml/mysqli_native_moodle_database.php(481): moodle_database->dispose()  #7 /public_html/lib/dml/moodle_database.php(152): mysqli_native in /public_html/lib/dml/moodle_database.php on line 443  """,Bug,"Backup,SCORM,Database SQL/XMLDB"
334298,"""I've tried to link my backpack on a website, then on Moodle.org (where it was already linked).    After logging in with my Persona I get:    {quote}  You could not be connected to an external backpack for the following reason: The data return from the backpack was invalid.  {quote}""",Bug,Badges
334354,"""Currently, if you want to debug smtp mail sending, you need to enable the debugsmtp setting.    However, this simply makes phpmailer echo it's debug messages.    To actually be able to read those error messages, you need to enable """"display debug messages"""" and set the """"debug messages"""" setting to NORMAL or higher, so that the page does not immediately redirect after the messages are shown.    Now, ideally, we should be able to log those messages someplace instead of seeing them on the screen.  For example when a problem is seen on a production server, that can not be reproduced on a test server.  phpmailer has a """"Debugoutput"""" variable, which allows specifying what happens with it's debug messages.  The default value is """"echo"""".  However, it can also be set to """"error_log"""", in which case it will use php's error log function.    Shouldn't Moodle expose this functionality?  I'd imagine it working like this:    {code}  if ($CFG->debugsmtp) {    if (empty($CFG->debugdisplay)) {      $mailer->Debugoutput = """"error_log"""";    }  }  {code}""",Improvement,Libraries
334588,"""If you're using Oracle as your back-end database and have more than 999 user accounts that need processed in one run (after a mass account maintenance, etc.,) the process will make it to    $remove_users = $DB->get_records_sql($sql, array('auth'=>$this->authtype));    and then will just die (no errors, etc.)    I tracked this down to the fact that we just purged a TON of inactive accounts from our system (~30,000,) after which point our Moodle user syncs stopped functioning.    I found that if you add """"and rownum < 999"""" to the affected sql (in our case, using LDAP for the backend, in auth/ldap/auth.php,) it will allow the code to continue:  {noformat}          } else if ($this->config->removeuser == AUTH_REMOVEUSER_SUSPEND) {                $sql = """"SELECT u.*                        FROM {user} u                   LEFT JOIN {tmp_extuser} e ON (u.username = e.username AND u.mnethostid = e.mnethostid)                       WHERE u.auth = :auth                             AND u.deleted = 0                             AND u.suspended = 0                             AND e.username IS NULL                             and rownum < 999"""";  {noformat}    But that's obviously not ideal because the reset of the code sections continue to have problems. So, you basically have to comment out the remaining code, run the process manually howevermany times it takes for 999 accounts to be processed at a time to clean up all of the accounts that need touched, and then you can uncomment the additional sections.    It looks like in order for this to function correctly, the user sync process needs to get a count of how many accounts need processed, and then loop through it a chunk at a time.    I'm going to label this as a minor security issue, because when the process dies from this error, it can't automatically disable the accounts of affected users, so they remain active until someone figures out what's going on.""",Bug,User management
334608,"""Hi, I'm Nghia.    I'm starting to translate Moodle language pack after a long holiday, but I realize that there was a really slow annoyed delay when I started to type text in AMOS translation box. Any box caused the same!  I think there was something got along inside whenever I type.  Please check it, now I have to type the translation in a text editor and copy back to the box.  I typed in my browser: Google Chrome 45, Firfox 40.    Best regards,  Nghia Huynh""",Improvement,Language
334633,"""Someone was having a hard time tracking down why an authentication plugin kept being disabled. There currently does not seem to be any type of logging of this activity. It might be nice to either log such activity as an event; however, since it does affect a change to the <USER>config table - at least in the case of an authentication plugin, I wondered if it would be a good idea to have it show up in the configlog report.     To accomplish this, at least for the authentication plugins, I think it may be as simple as replacing the set_config in with config_write and then allowing the config_write call set_config. It would be ideal however if we checked to make sure that all plugins that are being enabled - at least the ones that modify the <USER>config table - are being recorded. Perhaps those that do not impact the <USER>config table could be logged. I've not thought this through for each of the plugin types but wanted to create this issue in response to CONTRIB-5906.  """,Improvement,Admin
334757,"""Hi,    while looking @ MDL-50919, it was detected that tablelib blindly creates a table with $perpage rows, no matter the total is smaller or any other consideration.    So if you call to {{pagesize(50000, 1}}, it still creates a table with 49999 hidden rows.    As a temporal fix, in the linked issue I've applied a quick fix in the caller, only for master:    {code}  -            $this->pagesize($pagesize, $total);  +            $this->pagesize(min($pagesize, $total), $total);  {code}    As references, looking in the tracker about information justifying current/weird behavior, I've found MDL-41534, but not much else.    So this is about to consider if we could always """"optimize"""" the number of rows created within tablelib or that's not possible (for some unknown reason) and it's a caller responsibility to adjust it.    Also, note that this behavior is 100% reproducible in 28 and 29 manage tags page... and in any other page using tablelib and having a big """"SHOW_ALL_PAGE_SIZE"""" (or similar) defined.    Ciao :-)""",Bug,Libraries
334760,"""Hello, it would be great to have a """"read-only"""" capability in moodle. The capability that if chosen turns every role to read only. By """"read only role"""" i mean that the role can see anything in the moodle site (according to other permissions), but not to save/edit/change any data. This permission is different from GUEST role, because guest alowed to see mainly the course content. And the read-only role (if it based on the maneger's role for example, or a teacher) would be able to see also al the settings pages, such as course settings or activity/resourse settings, grades, users info etc (but not to change anything in the course).The main purpose of such role is for technical support. For example if there is a  tech-support team that i want them to be able to SEE all the user information throughout the site, course settings etc., in order to analize and understand the problems. But i don't want them to change anything (by mistake). Those tech-supporters will then give exact instructions to teachers/course managers what has to be changed to solve the issues. I saw """"read-only"""" permissions in other systems, not in moodle. I'ts a very usefull permission that can be used in many ways. In one system i worked with, the user with """"read only"""" permission was alowed to work with the system and see all the information, but if he tried to change/save data a pop-up message showed up: """"you are currently in read-only mode, you are not allowed to save any data.."""". There may be some flexibility with the message/style, but generally this is the idea. Thanks for considering this improvement  Inna """,Improvement,Roles / Access
334785,"""During the usability study, participants created links to YouTube videos, but they did not consider options of how the video should be displayed.    Based on the URL being used, Moodle could check whether that site can use object embed tags.    The user could be offered advice like """"It looks like you're trying to add a video resource. How would you like to display that?""""    Or … if we detect a video resource should be used, um, just switch it to a video resource. That was easy. (I'm not sure that is a good idea.)    This was a blocker for 1/6 participants.""",Improvement,Usability
334798,"""I get the above error when I try to update a folder with multiple subfolders and files.  I can download all of the files so I know they exist.  I have a number of files  that I need to update and cannot.  I am logged in as a program administrator and I have permissions to access the files as well as read and write permissions.  Where do I need to look to correct this issue?""",Bug,Admin
334813,"""During the usability study, participants were asked to change the section name.    Attempting to edit a topic name led to confusion and frustration. """"I want to edit the title of this section, by clicking on it.""""    The section name should be editable in a way that is consistent with activities and resources on the same page.  * Make that click-able with a pencil icon.  * Move the position of the cog to the right-hand side, and replace cog with """"""""Edit"""""""" text. (Although, is there anything else to go into that menu except the edit? Perhaps some of the controls to the right can also be put into this edit menu.)    If you hold over the icon, it says """"Edit summary"""" and clicking this leads to a page titled """"Summary of XXX"""", but there is also the potential to edit the title and restrict access, so instead of """"Edit Summary"""" it should probably be """"Edit settings"""" to be consistent with activities and resources.     If the section name is removed, it should automatically revert to the default name.    The presence of the word """"Section"""" should be generally checked.    This was a significant blocker for 2/6 participants and a lesser impediment for 2/6 more.""",Improvement,"Course,Usability"
334865,"""Create an assignment with all default settings (file submission)  Create a student, submit a file  As an instructor go to the assignment. You see 1 submission """"needs grading""""  Click on view all submission. Select filter """"requires grading"""". You see the student submission.  Click on the grade icon in the grade column.  On the new page, download the file (you probably don't need to download to reproduce the issue but it is what I did so I mention it), then without doing anything else, press """"save changes""""    => the submission is not listed anymore in """"requires grading"""" (<USER>assign/view.php?id=4&action=grading). However the submission is still marked as """"needs grading"""" (<USER>assign/view.php?id=4).    Expected result: as no changes were done I would have thought that the grading page would forbid to save changes. But if for a reason we need to be able to press save changes then the submission should still appear in """"requires grading""""""",Bug,Assignment
334875,"""I want to force the use of png-Icons instead of the svg-icons and therefore set $CFG->svgicons to the boolean false in my config.php. This didn't work as expected, as all values of $CFG will be casted to string through the call to """"initialise_cfg()"""" and its call to """"get_config('core')"""" in setup.php, while the method """"theme_config::use_svg_icons()"""" expects $CFG->svgicons to be of boolean type.    The bug lies in """"use_svg_icons()"""" as the global policy seems to be that every configuration value will be of type string. But it still made me doubt myself for a moment or two when I put a value of one type in and it magically transformed into a value of another type. ;)     I only checked for version 2.9.1 but I suspect previous versions will also be affected.""",Bug,Themes
334883,"""Our current namespace policy is slightly dangerous, especially when you consider that we use lots of third-party libraries. There is a potential for namespace collision because we _do not_ include the vendor (Moodle).    I'd like to propose that we add support for 'Moodle' as a top-level namespace.  Everything else will work as before, but for items in the Moodle namespace, we know that they belong to Moodle.    For example, rather than having a namespace of:  {code}  core\log  {code}  This should really be:  {code}  Moodle\core\log  {code}    In doing so we can ensure that, unless some third-party author is an a-hole, we avoid collisions in our code.    Ideally this improvement should be backported to stables, and should also support addition of the 'Moodle\' prefix when it was not specified. Longer term, non-use of it should be deprecated (3.4/3.5 possible).""",Improvement,Policy
334914,"""The `bootstrapbase` theme was added in Moodle 2.5 (June 2013 = 2.5 years from the 3.0 release), and was made default in Moodle 2.7 (June 2014 = 1.5 years from the 3.0 release).    Since then, the `base` theme has been less and less loved, and is gradually becoming technically expensive to maintain: It requires additional development time, especially for new JavaScript-based features; it requires additional testing time each week, and with various issues surfacing to clean up bits of the theme's CSS (MDL-50990 for example), it also requires dedicated integration time.    In addition, the base theme uses old practices, which in turn rely upon deprecated libraries which we wish to remove (cssreset from YUI).    I'd like to deprecate the `base` theme as of Moodle 3.0 and remove it as per the standard deprecation policy in Moodle 3.2.    Steps required to deprecate the base theme:  # raise a discussion regarding this change on moodle.org's theme's forum;  # add some warning that the `base` theme is deprecated when it is used as a theme parent;  # update the theme upgrade.txt.    When it is removed in Moodle 3.2, it can be moved to the plugins repository as usual.    As the `canvas` theme also depends upon base, this will also need to be deprecated and removed in the same manner.    Note: None of the themes marked as supporting Moodle 2.9 in the Moodle plugins directory use either base, or canvas.""",Improvement,"Themes,Policy"
335120,"""Hello,  I signed up on the Moodle website thinking it was going to enable me become an administrator but it didn't. Please advise on how I can access the website, first as an administrator, then a teacher. ( I've spent 6 long early morning hours learning how to design courses on moodle only to log in and be stuck with a basic profile).   Please help.  """,Bug,Admin
335344,"""MDL-49684 revamped timezone support (yay). According to the commit message """"phpunit has Australia/Perth as default timezone."""" I don't think this is working. Starting with 2.9 I'm getting 1800+ warnings like this one:    {quote}1) qtype_numerical_attempt_upgrader_test::test_numerical_deferredfeedback_history620  Warning: unexpected change of locale    /var/www/moodle/htdocs/lib/phpunit/classes/util.php:272  /var/www/moodle/htdocs/lib/phpunit/classes/advanced_testcase.php:125    To re-run:   vendor/bin/phpunit qtype_numerical_attempt_upgrader_test question/type/numerical/tests/upgradelibnewqe_test.php{quote}    I'm seeing this in instances where Moodle and PHPUnit have been installed from the command line. The locale on these systems is en_US.UTF-8.""",Bug,Unit tests
335773,"""What would be a helpful addition would be on the Plugins Check page, where you upgrade plugins, to have a series of checkboxes so that instead of being forced to upgrade everything that wants to be upgraded, you can choose to only upgrade certain things.    As a developer working with Moodle in git, and having multiple branches I'm always working on, I often find that because I've worked on a block in one branch and updated the version, then switching to other branches it complains about a Higher Version already being installed, so I have to constantly alter version numbers, which in the long run can lead to some branches missing out on upgrades they were supposed to have because the version number has had to be changed.    So if we could just untick certain plugins from being upgraded, that would be helpful.""",New Feature,Database SQL/XMLDB
335839,"""I've revised this issue a little from the original description:    Steps to reproduce:  # Login as an admin and student in two different browser windows  # As admin edit the student's profile - change the firstname, to add a '1' to the end of it  # As the student go to profile    Expected results:  # The users firstname should be the same in both the user menu and the profile pages    Actual results:  # The user menu ($USER) displays out of date information    This is a fairly niche issue, however it might have more serious consequences if $USER gets too far out of date. Note that we have manual workarounds for this in user/edit.php for the currently logged in user.    ----  Original description:      As seen in MDL-50162, when a user edits their own profile/properties we need to manually update the $USER object which is stored in the session. Failing to do so appears as if the value wasn't changed.    This can also potentially be a security concern as an admin that would update a field from another use who's currently logged in will not see the change happening.    I suggest we either:  # Invalid the cache in the session  # Implement auto update in _user_update_user_""",Bug,User management
336032,"""On /user/profile.php?id=<USER-ID>, there is a list of courses which the user is enrolled in. This list is divided by commas and is sorted in a way which can't be recognized at first sight. Furthermore, this list is limited to some n courses, a user has to click the """"..."""" link to see the complete list.     This list style is not really useful when you want to get a quick overview which courses a user is enrolled in.    I would like to propose to:  - change this list to a _real_ list with <ul> tags  - sort it alphabetically because this is the standard sorting a user is accustomed to parse quickly  - show all courses at the very beginning""",Improvement,User management
336033,"""On /user/index.php?id=<COURSE-ID>, there's the participants page for a course. By default, this page does only show the first 20 users and has a page switcher.    Our teachers have reported to us that they use this page to send messages to their students. If a teacher goes to this page, clicks on """"Select all"""" and then """"With selected users... -> Send a message"""", he will only send a message to the first 20 users!! To send a message to all users, he would have to click the """"Show all xx"""" links at the bottom first. This usability problem has already produced anger amongst our users.    I would like to see an admin setting to globally disable the page switcher or a UI improvement to remove this stumbling block.""",Bug,"Course,User management,Usability"
336091,"""When I was implementing new external functions (for 3.0) I noticed that I missed to add capability checks to that new function (and also for <USER>forum_view_forum_discussion).    <USER>forum/view.php and <USER>forum/discuss.php checks the <USER>forum:viewdiscussion capability prior to do logging and completion triggering.    I'm not marking this as a security bug since 2.9 is not yet released.""",Bug,Web Services
336100,"""I've noticed that in editing mode the text areas allow to insert line breaks, for e.g. making a listing. But when back in displaying mode, all the entered text is displayed just in a single line.   (see attached screenshots).    Although the field is an textarea (regarding to the HTML-Code), there's no text editor added to this area just like it is with the text area at the description section.    I've also tried to insert tags like <br /> into the text, but it was just displayed as text and not interpreted as code (because in HTML-Code it is inserted as a string). """,Bug,Grading methods
336140,"""To give some context I've put in 3-4 hours testing on this to figure out why activity completion doesn't work with group submissions. In conclusion there seems to be something missing, Groups Submission and Activity Completion don't 'communicate' properly, and the UI doesn't work correctly in enabling and disabling options based on choices made. For example:    Require all group members submit = No then activity completion should be disabled (which is not the desired outcome to this bug)    The test applied used two assignments the difference between them is one assignment used    Require all group members submit = No    and the other assignment used:    Require all group members submit = Yes    The Activity completion for the assignment is the Student must submit for the activity to show as complete.    The text for the 'Require all group members submit' is     """"If enabled, all members of the student group must click the submit button for this assignment before the group submission will be considered as submitted. If disabled, the group submission will be considered as submitted as soon as any member of the student group clicks the submit button.""""    This is true, however the side effect is Activity completion only tracks the submission for the individual submitting the assignment, not for other group members.    Therefore students in a group who cannot click the submit button because the Require all group members submit is set to No cannot complete the activity, which is a problem not only for activity completion but course completion as well.    1. There is a workaround in getting all students to submit, but the UI for this has issues and is confusing. Since the UI states that the submission is final and once submitted it cannot be edited, and then takes the user to a screen showing the 'Edit Submission' button. Furthermore all members of the group can edit the submission which can lead to potential issues and confusion over which version of the submissions is valid.     2. Is this something Moodle are aware of and is there a better solution to number 1. Have I done something wrong?    The assignment settings I have used are as follows:    Submission Settings:    Require students to click submit button: Yes    Require that students accept the submission statement: No    Group Submission settings:    Students Submit in Groups: Yes    Require all group members submit: No    Activity Completion settings:    Show activity as complete when conditions are met: Student must submit to this activity to complete it.    Look forward to hearing back on this, hopefully the settings are incorrect, if not then this needs to be fixed.""",Bug,Assignment
336228,"""If a site has debugging turned on and a debug notice is generated prior to calling send_file() that debug notice will be included as part of the file. This is likely to corrupt the file.    See MDLSITE-3924 for an example. Although a sites issue (which will be fixed) exposed this it is rather easy to reproduce and I would be surprised if core code as well as assorted plugins didn't also experience it under the right circumstances. Here is an example.  {code}  add_to_log(SITEID, 'local_hub', 'download backup', '', $courseid . $remotemoodleurl);  send_file($CFG->dataroot . '/' . $userdir . '/backup_' . $courseid . """".mbz"""", $course->shortname . """".mbz"""", 'default', 0, false, true, '', false);  {code}  add_to_log() is deprecated so it generates a debug message which is included as if it is part of the file.    Can send_file() clear the output buffer or something? Moodle admins should be able to turn on debugging without causing their users to receive corrupt files.""",Bug,Files API
336266,"""AS a USER enrolled in a course, I want to BE ABLE to """"mention o tag"""" another user that belongs to the course in any place where be able to write an info through the WYSIWYG and Moodle would send an email to notify the user was mentioned providing in the body of the email a link to go to the info referenced.    For instance: José is writing a post in a forum and he is talking about the work done by Carlos and <USER>which are a classmates and José decides to """"tag or mention"""" Carlos and <USER>using the syntax """"@Carlos Washington y @<USER>"". Exactly like Facebook or Instagram do when you start writing the name with the wysiwyg of someone after the """"@"""".""",Improvement,"JavaScript,User management,HTML"
336495,"""When viewing an mform (like when adding a module), if the form has a checkbox or radio element (or group of them), the alignment of the radio/checkboxes and their labels is mismatched with the item title on the left.    See attached screenshots for an example of what I mean (using workshop as an example - I've also copied the checkbox element used as an example to display a radio one underneath to highlight the differences there).    This affects bootstrapbase and its children.""",Improvement,Themes
336521,"""During MDL-48572 integration I've realised that site-wide suspended users are not being filtered, I am not 100% that it is expected that get_enrolled_users and count_enrolled_users are not filtering site suspended users when asking for $onlyactive.    To reproduce the problem:  # Go to a course with manual enrolments (students)  # Go to the gradebook, you can see all the students  # Go to one of the students profile page and edit it as admin, setting suspended to 1  # Return to the gradebook, the user is still there""",Bug,Admin
336574,"""The intent of this set of changes is to help with tracking down issues with the site and is a oft-requested feature.  Currently our recommendation is to write cron output to a log file and read it. This isn't really very administrator-friendly and means that failing jobs can go unnoticed for a considerable period.    This set of changes introduces a new task logging system which adds a new output buffer to:  # cron runs (web + CLI)  # scheduled task runs  # adhoc task runs    The output buffer uses a new function which both returns the same buffer (so that it is displayed to the user still), and writes to a filehandle.  This method allows capture of all content displayed via:  # mtrace  # echo  # var_dump  # print_r  # print_object  It does not capture output displayed via {{fwrite(STDOUT, 'Hidden from logs but displayed in CLI');}}  If the job itself runs an {{ob_start}} then the output of that nested capture is not considered. It does not interfere with any logic within a task.    When the task finishes, the task is sent to a log storage class to store as appropriate.  Filters can be applied at this stage to store all, or only failed jobs.  The functionality can be disabled entirely too.  If the task fails then the state is noted too.  if the task calls {{die}} or causes the thread to finish prematurely then this also counts as a fail and is logged. (segfaults are excluded from this).    The change allows for the storage class to be overridden such that a custom storage class can be used. This may be useful for those shipping to somewhere such as fluentd, graylog, etc.    The default implementation stores a range of metadata in the database (start and end time, number of DB queries, result, user that the task ran as for adhoc tasks, etc.). The output is stored in the file storage API.    A cleanup task is also present and used in the default implementation to keep the number of logs to a minimum.""",Improvement,"Admin,Tasks"
336897,"""When editing (say a course section) in Atto, I can choose to insert a link and then 'browse repositories'. From there I can upload a file and that gets pasted in as a link to my draft files folder.  When I then save the section, I see my link! But after a few days cron deletes the draft file and the link breaks.    This doesn't seem like intended behaviour? I would assume the draft file should be """"promoted"""" to being a """"proper"""" file (not sure of the terminology here) after the Atto document is saved.""",Bug,HTML
337204,"""Not sure if this issue is a Moodle bug, per se, or due to the plugin theme Shoelace.  I originally had contacted the theme maintainer, Gareth, but he could not reproduce the issue.  After loading the plugin, all links worked well and I was able to begin creating my first """"test"""" course.  After logging out and back in some time thereafter, all hyperlinks within the Administration component were missing; thus, not allowing me any administrative functionality.  It almost appears as a permission issue as I've attempted reinstalling both the database and the Moodle files.  The only thing that I haven't done yet is to do a scratch install and start over.""",Bug,Admin
337330,"""Reproduction steps:  * Create assignment.  ** Under """"Group submission settings"""" set """"Students submit in groups"""" to """"Yes"""".  * Save and display.  * Go to View/grade all submissions.  * Grade a user, either via quick-grading or editing the grade.  * Grade will be empty. (although if you click edit, you will see the grade was saved)    If you login as a user and submit something, tada! your grade is now there.    This appears to be a visual bug introduced since 2.8 as I've tested on 2.7 and she's fine.    """,Bug,Assignment
337347,"""Hi!    My name is <USER>and I'd like to propose a few things for the new version of Moodle.    That in the appeal """"Task"""" function was added to send work done in groups. When students send their work, which could select which students else has done (should be in your group).    It would be interesting to make a label by default, have a space between it and the resource above, in order that as usually done to put titles ...    That when a user enters the center section and see """"My courses"""" the display format mosaics, as the home screen of Windows 8, and the configuration of each course can select an icon that gives Moodle without uploading your one.    Finally you could make the app for Android happens to Material Design and has a horizontal format for tablets.    Thank you""",New Feature,Assignment
337356,"""As an administrator of Moodle, I want to received a notification that a token will expire in x days (I suggest 2 weeks) in order to be able to create a new token before the current token expire.    This verification could be triggered by cron.php""",Improvement,Web Services
337494,"""When $CFG->notifyloginfailures is set, the selected users get mails as soon as a user tries to login and does not succeed.    The mail gives information about time & date, IP and username of the failed login.    I would like to propose to add an additional column to the mail text which tells the real name of the user whose username was used to login. This way, admins could better assess if  a) the failed login belongs to a real user account in Moodle and was not caused by a typo in the username  b) the affected user account belongs to an important Moodle user (school director, member of teacher staff) and might be under attack""",Improvement,Admin
337739,"""I'm currently investigating failures with a ~16GB backup file. In our system, anything which pauses without displaying progress for more than about 5 minutes will cause a timeout. This also applies to Firefox users regardless of installation details; Firefox has a 5 minute timeout.    The step backup_zip_contents calls backup_general_helper::get_backup_information_from_mbz which takes over five minutes (with the tgz packer) and does not display progress.    The underlying function this calls does provide progress information so it should be easy to add progress reporting to this step and avoid the timeout. I hope!""",Bug,Backup
337904,"""While testing MDL-47892 and creating various rules, at some point the forms started to behave strangely, with the """"a draf version of this text was automatically restored"""" appearing all the time the form was visited.    After a bit of trial and error, I think these steps reproduce the problem happening on any form/browser/database (I've confirmed it both in the course edit and forum post edit forms, with both oracle and mysql and with safari and firefox).    1. Edit any form having editor fields. Fill them and save the form.  2. Edit that form again. The editor fields are there, correctly.  3. Reload the form page (F5). The """"draft"""" message is shown and contents are erased.  4. From that point, every time you visit that form page the """"draft"""" message is shown and contents erased (no need to reload anymore).    That is it, ciao :-)""",Bug,"JavaScript,HTML"
338070,"""I had this quick idea, so thought i'd record it somewhere..    As well as the regular 'timed' autosaves, we could have something which detects that no input has happened in the editor for 3/4 seconds and triggers an auto-save.    The purposes of this would be to trigger at natural pauses/finishing points that people have when typing without having constant saves going on in the server. It would also mean a save is done as soon as they are 'done' with the editor (maybe spend a few mins messing around with other form settings).    I think it has a potential fringe benefit of also making the use able to see the autosave has happened (which I think is a key part of the feature - to give them confidence in their text being saved).""",Improvement,HTML
338307,"""Looking for a solution @ MDL-47509, it was detected that the reserved-word """"uid"""" was also being used in badges.    {code}  $ grep -rP '(FROM|JOIN).* uid'  badges/criteria/award_criteria_profile.php:            $join = """" LEFT JOIN {user_info_data} uid ON uid.userid = u.id AND ({$extraon})"""";  badges/criteria/award_criteria_profile.php:            $join = """" LEFT JOIN {user_info_data} uid ON uid.userid = u.id AND ({$extraon})"""";  {code}    I've not verified it, neither seems to fail any unit test. But I'm 99% sure it leads to problems under Oracle.    And there are occurrences both in 2.7 and master.    So this is about:    1) get rid of all those """"uid"""" uses in badges.  2) hopefully get some simple unit test covering some of those queries so we can ensure they are working under all DBs.    Ciao :-)""",Bug,Badges
338453,"""When I create assignment without any submission type selected and using the workflow marking to yes, when I try to order by user Status on view/grade all submissions <USER>assign/view.php?id=XXX&action=grading we got a Database error message.    The problem is that creates the following SQL    SELECT                  u.id,u.picture,u.firstname,u.lastname,u.firstnamephonetic,u.lastnamephonetic,u.middlename,u.alternatename,u.imagealt,u.email, u.id as userid, s.status as status, s.id as submissionid, s.timecreated as firstsubmission, s.timemodified as timesubmitted, s.attemptnumber as attemptnumber, g.id as gradeid, g.grade as grade, g.timemodified as timemarked, g.timecreated as firstmarked, uf.mailed as mailed, uf.locked as locked, uf.extensionduedate as extensionduedate, uf.workflowstate as workflowstate, uf.allocatedmarker as allocatedmarker                   FROM <USER>user u                           LEFT JOIN ( SELECT mxs.userid, MAX(mxs.attemptnumber) AS maxattempt                                   FROM <USER>assign_submission mxs                                   WHERE mxs.assignment = '6085' GROUP BY mxs.userid ) smx ON u.id = smx.userid                           LEFT JOIN ( SELECT mxg.userid, MAX(mxg.attemptnumber) AS maxattempt                              FROM <USER>assign_grades mxg                              WHERE mxg.assignment = '6085' GROUP BY mxg.userid ) gmx ON u.id = gmx.userid                           LEFT JOIN <USER>assign_submission s ON                              u.id = s.userid AND                              s.assignment = '6085' AND                              s.attemptnumber = smx.maxattempt                           LEFT JOIN <USER>assign_grades g ON                              u.id = g.userid AND                              g.assignment = '6085' AND                              g.attemptnumber = gmx.maxattempt                           LEFT JOIN <USER>assign_user_flags uf ON u.id = uf.userid AND uf.assignment = '6085'                  WHERE u.id IN ('888','687','1049','1548','1523','1151','1530','769','437','1042','1051','1131','1654','1770','1756','1227','1233','1632','1336','1685','53','55','1071','1747','56','1423','1831','1639','1631','1560','1708','1286','1194','1554','1008','1390','1706','1513','603','1805','1798','435','1409','1296','1601','1761','1772','1097','1339','1544','999','1085','1660','67','1701','1588','1781','1055','1340','808','1655','71','1324','1329','1333','938','439','588','1570','853','1681','1115','1556','765','1774','1620','1220','1814','944','77','1827','1762','1205','1719','752','1507','1816','1585','953','1674','1241','1641','433','733','1670','1780','1758','401','1837','1156','1688','1602','1569','1603','1684','1516','1792','962','1181','1116','1623','1580','1771','1515','84','1424','1436','1439','1244','959','1323','86','1211','683','1610','1550','1500','1721','492','89','1312','1128','1800','1733','1726','764','1549','814','748','1739','1021','1807','93','1300','1279','934','1551','1189','95','458','99','1165','102','1703','1734','1532','598','104','453','1308','1846','428','1223','1821','1218','780','1405','107','863','1716','1785','452','1577','1849','1584','1155','1519','1575','427','1377','1511','1741','110','1832','1373','1547','1529','1645','584','1735','495','771','1848','1736','112','1009','113','1574','1571','745','1673','507','1420','116','688','469','644','1578','1369','1419','1851','1725','1694','1209','1326','1759','1162','122','1691','931','643','1538','1618','1528','728','1186','124','1370','1202','506','678','802','1546','513','399','1555','996','805','928','1806','1817','1699','1587','1294','1801','1509','844','1026','1599','1696','132','137','1757','1712','423','957','1581','1498','1238','1640','595','716','134','1190','941','744','1833','1682','641','593','1707','1016','1261','467','140','1072','1745','1769','1108','621','779','1646','1095','1847','1812','1522','1521','1627','743','1273','1558','155','150','1690','742','1254','741','1292','1636','1704','1819','639','449','1649','1109','933','973','762','1012','1174','1619','1740','1803','1783','1356','976','1346','1697','770','618','1319','1828','1693','1314','1325','870','1394','1779','988','1612','1354','1663','1744','1505','171','1389','1367','1643','1564','1705','1753','1541','857','172','1773','1635','972','1591','1170','1628','1501','1121','1010','1527','615','1838','1767','1615','1582','790','519','179','1589','1782','477','1829','1642','1616','782','777','1411','1622','1850','476','1714','1124','859','1514','494','1537','1835','1840','1506','500','1572','1764','1614','1752','1005','1397','1318','1818','1613','1653','1499','485','1036','1562','1000','1414','761','1750','1746','1748','1141','1644','1823','1457','1127','1662','1163','456','1788','209','849','1600','1738','1099','1648','475','210','1731','674','1596','1794','1787','760','1686','1598','1718','1406','1789','1586','1224','1402','1278','1197','484','1290','1058','447','1081','1425','510','1535','1715','1381','1668','1450','1775','1630','1777','978','1568','231','1542','1039','232','446','1360','1563','1545','1536','1540','975','1617','1567','1658','1755','238','792','1330','1722','483','1206','1592','1240','1427','1826','1607','1820','1328','1111','1583','1595','1804','1070','1303','525','707','1047','1749','1811','248','1754','1557','251','1822','1362','445','252','1503','1611','253','1096','1159','383','1634','1720','416','1504','1683','259','1553','1751','465','1799','1637','1518','1552','1791','1839','800','1813','1626','989','1524','1605','1717','1650','1675','1676','1664','1680','1510','1647','1576','264','737','1621','270','1604','993','1638','1088','1723','1710','464','1068','271','1172','1061','1651','1624','1508','1533','609','1778','1671','1808','1146','278','413','1594','1565','1305','1221','705','1730','704','1825','1289','736','1283','1841','1184','1512','1802','1139','1776','1724','1107','1531','412','1809','1579','759','1260','1048','1656','288','289','997','1609','1200','632','1629','1743','1766','841','701','1351','1590','1262','292','293','294','1689','1711','408','1692','1293','1217','812','1836','960','1593','1852','1561','1606','303','1698','1379','1534','1177','304','1059','1129','1270','1277','1713','1110','1608','1065','1410','397','1678','1687','1035','929','1543','1573','498','1657','1123','1502','482','1732','1830','629','497','1784','1416','1737','1652','605','758','1192','591','1810','735','1729','1797','1672','1702','1539','1677','592','1332','1728','322','1140','650','1666','1727','795','1763','522','1796','1700','855','801','1103','325','1566','326','1407','1625','846','1520','1667','1359','1526','328','1695','589','1786','1659','1525','1517','1253','329','1790','1633','1178','1014','755','731','1665','1597','331','754','1382','1321','1669','332','1045','1251','734','1661','1765','1815','516','1793','1742','662','1132','1679','336','935','1265','1709','1768','1795','856','1122','1431','1824','693','1559','627','480','1225','1760','1834','1093','461')                  ORDER BY workflowstatus;    but the ORDER BY column doesn't exist.    I think that the problem could be fix changing gradingtable.php file on line 281 on which the columname assigned to the array is wrong, I change it to 'workflostate' and it works!    If I'm wrong let check it out how to fix it properly.    Thanks for all  """,Bug,Assignment
338549,"""If you upload a .bmp file into Moodle (and I'm guessing the same applies to any image filetype that the GD graphics library cannot convert from) then when viewed in the filemanager it is recognized as an image file and a thumbnail is attempted to be generated.    When it fails, it returns instead a hardcoded fallback icon which cannot be overriden by themes, and doesn't take advantage of Moodle's image fallback system to provide SVG, then PNG when not supported.    The hardcoding is on line 2399 of lib/filelib.php:    if (!$preview_file) {  // unable to create a preview of the file, send its default mime icon instead  ...  $fileicon = file_file_icon($stored_file, $size);  send_file($CFG->dirroot.'/pix/'.$fileicon.'.png', basename($fileicon).'.png');    The ideal solution would be to recognize that only certain image filetypes can have previews generated for them and to not even try for other image filetypes but instead let the generic icon already provided by the theme remain in use.    Currently if you override the .bmp icon in a theme, when shown in the filepicker it will flash up for a short period of time before being replaced by a """"thumbnail"""" which is actually the default icon.  """,Improvement,"Themes,Files API"
338644,"""Sometimes it would be useful to be able to set a 'random' value for the scheduled tasks time value. i.e. When dealing with remote servers, its useful to avoid every single request from automated checks hitting the server at the same time.     What i'd like is define a 'random' minute value for my task, then when the task is installed - Moodle will choose a random time to set it in the database for each Moodle install.    I don't think its necessary for this to be exposed to users (and think its better that its defined in the database as a fixed time), its simply to distribute the timings across Moodle installs.    I need this for MDL-46210 - I can imagine other uses - like converting the updates API checker.""",Improvement,Libraries
338666,"""Hi,  We're using folders to try to eradicate the scroll of death on our courses, but the biggest bugbear staff keep coming back to is that they can't hide individual files within a folder.  For example, when a course is rolled over for the next year, a folder may contain several worksheets along with a file containing the answers given to the students the previous year. The current setup means that these need to be manually downloaded (to save a copy), then deleted, then added back to the folder at the appropriate time. If they could be hidden then selectively made visible it would make the process easier. A tick box in the editing window for each individual file would provide an easy way to specify which files were hidden. As an extension to this, it would be greatly improved if individual folder contents could be tied to conditional activities or dates, being made visible when the condition is achieved.   I'm anticipating a rebellion at any minute -  most staff love the folders, but hate this all-or-nothing lack of control, and I'm expecting to see some of them abandon folders to go back to endless individual files simply to be able to hide the occassional resource.  Regards, <USER>""",Improvement,Usability
338670,"""The internal php IMAP library is terrible and very poorly documented.  There are other libraries out there. The best I've found is the Horde framework, which has a compatible license.""",Sub-task,Libraries
338775,"""I'm using the clean theme, and when you have 2 sections hidden, both of which have activities also.     As an instructor when you go and open the action menu, on the hidden section above the other section, the action menu's absolute positioning is changed because opacity causes a different z-index context.    So even though the action menu's z-index is set to 1000, changing the opacity on a whole section causes that z-index to be ignored when rendering the sections.    Because of this, it makes the action menu unusable.  So for instance I can't click the delete button in my image below    Screenshot attached for a visual representation:        """,Bug,"Themes,JavaScript,HTML"
338786,"""If you back up a course including badges, then restore it and the usercreator  of the badge doesn't match a user in the system it gets set to 0. I would expect it to default to the user restoring the course (ie $USER->id), it being 0 causes issues if it tries to send messages to notify the creator about the badge being awarded.    Steps to replicate:  1) Create a user and give them a sitewide role like site manager  2) Create a course  3) Log in as the user and create a badge within that course     - Make sure it has a criteria (manual issue by role is probably the easiest)     - Set notify badge creator to everytime on the messages tab    4) Activate the badge  5) Backup the course making sure that users and badges are ticked  6) Download the backup file and upload to a course where no user matches the creator id (or delete the creator from the <USER>user table manually)  7) Restore the course (including the badge) into a new course  8) Check in the database, the creator id of the restored badge will be 0  9) Create a learner, enrol them in the course, and award them the badge  10) Run the cron, you will get the invalid user error below when it tries to send the creator a notification about the badge being awarded.    Default exception handler: Invalid user Debug: SELECT * FROM {user} WHERE id = ?  [array (    0 => '0',  )]  Error code: invaliduser  * line 1411 of /lib/dml/moodle_database.php: dml_missing_record_exception thrown  * line 1387 of /lib/dml/moodle_database.php: call to moodle_database->get_record_select()  * line 129 of /badges/cron.php: call to moodle_database->get_record()  * line 108 of /badges/cron.php: call to badge_assemble_notification()  * line 35 of /badges/cron.php: call to badge_message_cron()  * line 545 of /lib/cronlib.php: call to badge_cron()  * line 61 of /admin/cli/cron.php: call to cron_run()    !!! Invalid user !!!""",Bug,"Backup,Badges"
338837,"""The Moodle core LDAP synchronization script only provides the option to suspend _or_ delete Moodle accounts for LDAP users which have disappeared in LDAP.    This all-or-nothing solution does not fit for all scenarios.     For example, a LDAP account may disappear because a student has been exmatriculated automatically for formal reasons (missed paying student fee or anything like this). Some days later, he is immatriculated again when the formal problems have been solved and re-appears in LDAP.     If I configure Moodle to delete Moodle accounts as soon as a LDAP account disappears, the Moodle account is already lost if the LDAP account re-appears.  If I configure Moodle to suspend Moodle accounts as soon as a LDAP account disappears, Moodle accounts of re-appearing LDAP accounts can be revived, but I end up with a large amount of suspended accounts which will never be needed.    I would like to propose to add another option what to do if a LDAP user disappears:   This option should suspend users which have disappeared in LDAP for a configurable amount of days and delete them only after this grace period.""",Improvement,Authentication
338930,"""Currently on themes based off bootstrap base have a button that expands the header and displays the users logged in status. This button has a data-target property which is meant to specify (or so I would think) what DOM elements are to be hidden or displayed at narrow screen resolutions""",Bug,Themes
339006,"""Using the 20140814 build of 2.6.4 we upgraded our site. Prior to upgrade, SCORM activities set to open in new window opened as expected in a new window. Now, the same activities open in the same window. Tested using both Firefox and IE. Firefox javascript debug tool shows attached error that may have to do with it. I am not sure whether the bug exists only for upgraded sites or new sites. The issue affects our upgraded site. Issue did not exist in 2.6.3 when we did our initial testing. It only appeared in 2.6.4.    Since the window also doesn't have navigation once the SCORM package starts playing, users do not have a way to navigate back to the Moodle course without closing the window and starting the login process again.    I believe this is directly related to the bug fix for MDL-46236 since this is the file we see with differences between 2.6.3 and 2.6.4. We may have code to share but we need to test it first before posting it. I wanted to post the issue so it's known.    To replicate:  1. Set up a SCORM 1.2 activity to open in a new window using a version other than 2.6.4 (may also happen with fresh activities in 2.6.4 but I didn't test this).   2. Confirm that the SCORM opens in a new window as a student.  3. Upgrade to 2.6.4 - no new settings were changed in the SCORM area from the new settings page during installation.  4. Attempt to launch the same SCORM activity in 2.6.4 - it opens in the same window without navigation even though the SCORM activity settings are set to """"New Window"""".    """,Bug,SCORM
339008,"""As an admin I would like to have the ability to go in and Manage all the backed up courses in our site. I know I can log in as each faculty member and manage that way, but as an Admin I need the ability to control these, especially since they count against our storage space and over time can begin to add up to some serious space usage.     Another alternative is:     If we can't have the ability to pull up and manage all the backed up courses on the site, then maybe there can be something implemented that would control the number of backups a professor can have under their profile and then it would remove the oldest or let the professor choose which one to replace when they reach their threshold.   """,Improvement,"Backup,Admin"
339133,"""I am now using the ATTO editor.  I discovered that ATTO does not seem to support line breaks directly by entering shift-enter, as I am used to doing in the other editors and in products like Dreamweaver.  I thought that I would report this as a """"bug"""", but maybe this is done differently with ATTO.    I am attaching a screen shot showing this problem, where I get a normal newline instead of a line break.""",Bug,HTML
339135,"""In Moodle, $cm objects are sometimes stdClass and sometimes cm_info. The cm_info objects are required in certain places, and have performance advantages.    The main way to get a cm_info object is like this:    {code}  $modinfo = get_fast_modinfo($course);  $cm = $modinfo->get_cm($cmid);  {code}    As you can see, a course is required as well as a cmid. In the common situation where you only have a cmid, it requires some SQL-query gymnastics to do this without making unnecessary queries.    This compares to the nice simple function for getting stdclass-type $cm:    {code}  $cm = get_coursemodule_from_id('assign', $id, 0, false, MUST_EXIST);  {code}    I would like to implement two new utility functions:    1) An efficient function that returns $cm and $course, given a $cmid (and optionally checking the module type also).    2) A convenience function cm_info::from_mixed($cm) that returns a cm_info based on an existing $cm object that might be any type. (This will not be as efficient as ensuring that the $cm object was right first time round, but realistically, is needed in some places.)""",Improvement,Libraries
339307,"""Hello,    Scenario:  * Create an adhoc task class named """"local_test\task\test""""  * Create an adhoc task class named """"local_test\task\test2""""  * Schedule test2 for 5 days in the future  * Schedule test for 6 days in the future  * Delete the class  * Wait 6 days  * Run cron    What I would expect is that the adhoc task test2 fails but test is executed anyway, but in fact the cron run simply stops at test2 and no further tasks are executed.    This is because adhoc_task_from_record returns false to get_next_adhoc_task which uses it as an object anyway.  """,Bug,Admin
339434,"""_Using Chrome/Firefox with font color and background color button added to Atto toolbar_    Both buttons, when clicked, don't set the style at the current cursor position. For example:  * i'm typing """"this text going to be red""""  * select the color red in the fontcolor button  * continue typing """"this should be red"""" but it still black.     You need to select the """"this should be red"""" text and then select the color to actually change it. I thought there was a similar problem for bold and italic that was resolved since but I can't find the related issues in the tracker right now.    In the end, font color and background color button should behave like bold or italic.""",Bug,HTML
339493,"""It would be very nice if the conditional access system could have an option to restrict access by role:    My use case is that in multiple courses across multiple categories, I have a courses to provide training materials to system managers, but in those courses I want to have sections which are restricted to Sys Admins and I don't want to have to create a group / grouping in each course.    I'm sure there are other use cases.  It doesn't really make sense to have a conditional access system not utilize roles as an option.""",Improvement,Roles / Access
339803,"""The core\plugininfo\theme::uninstall_cleanup() method unconditionally deletes these configs:  {noformat}  unset_config('thememobile');  unset_config('themetablet');  unset_config('themelegacy');  {noformat}    I'm assuming that the values should be checked to see if the value is set to the name of the theme that is being deleted.    Other items that could be cleaned as well:  * themelist config value has a CSV of theme names.  * -theme config value could be set to the theme being uninstalled.  I forget what the default theme is, but should probably be set to that.- Never mind, just saw that the plugin code prevents you from uninstalling the default theme.""",Bug,Admin
339834,"""hi, we provide training to external partners. one of our partners recently experienced hangging when accessing different """"testing"""" courses. they have not had any problems before, but the last person to access our system from their side was about 200+ days ago.   to me it sounds like something specific to their company/network. their IT is now checking if the problem is due to their set-up. i would like to check if there is anything that i can suggest them to do.  No other partner is experiencing the same problem.""",Bug,Course
339988,"""Moved from MDL-45726 as I hijacked that bug to fix a different issue - sorry!    Hi Dan. I want to add some more informations:  I've looked at the scorm debugger and now I've a very clear idea of what happens in sequence.  My SCORM Package is made of a number of html files in which are loaded a swf video that contain the lesson's slides. These swf files call each one an external mp4 that contains the teacher's recorded video lesson. This is synchronized by the swf player with the sfw slides so that every X frames of the video, the swf movie is checked and the play status is set at the correct frame. In this way the slides are always in sync with the video and we can use a low res video and an hi-res slide (because into the swf the slides symbols, lines, fonts and so on are vectors)  Now when the first lesson (html) is opened an onload action is triggered. This action triggers the doLMSInitialize and the check of the status of the sco.  At the end of the movie the page is reloaded. There is an other action on the html that is trigered onbeforeunload or onunload, and this actions calls doLMSCommit and dLMSFinish and set the status of the sco.  Here we have the issue:  The SCORM has a grading based on the learning object count. To set the SCORM grade to """"5"""" the student has to complete 5 lessons.. Every lesson is marked completed when it ends. We must set the lesson complete to get the grade of the scorm.   A SCORM package made of 25 lessons will have 25 scoes completed.  Into the Moodle database in the <USER>scorm_scoes_track table we will have three rows for each sco:  the first row will <USER>the x.start.time, the value and the timemodified   The second will record the cmi.core.lesson_status the value (incomplete/complete) and the timemodified  The last will record the cmi.core.total_time its value and the timemodified   And this will be for each userid, scormid and scoid.  Now if i set the grading method on the learning-object status, the scorm can't be """"completed"""" if the whole scoes are not in completed status or the student will get review mode at the middle of his attempt.  Last but not the least: this scorm package worked correctly in the 1.9.x moodle version.  The question is: why not in 2.X?""",Bug,SCORM
340002,"""I recently upgraded from 2.3.3 to Moodle 2.5.2+ (Build: 20131025). I needed to move a person from one course to another and transfer grades for quizzes that were identical between the two courses. After manually entering the grades, they would not show up in the course completion report in 2.5.2. I never had this issue in version 2.3.3. All settings are as they were before the upgrade.    In the grader report, I expected to be able to turn editing on, record a score for a quiz, update, and that score show up as a check <USER>in the course completion report because it was a passing grade for that quiz.    I was able to enter the appropriate score and the """"box"""" changed to the shaded box showing that a score had been manually added. However, the course completion report didn't acknowledge the input of the score as had happened in the earlier version.    I have screen shots if you need them but I didn't know how to add here. Thank you.    **Additionally, when a quiz is taken, that attempt doesn't show up in the completion report either. Again, no settings have been changed between the 2.3.3 version and 2.5.2+ version.    **Further information, both instances are recorded in the """"activity completion"""" under the reports menu in course administration with the 2.5.2+ version. However, when using the """"view course report"""" in the """"course completion status"""" block created in 2.3.3, the added grade AND the passing quiz attempt are not showing as complete. Again, I have screen shots is needed. Thank you.""",Bug,"Course,Reports"
340079,"""I've created an activity that has assigned roles.  When I duplicate the activity, it keeps the groups and groupings settings, but loses the role assignments.  Is this the expected result?""",Improvement,"Course,Backup,Roles / Access"
340196,"""I'm having some trouble getting the CSS to load when using the treeview module in a plugin.    _Steps to reproduce_    # Install moodle/master  # Install my module from github.com/<USER>moodle-tool_capexplorer/ (branch capexplorer-jsmodule-for-<USER>45606)  # Visit Admin > Users > Permissions > Capability explorer  # Notice the context treeview has circular bullet points and is not fully styled. Inspector shows the file:    {code}  """"[SITEURL]/theme/yui_combo.php?gallery/-1/skin-<USER>gallery-sm-treeview/skin-<USER>gallery-sm-treeview.js&3.15.0/template-micro/template-micro.js&3.15.0/tree-labelable/tree-labelable.js&3.15.0/tree-openable/tree-openable.js&3.15.0/tree-selectable/tree-selectable.js&3.15.0/view/view.js""""  {code}    is loaded. But looking in the file there is a line:    {code}  // Combo resource gallery/-1/skin-<USER>gallery-sm-treeview/skin-<USER>gallery-sm-treeview.js ($CFG->dirroot/lib/yuilib/gallery/skin-<USER>gallery-sm-treeview/skin-<USER>gallery-sm-treeview.js) not found!  {code}    This path is wrong - it shouldn't including 'skin-<USER>'.    5. Revert the commit 051548b897dd86bc5386e85c0e66a9fa888fde9d  6. Reload the page  7. Notice the context treeview has arrows and is fully styled. The inspector shows two separate files loaded:    {code}  [SITEURL]/theme/yui_combo.php?3.15.0/template-micro/template-micro.js&3.15.0/tree-labelable/tree-labelable.js&3.15.0/tree-openable/tree-openable.js&3.15.0/tree-selectable/tree-selectable.js&3.15.0/view/view.js  {code}    and    {code}  """"[SITEURL]/theme/yui_combo.php?gallery/-1/gallery-sm-treeview/assets/skins/<USER>gallery-sm-treeview.css""""  {code}    The styles that were missing are found in the second (separate) file.  """,Bug,JavaScript
340203,"""When I was implementing the new Plugins overview screen, there was a need to distinguish plugins installed as a part of Moodle standard distribution from those that were installed (added) manually by the admin. Various alternatives like """"3rd party"""", """"contributed"""", """"non-standard"""", """"extension"""" etc were considered (with [~<USER> and [~<USER> involved) and finally it was decided to use the """"add-on"""".    However, after some time, it now appears to me that we (well, it was just me actually) overacted a bit and used the term at much more places in the admin UI. This sometimes lead to confusion as people are asking what's the difference between a plugin and an add-on (I've seen some sophisticated slides about it even).    The true is, there is no conceptual difference. Or better, nothing that deserve so much attention as it is now.    So, my proposal in this issue is:    1. Use terms """"add-on"""" and """"standard"""" only (and only) in that table that lists all installed plugins (the """"Source"""" column)  2. Change all other occurrences of the """"add-on"""" string to """"plugin"""". So the admin Settings block node would read """"Install plugin"""" etc.    I believe that such consistency would help. Looking forward your comments here.""",Improvement,"Language,Admin"
340341,"""Each section of a course is structured as a list item within one unordered list.  Highly simplified example:    {code}  <ul>  <li>section one</li>  <li>section two</li>  </ul>  {code}    When you move a section via clicking the crosshairs and selecting a destination via the popup dialog, the section you moved is placed within a new unordered list element inside the list item inside the main unordered list.  Again, highly simplified and assuming that section one was moved to section two:    {code}  <ul>  <li>section two</li>  <li>  <ul>  <li>section one</li>  </ul>  </li>  </ul>  {code}    Refreshing the page puts everything back to how one would expect with just one unordered list containing list items for the sections and no sub-lists.    In any theme that does not have margins on the list items, the appearance (before refreshing) is simply that the moved section has a slightly thicker border.  Each list item has a 1px solid border, so the nested list items with no margins effectively produce a 2px solid border.  See the screenshot moved_section_no_margins.png.    Without looking at what's happening in the HTML, the thicker border appears that it could be intended behavior to highlight the moved section.  If that were the case, I would expect the border to be restyled, not for the section to be moved into a new list within the main list.    The problem is most obvious if a theme has margins on the list items (which is a valid case).  In this case, the border of the section is not simply 2px instead of 1px.  What you get is nested boxes that look messy and certainly do not appear to be intended behavior.  See the screenshot moved_section_with_margins.png.    Steps to reproduce:  # Start with a course in topics or weekly format and multiple sections.  # Open the developer tools of your browser so you can inspect elements.  # Turn editing on.  # Click the crosshairs of the first section.  # Choose any destination in the popup dialog.  # Depending on your theme, the moved section may now have either a slightly thicker border or a nested box situation.  # Using the developer tools, inspect the element of the moved section and note the new <ul> in the <li> in the outer <ul>.""",Bug,Course
340585,"""Along the last days, I've been performing a lot of unit tests runs, in parallel, putting the laptop under stress and I've seen this a bunch of times:    {code}  1) core_setuplib_testcase::test_localcachedir  Time is lower that allowed start value  Failed asserting that 1398075623 is equal to 1398075624 or is greater than 1398075624.    /Users/<USER>git_moodle/integration/lib/phpunit/classes/advanced_testcase.php:396  /Users/<USER>git_moodle/integration/lib/tests/setuplib_test.php:162  /Users/<USER>git_moodle/integration/lib/phpunit/classes/advanced_testcase.php:80    To re-run:   vendor/bin/phpunit core_setuplib_testcase lib/tests/setuplib_test.php  {code}    More yet, executing the same test alone, N times, I also have been able to reproduce it randomly (like 1 time every 500-600 executions, but my SSD is pretty fast, surely it happens often is slower disks).    {code}  vendor/bin/phpunit --repeat=100 core_setuplib_testcase  Moodle 2.7beta+ (Build: 20140417), mssql, c8ca0cea7712b6c2aaecaeafaeee8132b293608a  PHPUnit 3.7.34 by <USER>Bergmann.    Configuration read from /Users/<USER>git_moodle/integration/phpunit.xml    ...............................................................  63 / 800 (  7%)  ............................................................... 126 / 800 ( 15%)  ............................................................... 189 / 800 ( 23%)  ............................................................... 252 / 800 ( 31%)  ............................................................... 315 / 800 ( 39%)  ............................................................... 378 / 800 ( 47%)  ............................................................... 441 / 800 ( 55%)  ............................................................... 504 / 800 ( 63%)  ......................................F........................ 567 / 800 ( 70%)  ............................................................... 630 / 800 ( 78%)  ............................................................... 693 / 800 ( 86%)  ............................................................... 756 / 800 ( 94%)  ............................................    Time: 1.36 minutes, Memory: 110.25Mb    There was 1 failure:    1) core_setuplib_testcase::test_localcachedir  Time is lower that allowed start value  Failed asserting that 1398075623 is equal to 1398075624 or is greater than 1398075624.    /Users/<USER>git_moodle/integration/lib/phpunit/classes/advanced_testcase.php:396  /Users/<USER>git_moodle/integration/lib/tests/setuplib_test.php:162  /Users/<USER>git_moodle/integration/lib/phpunit/classes/advanced_testcase.php:80    To re-run:   vendor/bin/phpunit core_setuplib_testcase lib/tests/setuplib_test.php    FAILURES!  Tests: 800, Assertions: 7463, Failures: 1.  {code}    The only reason I can imagine is the filemtime() call in test_localcachedir() returning sort of cached (old) data.    So I've tried adding a clearstatcache() call:    - at the end of reset_all_data() => random failures continued.  - at the end of make_localcache_directory(), unconditionally => random failures continued.  - before the call to filemtime() in test_localcachedir(), global and applied to the .lastpurged file only => random failures continued.    So it seems that cleaning the caches is not enough and, for some reason the .lastpurged file is older than time(). So in some way... it's truly being created early or filemtime() has a different """"rounding"""" algorithm...  something nasty is going there.    My last attempt was to add to test_localcachedir():    {code}  > unlink($timestampfile);  $dir = make_localcache_directory('', false);  ...  {code}    And with that I've got > 10000 executions without error. But I'm not entirely happy putting that there.    So, for your consideration... perhaps we should consider both deleting that file and performing the clearstatcache() call in reset_all_data, or perhaps within make_localcache_directory() or perhaps it's ok to put it there in the test... don't know.    Ciao :-)""",Bug,"Caching,Unit tests"
340627,"""I'm marking this as a bug because I forgot to add this functions to the Mobile Service once it where implemented (see MDL-30085)""",Bug,Web Services
340732,"""The intention of this policy is to define how we manage the usage of Bootstrap classes in core in the other base themes (Base/Canvas).    It has been seen that now that we are using Bootstrap as default we are using Bootstrap classes in core, which is good and nice step forward the element library. But what happens with base?    So far I noticed:    # The Bootstrap class and styles are NOT copied to base  # The Bootstrap class and styles are copied to base  # The Bootstrap class and styles are copied to base, but made more selector specific    *About #1*    As the styles are missing, case can potentially look really odd especially when the classes are playing with positioning.    *About #2*    I see Base as a scaffold for themes. If you browse Moodle using base you notice that colours are mostly absent, and that the bare minimum was used so that things are positioned where they should be, but without any styling.    If we backport the Bootstrap rules to base one by one, we will end up with a proper Bootstrap in bootstrapbase, and a half-Bootstrap in base. Also, base will now have colours, and advanced styles such as shadows, rounded corners, etc... And those will be visible, and by default not differently styled, in all children themes.    *About #3*    Exactly the same thing than #2, but instead of being able to re-use Bootstrap classes you would have to re-define them again, and again, because the selectors are too specific.    On one side you are sure that you will not mess up with existing themes that might be using the same classes. But on the other side it makes themers job impossible because they need to override all those specific selectors.    *What I am suggesting*    Close to #1, I would not backport Bootstrap rules to Base. But, if while testing on base, I realise that a bit of styling is required (and let's remind ourselves that base is a minimalist) then I would create the class and style it myself. Without colours, or fancy styles.    That way:    - We do not mess with existing themes  - Base is still what it was, simple and a base  - We do not end up with a base that randomely has bootstrap rules""",Bug,"Themes,Policy"
340744,"""On my site with 11 courses and 1K course modules, i'm getting 500 queries on the front page when logged in as admin.    Some things I have enabled:  * Display courses on the front page when logged in  * Activity names filter    A very hacky investigation in the postgres log indicates a problem checking the course record many many times:    {code}  12  SELECT active.filter, fc.name, fc.value    12  SELECT ctx.*    30  SELECT f.id AS id, f.contenthash, f.pathnamehash, f.contextid, f.component, f.filearea, f.itemid, f.filepath, f.filename, f.userid, f.filesize, f.mimetype, f.status, f.source, f.author, f.license, f.timecreated, f.timemodified, f.sortorder, f.referencefileid, r.repositoryid AS repositoryid, r.reference AS reference, r.lastsync AS referencelastsync    33  SELECT * FROM <USER>context WHERE contextlevel = $1 AND instanceid = $2    33  SELECT * FROM <USER>enrol WHERE courseid = $1 AND status = $2  ORDER BY sortorder,id  1332  SELECT * FROM <USER>course WHERE id = $1  {code}    {code}  im=# select count(*) from <USER>course;   count  -------      11  (1 row)    im=# select count(*) from <USER>course_modules;   count  -------    1040  (1 row)  {code}""",Bug,Performance
340895,"""See screenshots for the following:    - The checkboxes are not well displayed  - The padding on the right of the text inputs is too small, they seem to close to the edge  - The info at the bottom do not have separator  - When there is a separator at the bottom, it is glued to both sides  - From an accessibility point of view, I'm pretty sure the file info are not properly ready as SPAN is not considered as a word separator  - Margins below and above the fields seems unnecessarily high    All themes are affected, so _clean_ and _base_ should be tested. To select _base_ pass ?theme=base to the URL.""",Bug,"Themes,Accessibility"
340936,"""Real case:    - there is the """"assignfeedback_editpdf|gspath"""" setting, where the path to ghostscript is set. Some divine developer (joking!) decided that the linux path was a good default. That automatically leads to problems running in other systems/installations.    - so, in order to get those tests executed here i needed to set, following the official way to do so, in my config.php:    {code}  $CFG->forced_plugin_settings = array('assignfeedback_editpdf'  => array('gspath' => '/opt/local/bin/gs'));  {code}    - but that setting, """"forced_plugin_settings"""", is not allowed and is kicked when running tests, leading to skipped tests (those were the mysterious 47 steps skipped I was getting here with behat).    So finally, I ended adding also this:    {code}  $CFG->behat_extraallowedsettings = array('forced_plugin_settings');  {code}    aka, whitelisting manually the whole 'forced_plugin_settings' array.    No matter it worked, IMO we should consider (one of):    1) Perhaps it's better to move the """"assignfeedback_editpdf|gspath"""" to main config, together with other """"paths"""" that we have in Moodle. And then, whitelist all them, so I can define where the hell my """"du"""" (CFG->pathtodu)... and my """"ghostscript"""" (CFG->pathtogs) are.    2) Whitelist the whole """"forced_plugin_settings"""" and assume that anything set there has precedence. So there is no need to use $CFG->behat_extraallowedsettings.    If you ask me I'd say I prefer 1), keeping all paths together (in the UI) and named in a similar way, so they can be configured via config.php and easily whitelisted in phpunit/behat.    The change, in any case, is really trivial. For your consideration, ciao :-)  """,Bug,Assignment
340953,"""While testing MDL-44690 I encountered a bug within the restore system.  I tested, as admin, on a freshly installed site, with a single category (Misc) and a single course.  When I attempted to restore the attached backup file I consistently encountered an error at stage 2: destination.  {quote}  There are no categories or existing courses you can restore to.  {quote}  I've attempted restoring through three means:  # Frontpage + Admin Block: Site administration > Courses > Restore Course.  # In the course + Admin block: Course administration > Restore  # In a newly created quiz + Admin block: Quiz administration > Restore.    All failed the same.    I found as soon as I created a second category + course the issue was no longer present.    I suspect after talking with Eloy that there is a bad course count somewhere in the code.""",Bug,Backup
340960,"""Restoring backup course in Moodle 2.6 takes considerably longer than in 2.5. Even modest size courses (30 - 40MB) take 20 - 30 minutes to restore. Occasionally there is no confirmation message that the course has actually finished restoring.     Based on previous experience backing-up and restoring courses of this size, I would expect the process of restoring to take about 3 - 4 minutes.    To replicate this problem, simply backup and restore a course in 2.6.1.""",Bug,Backup
341048,"""As part of the availability changes, I would like to be able to access certain fields from a cm_info object during the process of adding dynamic data to the modinfo, specifically name.    I don't think this should be a controversial requirement. There is already code in modinfolib that was intended to support this kind of 'recursive' call.    Now, there is a very strange PHP behaviour as follows:    - If you recursively access a property when there is a __get function, the recursive access is treated as if the property does not exist at all.    In other words, the following sequence:    * $var = $cm->name;  ** __get('name')  *** ...before returning name it builds dynamic data...  *** ...somewhere in there it calls a function which calls a function which accesses...  *** $cm->name    Will cause a horrible error.    (If it isn't clearer how horrible this is, let me explain that this depends on how dynamic data was obtained in the first place. In this case dynamic data was obtained due to accessing the name property, $var = $cm->name in the above. If you used the exact same code, but prior to the $var = $cm->name line you added $var = $cm->url, then the remaining code would all work perfectly. Assuming there isn't anywhere that tries to access $cm->url, of course.)    The obtain_dynamic_data function was already designed to handle recursive calls correctly, but it never gets called because PHP is broken in this regard.    There is no workaround and I really need to know the name (I don't even much care if some code changes it later, the one from module settings will do) so I suggest adding a function which we can use to access properties without going through the <USER>function. The behaviour will be completely identical except that, if called while building dynamic data, it will return the name value instead of giving a PHP error.    NOTE: An alternative solution would be to make it so you can access the name without requiring dynamic data. This would mean that it is not possible e.g. for a module to dynamically change its own name. Might be a good idea but I assume that code is there for a reason so I didn't take that simple approach.""",Improvement,Course
341056,"""I'd like to obtain/generate a set of simple valid SCORM 1.2 and AICC packages to include in tests.    These should replace the existing valid packages being used in tests.  singlesco_scorm12.zip  singlescobasic.zip  validaicc.zip  validscorm.zip    We should also find a better naming convention for the invalid packages:  invalid.zip  badscorm.zip    And we should also include packages that allow testing features like MDL-41765    If we can generate a specification for these packages I can ask someone who has a valid licence of something like Articulate to generate some packages for us.""",Improvement,SCORM
341143,"""By default user email addresses are to be unique, but several threads are asking queries how to circumnavigate this.     The functionality can be 'disabled' via:  - open users/editadvanced_form.php in the moodle installation folder  - comment out $err['email'] = get_string('emailexists'); in the file (around line 222) by adding a hash in front of it (e.g. # $err['email'] = get_string('emailexists');)    As this will impact potential upgrades, I would like to add a new feature, where the system administrator has a checkbox in the Plugins > Authentication > Manual Authentication > Settings to disable the unique email address requirement.     If checked, the functionality is as it is now, if unchecked, the error will not appear, allowing the user to continue with a non-unique email address.   """,New Feature,"Authentication,Usability"
341284,"""When we converted from SimpleYUI, I must have made a transposition error. We include the same module twice in slightly different rollups. The first is never used as a result.    The duplicates I've removed are:   * yui-base;  * get;  * features;  * loader-base;  * loader-rollup; and  * loader-yui3.    These contain all of the metadata for Moodle, the loader itself, and the fetching mechanism. If they were broken in some other way, we would see a very large bang and no working Javascript within Moodle.    To clarify, these were already included by us using 'yui' which is a rollup of the above 6 modules.    File-size savings:    || || Content size || Gzipped size ||  || Before | 388KB | 112KB |  || After  | 289KB | 86.6KB |""",Bug,JavaScript
341296,"""It should be possible for a plugin to declare admin pages in the settings tree that are available for permissions there than moodle/site:config.    Example - I have a <USER>that has a part that connects to external files. The users are able to delete these files, but in the <USER>structure, they are actually held for some number of hours. We have a page that allows user support to undelete them.    The problem is that right now, the full admin tree isn't parsed - most parts are skipped unless the user has site:config.    I've done some preliminary testing and the admin the changes needed seem to be small, and the admin tree enforces permissions well anyways.""",Improvement,Admin
341303,"""I'm raising this as a corollary of MDL-44334.    [~<USER> raised a concern about the EditPDF window not being scrollable because of the lockScroll changes but, in my opinion, we should not be creating dialogues which are larger than the screen anyway.    What we should do is to have the dialogue curtailed to the height of the screen (within reason), and for scrolling to happen *within* that dialogue. This is the method for the activity chooser.    Making EditPDF work in this fashion would mean that:  * the editing tools should always be visible (if implemented correctly) rather than off the top of the page; and  * we can lock scroll so that scrolling through a long pdf does not drive you crazy.""",Bug,"Assignment,Usability"
341390,"""As a follow-on to MDL-43524 which removed global text caching, we need to examine the activity names filter and probably add MUC caching there to improve performance.    The activitynames filter is what puts links in texts to other activities in the same course.     Petr has raised that the fully correct way would be to use new events or the as-yet-unwritten hooks to invalidate the caches, but I think that might be too much to do before 2.7, so I want to see a quick solution here even if it may mean relying on less-perfect cache invalidation.    Tests showing performance before/after are required.  """,Improvement,Caching
341391,"""As a follow-on to MDL-43524 which removed global text caching, we need to examine the database filter and probably add MUC caching there to improve performance.    The database filter is what puts links in texts to database entries. There is quite a lot of processing going on in the background even before the simple text replacement, and this needs to be redone every time the cache is invalidated.    Petr has raised that the fully correct way would be to use new events or the as-yet-unwritten hooks to invalidate the caches, but I think that might be too much to do before 2.7, so I want to see a quick solution here even if it may mean relying on less-perfect cache invalidation.    Tests showing performance before/after are required.""",Improvement,Caching
341392,"""As a follow-on to MDL-43524 which removed global text caching, we need to examine the glossary filter and probably add MUC caching there to improve performance.    The glossary filter is what puts links in texts to glossary entries.  There is quite a lot of processing going on in the background even before the simple text replacement, and this needs to be redone every time the cache is invalidated.    Petr has raised that the fully correct way would be to use new events or the as-yet-unwritten hooks to invalidate the caches, but I think that might be too much to do before 2.7, so I want to see a quick solution here even if it may mean relying on less-perfect cache invalidation.    This issue requires performance comparison tests before and after the fix.""",Bug,"Performance,Caching"
341434,"""When assigning roles at the site level:    moodle/admin/roles/assign.php?contextid=1    there's a warning message which is just outputted as an anonymous box (div.box.generalbox). It should use the notification('notifyproblem') API so that it actually looks like a warning.    (There's actually a bunch like this, that you can find because the text string contains the word """"warning"""" but there's no appropriate API used but I wanted a specific bug number to put next to a workaround so people will be able to check and then delete the workaround).""",Bug,"Admin,Roles / Access"
341588,"""There's a border=1 set on the device detection regex table in admin -> theme settings:    /moodle/admin/settings.php?section=themesettings    This isn't usually visible because Moodle includes YUI CSS that add a border to every single td and th, and then Moodle CSS fixes that by including CSS that removes borders from every single td and th.    The Bootstrapbase/Clean theme doesn't use the YUI CSS, and has removed the workaround that removes all borders from tables (in order to fix MDL-27774) and now you get the border=1 on this table showing through again.    When looking at this I noticed that the code to write out this table has a another bug. Normally the first row has r0 and r1 alternating, while each cell is c0, then c1, then c2 etc. This code adds c0 to each cell on the first row, c1 to each cell on the second row, c2 on each cell on the third row etc.    I don't know if it's worth messing with, I don't think it has any actual effect, but I thought I'd note it anyway.""",Bug,"Themes,Admin,HTML"
341592,"""I've put together a (mostly) working PoC to restructure the atto code.    At present, the code is based around a structure which must be aware of every editor on the page. We have to pass around an elementid, and each plugin adds buttons to their own element, but with a single click handler.    The code also does not meet the coding style guidelines.    IMO, we need to refactor this to:  * not pass around the elementid;  * comply with coding style requirements;  * include API documentation to assist plugin developers;  * have one editor instance per editor;  * have instances of the plugins on each editor rather than globally.    This will:  * make our lives far easier in the future to maintain because we'll be reducing the code duplication;  * allow people to write plugins more easily and with much less code duplication;  * which will allow for a greater diversity of plugins in a shorter period;  * give better confidence from third-party developers writing plugins for the new editor, and as a result the community as a whole (just think how much we've complained about the TinyMCE plugin structure).    Personally, I feel that this needs to be done before integration, and certainly before release.    I've been working on a PoC which is semi-functional (35% of plugins are already done and the others aren't far off) and it only took a few hours to write including appropriate API docs.  """,Improvement,HTML
341684,"""Note: I would appreciate anyone who could provide better language to describe this problem.     Instructions to replicate:  1. Create a new course, or use an old course.  2. In one of the sections on the main course page, create a Label. In the Label, put a table that has a width of 75% (or anything less than 100%). Put some text in the table. It is helpful to make the table background a color to see the problem visually. Center the table. I have put the html I used for this test in a txt file below.   3. Duplicate the table, and this time outside of the table write enough text so that the text would reach across the width of the main content area.  4. Verify that only the second table is 75% of the main content area, while the first table has a much smaller container, and hence the table is only 75% of that width.    You can also see that changing the amount of text inside the table (when there's no outside, long text), also changes the way the table is displayed in terms of width and centering.    I have replicated this in 5 or 6 themes, but have only uploaded screenshots in Standard and Leatherbound. It's not actually a theme problem, I don't think, but I wasn't sure where to put it.     """,Bug,Themes
341704,"""I've found an inconsistency at the assignment plugin.     If a teacher changes the group of a student, which did a submission on a group assignment earlier. Than the student, which did the submission, can't view the assignment anymore and gets a """"nopermissiontocomment"""" exception (with half of the side loaded).    It was a struggle to find that error, since we got only the report of a trainer saying, that the assignment can't be viewed by some students. And therefore I don't think there is an easy fix for this.    Edit: I can give this additional information as well. It may lead to a solution of this issue, but it's hard to decide, which direction this should go. I don't know that for sure, but it seems reasonable for me, that, if it is a group assignment and somebody has submited a solution, there will be created 2 entries in the assign_submission table. One for the student and one for the group. If the group of the student is changed both entries are untouched, which results in status """"submitted for grading"""" for the user and """"no submission"""" for the group in which he's moved. This conflict is badly resolved by moodle and gives the mistakable error message """"nopermissiontocomment"""", which is thrown by {{line 61 of /<USER>assign/submission/comments/lib.php}} as an result of {{assignsubmission_comments_comment_validate() in line 33}}  of the same file.    Assume a changed group after a successful submission of a student, called """"the student"""".      Consider:    {code:title=/<USER>assign/submission/comment/lib.php:55-62}  $group = $assignment->get_submission_group($USER->id);  $groupid = 0;  if ($group) {      $groupid = $group->id;  }  if ($groupid != $submission->groupid) {      throw new comment_exception('nopermissiontocomment');  }  {code}      In our case {{$group}} will be the new one, in which the student was moved and {{$submission->groupid}} will be the old one, in which he submitted his solution. Therefore, {{$groupid}} wont be same as {{$submission->groupid}}.    The reason, why moodle evaluates this statement at all is located at {{line 640 of /<USER>assign/renderer.php}}. it says:      {code:title=/<USER>assign/renderer.php:640-641}  $submission = $status->teamsubmission ? $status->teamsubmission : $status->submission;  if ($submission) {  {code}    Conclusion: Team has no submission, but the student has this entry from earlier, which leads to {{true}} for him, but {{false}} for all other group members. After that there is a series of renderer called which leads to the the if-statement mentioned above and to the exception.    Edit: The whole backtrace is:    {code:title=Debug info: Error code: nopermissiontocomment: Stack trace}      line 61 of /<USER>assign/submission/comments/lib.php: comment_exception thrown      line ? of unknownfile: call to assignsubmission_comments_comment_validate()      line 7366 of /lib/moodlelib.php: call to call_user_func_array()      line 7348 of /lib/moodlelib.php: call to component_callback()      line 828 of /comment/lib.php: call to plugin_callback()      line 839 of /comment/lib.php: call to comment->validate()      line 415 of /comment/lib.php: call to comment->can_view()      line 73 of /<USER>assign/submission/comments/locallib.php: call to comment->output()      line 890 of /<USER>assign/renderer.php: call to assign_submission_comments->view_summary()      line 221 of /lib/outputrenderers.php: call to <USER>assign_renderer->render_assign_submission_plugin_submission()      line 661 of /<USER>assign/renderer.php: call to plugin_renderer_base->render()      line 221 of /lib/outputrenderers.php: call to <USER>assign_renderer->render_assign_submission_status()      line 3628 of /<USER>assign/locallib.php: call to plugin_renderer_base->render()      line 3913 of /<USER>assign/locallib.php: call to assign->view_student_summary()      line 525 of /<USER>assign/locallib.php: call to assign->view_submission_page()      line 53 of /<USER>assign/view.php: call to assign->view()  {code}""",Bug,Assignment
341764,"""In version 1.9, teachers could control whether or not students received notification when assignments were graded or when grades were changed. In the upgrade to version 2.X, this functionality was lost. According to MDL-33600, this will be fixed for version 2.7 which is great.    However, not all institutions, especially larger ones like the University of Illinois will be able to upgrade to version 2.7 right away. Like is stated on the instruction page for requesting backporting of issues, """"Large amounts of change on the stable branches make the lives difficult for institutions to manage upgrades between point releases.""""    This is not really an improvement in Moodle functionality; rather it is restoration of functionality that was available in an earlier version and was lost when that version was upgraded. We have found that very large classes (those with several hundred students) are most affected by this lack of control over the sending of feedback to students since assignments may be graded by multiple people and at different times. Instructors often hide grades so that students will not receive access to grades at different times. Additionally, the more students in the class, the more chance there is for grade adjustments to be necessary. In short, teachers of courses should have control over whether students are notified of grade changes in their courses.    Large institutions are more likely to have larger courses and are also less likely to install the latest and greatest Moodle code due to the issues of change management.    So, since this is not really a new feature and because it will most likely have the greatest impact on institutions that are least likely to be able to install version 2.7 in May, it is a perfect candidate for backporting. I would request that it be backported to any versions that are currently being supported.""",Improvement,Assignment
341767,"""The table that the badges interface uses, which it gives the class """"collection"""" overrides the default generaltable class.    Most (though not all) other uses seem to pass this class in as well as the actual classname for the table:    git grep table..attributes..class    If it did then it would pick up the standard styling and not require to rebuild it's own styles. And many lines of CSS could be deleted, and as a result the table would be easier to theme.    While I'm here there seems to be many lines intended to tweak the table until it displays absolutely perfectly. Simply deleting these doesn't seem to make much difference after the generaltable class is added.    The table.collection seems to be created twice, so it's probably used in two contexts and possibly there's similar issues with table.issuedbadgebox, table.badgeissuedimage, table.badgeissuedinfo which don't sound like tables of data, just going by the CSS for them.""",Sub-task,"Themes,Badges,HTML"
341787,"""Hi all,  I'm new to moodle and just installed moodle_26_stable branch.I installed this branch like 3 days ago.And today i opened it but i forgot my password.I clicked the [forgot your password] button but there was a error due to the fact that smtp.host and other necessary details were not filled.But now i can't access my account .    I guess during registration if these details like smtp.host etc are filled then there would be no problem at all.""",Improvement,"Admin,Authentication"
341942,"""The ability to switch off 'Display Page Name' on both the Site administration > Plugins > Activity modules > Page, settings and on the Page settings area itself.  Our client wants to remove Page Headings, this option was available in Moodle 2.5 but seems to be missing in Moodle 2.6.1. also in 2.6 (20131224)    I'ved added before (2.5) and after (2.6.1) screen shots below.  The option is also removed from admin settings in the upgrade.       I've used custom CSS to remove the <H2> headings on Pages < not ideal solution obviously.     .empty-region-side-pre #region-bs-main-and-pre.span9 #region-main h2 {  float: none;  width: 100%;  display: none;""",Bug,"Accessibility,Usability"
342060,"""It would be handy to be able to use PHP count() function on recordsets    I don't know if this achievable for all Moodle supported PHP database entensions, but the few I've looked at return number of rows in result. Could the row count be accessed through PHP Countable interface?  {code:title=Quick ol workable hack}  class pgsql_native_moodle_recordset extends moodle_recordset implements Countable {        protected $result;      /** @var current row as array.*/      protected $current;      protected $bytea_oid;      protected $blobs = array();      protected $count;        public function __construct($result, $bytea_oid) {          $this->result    = $result;          $this->bytea_oid = $bytea_oid;          $this->count = pg_num_rows($result);            // find out if there are any blobs          $numrows = pg_num_fields($result);          for($i=0; $i<$numrows; $i++) {              $type_oid = pg_field_type_oid($result, $i);              if ($type_oid == $this->bytea_oid) {                  $this->blobs[] = pg_field_name($result, $i);              }          }            $this->current = $this->fetch_next();      }        public function count() {          return $this->count;      }  {code}""",Improvement,Database SQL/XMLDB
342068,"""I'm attempting to enable the one section per page feature under course layout settings of a course and it does not work.  I am running 2.6 (Build: 20131220).  I thought it was an issue with my installation, however I cannot enable the setting and have it take affect on the demo.moodle.org website either.""",Bug,Course
342140,"""There is a bug somewhere in one of the following modules:  * modchooser  * chooserdialogue  * dialogue    To reproduce:  # Log in as an admin  # Browse to a course that requires scrolling to get to the bottom section.  # Turn on editing  # Scroll down to the bottom section and click to add an activity.  # Notice that the scroll position jumps up the screen (usually to the top but depends upon your browser).    Of interest in all browsers except Chrome I've observed that this only happens the first time the dialogue is disabled.  In Chrome it seems to jump up each time you open the dialogue.  """,Bug,JavaScript
342141,"""In course/edit.php, there is some code that call require_login() in the following way:  {noformat}  $course = course_get_format($id)->get_course();  require_login($course);  {noformat}    At first, this looks harmless, but it actually causes a really odd edge case.  Basically the above code populates the $COURSE global with the course format options.    At first, this doesn't really harm anything, until course caches are cleared.  Once cleared, the format_base::get_course calls get_course() which reads the course from the $COURSE global.  Then it reports that the course format options are not valid because the _appear_ to be the same as a field in the course table.    I'm not sure if this can be replicated with stock Moodle, but we have a customization listening to the course_updated event which is fired after caches are cleared and the customization then uses the format_base API.    So, my two suggested solutions are:  # Ensure that when require_login() is called, it is not using a $course object from format_base::get_course  # Ensure that when $COURSE global is set, it doesn't include course format options.    The latter is probably more appealing since 3rd party code could be doing this.  Of course, other solutions are probably out there!""",Bug,Course
342158,"""While maintaining Moodle sites, I have realized that it's a good thing to prevent users to log in couple of hours prior to putting the site into the maintenance mode. The reason is obvious. I don't want to terminate existing sessions so that users typing their forums post or attempting a quiz won't loose their work. But I want to prevent new visitors to log in.    My typical use case looks like this:    * Firstly I """"close"""" the site from new logins (I have a custom hack for this that I would like to get as a new feature into the core).  * Then I wait some time - typically an hour or two. I check the logs and/or the Logged-in users block to be almost sure there is nobody logged into the site.  * Only after that I put the site into the maintenance mode.    The new feature I am suggesting here (and I will be happy to code it) is that:    * If a file called """"clinologin.html"""" is found in $CFG->dataroot then noone can log in (or roam in via MNet).  * The contents of the file is displayed to users as an explanation message.  * If a file called """"clinologinkey.txt"""" is found in $CFG->dataroot then users are able to provide a string called """"unlock key"""". If the provided text (submitted by the visitor) matches the value defined in the txt file, then a special cookie is set in their browsers and the login is allowed for them as normally. This will allow the admin (and/or other people as required) to continue using the site even if it is closed.    Because this is controlled from CLI, we can be sure that the admin does not lock the site accidentally.    Again, this will affect creating new logging/roaming in only. Currently logged in users are not affected (unless they log out).""",New Feature,Admin
342208,"""I have set up several test sites in the past week, each into which I've had to create a category structure and to restore courses into.  I wished during this process that there was a means to restore a course through the course management interface as browsing to a random course to restore a backup as a new course in another category really felt pretty wrong.  This is perhaps a developer situation more than a real world situation but I still think it is something that should be done.""",Improvement,"Course,Backup"
342350,"""I'd like to see the all available courses list be available as an option for the """"Courses"""" page. We want to display the list, just not on the front (home) page of our site. This is a fairly easy addition.    1. copy the code from the front page to display all available courses into the course/index.php page.    2. add an option somewhere to display the """"all available courses"""" view on the course page.""",Improvement,Course
342775,"""On a blog page, it would be easier for the reader to read only a blog preview and have a link to open the blog post and read more if interested. Same feature as wordpress, drupal...    Admin case 1:  In order to set the """"Read more..."""" link in a blog, as an admin, in the Blog presentation settings, I change the option """"Entry readability"""" from """"Show the whole entry"""" to """"Show a preview"""".    Admin case 2:  In order to change the number of lines of a blog entry shown in the blog page, as an admin, in the Blog presentation settings, I can set a numeric value in the option """"Number of lines in the blog preview"""".    User case:  As a user, when I am on a blog page, when I want to read a previewed blog entry, I click on the entry title or the """"Read more..."""" link, both redirect me on the entry page""",New Feature,Blog
342845,"""(This is a re-post because I originally accidentally placed this under """"tracker.moodle"""" and got no response.)    I recently upgraded from 2.3.3 to Moodle 2.5.2+ (Build: 20131025). I needed to move a person from one course to another and transfer grades for quizzes that were identical between the two courses. After manually entering the grades, they would not show up in the course completion report in 2.5.2. I never had this issue in version 2.3.3. All settings are as they were before the upgrade.  In the grader report, I expected to be able to turn editing on, record a score for a quiz, update, and that score show up as a check <USER>in the course completion report because it was a passing grade for that quiz.  I was able to enter the appropriate score and the """"box"""" changed to the shaded box showing that a score had been manually added. However, the course completion report didn't acknowledge the input of the score as had happened in the earlier version.  I have screen shots if you need them but I didn't know how to add here. Thank you.    **Additionally, when a quiz is taken, that attempt doesn't show up in the completion report either. Again, no settings have been changed between the 2.3.3 version and 2.5.2+ version.    **Further information, both instances are recorded in the """"activity completion"""" under the reports menu in course administration with the 2.5.2+ version. However, when using the """"view course report"""" in the """"course completion status"""" block created in 2.3.3, the added grade AND the passing quiz attempt are not showing as complete. Again, I have screen shots is needed. Thank you.    Further information, I have discovered that there is a cron script (site administration>notifications)that, when manually run, will populate a check <USER>on the appropriate """"view course report"""" where a quiz with a passing score has been taken. It will not however, place an X for a quiz that has a failing score. Version 2.3.3 would populate both in real or near real time without having to run the mentioned cron script. Thank you.    **Additional information, after setting the cron script described above to run every 30 minutes, it appears that some SCORM packages I have set up in a course no longer show when the package is completed. Could this be related?  """,Bug,Reports
342857,"""Feedback starts here (Tested on 2.6 beta):    The highlight and hide links do not announce whether the topic is hidden or highlighted.  This is not immediately apparent to a non-visual user.    Editing Titles is straight forward    When editing a summary, the label for the section name is not read.    When moving resources, the dialog is read clearly, but is difficult to follow in the same way that a page full of nondescript links is difficult.  Instead of """"To item""""  it would be more clear if it were something like, """"to beginning of section"""", """"before assignment [name]"""", """"before quiz [name]"""" were used.  This would help non-visual and persons with cognitive barriers to have a clear picture of what they're doing.    For AJAX Edit Settings/ Move /Hide /Duplicate, it is not apparent that one needs to wait after hitting duplicate.  Instructions would be helpful.  An alert that the duplication is complete would also be helpful.    For adding a new activity, arrow key capture is working and interaction can be followed using a screen reader.  This is contrary to my other experiences.    When the """"Add"""" button is clicked, the word """"unavailable"""" is spoken.  This is not so informative.  Maybe add in some form instructions that are revealed via aria-describedby or a title attribute.  This could tell the blind user that they should expect to wait after pressing the button.    Help buttons produce an alert that is read immediately    When creating a new resource, I could not reverse tab out of description text-editor for the description field, I could tab forward normally.  This was inconvenient but not a """"deal breaker"""" due to the other ways of navigating with a screen reader.    Error text when name not entered is read but is not understandable in JAWS 12.  Example """"You must supply a value here."""" is read after the label for the next field is spoken.  This makes it sound like an instruction for the subsequent field instead of error handling for the previous one.  The sequencing felt off.  Maybe render as an alert with the name of the field it applies to supplied for context    For the Content / Appearance / Etc buttons, add descriptive text indicating that fields will be hidden or shown.  Also announce the state of these fields within the button.    Course Category Management feedback:    When visiting the Viewing Course categories and courses link, JAWS announces that I should use the arrow keys to navigate the collapsible menu.  Nothing happens when the arrows are pressed. The enter key must be pressed to open the menu, but this is not stated.  Once opened, tab and shift+tab are used to navigate normally.  My guess is that the JavaScript used to capture the arrow key presses may not be implemented or may not be working as intended.  I would need to look at the source.     • The use of double square brackets """"[[""""  and """"]]"""" clutter the interface for screen readers.  Screen readers will say """"left-square-bracket left-square-bracket"""".     • It feels like either legend, title attributes or aria describedby is out of whack.  Upon focusing each input, the following is read:     • Controls are screen reader accessible and seem to reveal the state of each control, however real world usability is an issue due to JAWS reading the same information repeatedly.  only the menus were actually broken.     • Visual users who are forced to use the keyboard only, may have difficulty determining where their focus is due to how feint the icon font color is.  """,Improvement,"Accessibility,Course"
342859,"""MDL-42705 adds the CSS to display a progress bar using the standard bootstrap HTML. The CSS to support this was also added to the base theme - but it's huge. I'm sure we could come up with a smaller and nicer alternative.    """,Bug,Themes
342918,"""Greetings. I believe I have 2 Moodle accounts. I am new to Moodle and wish to create a course. I created an account as a teacher and not an administrator and thus have no access privileges. I would like to delete both existing accounts and start over. I would then like to create 1 account with administrator access privileges.    Email associated with the accounts  <EMAIL> <EMAIL>    Log-ins:  csemedia  csemediacenter  <EMAIL>   Please help! <EMAIL>or 651-913-0073 x149.  """,Bug,"Accessibility,Roles / Access"
342954,"""While testing MDL-42739 I discovered the action menu is not operation within a docked block.    To replicate:  # Log in as admin  # Turn on editing  # Dock a block  # Refresh the current page.  # Toggle the display of the docked.  # Notice that the block actions have not initialised.    I've tested in standard and can confirm the issue there, we should also of course test bootstrapbase based themes.  """,Bug,Themes
343006,"""The following error is reported when running unit tests under MSSQL...    {noformat}  2) core_course_courselib_testcase::test_course_change_sortorder_after_course  dml_write_exception: Error writing to database (SQLState: 42000<br>  Error Code: 102<br>  Message: [Microsoft][SQL Server Native Client 11.0][SQL Server]Incorrect syntax  near 'c'.<br>    UPDATE phpu_course c                         SET sortorder = sortorder + 1                       WHERE c.category = '2'                         AND sortorder > '20003'  [array (    0 => '2',    1 => '20003',  )])    D:\xampp\htdocs\master_integration\lib\dml\moodle_database.php:444  D:\xampp\htdocs\master_integration\lib\dml\sqlsrv_native_moodle_database.php:245    D:\xampp\htdocs\master_integration\lib\dml\sqlsrv_native_moodle_database.php:352    D:\xampp\htdocs\master_integration\lib\dml\sqlsrv_native_moodle_database.php:743    D:\xampp\htdocs\master_integration\course\lib.php:3469  D:\xampp\htdocs\master_integration\course\tests\courselib_test.php:2241  D:\xampp\htdocs\master_integration\lib\phpunit\classes\advanced_testcase.php:80    To re-run:   \xampp\php\phpunit core_course_courselib_testcase course\tests\courselib_test.php  {noformat}    In the function course_change_sortorder_after_course() there are a number of raw DB calls (I'm not sure why this was allowed). In each of the queries an alias is used for the table, however MSSQL seems to dislike this alias syntactically. As there is only a single table involved in each query the alias is not needed and can be removed. When it is removed, the queries work in MSSQL.""",Bug,"Course,Database SQL/XMLDB"
343031,"""I've been experiencing some randomly Moodle server crashes and weird error messages using Moodle as a teacher and admin.     Server crashes were related to: """"Fatal error: $CFG->dataroot is not writable, admin has to fix directory permissions! Exiting."""" and weird error messages using Moodle to """"... Failed to unserialise data from file. Either failed to read, or failed to write.""""    I posted about all of this in my blog but it's a long post and I don't know if I should paste here a link or paste the long post here. Lete me know please.    In summary:    a.- Me servers crashed 6 times randomly (never at peak times) and I had to reboot it manually.     Date of freezing --> Errors showed when trying to access Moodle WEB Server        Friday 4th October at 00:00h AM --> Fatal error: $CFG->dataroot is not writable, admin has to fix directory permissions! Exiting.      Sunday 6th October at 04:00h AM --> Fatal error: $CFG->dataroot is not writable, admin has to fix directory permissions! Exiting.      Wednesday 16th October at 04:00h AM --> Fatal error: $CFG->dataroot is not writable, admin has to fix directory permissions! Exiting.      Saturday 26th October at 19:00h PM --> Fatal error: $CFG->dataroot is not writable, admin has to fix directory permissions! Exiting.      Monday 29th October at 00:00h AM --> Fatal error: $CFG->dataroot is not writable, admin has to fix directory permissions! Exiting.    As soon as the server rebooted it started to rebuild the raid1 and everything started to work fine again... until the next incident.     b.- We had also incidents reported by teachers and me (as admin) where the error message was """"Getting Error:Coding error detected, it must be fixed by a programmer: Failed to unserialise data from file. Either failed to read, or failed to write"""".    Initially I managed to sort them out from Settings > Site administration > Development > Purge all caches but the last 'similar' incident could not been sorted out same way because the error """"Getting Error:Coding error detected, it must be fixed by a programmer: Failed to unserialise data from file. Either failed to read, or failed to write"""" affected to 'Purge all caches' too. I tried for console (cli) but I also failed. Look at the debugging message below:    Error code: codingerror  * line 468 of /cache/stores/file/lib.php: coding_exception thrown  * line 371 of /cache/stores/file/lib.php: call to cachestore_file->prep_data_after_read()  * line 295 of /cache/classes/loaders.php: call to cachestore_file->get()  * line 1358 of /cache/classes/loaders.php: call to cache->get()  * line 522 of /lib/dml/mysqli_native_moodle_database.php: call to cache_application->get()  * line 1269 of /lib/dml/mysqli_native_moodle_database.php: call to mysqli_native_moodle_database->get_columns()  * line 1565 of /lib/dml/moodle_database.php: call to mysqli_native_moodle_database->set_field_select()  * line 1302 of /lib/moodlelib.php: call to moodle_database->set_field()  * line 1205 of /lib/outputrequirementslib.php: call to set_config()  * line 1503 of /lib/moodlelib.php: call to js_reset_all_caches()  * line 51 of /admin/cli/purge_caches.php: call to purge_all_caches()      Finally I went to administration ► Plugins ► Caching ► Configuration. From there I found the 'Configured store instances' section and the three links to purge content straight from there. Purging cache stores ended up solving the problem.... until now.  No more server freezes either.    A further investigation on server logs at crashing time (actually the last logs before crashing) showed me nothing in particular at syslog, messages, apache2 logs. But in kern.log I noticed this:    Oct 29 00:00:58 server kernel: [52934.011290] ata1.01: exception Emask 0x0 SAct 0x0 SErr 0x0 action 0x0 frozen  Oct 29 00:00:58 server kernel: [52934.011297] ata1.01: failed command: FLUSH CACHE EXT  Oct 29 00:00:58 server kernel: [52934.011304] ata1.01: cmd ea/00:00:00:00:00/00:00:00:00:00/b0 tag 0    Cacti charts (Apache, MySQL, IO disks, Memory,..), APC behaviour and smarttools tests over our raid1 disks didn't give any evidence of failure.     Today I think that all of this has been caused by 'something' related to Moodle Cache but I don't know yet how and when. I though it could be valuable to share it here.     I really was afraid of losing data because every time I had to manually reboot the server it started to rebuild the raid1 --> Panic!    Thanks for reading and for working on this amazing software.    Viva Moodle!    <USER>",Bug,Caching
343080,"""I'm creating it as a bug but maybe it should be considered an improvement. When I use the filemanager using a RTL language the mentioned list of icons are shown as in LTR languages when using the standard theme, but the order is swapped when using clean; in my opinion this is not consistent, feel free to close the issue if there is a known reason for it or you think it should behave like it does.""",Bug,Themes
343086,"""*Updated* This issue now refers to a custom tool that lets administrators add new file types. You can define the MIME type and text description, and select an icon from the existing Moodle file type icon set.    The below is the original description from the problem we were experiencing (when it misdiagnosed a file as a zip file):    A fairly frequent occurrence is that certain courses will use files of a specific, obscure format intended for a particular software program. Often these formats are based on standard formats. Students are expected to download the files (and the software, either free software or commercial software that's been licensed for them) and then open the files in the software.    Moodle makes this difficult because it automatically identifies generic file types, incorrectly. The worst offender is zip. I'm not sure if it also does the same for XML.    This then causes problems for students because when they download the files, depending on the browser (a) they may get renamed to the zip extension, and (b) it also offers to open them in an unzip program. Both these things cause problems for students, who are generally not technically skilled. Even if renaming a file isn't a challenge, we still have to tell them to do it, and they're going to be asking the obvious question, why on earth are you making us download files with the wrong name and then renaming them...    Here is an example of the type of issue request I get:    <<  The three GeoGebra files on this page [...] are displayed as .zip files when they should be .ggb files. When I downloaded the files they saved as .ggb files, but when Zoe tried this they saved as .zip files that couldn't be used.    The files are displaying correctly on the 12J presentation website [...]. Has something changed in the VLE updates that now causes the files to be associated with Winzip?    This is pretty urgent as the students need to use these files now.  >>    I don't think we should add a complete list of obscure random MIME types and icons (GeoGebra? Come on) to Moodle core. However, Moodle really needs to stop breaking these file downloads.    I can do temp fix for this kind of problems by running a database query like the following:    <<  update <USER>files set mimetype='document/unknown' where filename like '%.ggb' and mimetype != 'document/unknown'  >>    But obviously I have to do that each time this is reported and if they upload any more files they will be screwed again. And it will only ever be reported when students are actually due to be using the files at that moment, so it always turns into an urgent request...    To be honest I'm not sure what the right solution is here, but I can see two good possibilities:    1. Stop the auto-detection logic for common container formats (.zip, .xml), so that only files called .zip get the zip mimetype and others get a suitable unknown type mimetype.    or    2. Make it easy to add user-configured MIME types in the admin user interface (this is roughly MDL-7101 - yay four-digit issue). Since I'd like this to happen in stable branches, I suggest this could be a minimal implementation using a single text area where you enter a list of types in a basic text format, similar to mime.types but also including the format description and which icon to use.    I am very happy to work on this, with the proviso that I'd like to do a minimal implementation which is easy to implement and backport rather than a fancy GUI - fancy GUI could be left for later work if required. :)      Before I do anything, could I get opinions on this - what's the best approach? To me it sounds like adding the mime configuration would be better but...""",New Feature,Libraries
343215,"""I've noticed that the graphs/data for statistics reports need some fine-tuning.    At the site  level, Statistics Report graphs show two data points for each day or week period. Same label for day increments, and for week increments it shows the first and last day of that week. The first is a value of zero for the """"period ending"""" and the second has a accurate (I'm assuming) values. The table below the graph doesn't list this duplicate data point.    At the course level however, the statistics report shows a graph as described above AND the data table lists a row of zero data for each week of good data - which seems to be indicating the start of the period not the end.    """,Bug,Reports
343310,"""Doing some innocent testing of MDL-38743 it was discovered that etag verification (and last-modified verification consequently) were no matching ever.    See point B2 in that issue.    This is about to fix the etag matching over all stable branches.    It has a big impact in pages heavily using stored files (forum discussions… long books… long courses with labels…)    I reviewed all the rest of etag uses in code base (js, css, yui combo…) and all them seemed to be using the /revision/ approach, so they are not affected. In any case I'd recommend to review all them again, just in case I missed anything.""",Bug,"Performance,Files API"
343784,"""When an mp3 file is included as part of a quiz question, with quiz security set to """"Full screen pop-up with some javascript security,"""" the mp3 file won't play for students or when a teacher switches to student role only when using the Chrome browser.  The band-aide solution is to change the browser security setting for the quiz to """"none.""""  This issue seems to be connected to the pop-up window, but ONLY in Chrome.    To reproduce the problem:  1) create a quiz with a question that includes an mp3 file, uploaded using the Moodle Media icon of the editor. Ensure the browser security setting for the quiz is """"Full screen pop-up with some javascript security.""""  2) as a teacher, review the quiz and prove that the flashplayer loads AND you can play the file.  Also note that because you are a teacher, the quiz does not open in a pop-up window.  That's understandable and expected.  Exit the quiz.  3) switch role to student and attempt the quiz.  The pop-up window opens, as expected, and the flashplayer will load but the play button does nothing.  4) switch role back to teacher.   5) change the browser security setting of the quiz to """"none.""""  6) switch role to student and attempt the quiz.  The pop-up window no longer opens (understandable and expected), the flashplayer will load and the play button now works.    I first noticed the problem in our 2.2.4 production server, but have reproduced it with our 2.5.2 development server.    I'm using -  Windows 7, but have also tested on XP  Flash 11.8.800.170, but have also tested with 11.8.800.94  Moodle 2.2.4 AND 2.5.2  """,Bug,JavaScript
343818,"""To reproduce:    1. go to any course and choose Import.  2. Select any other course to import from.    Look at the numbered 'stage bar' that indicates which import stage you're on:    1. Course selection / 2. Initial settings / 3. Schema settings / 4. Confirmation and review / 5. Perform import / 6. Complete    As a minor point, this bar didn't appear on the first page where you select a course, which might possibly be considered a bug. However, more obviously, when you are on the second page 'Initial settings', it actually highlights both the first and second stage as bold (current) even though you are clearly on the second page not the first.    I don't intend to fix this (sorry) but it was raised while testing something else I'd done so I'm filing the issue to have it on record.    Possible solutions include leaving out the current step 1 and changing the numbering (so step 2 becomes step 1), or fixing it so that only step 2 is highlighted.      """,Bug,Backup
343846,"""I recently added code for the zip packer to report progress, but there were some accidental omissions: three recursive calls missed out the $progress variable.    As a result it was possible that the progress callback did not get called between files when processing sub-folders.    As well as the fixes, I'm also adding a new unit test that covers this case that didn't work correctly before.    NOTE: There is still a long pause without progress calls in the zip archive 'close' method which is where it actually does most of the work, and because this ends up being a PHP built-in function we cannot do anything about it on a trivial level. I had some experimental work to improve this by closing it periodically but this is a bit inefficient; I think it's probably better to work on this issue with a new format as part of MDL-41838.""",Bug,Files API
343857,"""My course only shows one section at a time, so i'm in a section and editing it.  (course/view.php?id=489&section=2&notifyeditingon=1)    I was adding an activity to a course section. I clicked on 'Add an activity or resource'    As expected i see that the page-content's first h2 reflects this with """"Adding a new Assignment to <section name>"""" but it was slightly confusing when seeing my navbar change from being inside the section to 'navigating' outside of the section.    go usability!""",Bug,Usability
343874,"""Rewriting the issue description after investigation:  grade_item->get_name() in conditionlib returns the name that has already been formatted and passed through get_string(). The item name is cached in course modinfo/sectioninfo if there is some module or section with availability depending on the grade in this item.  This was discovered via the debugging message if format_string() is called before page context was set (see below), but it also means that get_string and format_string are called for language, user and context that were used when cache was build and not when the string was displayed. This may cause problems in multilang environment and comprehensive filters that depend on particular course.    Example: with multilang filter enabled, cache was rebuild when english-speaking teacher was accessing the course. The English name of the activity will be displayed to French-speaking student.    ---------------  Original description:    It seems that something has been recently broken:    1) master branch (I've not tried stables)  2) Go to ANY activity (DEBUG_DEVELOPER enabled). I've tried glossary, forum, lesson.  3) Purge caches  4) After the redirect, this is shown:    {code}  Coding problem: $PAGE->context was not set. You may have forgotten to call require_login() or $PAGE->set_context(). The page may not display correctly as a result  line 451 of /lib/pagelib.php: call to debugging()  line 734 of /lib/pagelib.php: call to moodle_page-><USER>get_context()  line 1342 of /lib/weblib.php: call to moodle_page->__get()  line 1223 of /lib/grade/grade_item.php: call to format_string()  line 575 of /lib/conditionlib.php: call to grade_item->get_name()  line 525 of /lib/conditionlib.php: call to condition_info_base::get_grade_name()  line 131 of /lib/conditionlib.php: call to condition_info_base::fill_availability_conditions_inner()  line 1064 of /course/lib.php: call to condition_info::fill_availability_conditions()  line 617 of /lib/modinfolib.php: call to get_array_of_activities()  line 450 of /lib/modinfolib.php: call to course_modinfo::build_course_cache()  line 411 of /lib/modinfolib.php: call to course_modinfo->__construct()  line 2003 of /lib/modinfolib.php: call to course_modinfo::instance()  line 2837 of /lib/moodlelib.php: call to get_fast_modinfo()  line 42 of /<USER>lesson/view.php: call to require_login()  {code}    Weird, because it is require_login() the function being executed.    Ciao :-)""",Bug,"Course,Caching"
343912,"""Given the <USER>grrr contrib plugin (or subplugin) as example, right now it's possible to specify a custom css sheet:    <USER>grr/styles.css    And also it's possible to specify custom css by current theme (assume standard):    <USER>grrr/styles_standard.css    The 1st is used everywhere (both in core and contrib plugins). And the 2nd is way less common, but still has a point in some rare situations.    But it's impossible to point to the parent themes so this does not work:    <USER>grrr/styles_base.css    With the arrival of the """"base"""" and """"bootstrapbase"""" duality in 2.5, and more if the later is going to become the """"default"""" one… we need to be able to specify any of these, per plugin:    - <USER>grrr/styles.css (to be applies to all themes)  - <USER>grrr/styles_base.css (to be applied to all """"base"""" themes)  - <USER>grrr/styles_bootstrapbase.css (to be applied to all """"bootstrapbase"""" themes)  - <USER>grrr/styles_standard.css (to be applies to """"standard"""", and any """"child"""" of it.  - <USER>grrr/styles_clean.css (to be applied to """"clean"""", and any """"child"""" of it).    I other words, we need to add support to parent themes in css_files() also for plugins, if I'm not wrong. Right now only the """"current"""" theme is supported. We should be looking for the existence of any parent there.    That's the only way I can imagine to keep things organized. Right now we are using a modules.less (and .css) in bootstrapbase but it has 2 limitations:    1) It only works for core plugins.  2) It is against the whole organization of plugins (self-contained).    So this is about to implement and document the support of parent themes css for plugins.    Ciao :-)""",Bug,Themes
344006,"""(This is a continuation/final part of the efforts I've been going through to try to get Moodle to restore my 'L' test course.)    1. The restore system does not display a progress bar for precheck, only for the main restore stage. As a result it can time out during the precheck.    2. There are some aspects of precheck (and main restore) where an individual task step can take long enough that restore still times out even though there is a progress update between each step. We need to add indeterminate progress display during these steps.    Note: I've already done work on this (it needs tidying up). The L course restore completes, albeit in 4.5 hours. :)    """,Sub-task,Backup
344080,"""In latest version of moodle 2.5.1+ (Build: 20130830) and probably earlier user who has no permission (eg. guest) to view hidden categories of courses can see them as administrator - hidden (dimmed) but when category is clicked error unknowcategory is displayed.  """"Affected categories"""" are caregories without children and parent=0.  I spend hours to learn how it works an why that way (upgrading finally from v1.9) and I found a solution of this bug :). In method get_tree($id) in file coursecatlib.php    {code}   --- /tmp/moodle.org/lib/coursecatlib.php        2013-08-30 03:42:53.000000000 +0200  +++ coursecatlib.php    2013-09-04 13:49:15.000000000 +0200  @@ -581,6 +581,9 @@               } else {                   // parent not found. This is data consistency error but next fix_course_sortorder() should fix it                   $all[0][] = $record->id;  +                   if (!$record->visible) {  +                           $all['0i'][] = $record->id;  +                   }               }               $count++;           }  {code}     Is there any way to disable permanently cache? - debugging is much simpler then ;)""",Bug,Course
344134,"""I've a small patch (5 lines removed, 58 added) which add the possibility to choose CAS authentication through phpCAS::getUser() or phpCAS::getAttribute().  The patch include both the config page change to add the 2 required options and the internationalization support.    Not included in the patch is phpCAS 1.2.1 (version included in Moodle, 1.1.3 don't support attributes properly). I've not tested more recent versions (1.2.2, 1.3.0, 1.3.1 or 1.3.2) but they should work just fine (and phpCAS web site recommend to upgrade to latest as there are several security issues fixed).  {noformat}  --------------BEGIN-DIFF------------------  diff -ruN old/auth/<USER>auth.php new/auth/<USER>auth.php  --- old/auth/<USER>auth.php 2013-08-28 11:00:00.000000000 +0200  +++ new/auth/<USER>auth.php 2013-08-28 14:30:21.000000000 +0200  @@ -61,7 +61,11 @@        */       function user_login ($username, $password) {           $this->connectCAS();  -        return phpCAS::isAuthenticated() && (trim(textlib::strtolower(phpCAS::getUser())) == $username);  + if ($this->config->casusersource == """"getUser"""") {  +            return phpCAS::isAuthenticated() && (trim(textlib::strtolower(phpCAS::getUser())) == $username);  +        } else {  +         return phpCAS::isAuthenticated() && (trim(textlib::strtolower(phpCAS::getAttribute($this->config->casuserattribute))) == $username);  +     }       }          /**  @@ -115,7 +119,11 @@              if (phpCAS::checkAuthentication()) {               $frm = new stdClass();  -            $frm->username = phpCAS::getUser();  +            if ($this->config->casusersource=='getUser') {  +                $frm->username = phpCAS::getUser();  +            } else {  +         $frm->username = phpCAS::getAttribute($this->config->casuserattribute);  +         }               $frm->password = 'passwdCas';               return;           }  @@ -280,6 +288,12 @@           if (!isset($config->casversion)) {               $config->casversion = '';           }  +        if (!isset($config->casusersource)) {  +            $config->casusersource = 'getUser';  +        }  +        if (!isset($config->casuserattribute)) {  +            $config->casuserattribute = 'samaccountname';  +        }           if (!isset($config->baseuri)) {               $config->baseuri = '';           }  @@ -366,6 +380,8 @@           set_config('hostname', trim($config->hostname), $this->pluginconfig);           set_config('port', trim($config->port), $this->pluginconfig);           set_config('casversion', $config->casversion, $this->pluginconfig);  +        set_config('casusersource', $config->casusersource, $this->pluginconfig);  +        set_config('casuserattribute', trim($config->casuserattribute), $this->pluginconfig);           set_config('baseuri', trim($config->baseuri), $this->pluginconfig);           set_config('language', $config->language, $this->pluginconfig);           set_config('proxycas', $config->proxycas, $this->pluginconfig);  diff -ruN old/auth/<USER>config.html new/auth/<USER>config.html  --- old/auth/<USER>config.html 2013-08-28 11:00:10.000000000 +0200  +++ new/auth/<USER>config.html 2013-08-28 11:18:13.000000000 +0200  @@ -6,6 +6,12 @@   if (!isset ($config->hostname)) {       $config->hostname = '';   }  +if (!isset ($config->casusersource)) {  +    $config->casusersource = 'getUser';  +}  +if (!isset ($config->casuserattribute)) {  +    $config->casuserattribute = 'samaccountname';  +}   if (!isset ($config->port)) {       $config->port = '';   }  @@ -226,6 +232,33 @@           <?php print_string('auth_cas_logout_return_url', 'auth_cas') ?>       </td>   </tr>  +<tr valign=""""top"""" class=""""required"""">  +    <td align=""""right"""">  +        <?php echo html_writer::label(get_string('auth_cas_casusersource', 'auth_cas'), 'menucasversion'); ?>:  +    </td>  +    <td>  +        <?php  +             $cassources = array();  +             $cassources[""""getUser""""] = 'getUser';  +             $cassources[""""getAttribute""""] = 'getAttribute';  +             echo html_writer::select($cassources, 'casusersource', $config->casusersource, false);  +             if (isset($err['casusersource'])) { echo $OUTPUT->error_text($err['casusersource']); }  +        ?>  +    </td>  +    <td>  +        <?php print_string('auth_cas_sourcetip', 'auth_cas') ?>  +    </td>  +</tr>  +<tr valign=""""top"""" class=""""required"""">  +    <td align=""""right""""><?php print_string('auth_cas_userattribute', 'auth_cas') ?>:</td>  +    <td>  +       <input name=""""casuserattribute"""" type=""""text"""" size=""""30"""" value=""""<?php echo $config->casuserattribute ?>"""" />  +       <?php if (isset($err['casuserattribute'])) { echo $OUTPUT->error_text($err['casuserattribute']); } ?>  +    </td>  +    <td>  +        <?php print_string('auth_cas_user_attrtip', 'auth_cas') ?>  +    </td>  +</tr>   <tr>      <td colspan=""""2"""">           <h4><?php print_string('auth_ldap_server_settings', 'auth_ldap') ?></h4>  diff -ruN old/auth/<USER>lang/en/auth_cas.php new/auth/<USER>lang/en/auth_cas.php  --- old/auth/<USER>lang/en/auth_cas.php 2013-08-28 11:00:27.000000000 +0200  +++ new/auth/<USER>lang/en/auth_cas.php 2013-08-28 11:15:21.000000000 +0200  @@ -64,3 +64,7 @@   $string['CASform'] = 'Authentication choice';   $string['noldapserver'] = 'No LDAP server configured for CAS! Syncing disabled.';   $string['pluginname'] = 'CAS server (SSO)';  +$string['auth_cas_casusersource']='CAS User Source';  +$string['auth_cas_sourcetip']='Use getUser for standard CAS auth or getAttribute to use a specific attribute returned by the CAS server';  +$string['auth_cas_userattribute']='CAS attribute holding username';  +$string['auth_cas_user_attrtip']='If getAttribute used, specify the CAS-returned attribute to use';  {noformat}""",Improvement,Authentication
344162,"""In our Moodle installation, we use a contributed plugin which is available in the Moodle plugins repository. And we made some changes to the plugin so that it matches our needs better.    Now, when a new version of the original plugin appears in the Moodle plugins repository, our Moodle reports that a new version is available (based on the version number in the Moodle plugins repository which is higher than the version number in our locally installed plugin). That's great for normal / unchanged plugins, but not for modified plugins.    I would be grateful if we could have a possibility to prevent certain plugins from being mentioned in the plugin updates list and to prevent them from being updatable through the automatic plugin update mechanism (without disabling the whole plugin check system as a whole which could be done already with $CFG->disableupdatenotifications).    How about a new option to include in the plugin's version.php like  $plugin->updateable = 0;  or a global option in config.php like  $CFG->disableupdateforplugins = '<USER>checklist,block_people';    Thanks in advance  <USER>",Improvement,Admin
344243,"""First, this only really affects administrators with debug developer turned on.    But, as developers, I would think that 90% or more of the time, we are logged in as the admin user and have debug maxed out.    OK, onto the problem: admin_category::add() has a statement where if debugging is turned on, then it attempts to find an admin page with the same name and print out a debug statement.  Attaching webgrind screen shots of the performance of this function.    The short story, with this code enabled, its total inclusive cost is 40% of the page load.  Without this code enabled, its total inclusive cost is .55% of the page load.    Please perform your own profiling, sometimes things are funny with performance, but some of the raw numbers stand on their own (Like how admin_settingpage->locate is called 46,000+ times).""",Bug,Admin
344255,"""Sorry if this has already been reported I tried searching for it on tracker but every time I searched I got an error from tracker.    I've encountered this error a couple of times while upgrading my master development site after the release of the weeklies.""",Bug,Libraries
344259,"""It seems that Moodle  does not respond with a 302 in case of a failed login. Ideally a user should be redirected to the next page via a 302 Redirect.    In case of a failed login a user gets a 200 OK response from the server.   Now as there is no 302 redirect the browser will store this request.    I consider it as a vulnerability because.  1. Users may often miss type a letter or a number in their password. (Eg. user types password as Harrt@123 instead of Harry@123)  2. Users may often miss type a letter or number in their usernames as well. (Eg. user types admim instead of admin).  3. In commonly shared machines if an attacker gets hold of these requests he/she can easily guess the victims actual credentials.    I would like you to please redirect (302 redirect) any user irrespective of whether he/she has entered the correct credentials or not. (Eg. Moodle can redirect a user to the """"Invalid Credentials"""" page instead of issuing a 200 OK)     I have tested this on version 2.5.1, but i assume all the other versions may be affected.   --   Cheers :}""",Bug,Authentication
344347,"""My site has hundreds of courses and I can't seem to find where I track students' submissions sitewide. It is highly impractical for me to constantly be opening every activity in every one of the hundreds of courses to check the submissions tab.     I have email notifications turned on everywhere that allows it (assignments, forum subscriptions, etc.), but not everything has an email notification option. And even if there were, this isn't an efficient way to handle this task    I have been looking at the logs to keep up with it so far, but again this is highly impractical as the logs track every single page view for every single user and the more users that join and become active - the more cluttered and useless logs become for tracking activity.    In order to grade assignments, track students' progress and keep my site fully interactive I really need a way to track all submitted activity.     I would think that this would be a function that would be absolutely NECESSARY for Moodle installations on a University scale!    Ideally the function would work like this:    In reports (or other relevant section of the Administration) there would be a """"completed activites"""" link that would pull up a table with usernames, the course name, the activity name, a link to the completed activity for grading or commenting, the date of completion, the grade if applicable, a """"needs grading"""" or """"needs attention"""" setting letting the teacher know that a response is required.    Thank you!""",Improvement,Admin
344377,"""we have an ajax method to set a user pref but no way of getting the existing pref.    It would be good if we could implement M.util.get_user_preference to match M.util.set_user_preference()    I'd like to use this to replace the use of cookies to mange debugging in SCORM.    Of course - I'm open to other ideas on how to remove the use of cookies from SCORM debugging if someone has a better idea?  """,Improvement,JavaScript
344492,"""Using the ajax quick edit of an activity name does not also update the corresponding calendar event name if a due date was applied.    Current behaviour:  1. Create an assignment with a due date into the future, and confirm after save that the assignment appears in the course calendar and upcoming events.  2. Do a 'quick edit' to change the assignment name (using ajax)  3. Check the calendar and upcoming events and note that the event name has not been changed. A page refresh (F5) doesn't resolve anything.    Expected result:  I would expect that the calendar event would have the title updated automatically. If the title of the assignment was changed via the traditional method (full edit - not ajax), then the event title is properly updated.""",Bug,"Assignment,JavaScript"
344586,"""I've been beavering away in the back of my mind some alternatives to the current course editing layout.    I've put some mockups together (I'll attach).    These will be blocked by a lack of JS forms - [~<USER> suggested we could look at a web service wrapper of some kind to do this instead though...    This is only a static (ish) mockup at the moment and the functionality isn't present, but it's getting there.    Other notes:  * It should be possible to drag a module straight to its intended new location - not just the end of a section  * needs a paginator for excess modules  * should be resizable when not collapsed (to make more columns - this will work, I just haven't plugged the resize plugin to the drawer node yet  * you can flip the drawer from right to left for when you want to add a block to the right-hand side of the page  * perhaps a bottom version too for portrait tablets? (this was my first idea, but I didn't implement it immediately as it doesn't fit well with widescreen laptops)  * what about smaller devices (e.g. phones) should we support this kind of interface on a phone?  * the search box will implement the YUI AutoComplete like MDL-37325  * this will replace the existing activity chooser entirely  * need to add a margin/padding to the page-content somehow so that when the drawer is collapsed you get the full page    Further development for the future:  * I'd like to extend the concept to remove the 'Add a block...' dropdown and make this handle them too. We'd add a tab at the top to switch from Modules to Blocks to ??? (we need both Icons, and Help for blocks - we should encourage developers to start providing these ASAP - e.g. 2.6)  * We should be able to increase and decrease the number of sections here too. Preferably by dragging and dropping them into their *desired* location rather than popping them on the end. The ability to delete a section from the middle also fits here.  * Once we have JS forms lib or equivelant, we can modify the editing cogs to pop this open as a JS dialogue - advanced can probably still go back to the existing editing from for the moment. This could replace the quick rename though?""",Improvement,"Course,JavaScript,Usability"
344665,"""I like the way the Grid-format works but I would like this to work only on a section on the course.    Add an option to the Page Resource so it will open as a lightbox/shadebox and not a new page.  Or create a new resource 'Lightbox Page'.    I don't know how this would work if the Gird-format is being used.""",New Feature,JavaScript
344736,"""I have tried forums, and I've tried Google, and I've tried StackOverflow.com and have had NO answers. I've seen several posts from people with similar problems.    Fresh install of Moodle 2.5.    I installed successfully, I thought. I have not customized the theme, so it can NOT be theme-based.    TinyMCE's File Picker is not loading...whether trying to upload a file or creating/editing a course under the Course summary files section...all I get is the """"Loader"""" star. I did some research and found no solutions that worked. Quite a few <USER>""""custom themes,"""" but I have no custom themes yet. Now, in checking the console, I got these errors:    Uncaught SyntaxError: Unexpected Identifier /lib/editor/tinymce/tinymce/3.5.8/tiny_mce.js:2  Uncaught ReferenceError: tinymce is not defined module.js:1  Uncaught ReferenceError: tinyMCE is not defined yui_combo.php?moodle/1374671241/editor_tinymce/collapse/collapse-min.js:1    Digging further, I found the error on tiny_mce.js with the """"uncaught syntax error"""" is around this area (I beautified the javascript for better readability and it ended up on line 3344):    {code}  pi: function(h, i) {     if (i) {       c.push("""""""")     } else {       c.push("""""""")     }     if (a) {       c.push(""""\n"""")     }   }  {code}    I also have no clue why the page is not seeing TinyMCE as defined.    Please help, this is urgent.    Also: In my browsing of multiple sites, one person mentioned being able to open the file picker in Moodle 2.5 when logged in as a regular non-Moodle user. They, too, have no answer to the problem, but I tried that and the file picker did, indeed, open.    This, however, doesn't help my situation as I absolutely need to have the file picker load up for the admin.""",Bug,JavaScript
344874,"""We recently had a couple of students on a distance learning course who didn't realise that by changing their profile Timezone setting, it would display their assignments local to their current time. As a result, they submitted five hours late and blamed this on their having set the Timezone field without understanding it's meaning :|    I'd like to suggest we explain what it does. Perhaps something like:    """"The Timezone setting is used to display all dates and times in moodle relevant to your current time zone. This will affect all dates and times, for example your assignment due dates""""""",Improvement,"Language,Admin"
345078,"""Extra profile fields marked as required in signup page are shown as optional into the new user account form.  If i create a new profile field and set it as required in the signup page i expect to have it as a mandatory field during the account creation.    What I obtain is that the field is _not_ set as mandatory but as optional.    The problem seems to be localized into user/profile/lib.php in two functions:  In function edit_field_set_required(&$mform) the condition   {code}  if ($this->is_required() && ($this->userid == $USER->id))   {code}  is always false. the $this->userid== $USER->id can't be matched (if i'm creating a new user how can i verify the user id?).  The second problem is caused by the """"is_required"""" function in the same file.   The casting (boolean) seems to not work correctly.  wORKAROUND (that seems to work in the signup page) is to modify the is_required function changing:  {code}  return (boolean)$this->field->required;  {code}  into   {code}  return $this->field->required;  {code}  and changing the condition in function edit_field_set_required modifying:  {code}  if ($this->is_required() && ($this->userid == $USER->id)) {  {code}  into  {code}  if ($this->is_required()) {  {code}    I don't know how this change can affect the behaviour of moodle with user profile fields  Is there a better solution for this problem? Thanks a lot.  Giovanni""",Bug,Authentication
345125,"""Under a SSO portal, I wanted, as 1.9 was able to do, to make a direct link to a course a user is enrolled in.  Moodle and the portal are under  the same SSO. Moodle has """"autologinguests"""".    First try, link to : moodle/course.php?view.php&id=999 leads to a page enrol.php with a message about anonymous user.    Second try, link to : CAS_URL/login?service=moodle/course.php?view.php&id=999 to force it : same result.    After some time in the code, the redirects are done in moodlelib.php:require_login() function.  This one is done in the if at l.3121    I fixed it but I'm really not sure if it is the good way to do it.  I replaced  redirect($CFG->wwwroot .'/enrol/index.php?id='. $course->id);  by  if (isset($_GET['ticket'])) {   redirect($CFG->wwwroot .'/login/index.php?gateway=true');  }  else {   redirect($CFG->wwwroot .'/enrol/index.php?id='. $course->id);  }  because when I force the URL to go through the CAS, I get a ticket from it, so I can use it as a proof I'm logged in but out of Moodle, in the SSO.    Hope this will help.      EDIT (28/08) : this fix doesn't work, I tried too quickly before to go on holidays !  But adding the test on the redirection tests      if ((isset($_GET['ticket'])) or ((!isloggedin() or isguestuser()) && !empty($SESSION->has_timed_out) && !$preventredirect && !empty($CFG->dbsessions))) {          if ($setwantsurltome) {              $SESSION->wantsurl = qualified_me();          }            redirect(get_login_url());      }  works. Maybe it's better to separate this test.""",Bug,Authentication
345150,"""I have a addon that uses the Moodle Excel library to create spreadsheet files on the server and then emails them out to relevant users.    During the upgrade to Moodle 2.5, I've discovered that MDL-35356 has removed the option of saving the generated file on the server - instead it immediately outputs the headers and the file contents.    The same fix has also removed the ability to set custom formats and replaced it with a very narrow set of predefined formats (in my case I need a 'h:mm' format, the only options available are date-only or date + time).    I can work around these issues by subclassing the Moodle Excel class, but it would be easier if this was fixed in core.""",Bug,Libraries
345207,"""I'm struggling to work out how to replicate this, but I do have a fix.    Basically, it seems that if the source course has a section with an image inline, then that image is sometimes included twice in the backup. As a result, it's inserted twice by restore_dbops.class.php and because of unique key constraints on the pathhash, an exception is thrown.    It's easy to fix (check for the file first) but I'm struggling to write test instructions because I can't reliably replicate the issue. It seems to occur on a course if you've added a file to a section, and then later modified it.""",Bug,"Backup,Files API"
345356,"""This value is housed in two locations. The value is 10 by default and you can change it in the php file but that doesn't stick with upgrades.     It is currently treated as a user preference but I've had several institutions request the setting change on a system wide level.     New Assignment Type:    <USER>assign/locallib.php  Line 2483 -  $perpage = get_user_preferences('assign_perpage', 10);      Old Assignment Type:    <USER>assignment/lib.php  Line 856 -  $perpage = get_user_preferences('assignment_perpage', 10);      Ideally these settings would be housed under:    Site Administration -> Plugins -> Activity modules -> Assignment    Site Administration -> Plugins -> Activity modules -> Assignment (2.2)              """,Improvement,Assignment
345489,"""Moodle currently has a """"Bootstrapbase"""" theme, which attempts to map the Bootstrap toolkit to Moodle. That theme is hidden and another theme called """"Clean"""" inherits from it. Initially this was to give people a theme to clone, but possibly it's goals have changed since then.    There appears to be at least one school of thought that believes the default theme (which Clean may become in 2.6) should be familiar to users of the previous default theme, and so a variety of """"bugs"""" in Clean are being fixed by porting things directly from Base/Standard.    There's another trend suggesting that now we have a Bootstrap-based theme we can use our knowledge of Moodle's audience and goals to improve it further and not feel bound to follow the decisions made by upstream Bootstrap and can create our own solutions.    Both of these are entirely sensible points of view, but without at least one theme in Moodle that aims to follow upstream Bootstrap closely, there's no clear indication that Moodle is actually capable of supporting a Bootstrap theme. It's fairly clear that Moodle can support the traditional Moodle way of doing things as it's been hard coded into Moodle for a long time. It's far less clear that it has the flexibility to support other approaches without extreme effort.    External parties who hear that Moodle supports Bootstrap themes will expect to be able to use their existing Bootstrap skills, knowledge and resources to build Moodle themes, possibly to match with other systems that are similarly customized with Bootstrap-variant themes and tools.    I'd quite like to work on improving core Moodle's ability to be themed, and a straightforward Bootstrap theme gives an easy way to measure progress. """"Does it look like Bootstrap?"""" yes/no. The tools and techniques that are created should be immensely helpful to people who want to make a Moodlier Bootstrap theme, or an improved Moodle/Bootstrap theme, but working on all three goals together in a single theme just creates distracting noise.    This work can't be done externally from core Moodle since the required changes are mostly in Moodle's core code.    (I've also just added a similar issue about adding a YUI Pure them to core Moodle. Very similar issues apply and much of the work would be shared)""",Improvement,"Themes,HTML,Policy"
345608,"""On action=viewpluginassignsubmission, the plagiarism library isn't called.  Just a list of files is shown.    I'd like to see the file details for plagiarism so our customers can see the reports generated for this files.    -- Reproduce --  * create an assignment  * submit max files (atleast, I submitted 20)  * view the submitted files as a teacher  """,Improvement,Assignment
345649,"""In testing MDL-39884 I discovered the menu items for Moodle .org where presenting as alt names and not the correct titles like About, Community, etc instead they show as overviewabout, overviewcommunity, etc...etc...    I've reported this in MDLSITE-2297 as it is currently affecting Moodle Forums when seen on Mobile device as Clean theme must be set for Mobile and Tablet in theme selector.    I thought at first it was a new menu, but see it's not.    """,Bug,Themes
345662,"""If you create three topics the default names will be Topic 1, Topic 2, Topic 3.    If you use drag'n'drop to re-order these Topics then the names will be changed so that no matter what re-ordering you do the first one will be called """"Topic 1"""", the next """"Topic 2"""" and then """"Topic 3"""".    If you change the names to be customised but still end in numbers e.g. """"Something part 1"""", """"Something part 2"""" then when drag'n'drop re-ordering the names will sometimes(!) have the last digit changed in the same manner as above.    I'd got reports of this on our 2.3 server, the reporter could reproduce it on our 2.4 test server and I just recreated it on qa.moodle.net which claims to be running a very early 2.6.    There seems to be various weird bugs surrounding drag'n'drop topic re-ordering and maybe the all have the same underlying cause but this seems to be a new symptom at least.""",Bug,"Course,JavaScript"
345770,"""This is not a bug, it is more of a """"code cleanup"""" suggestion.    For awhile (at least back to Moodle 2.4, and maybe earlier), when I do a bulk delete of users (with debug on), I get some error messages similar to what I have attached.  Everything works fine, but I just thought that I would bring this to your attention just in case some other problem is occurring.    Steps to reproduce:  # Log in as admin and turn debugging on  # Go to Site administration > Users > Accounts > Bulk user actions  # Select few users  # Select delete option and confirm.  You will see following notice.  {noformat}  You should really redirect before you start page output  line 728 of /lib/outputrenderers.php: call to debugging()  line 2540 of /lib/weblib.php: call to core_renderer->redirect_message()  line 38 of /admin/user/user_bulk_delete.php: call to redirect()  {noformat}    """,Task,Admin
345886,"""When viewing assignment submissions, users are given a dropdown to select numerous available actions to perform on the selected submissions.  The """"grant extension"""" option shows up regardless of the <USER>assign:grantextension capability.  If a user without the capability chooses the """"grant extension"""" option, they will will be presented with the form to grant an extension, but when they submit it they will see a capability error and the extension will not be saved.    I've attached a patch file for 2.4, but it is not necessarily the """"proper"""" way to fix it.  To make this fit more into the rest of the code design, perhaps the form definition shouldn't have any capability checks, but instead call to the assignment to add the extension option if its available.  eg. $assign->add_extension_elements() or something.    Patch attached as a starting point.""",Bug,Assignment
345953,"""The 'Clean' theme normally has adequate spacing around buttons, however in places this gets lost and the button actually touches the field next to it, e.g. a text entry field. It's only a visual issue but it detracts from the professional look and feel that 'Clean' brings to Moodle. I've seen this in a few places and trying to search them out, but there is an example in the Course Administration > Badges > Add a New Badge page. """,Bug,"Badges,HTML"
346015,"""Now that Moodle has a new logo (<USER> moodle.net and soon moodle.org), we should use the same in core.    - On the front page (100x30)  - The favicon in standard theme  -- Should we make a copy from theme/clean/pix/favicon.ico?  -- Is the standard for favicon a 16x16 .ico file? I've read that .ico could be .png renamed.    Idea: Place the favicon in pix/ to have a default favicon for all the themes.""",Improvement,Usability
346120,"""Inspired by <USER>and <USER>s work, I've just been having a play with xh_prof.    I noticed that in admin_category we use $this->category_cache to cache records.     This is a protected variable, which is set to an array in the constructor, and also in a purge function. The only operations on it are:  * $this->category_cache = array();  * isset($this->category_cache[$key])  * $this->category_cache[$key] = $value;  * return $this->category_cache[$key];  * while($this->category_cache)  * array_pop($this->category_cache);    In every function which accesses it, we still call is_array($this->category_cache) even though it is only every treated as an array.    On a default install of Moodle with no additional plugins, the locate() function is called around about 8,600 times with up to two is_array() calls.    By removing this, I'm seeing savings of around about 100ms on my laptop.""",Sub-task,"Performance,Admin"
346129,"""The ID number of assignment grades items is overwritten every time a student edits a submission or a teacher grades a submission. Only the ID number of the grade item changes, the ID number of the assignment does not.     Steps to reproduce:    # Create/edit and assignment and set the ID number to """"A"""".  # Edit the grade item of the assignment (in Grades -> Categories and items -> Simple view). The ID number is """"A"""".  # Edit a submission as a student or grade a submission as a teacher.  # Edit the assignment settings. The ID number is still 'A'.  # Edit the grade item of the assignment. The ID number is set to a number.    I've traced the bug to function {{gradebook_item_update}} of {{<USER>assign/locallib.php}}. It updates the grade item, using the course module id as id number.    """,Bug,Assignment
346233,"""With the recent commit from MDL-38565, a new support option was added, {{IS_SEARCHABLE}}. However, the way this """"SUPPORT"""" is detected differs than other options.    Since this feature implements a new interface, I would expect support for it to act similarly to """"key awareness"""" and """"native locking"""". Those do not have a corresponding cache_store::SUPPORT_* option but rather are determined by the stores implementation of {{cache_is_key_aware}} and {{cache_is_lockable}}.    Therefore, I would recommend removing the {{cache_store::IS_SEARCHABLE}} option and change the support detection to how key awareness and locking works. At the very least, rename the constant to {{cache_store::SUPPORT_IS_SEARCHABLE}} to match the naming convention.""",Improvement,Caching
346302,"""I need a custom user profile field that will be editable by only admin and it will be read only for students and their guardians.  I need to implement registration number with this custom profile field that should be unique for each student. But in case of guardian it should be either empty or it should contain some default value(Say 0). Now problem is when admin update guardian profile(Other info than registration number) there is validation error is showing    """"This value has already been used.""""    How can we use default values with uniqueness? I need to profile field type """"text input"""".""",Bug,Admin
346410,"""The error messages on login are styled as an alert (a colored box), but is currently wrapped in a span, which is display: inline by default. This causes visual problems when the text gets long enough (or the screen narrow enough) to cause line wrapping.    One option is to set display: block on the span but then you need to constrain it's width.    The other option is to remove the alert styling and just have red text.    For the short term I'd probably favour the latter.      """,Sub-task,HTML
346470,"""When using Bootstrap/Simple theme the 'Participants' and 'Question Bank' nodes are links, but clicking on the trinagle should also expand them. However, they aren't working and take you straight to the target.    In the standard theme, the anchor is only the width of the text, and the li is clickable. In the BS theme, the anchor is the full width of the block.    I'm going to say that this is (initially at least) not a JS issue as we need to have something to click on. The current system relies on CSS classes to change the triangle to show whether the nav node is expanded or not, and as a result we only have the <li> that the class is applied to apply event handlers to.""",Bug,"Themes,JavaScript"
346510,"""I've been looking at Moodle's CSS a lot recently, and if I had to point to one core issue with the current CSS, I'd say that's there simply too much of it.    This has two key impacts: complexity for developers, and size for browsers.     There's simply too much CSS in Moodle for any one person to understand. What is there contains repetition, near-duplication, unused code as well as simple bugs and typos. It's not well laid out or formatted, it's often over specific which makes it hard to override in themes. Mostly it's like looking at the CSS of a collection of vaguely related websites rather than one web app. This makes it very hard to work on, which encourages people to avoid what's there and just do their own thing, creating a vicious circle which leads to mounting technical debt as well as simple bulk.    That bulk then leads to issues on the receiving end. Every line needs to be compressed and sent across the internet, possibly over slow mobile networks and then stored in browser memory and each line scanned for each item on the page (again possibly on low powered phones).    For end users of Moodle these issues translate into Moodle being uglier, slower, harder to use and buggier than it needs to be.    I don't think there's any one easy solution to this, but there's plenty of smaller things that together would add up to a noticeable difference. So here's a list of things that could be done in the 2.6 dev cycle to reduce the amount of CSS in Moodle. Note that often, in order to reduce CSS you need to fix the HTML that it applies to as well.    It's just a list in no particular order, when/if work begins then sub-tasks that can be prioritised would probably make more sense:      1. Locate and remove CSS that no longer has corresponding HTML. There's lines in core CSS aimed at IE5 for example. (There's tools to automate locating unused CSS like this they'd possibly need tweaked for a dynamic application like Moodle)    2. Reformat the CSS so that it's readable. Currently it's half compacted and difficult to read and edit, possibly a holdover from the days before we had a CSS compressor built in.    3. Reformat the CSS so that it's more compressible. Well formatted CSS that does the same thing in the same manner is easier for gzip to compress. There's tools that will help with this.    3. Include a new, better CSS minifier. Currently we have two, one that doesn't do very much and an experimental one that sometimes breaks things. The key feature of the latter is (I believe) that it removes duplicated CSS. We should avoid having duplicate CSS in the first place. There's 3rd party code that tackles this tricky problem well and is already used across the web. (edit: note I found out later that this isn't actually necessary. Moodle already includes a good quality, 3rd party minifier, just in a slightly odd way that made it non-obvious to find. That and the fact that a new homegrown one was being written, confused me into believing they didn't already have a very good one included in the codebase.)    4. Standardise the component HTML of Moodle. There may be as many as 20 different styles used to layout forms in Moodle, I lost count at some point after 10. These should all use the same styles as far as possible.    5. Minimise admin styles. Moodle doesn't have an admin/user split like some other web apps. This means all admin styles get sent to every user (and all user styles get sent to every admin). A specific effort should be made to minimise the variation of the admin interface to reduce this load.    6. Use a CSS grid. This saves having to specify floats repeatedly.    7. Debug code shouldn't have any unique CSS styles. Debug styles get sent to every user, but probably get seen on 1 in a million page views or less. If it can't be done with CSS that's already in the theme, then in extreme cases inline styles are appropriate for this use case. A simple first step would be to move all the debug code into a single file that can be excluded from the theme if necessary.    8. Add variables to CSS preprocessor so themes can override parts of the core theme by supplying new variables (colors, font faces & sizes) rather than repeating the CSS.    9. Officially deprecate IE7. Currently 2.5 doesn't support IE8, but that doesn't mean it intends to be totally broken, just that bugs aren't such a high priority. IE7 needs a bunch of workarounds for inline-block etc. We should removed these and drop actively drop support for it. (Bootstrap 3 only supports IE8)    10. Upgrade to Bootstrap 3. This version is more focussed and streamlined which should lead to smaller CSS.     11. Fix the two main causes of extra CSS for the Bootstrap theme: make a renderer for buttons, <USER>standard simple tables with a .table class. Then continue with the next largest wins.    12. Fix bugs. There's lots of little issues that, mostly as a side-effect, have an impact on the amount of CSS output. These should be tagged in the bug tracker to allow their impact on CSS to be considered in their level of priority.    13. Break the styles into smaller files with related content. This helps you find existing styles that fit together, and any needless duplication.    14. Find a better way to do RTL. Some of this is just an extension of the above. If you have a rule for ie5 then you might also have an RTL rule for ie5, which doubles the impact. But also, there should be a system so that RTL CSS doesn't include LTR CSS and vice-versa. Currently we just tag the RTL stuff onto the end of the LTR stuff, which can lead to 3x as much CSS. (First the LTR code, then something to reset that, then the actual RTL code). One simple fix would be to enforce a layout for RTL add a simple preprocessor to strip it out when not needed.""",Improvement,HTML
346614,"""Has anyone started work on a WinCache store for MUC? If not, I would be willing to do so and submit a pull request when complete. Just don't want to put extra work into something which will be resolved by someone else at the same time.     Also, since PHP 5.5+ is going to include a bundled opcache extension, is it even worth making a store for WinCache? I'm not not sure if WinCache and the likes will continue to be maintained. But perhaps it would be worth it for those who stick to PHP 5.4 and below..""",Sub-task,Caching
346939,"""When an assignment has team submissions enabled, the admin user can not view images in submitted as part of online text or download a file submitted.    Reproduction steps:  # Create an assignment and enable team submissions with online text.  # As a student make a submission to the assignment and include an image in the online text (via the TinyMCE editor).  # View the submission as an admin.    Expected results:  * Admin can view the image submitted as part of the online submission.    Actual results:  * Admin can not view the image submitted as part of the online submission.    This can also be reproduced using file submissions, which is how I originally found the issue. And while I haven't tested it is likely to affect submission comments as well because it uses the method where the issue is found.    I believe the issue caused by the {{is_enrolled()}} check made in the assign class {{can_view_group_submission()}} method. The {{is_enrolled()}} call returns false for the admin, which in turn causes {{can_view_group_submission()}} to return false as well. {{can_view_group_submission()}} is used by {{assignsubmission_onlinetext_pluginfile()}} and {{assignsubmission_file_pluginfile()}}.    Possible solutions:  * Move the {{has_capability()}} check for '<USER>assign:grade' above the {{is_enrolled()}} check.  * Add a check for admins in the conditional with {{is_enrolled()}} (e.g. {{if (!$isadmin && !is_enrolled()}})    If either of these sound good (or if even you'd like to go another route), I'd be glad to supply a patch or pull request. Just let me know.""",Bug,Assignment
346963,"""I discovered that when trying to resolve MDL-37365. I needed to use a SVG file as a URL in CSS files, using the [[pix:theme]] function and its not working.""",Improvement,"Themes,Libraries"
347002,"""<USER>brought it up :   """"  Is there for an admin deleting a block which may be a dependency for something else?  I'm wondering if it might be helpful to have a database table of dependencies to check against.    Here is what happened that made me think of this: I was receiving a PHP notice for a block (in this case formal languages) which the correctwriting question type lists as a dependency.  After deleting the formal languages block I received a PHP error that prevented me from accessing the blocks page.     I think we may want to do better at protecting admins from potentially breaking their site - we do not allow addons to be installed that do not meet the dependencies, I think we should check for dependencies when attempting to delete a plugin  """"    We just need to re-invoke the dependencies check when deleting.""",Bug,Admin
347006,"""Yet again I've had to puzzle out the meaning of the pop-up Help text for Quiz 'Allow access from'. Here's a suggestion for better wording. I'm assuming this text is used in locations other than Quiz, otherwise I'd make additional changes.    Currently:  ------  Allow access from    Access from/to dates determine when students can access the activity via a link on the course page.    The difference between access from/to dates and availability settings for the activity is that outside the set dates the latter allows students to view the activity description, whereas access from/to dates prevent access completely.  ------    Suggested:  ------  Allow access from    Access from/to dates determine when students can access the activity via a link on the course page.    The difference between access from/to and availability settings for the activity is that outside the set dates, access from/to prevents access completely, while availability allows the students to view the activity description.  ------    Priority: Trivial""",Improvement,Language
347094,"""Currently, the parentable_part_of_admin_tree::add() method appends the new part as the last child of the given destination. Therefore, the order of items in the admin tree is given exclusively by the order of $ADMIN->add() calls. It would be really nice if, for example, plugins had an option to add their nodes anywhere into the tree.    My particular use case that led me to implement this was that I am working on a new admin tool and I want it appearing just below the Site administration > Plugins > Plugins overview (not as a child but as a sibling). But because the core already put other nodes there (Activities, Blocks, ...), my admin tool has got no way to inject itself there.""",Improvement,Admin
347121,"""There is currently a bug in shifter which has meant that the recursive option doesn't quite work as it should. I have a patch in to fix it and I'm hoping it gets addressed soon.    As a result though, moodle-core-tooltip was integrated with a different build (<USER>modified my commit to change whitespace and the recursive build he ran modified moodle/../)    In the mean time:  {code}  npm install shifter@0.2.15 -g  {code}    And here's a rebuild of moodle-core-tooltip with correct whitespace etc.""",Sub-task,JavaScript
347230,"""Teacher's often ask for the ability to have a resource or activity which are placed inside a meta course be automatically (or manually) linked into all sub courses. And be updated in all those courses when it is updated in the meta course. (manually or automatically depending on the content creator's management perspective)    As of Moodle 2+, we are able to link resources from one course to the other. which is a nice work-a-round to part of this issue. But then, teachers have to manually go through all the resources from within each sub-course and re-select each resource from the meta-course (and make sure they do not miss anyone, too). problematic.    What would be helpful is...  In the meta-course, Add a new page that list all the resources and activities with  checkboxes beside each one. Components that are selected, will be propagated into the sub-course(s). And from that moment on, Component updates could be pushed as frequent as it updates in the meta-course. (In case the meta-course content creator is also responsible to the content in the sub-courses) Or just available to be pulled from the sub-course(s).    An alternative and a more """"safe"""" approche, from the sub-course pov, is to request an update from the meta-course when an indication for a content updated (time <USER>change) was sent to the sub-course teachers. which enable them to decide if they wish to pull the update.    Considering this is integrated into core, my initial thoughts are to...   # Add a new field to the <USER>course_module table which is called parent_cmid which will hold the parent cmid of the course module that the current course mode was initiated from.  # On the meta-course, Add a new entry to the course setting block: """"propagate"""" - which will list the course's resources and activities with checkboxes beside each one to enable and indicate which component should be propagated into the sub-courses. (Propagate will duplicated the selected component, using moodle's duplicate function, and move it into each sub-course. then, set it's parent_cmid to the proper cmid)  # On the sub-course, Add a sub-course view to the """"propagate"""" course setting - that lists all the current course components that could be updated with new settings/content from the meta-course. And use checkboxes to select which ones are to be updated, manually by the sub-course teacher.  # Add some kind of indication to the teacher in the sub-course, that the current component is linked to a parent component in the meta-course.    Teacher in the sub-course can always change contents and settings for those propagated components. And can always overwrite them with new and updated settings from the meta-course.    I imagine this could also be done as a plugin with a separate tables and a block for managing it on a course level. but I am not sure if this is the right way to go.    I would LOVE to get feedback and ideas how to implement this properly into Moodle.""",New Feature,Course
347259,"""Scorm in Moodle 2.4 has an error when review a lesson.    Reproduction steps:  - Attempting a Scorm lesson the first time, everything goes well.  - Enter that lesson again with Start new attempt checked => fine  However, enter that lesson again with Start new attempt unchecked => error    The problem is in the player.php file, scorm_get_toc is expected to always return a sco object:  {code}  $result = scorm_get_toc($USER, $scorm, $cm->id, TOCJSLINK, $currentorg, $scoid, $mode, $attempt, true, true);  $sco = $result->sco;  ...      if ($trackdata = scorm_get_tracks($sco->id, $USER->id, $attempt)) {    {code}    However, the scorm_get_toc function return either a sco object or an array of sco objects:  {code}      if (empty($scoid)) {          $result->sco = $scoes['scoes'][0]->children;      } else {          $result->sco = scorm_get_sco($scoid);      }  {code}    Therefore, when reviewing, the array of scos is returned and it cannot select any sco to display. Placing an if like below seems to work but I'm not sure it works in every case, since I'm not familiar with the scorm specification.  {code}  $result = scorm_get_toc($USER, $scorm, $cm->id, TOCJSLINK, $currentorg, $scoid, $mode, $attempt, true, true);  if (is_array($result->sco)) {      $sco = $result->sco[0];  } else {      $sco = $result->sco;  }  ...      if ($trackdata = scorm_get_tracks($sco->id, $USER->id, $attempt)) {...    {code}    """,Bug,SCORM
347283,"""We have recently moved to Moodle 2.4 (from 2.2) and while I love the additional options, I keep having discussions with 'normal' teachers who still regard moodle as un-intuitive and frustrating to navigate. They understand that it is very powerful, but feel overwhelmed by the mass of buttons and options throughout pages, just to get started in a new course.    Instead of all teachers being given the """"Course Creator"""" role I have been considering if it was possible to create a """"Simple Course Creator"""" role which I could assign to teachers by default.    This role would not have as many options in the Course Settings page, and in particular, would get rid of the complexity of needing to set the enrolment key inside enrolment methods, as well as enable self-enrolment etc. (and the other issue around enrolment methods that I have been encountering is teachers accidentally clicking the hide button next to manual enrolments, therefore disabling their own access to the course!!).    Of course, underneath everything would be the same, but by simply adding an """"enrolment key"""" in the settings page, it would automatically enable self-enrolment and apply that key.    It would be good if the Administrator/Manager was able to customise what this role could do, and perhaps make some decisions based on the situation as to which settings are already decided upon and which can be modified (ie. we pretty much only use 'topic mode' for courses, so I would just remove that as an option to reduce the complexity). Or the """"Show gradebook to students"""" option, which in our situation wouldn't make sense to disable.... but I would think that if I (as an administrator) went into their course, I would be able to change the settings myself and then customise it a bit for them, or when they have mastered the basics I could make them into a """"Proper Course Creator"""" with all of the options available to them.""",Improvement,Roles / Access
347326,"""I've just freshly installed Moodle 24 STABLE.    Doing a simple editor is ridiculously inefficent and costs **100** queries for a student:  {code}  $mform->addElement('editor', 'test', 'test', null, array('maxfiles'=>EDITOR_UNLIMITED_FILES, 'noclean'=>true, 'context'=>$PAGE->context));  {code}    Here is a test file which demonstrates the issue:  {code}  <?php    require_once('config.php');  require_once($CFG->libdir . '/formslib.php');    class test_form extends moodleform {      public function definition() {          global $PAGE;          $mform = & $this->_form;          $mform->addElement('editor', 'test', 'test', null, array('maxfiles'=>EDITOR_UNLIMITED_FILES, 'noclean'=>true, 'context'=>$PAGE->context));      }  }    $PAGE->set_context(context_system::instance());  $PAGE->set_url(new moodle_url('/testform.php'));  echo $OUTPUT->header();  $mform = new test_form();  $mform->display();  echo $OUTPUT->footer();  {code}    Repositories list should not be populated and passed in JS variables three times for each textarea. Actually it should not be pre-populated at all, filepicker should do it in AJAX request when opened for the first time (if ever opened at all). Also draftareaid should be passed only once as mentioned in MDL-41046 (closed as a duplicate)""",Bug,HTML
347528,"""This task is a research project that will show where we can face the difficulties in implementing """"Parenting functions"""" in Moodle.    The main goal is to allow some users (Observers) to monitor other users progress on course(s) completion. The two common examples are:  - Parent monitoring the child  - Manager monitoring the employee on the course paid by the company    The task in endless,  there can be a lot of additional permissions, comprehensive administrative UI, endless progress reports, etc. But for the first stage we try to make just the most important features.    My vision of the implementation, open for discussion:    Stage 1 (base)    - there is a local plugin that adds a DB table that links an Observer to other users (note that one Observer may watch several students) and provides a simple interface for populating it. (*1)  - local plugin also makes Observer receiving all emails that are sent to the users he watches (*2)    Stage 2 (optional UI)    - there is some kind of report of what is going on in the life of the watched student. Different plugin types can be used for showing this - local plugins, reports, block (to be used on the side or on the /my/ page), etc. Later yourself or other developers can create different types of reports but the plugin from stage 1 will be a pre-requisite for all of them.    Stage 3 (advanced)    The goal is to allow Observer to view the course content where his students are enrolled. Better to develop as a separate plugin that can be enabled or disabled by administrator.    Autoenrollment approach:    - local plugin creates an Observer role and by default copies to it all Student's READ permissions (Observer is not allowed to submit assignments for example)  - local plugin listens to events when user is enrolled in the course, added or removed to/from the group or groupping. At the same moment the watching user will be enrolled with the role Observer in the same course/group. (*3)    Problem I can see with this approach is how to exclude Observers from course reports, such as gradebooks, recent activity, etc.      Footnotes    *1   - observer-student link may be inside a particular course or for any course. I would recommend to define both course-level and system-level permissions that allow teacher/admin/manager to view, create or change the links.   - Design admin UI extremely simple, just to allow linking. But at the same time design it keeping in mind different possible entry points for listing: observers of this student, students of this observer, all students in the course and their observers, all students-observers links in the system.    *2 - will require core changes, some hook or event, allowing plugins to do something with the sent emails. This hook must be very generic. This is a challenge in the issue.    *3 - will most likely require core changes because some necessary events may not yet be triggered. Also still Observer will not be able to see the activities with conditional access unless we hook into cm_info      Don't go too deep into the details!    Always remember - the most important outcome of this task is not creating the complete feature with advanced UI but a research what Moodle core lacks to allow implementation of Parenting functionality.  """,Improvement,Roles / Access
347668,"""Getting some inconsistent behavior with Drag and drop text/url:    Safari 6.0.2: If I drag text or text that is a URL into a course section, then it always uploads as a page.    Chrome 24.0.X: Seems to detect text or URLs (EG: says Add page here or Add link here) but fails to do anything on release of the drag and drop.    Firefox 18.0.2: Fails to detect if a page or URL and doesn't upload anything on release of the drag and drop.    The above is what I experienced.  Our QA tech was actually getting things to work just fine in Chrome but Safari and Firefox were just not recognizing URLs vs text.  I'm on OS X, the QA tech might be on Windows.""",Bug,"Course,JavaScript"
347730,"""Our staff are very pleased with the drag'n'drop they got when we moved from 1.9 to 2.3    I would like to propose a similar feature that works for mobile devices.    Obviously mice, multiple windows etc. are missing from these devices, but I'd say that the actual dragging and dropping isn't the key to why people like drag'n'drop. Instead, people have a PDF (or whatever) on their computer and they want to get it into Moodle with minimum fuss.    On the desktop drag'n'drop achieves that. You grab the file, you drop it into a course, and several small tasks are done for you. A resource is created, it's given an appropriate name automatically etc.    The equivalent on mobile would be a standard file upload button positioned at the top of the course. You press the button, it asks for a file. You choose one from your device and it does all the small tasks for you, placing it a the end of the current section (if on a section page, at the bottom of the course if on the main course page).    I'd guess all the hard work has already been done for drag'n'drop and all you need to do is pass the uploaded file into the same system. (This could also be useful for IE8 or 9 users.)    This was inspired by playing with the file picker on a variety of Android device/browser combinations. The size and position of the pop-up file-picker and various other steps of a the file upload process were very clunky, but once it actually got to the choosing a file stage it was very smooth. Even with the clunky steps I think I could get a file from my dropbox into Moodle faster on my mobile device (with no need for dropbox integration on the server). Taking a photo and uploading it would be no contest at all. I've not tried it on an iOS device yet, but I'm assuming it's a similar process.""",New Feature,"Course,JavaScript"
347796,"""First I logged into my Moodle account then I went straight into my Isothermal 2013SP MAT 161 College Algebra 970IN - <USER>course. After that I find the link to the syllabus quiz and click it. A page that asks if I want to attempt the quiz is supposed to come up but instead it says """"Sorry, but you do not currently have permissions to do that (View quiz information)."""" I know that I am supposed to be in this course and my teacher has already tried to fix this problem once but it did not work.""",Bug,"Accessibility,Assignment"
347813,"""The MyMobile theme contains the following code in 'custom.js':    {code}  //collapsed topic only stuff  $('div#page-course-view-topcollPAGE').live('pagebeforecreate',function(event, ui){      $('#page-course-view-topcollPAGE ul.section').attr(""""data-role"""", """"none"""");      $('.section li img').removeClass(""""ui-li-icon"""");      $.getScript('../course/format/topcoll/module.js');      $('#page-course-view-topcollPAGE tr.<USER>a').attr(""""data-role"""", """"button"""").attr(""""data-icon"""", """"arrow-r"""");      $('#page-course-view-topcollPAGE #thetopics').attr(""""data-role"""", """"controlgroup"""");      $('#page-course-view-topcollPAGE td.<USER>centre').each(function(index) {          var cpsc = $(this).text().replace('<br>','').replace(')','');          $(this).prev('td').find('a').append('<span class=""""ui-li-count ui-btn-up-a ui-btn-corner-all"""">' + cpsc + '</span>');      });  });  {code}    In 'core.css' the following selectors need to be removed:    {code}  /*collapsed topic format*/  #thetopics {      table-layout: inherit !important;      width: 100%;      display: block !important;  }  #page-course-view-topcollPAGE .section td.content, col.content {      text-align: left;      width: 100% !important;      overflow: hidden;  }  #page-course-view-topcollPAGE .section td.content {      padding: .5em;  }  #page-course-view-topcollPAGE td.left.side, #page-course-view-topcollPAGE td.right.side {      display: none;  }  #page-course-view-topcollPAGE tr.<USER>td a {      background: none !important;      color: inherit !important;      padding: 7px 0 7px 0px !important;  }  tr.<USER>td span {      font-style: inherit !important;      font-size: 1.12em !important;  }  #page-course-view-topcollPAGE .section.separator, td.<USER>centre {      display: none;  }  tr.<USER>{      background-color: inherit !important;      color: inherit !important;  }  .opencps .ui-btn-inner .ui-icon-arrow-r {      background-position: -216px 50%;  }  tr.<USER>td span.ui-li-count {      font-size: .6em !important;      right: 12px;  }  #page-course-view-topcollPAGE .<USER>.ui-btn-inner {      white-space: normal;  }  {code}    It is out of date as Collapsed Topics has moved on and as discussed on MDL-33115, core really should not have code that copes with contributed plugins.  I am now in a position to adapt my code both with jQuery and styles to support MyMobile but this code is redundant.    I'm raising this as a separate issue to MDL-33115 because I believe that MDL-33115 is a container for a whole range of issues that are no longer relevant and this small change would get lost within it.  A big thank you to <USER>for assisting me with it.""",Task,Themes
347859,"""(Note: I thought I might have filed this before, but can't find it.)    It would be nice if Moodle course sections could be 'owned' by a course module. This essentially allows modules to contain embedded activities. Key results for users are:    * Owned activities don't show as part of the normal course format, but are displayed by the activity somehow.    * When looking at an owned activity, the parent activity shows in the breadcrumb (hierarchical structure). You can have multiple levels if you like.    At the OU, we have two modules (subpage and an internal one that isn't released) which achieve the same result already in current Moodle by relying on 'orphan sections' within the course. This works, but has disadvantages.    Here's a quick summary of how this feature could be implemented. We at the OU would be interested in implementing it if the idea is accepted, although I'm not sure on what timescale. Maybe for 2.5 or 2.6 though?    h2. Back-end changes    * Add optional cmid field (default null) to course_sections table in database. If specified, this must be set to a course-module on the same course, and the section number must be 0.    * Change section cache to include field.    * Change existing functions which return sections so that by default they return only sections with null cmid    * Add API/parameters to modinfolib functions so that it's possible to retrieve all sections, or all sections owned by a cmid.    * Change login checks so that require_login on an owned cm also ends up requiring access to the owner cm (i.e. if the owner has a date restriction or is not visible, you can't access its children either).    * Change section name-related functions so that section name for owned activities is done by a callback to a parent activity (normally section names come from callback to course format).    h2. Navigation changes    * Make system handle breadcrumbs (course > section > owning activity > owned activity), including multiple levels of nesting.    * The parent activity (via callback) should be able to customise the breadcrumb link. For example we have cases where the breadcrumb to parent actually takes you to a specific page within the parent (the page where this child appears) and not the main view.php.    * Make child sections/activities go in the right place in nav tree as well.    h2. Backup/restore changes    * Add field to section backup/restore.    * In backup UI, show owned sections 'within' the owning activity.    * Handle options sensibly (i.e. you can't backup the owned section unless you also back up the activity that owns it).    h2. Editing UI changes    * Where not already available, add a way so that the URL to return to after editing an activity ('Save and return to website') can be customised by the code that is printing out the edit icons/links. Not just for the edit form but also for the various other similar scripts (move, delete, duplicate, etc).    * Also change the button label ('Save and return to (name of parent)').""",Improvement,Course
347879,"""I have three Moodle systems two of which I've just upgraded to 2.4.1+ (20130118). I've found that FlowPlayer is no longer working for me. I've tried multiple browsers and different machines. A slightly older Moodle system I have [2.4.1 (Build: 20130114)] has a fully functional FlowPlayer. I've even downloaded videos from that site, loaded them on the newer ones and confirmed the issue remains.    My browser's console is returning no error messages and it seems as though FlowPlayer loads, but nothing appears in it, just a black box.    Please investigate!""",Bug,JavaScript
347961,"""In Moodle 2 a lot of the notifications controls have been merged together into something called """"Messaging"""", so a user can access """"My Profile Settings"""" -> """"Messaging"""" and have full control over how they receive these """"messages"""" from the system.    There is also a system, also called """"Messaging"""" that allows people to send messages to each other. This can be turned on/off in """"Advanced Settings"""" with the following option:    name: Enable messaging system  shortname: messaging  Default: Yes    With just those settings there's scope for confusion between these two """"Messaging"""" systems. It is made very slightly clearer by the help (""""Should the messaging system between site users be enabled?"""") but I'm guessing that text was written before Moodle had 2 """"Messaging"""" systems.    This messaging system is referred to (I assume) in the last entry of the """"Messaging"""" preferences, where it says: """"Prevent non-contacts from messaging me [x]"""" under the heading of """"General Settings"""" which implies the two systems have merged.    As a final confusion, you can go into the participants list of a course, tick a checkbox next to some user names and then select the option """"Send a Message"""" to send a message.    This isn't turned off by the previously noted """"Enable Messaging System"""" option, nor is it obviously controlled by the """"Messaging"""" options in your profile. I'd quite like to be able to turn this off, as we already have systems for contacting students (individually or en masse) and this duplication leads to confusion. But so far I can't find any way to disable this feature.""",Bug,"Admin,Usability,Documentation"
347973,"""There is a possible performance issue with function notify_login_failures() the first time login failures are checked. If they were never checked previously (e.g. $CFG->notifyloginfailures was not set), $CFG->lastnotifyfailure will be empty. In such a case, the whole log will be checked as per this logic:  {code}      if (empty($CFG->lastnotifyfailure)) {          $CFG->lastnotifyfailure=0;      }  {code}    If the log is big, this potentially matches thousands of records. Even worse, if the function does not finish (e.g. because connection to DB will time out), $CFG->lastnotifyfailure will never be set to the current date. Hence the same situation will repeat with the next cron run.    As a fix, I would suggest to never check more than (say) 1 last month.""",Bug,Performance
348027,"""When a SCO has 'Student skip content structure page' set to 'Always' or 'First Access' and 'Grading method' is 'Learning objects,' then tracking data populates in the database but is inaccessible from Moodle, showing a SCORM status of """"Not attempted"""" even though complete tracking data are stored. In my investigation, the inaccessible tracking data within the '<USER>scorm_scoes_track' table is linked to the wrong 'scoid.'     There appear to be two 'scoid's associated with a single SCO as mapped in the table '<USER>scorm_scoes'. One entry has a value for every field and 'title'=""""Course Object title."""" This 'scoid' appears for all SCORM attempts that render correctly in Moodle. A second entry in '<USER>scorm_scoes' for the same SCO has many empty fields with 'title'=""""Captivate E-Learning Course."""" This 'scoid' appears for all SCORM attempts that render incorrectly. SCORM player seems to select the wrong 'scoid' to link tracking data to when skipping the content structure page.    Steps to recreate:  1. add a new SCORM activity   2. set 'Grading method' = 'Learning objects'  3. set 'Student skip content structure page' = 'Always' or 'First Access'  4. save SCORM  5. access SCORM as a student enrolled in the course  6. exit the SCO after completing it or sometime before  7. as teacher, view the SCORM report    EXPECTED:  1. attempt details to display including 'Attempt', 'Started on', 'Last accssed on', 'Score', and 'Course Object title' (showing Incomplete or Complete as a hyperlink)  2. click on 'Incomplete' or 'Complete' hyperlink to view tracking detailed tracking data for that attempt    ACTUAL  1. attempt details display including 'Attempt', 'Started on', 'Last accssed on', and 'Score'. But 'Course Object title' shows 'Not attempted' as plain text  2. no link is available under to 'Course Object title' detailed tracking data  3. clicking 'Attempt' number hyperlink shows status of 'Not attempted' and no tracking data    As I <USER>at first, I have identified the field 'scoid' in '<USER>scorm_scoes_track' as the culprit. It could be one of two values for these types of SCOES and all the errors occur when it is linked to the '<USER>scorm_scoes.title'=""""Course Object title"""" rather than the other, which is """"Captivate E-Learning Course."""" SCORM API debugging always shows expected behavior throught with no errors. The data are reported and received correctly, they just appear to be linked incorrectly and therefore inaccessible through Moodle. It all started with upgrading to Moodle 2.4, by the way. I've been through all previous versions down to 1.9 and never seen this before.""",Bug,SCORM
348076,"""Presently cache::get($key) returns false if the key was not found in any of the assigned caches and could not be loaded (if a loader is available).    Its been suggested several times now that returning null instead of false would be a better idea.    Have a read of the conversation <USER>and myself had about this in dev chat (quoted here because linking to HQ chat logs means some can't see it)    {quote}  (10:12:59) DavidMudrak: hmm  (10:13:21) DavidMudrak: $data = $cache->get('key'); returns false if the key was not found. I'm wondering why NULL has not been chosen instead.  (10:15:11) DavidMudrak: <USER>  (10:16:22) <USER>left the room.  (10:26:59) DavidMudrak: Otherwise I must say I am pretty impressed with the MUC API. Well done <USER>  (10:27:09) <USER> Thanks  (10:28:29) <USER>left the room.  (10:28:46) <USER> False was perhaps not the right choice, I know <USER>H suggested changing it to NULL at hackfest but I never got time to properly consider it/make the change  (10:30:24) <USER> Originally I had chosen false because it is commonly used by PHP cache extensions as the return value when they don't have the requested value.  (10:30:48) <USER> APC and memcached extensions for example  (10:31:33) <USER> I'm also not really sure whether there is a good use case for storing false (causing a collision of interest)  (10:32:33) <USER> But then at the time of thinking that I was more focused on the application cache  (10:32:51) DavidMudrak: Right.  (10:32:53) <USER> *cache idea  (10:33:04) DavidMudrak: I know that many PHP core function return FALSE in such cases  (10:33:21) DavidMudrak: It's just that in my mind, the NULL is exactly this - unknown value.  (10:33:22) <USER> That was really my driving reason in this instance  (10:33:35) <USER> Yip - that is what <USER>H pointed out to me as well  (10:33:46) DavidMudrak: False is something known. Imagine caching of an array of flags.  (10:33:48) <USER> NULL would be more commonly used within our code as well I imagine  (10:34:42) DavidMudrak: Anyway, I agree on the point of FALSE being used by common cache products.  (10:35:22) DavidMudrak: Still. It is not late to change it IMHO. Better now (than never ;-)).  (10:36:29) DavidMudrak: See for example how isset() works.  (10:36:30) <USER> Certainly if we are going to change it sooner would be better  (10:37:06) DavidMudrak: $a = false; $b = null; print_r(isset($a)); print_r(isset($b));  (10:38:08) <USER> I see that as an example of a php function that returns null, but I don't see its relevance to the fetching something from a cache  (10:38:10) DavidMudrak: and as we have set() method ...  (10:38:28) DavidMudrak: yeah  (10:38:54) <USER> I think null is worth looking into, but I think to change it at this point we would be best to have a really good reason to  (10:39:16) DavidMudrak: I mean - it makes sense to store FALSE value in a cache (like a flag) but it does not make sense to store NULL.  (10:40:22) DavidMudrak: $a = null; $cache->set('a', $a); // I can even imagine this throwing exception  (10:41:11) <USER> But can you think of a use case for storing false in an application cache, that would apply to everyone and wouldn't be better suited as a config variable? (I am just curious because I struggle to think of one)  (10:41:37) <USER> Hehe I wonder if it throws an exception if you try to set $a = false presently!  (10:42:07) <USER> Nope it would permit it  (10:43:04) DavidMudrak: The relevance in my mind was that if isset($a) returns false, it makes no sense to call $cache->set('a', $a) - you can't call set() with a value that is not set  (10:43:39) <USER> Hmm that is a pretty good point  (10:44:45) <USER> I'll create an issue to convert the return from false to null and add a few watchers who may be interested  (10:45:03) DavidMudrak: And regarding the use case - imagine cache of flags like $cache->set('update_available', false)  (10:45:55) DavidMudrak: Using FALSE effectively disables caching of bool values, unless FALSE is the default (fallback)  {quote}    Lets gets some ideas and votes going here.  If we are going to make this change it has to be done ASAP!""",Improvement,Caching
348504,"""We have a SCORM object with 134 interactions. It's an assignment for a stats module which involves filling in lots of data tables, so we genuinely do need to <USER>that many separate interactions.    Because the SCORM module submits every single element in the data model as a separate form value each time an API.Commit() call is made, and because each completed interaction has ~8 data model elements associated with it, the Commit request contains over 1000 values.    PHP has a configuration variable called max_input_vars which limits the number of values it will accept in a GET or POST request. By default, it's set to 1000. That means the remaining values just get chopped off, so the request fails in an interesting way: scoid is the last parameter added to the request, so the request returns """"404, scoid not given"""".    I'm filing this as a bug because there are much better ways of sending the data that don't involve breaking this limit, but it's low priority for us because we just increased the value of max_input_vars to 10000 and left it at that.""",Improvement,SCORM
348616,"""I would like to see a feature added to the Course Request where a user can request that a New Course be copied from an existing course.    For example, the user would complete the form as it exists, however they would have an additional dropdown menu field in which they could select any one of the existing course shortnames that exist on the site to serve as a template for the new course.  Then when the administrator approves and processes the request, the new course will be automatically created from the existing course template.""",New Feature,Course
348817,"""I'm still seeing some caching effects in toggling the visibility of activities/resources.  *Replication steps:* # Log in as admin/teacher # Navigate to a course with some activities/resources # Hide a couple of activities/resources # Show the activities/resources # Hide a couple of activities/resources # Reload the page # Show the activities/resources # Reload the page  _Expected result:_ The link of the activities/resources should appear normally after showing the activity/resource and before reloading.  _Actual result:_ The link appears as if hidden after showing the activity, until the page is reloaded""",Bug,Course
348820,"""To allow a feedback plugin to add elements to an assignment grading form the function 'get_form_elements' is called.  This is passed 3 parameters - grade (which is false, if the submission has not yet been graded), mform (the form to add elements to) and data.  If the submission already has a grade, then the 'grade' parameter includes the userid for the assignment that is currently being edited, which can then be used in whatever way required (including looking up the submissionid).  If the submission does not yet have a grade, then there appears to be no sensible way to retrieve the userid for the current submission (or the submissionid, for the submission).  For many feedback plugins, this may not be a problem (as they do not need the userid until the form is saved, by which point the grade will have been created), for my feedback plugin I need to know the userid, before I can start the grading process.  """,Bug,Assignment
348863,"""When editing the submission settings of an assignment, the Course upload limit option appears as the last item in the list of available options. Generally, the course upload limit will be largest available option and should therefore be positioned at the begining of the list - given that the list is already sorted by available sizes.  Steps to reproduce:  1. Login as a teacher and modify the maximum upload limit of a course. Set the value to 1MB. 2. Create an Assignment activity within the same course. 3. In the Submission Settings area, click on the drop down list for """"Maximum submission size"""". The option for """"Course Upload limit (1MB)"""" is present as the last available item in the list, and not in a consistent order with the other options.  I would expect the """"Course Upload limit"""" option to be the first item in the list of options as it is the largest value available for selection.  """,Improvement,Assignment
349074,"""It is possible to base a grade condition on an assignment, but when the condition is evaluated, it appears to rely on the student having made a submission. In most cases, this would be acceptable, but when the student is given a <USER>without having made a submission (a real possibility), the condition is not seen as completed.  *Replication steps:* # Log in as admin/teacher # Create two assignments # In the second, create a condition that relies on a grade value in the first # Go to the first assignment and <USER>an assignment without a student submission # (I ran cron manually at this point, I'm not certain I needed to) # Log in as the student who received the <USER># Check if the second assignment is accessible # Submit a submission to the first assignment # (I didn't run cron here) # Check if the second assignment is accessible  _Expected result:_ The second assignment should be accessible after the grade is set, even without a student submission  _Actual result:_ The second assignment is not accessible after a grade is set for the first assignment until the student makes a submission""",Bug,Assignment
349210,"""I am attempting to use the Dropbox plugin as a document repository. I have set up my app, uploaded documents to my dropbox, and then linked them in to my Moodle install via the file picker. All of that seems works fine.   However.   1. If I set a cache value, I get a 'Failed to Load PDF document' error when I try and click on the link. If I 'save link as', the file downloads but is only 1kb in size, and when you try and open it the following error message comes up: Acrobat could not open 'file name here' because it is either not a supported file type or because the file has been damaged (for example, it was sent as an email attachmnet and wasn't correctly decoded).  2. If I don't set a cache value, I get the following error message when trying to click on the link: Notice: Undefined property: stdClass::$url in C:\Moodle Server\server\moodle\repository\dropbox\lib.php on line 680. Again, the file downloads, but is 1kb in size and cannot be opened.   It looks as though the files are not being correctly brought across into Moodle, but I wouldn't know where to begin troubleshooting.  At present, I am running this using the Windows installer. It's a completely clean install: this is the only thing I have played with on it.   Thank you in advance!""",Bug,Repository
349240,"""It's not possible to add this block to the Logs page.  *Replication steps:* # Log in as admin # Navigate to the Site Home # Ensure editing is turned off # Navigate to the Logs page using either: #* Site pages > Reports > Logs #* Settings > Site admin > Reports > Logs # Click Blocks editing on button (Not """"Turn editing on"""" link from navigation) # Attempt to add a block  _Expected result:_ You should be able to add the block  _Actual result:_ Nothing is added, the editing mode turns itself off  I couldn't find another page that has the same behaviour. I would have thought that the Statistics report, which has similar properties, would be the same, but it is not.""",Bug,Reports
349255,"""The title is quite explicit in itself.    I was able to correct the situation by editing the assign_user_outline in /<USER>assign/lib.php based on the quiz function quiz_user_outline. I'm not sure tho if it's good enough but as for now, it resolved the issue for us. And instead of displaying the grade as """"Grade : 9.0000"""", it's now """"Grade : 9.00 / 10.00"""".    {code}  function assign_user_outline($course, $user, $coursemodule, $assignment) {      global $CFG;      require_once($CFG->libdir.'/gradelib.php');      require_once($CFG->dirroot.'/grade/grading/lib.php');        $gradinginfo = grade_get_grades($course->id,                                          '<USER>,                                          'assign',                                          $assignment->id,                                          $user->id);        $gradingitem = $gradinginfo->items[0];      $gradebookgrade = $gradingitem->grades[$user->id];        if (!$gradebookgrade) {          return null;      }      $result = new stdClass();      //$result->info = get_string('outlinegrade', 'assign', $gradebookgrade->grade); //remove this      $result->info = get_string('grade') . ': ' . $gradebookgrade->str_long_grade; // and add this one, inspired from quiz      $result->time = $gradebookgrade->dategraded;        return $result;  }  {code}   """,Bug,"Assignment,Reports"
349276,"""In the backup process, even though the option """"Include grade history"""" is checked (and confirmed in subsequent screens), grade history are not included in the backup file.  I put a debug message in function backup_gradebook_grades_history_info() and verified that the function doesn't get invoked.  I searched the backup file and couldn't find any file containing the string """"GRADE_GRADES_HISTORIES"""" or """"GRADE_GRADES_HISTORY"""".  I'm not familiar with the backup process so I can't tell what's wrong, perhaps the """"grade_histories"""" setting never get considered and added as a step?""",Bug,Backup
349325,"""The database activity has a great potential for more learner centric learning. In lots of discussions teachers told us that the idea is great and they would like to used it but usability is really a mess. Specially the list view editing process is not usable for a normal teacher. To create acceptable results some understanding of HTML is needed that most teacher don't want to use. The text editor often breaks the process.     I've thought for a long time how to simplify this process and I think there is a way we can do it.    Lets implement a new simple form for standard view definition. If the user opens the form he see a table with all data fileds created earlier plus the system fields ''user## et al. He can <USER>the fields that should be used for the table, a second column shows the order of the fields and the last one is the title as defined when fields are edited.    If fields are selected and ordered they can save the changes and the view will be created.     Most teacher accept to use a standard design for the tables and are not interested to change them.  Advanced users can click at the link 'advanced' modus to use the traditional way to define the views.     This process could be implemented for all four views: new entry, search, list, and single. Top priority has the list view, because it creates most of the problems.""",Improvement,Usability
349377,"""We're now beginning to adds more and more YUI modules to moodle. At present, each call to yui_module means that we add something like:  {code} Y.use('moodle-course-modchooser', function(Y) {   M.course.init_chooser({""""courseid"""":""""6""""}); }); {code}  The way that the YUI loader works what then happens is: # the Y.use() is processed by the browser # the YUI (JS) loader then loads the JS for the specified modules (moodle-course-modchooser) and issues a call for the JS (/theme/yui_combo.php?moodle/-1/course/yui/modchooser/modchooser.js) # the newly downloaded JS is parsed and its list of dependencies is read # The list of dependencies is now downloaded (base, overlay, moodle-core-chooserdialogue, transition) # The list of dependencies for moodle-core-chooserdialogue is now read and they are downloaded  As a result, for the modchooser we've now made: * 1 call for modchooser.js * 1 call for modchooser's dependencies * 1 call for the dependencies of the dependencies * possibly more calls for yet more dependencies.  That's at least three browser fetches.  The way that the core YUI side of things gets around this is to preseed its list of dependencies somehow. We should really look at putting some way of doing this in place to reduce the amount of unnecessary page calls.  Additionally, we should look at combining the list of YUI modules that we're using into a single Y.use(), for example:  {code} Y.use('moodle-course-modchooser', 'moodle-course-toolboxes', 'moodle-core-blocks', 'moodle-course-dragdrop', function(Y) {   M.course.init_resource_toolbox({""""courseid"""":""""6"""",""""ajaxurl"""":""""\/course\/rest.php"""",""""config"""":{""""resourceurl"""":""""\/course\/rest.php"""",""""sectionurl"""":""""\/course\/rest.php"""",""""pageparams"""":[]}});   M.course.init_section_toolbox({""""courseid"""":""""6"""",""""format"""":""""topics"""",""""ajaxurl"""":""""\/course\/rest.php"""",""""config"""":{""""resourceurl"""":""""\/course\/rest.php"""",""""sectionurl"""":""""\/course\/rest.php"""",""""pageparams"""":[]}});   M.course.init_section_dragdrop({""""courseid"""":""""6"""",""""ajaxurl"""":""""\/course\/rest.php"""",""""config"""":{""""resourceurl"""":""""\/course\/rest.php"""",""""sectionurl"""":""""\/course\/rest.php"""",""""pageparams"""":[]}});   M.course.init_resource_dragdrop({""""courseid"""":""""6"""",""""ajaxurl"""":""""\/course\/rest.php"""",""""config"""":{""""resourceurl"""":""""\/course\/rest.php"""",""""sectionurl"""":""""\/course\/rest.php"""",""""pageparams"""":[]}});   M.core_blocks.init_dragdrop({""""courseid"""":""""6"""",""""pagetype"""":""""course-view-topics"""",""""pagelayout"""":""""course"""",""""regions"""":[""""side-pre"""",""""side-post""""]}); }); {code}  This would mean that we combine all of the modules into one page load, those are all evaluated at once, and their dependences loaded once. This should reduce page load count.  I've put a quick proof-of-concept together.  On a course editing page, with the fix for MDL-36099 included, the number of requests related to JS reduces from 23 to 18. Adding in some of the module dependencies reduces this further to 15. If the file drag/drop were rewritten as a YUI module this would reduce further to 14.""",Improvement,JavaScript
349657,"""I have Problems to see the full profiles of users in a course, where I'm enroled as a teacher. After clicking on a user from the users list, I can see some Information about the user. As teacher I have the capability """"user:viewdetails"""" but there is no link to see the full profile.""",Bug,"Course,Roles / Access"
349761,"""it would be very helpful if you add some more CLI scripts such as:  Language packs update (can get language code \ """"all"""") MOD remove (if i want to uninstall a MOD / theme i have to do it manualy) CLI update of user pics - DONE ! attached here  I thinkevery one has a script he/she would loveto have..  Thanks""",Improvement,Admin
349797,"""When logged in as an admin user with a large number of courses/categories in a site, when browsing to the server files root in the file picker it takes too long to load in the browser - it looks like it's hitting the db pretty hard during the load.  I'm testing this on a site with a massive number of courses/categories - MDL-35179 has a script that lets you create 900 categories quickly.  MDL-33857 is partially related to this.""",Bug,"Performance,Repository"
349850,"""In moodle 1.9 and possibly later moodles, the db_init function in auth/db/auth.php does not test if it has connected to the database. A function like get_userlist (in the same) will return an empty array suggesting no records returned as a result of this, when it should be failing or returning something else. This behaviour is occurring for me when running sync_users with an external db for moodle 1.9.  One possible fix is for db_init to throw an exception after testing whether $authdb->Connect has returned true or not. I've only tested and had issues with moodle 1.9 -- I have not tested if this is an issue with later moodles although none of them appear to be checking $authdb->Connect either.  So I'm suggesting something like this:  diff --git a/auth/db/auth.php b/auth/db/auth.php index e3582f5..908adb9 100644 --- a/auth/db/auth.php +++ b/auth/db/auth.php @@ -118,7 +118,11 @@ class auth_plugin_db extends auth_plugin_base {              $authdb->debug = true;              ob_start();//start output buffer to allow later use of the page headers          } -        $authdb->Connect($this->config->host, $this->config->user, $this->config->pass, $this->config->name, true); +        // W/R 98046: +        $result = $authdb->Connect($this->config->host, $this->config->user, $this->config->pass, $this->config->name, true); +        if(!$result) { +          print_error('auth_dbcantconnect','auth');  // Replace with cli friendly alternative ? +        }          $authdb->SetFetchMode(ADODB_FETCH_ASSOC);          if (!empty($this->config->setupsql)) {              $authdb->Execute($this->config->setupsql);  Ignore the w/r codes. I'm not sure print_error is necessarily the best thing here, I noticed it was being used somewhere else. We just need to fail in a cli friendly manner.  Thanks, <USER>",Bug,Authentication
349875,"""After spending some time playing with video formats to work out best compatibility for desktop, tablet and phone I realised that the medialib.php may work better with a couple of minor changes.    Changes I would suggest are to raise the rank of core_media_player_html5video from 20 to 90 to give it a higher priority than core_media_player_flv, also to change the supported formats for core_media_player_flv to include mp4 and m4v.    This would mean that the mp4 alone would provide video playable by almost all devices.    Using HTML5 video it would work on Chrome, Safari and almost all mobile devices.  as a fallback it would use Flowplayer to work on Firefox and IE.    During testing I discovered that Firefox only handles stereo sound (no 6 channel surround) for ogv and webm video formats making mp4 preferable.       """,Improvement,Libraries
349891,"""The SCORM module doesn't consider the result of the activity as a percentage, so when you set a maximum grade minor to 100% the results are unexpected.  h2. For example  # Set a maxgrade for a SCORM object of 50. # Achieve a grade of 60%.  h2. Expected result  The grade obtained (60%) in the SCORM activity represents 30 as the final grade (having set a maximum grade of 50 in the SCORM activity configuration).  h2. What actually happens  The SCORM module reports 60 as the result, not the 60% of 50. So the finalgrade is capped to 50 by the grade lib.  h2. Pull request  I've fixed this already so I'll be posting a pull request later today.""",Bug,SCORM
349894,"""init_url_select is woefully inefficient, and init_select_autosubmit may have similar issues. We should rewrite it, possibly as a YUI module, and improve it's performance. We should try not to: * use loops of Y.use() * use loops of Y.Node.on()  We also need to <USER>the accessibility issues in mind  I believe that the attached branch should do that, but I'm not sure whether it's best to run it as a YUI module though or not.""",Sub-task,"Performance,JavaScript"
349980,"""Assigning a system role of """"student"""" to a user does not give that user student permissions within all courses on the site, in the way that assigning other roles, like teacher, does.  Steps to reproduce ================== 1. Set up a new user A, a category and a course with self-enrolment and an enrolment key set.  2. Users > Permissions> Defines roles, and change the student role to allow it to be defined at system and category level.  3. Users > Permissions > Assign System Roles, and assign user A the student role.  4. Go to the category, then Assign Roles and choose Student. - Observed bahaviour: I see user A listed, in grey text, under """"users from system"""", as expected. - Expected bahaviour: As above  5. Log in as user A, and navigate to the course (which user A is not already enrolled in). - Observed behaviour: I am asked for the enrolment key. - Expected behaviour: As user A already has the student role in this course, I would expect to enter it directly.  6. As admin, go to the category, then Assign Roles and choose Student. Assign the student role to user A. - Observed behaviour: The """"users from system"""" section disappears, and user A appears on its own in the list. - Expected behaviour: As above.  7. As user A, go back to the course. - Observed behaviour: I enter straight into it, without needing an enrolment key. - Expected behaviour: Aa above.  Therefore the system-assigned student role is behaving differently from the category-assigned role. """,Bug,Roles / Access
350074,"""After a course is restored (either as a new course or into an existing course) I want to be able do some post processing steps without modifying core code.  For example, I want to: * Make sure the course has the right format * The right blocks are installed * The right enrollment plugins are installed""",New Feature,Backup
350122,"""Oracle doesn't support more than 1000 items in an IN() clause - any more and the query fails with error ORA-01795.  The problem is that often the list of items is generated dynamically, so most sites work fine but a big site will suddenly get breaking queries when it gets big enough.  Back on 1.9 we had a function that we used when building an IN() clause from an array that would check the number of items and the database in use, and split the IN() for Oracle if there are too many items - changing this:  IN (1,2,3,...,2009,2010)  to:  IN (1,2,3,...,999,1000) OR IN (1001,1002,...,1999,2000) OR IN (2001,...,2010)  In Moodle 2, we use get_in_or_equal(), which could have the same issue - I was wondering if it would make sense to have an Oracle-specific version of get_in_or_equal() which did something similar?  I'd be happy to write it (based off the code below) if it's something that would be wanted in core.  Here's the code we were using (ported to 2.x):  {code} /**  * Return the proper SQL to compare a field to multiple items  *  * By default it uses IN but can be negated (to NOT IN) using the 3rd argument  *  * The output from this is safe for Oracle, which has a limit of 1000 items in an  * IN () call.  *  * @param string $field The field to compare against  * @param array $items Array of items. If text they must already be quoted.  * @param boolean $negate Return code for NOT IN () instead of IN ()  *  * @return array In the form array(sql, params) The SQL needed to compare $field to the items  *              in $items and associated parameters  */ function sql_sequence($field, $items, $type=SQL_PARAMS_QM, $negate = false) {     global $DB;      if (!is_array($items) || count($items) == 0) {         return ($negate) ? array('1=1', array()) : array('1=0', array());     }      $not = $negate ? 'NOT' : '';     if ($DB->get_dbfamily() != 'oracle' || $count($items) <= 1000) {         list($sql, $params) = $DB->get_in_or_equal($items, $type, 'param', !$negate);          return array("""" $field """" . $sql, $params);     }      $out = array();     while ($some_items = totara_pop_n($items, 1000)) {         list($sql, $params) = $DB->get_in_or_equal($items, $type, 'param', !$negate);         $out[] ="""" $field """" . $sql;         $outparams = array_merge($outparams, $params);      }      $operator = $negate ? ' AND ' : ' OR ';     return array('(' . implode($operator, $out) . ')', $outparams); }  /**  * Pop N items off the beginning of $items and return them as an array  *  * @param array &$items Array of items (passed by reference)  * @param integer $number Number of items to remove from the start of $items  *  * @return array Array of $number items from $items, or false if $items is empty  */ function totara_pop_n(&$items, $number) {     if (count($items) == 0) {         // none left, return false         return false;     } else if (count($items) < $number) {         // return all remaining items         $return = $items;         $items = array();         return $return;     } else {         // return the first N and shorten $items         $return = array_slice($items, 0, $number, true);         $items = array_slice($items, $number, null, true);         return $return;     } } {code}""",Improvement,Database SQL/XMLDB
350139,"""Firstly, I apologize if this has already been addressed. I searched the issue but couldn't find anything relating exactly to my issue. When you create an assignment and the users have submitted their assignments, you get an """"Error reading from database"""" message when you click on """"View/grade all submissions."""" Sometimes it will even go to the list of submissions, but when you try to sort by any field, you get the same error.  My environment:    -Moodle ver - 2.3.1    -Windows Server 2008 R2    -IIS 7    -PHP 5.3.10    -PHP driver = FreeTDS    -Database MSSQL  I should note, if you need any information from me please give me the exact steps. I'm a php/database beginner. Thanks!""",Bug,Assignment
350397,"""I was of the understanding that when a Moodle administrator logged in as another user, no logs were created. However, it seems that in Moodle 2.3, a log record is created on the user logout event.  To replicate:  1. Log in as Moodle administrator.  2. Browse through the list of users, then select a user and and view their profile.  3. Select Login As for that user.  4. Log out. (Note the time.)  5. Log back in as a Moodle administrator.  6. View the logs for that user. A log entry will have been created at the time of log out in Step 4.  This is an extremely minor issue. However, because there is no indication that the log was generated by an administrator, there is some chance that the record will be interpreted as an actual user event. This has implications for support and audits. For example, it might be difficult to tell whether the record related to the user logging out or one of our administrators.  I would expect that the log record not be created. But if there is a reason why this log entry is created, perhaps it should include the name of the administrator.""",Bug,Admin
350422,"""I'm building out a webservice that retrieves a given user's courses. I've created an external webservice with the core_enrol_get_users_courses() function, allowed it for authorized users only, assigned a role account to it, and verified that the user has all the required capabilities. When I try to retrieve courses for the passed user id, I get a null return.  After turning on debugging I find that validate_context() is throwing a require_login_exception exception, which doesn't make any sense to me. I've verified that if I skip the validate_context tests the proper courses would be returned.""",Bug,Web Services
350528,"""I'm using the advanced section of the filtering on the browse list of users page and bulk user action page.  I'm trying to find all users that are not within a specified Cohort.  So I choose under Cohort ID filter option """"doesn't contain"""" and enter the Cohort ID I'm not looking for, for instance """"allstudents"""".  I have a total of 4500 users and only 4200 are in the """"allstudents"""" Cohort.  So I would expect to get the other 300 as a result.  I get nothing unless I have another cohort that has at least one user in it.  Otherwise I won't get any users.  The way I'm thinking it should work is that it should return any user not within the """"allstudents"""" Cohort whether they are in another Cohort or not.  There is also another thing that happens is that if a user is within multiple Cohorts (""""allstudents"""" and """"allusers"""") and I don't want any users that are within the """"allstudents"""" but is contained in an """"allusers"""" Cohort that user is returned.  Even though I don't want any users within the """"allstudents"""" Cohort it is seeing that they are in the """"allusers"""" Cohort and returning them. """,Bug,Admin
350649,"""I'm currently working on a course on digital photography. Our students are divided into groups and are using the Moodle blog as a progress diary.   Currently the blog menu block gives students and teachers the possibility to see all blog entries, the users entries or add a new entry about the current context, ie. the block has no notion of the course's group mode.   I therefore propose an extension of the blog menu block to give the user the option to see all blog entries by members of the users group.   Edit: The provided patch also solves a bug that causes no entries to be selected when filtering both on module and group.""",Improvement,Blog
350729,"""The pop-up date picker shows an incorrect calendar and the days are starting at Saturday and running until Friday for some reason.  I'm using the en_us language pack. See the screenshot, it has August 1st listed as a Tuesday and not a Wednesday. I'm running Moodle 2.3.1+. This is also happening on Moodle 2.2.4+. This does NOT occur on the latest Moodle 2.1.7+.""",Bug,Forms Library
350762,"""I noticed that the current layout, which places the titles for the current, previous and next section all on the same line, doesn't work that well if users replace the default short titles (e.g. """"topic 1"""" or """"1 June - 7 June"""") with their own text. Similar happens at the bottom where the next and previous titles share the same line with the """"Return to main course page"""" link. Not only do the titles get squashed as they grow longer, if one is much longer than the other then the center title gets pushed to the opposite side and looks odd as a result.  Something like """".single-section .section-navigation .<USER>align {clear: both;}"""" lets the next and previous float up onto their own line and recenter the title/main course page link. However, for the title as I mentioned in MDL-34613 I'd suggest placing it back inside the content, and inside a header tag and use CSS to position it from there.""",Bug,"Themes,Course"
350777,"""As a course grows, resource items accumulate, and as a natural progression, teachers would want to pop those items in a new folder to avoid the scroll of death.  Folders in Moodle however, seem to have been implemented only to work properly when the resource items appear after that folder has been instantiated. You make your folder and then you upload your files into it. Er, -no.  Why, oh why, oh why is it not possible to drag and drop resource items into a newly-made folder? Gosh, if I could programme this I would. thoughtful  At the moment, there is the file picker, but this currently does two counterintuitive things:  1. it presents resources in a separate window from the course page, ordered by course category and course (yes); but then all logic seems to be abandoned. Resource items in the file picker are not presented by topic (or week), and they seem to be in upload-date rather than alphabetical order. - How are users supposed to find items in larger courses from the file picker?  2. Every resource item in the file picker has been put into a separate folder. - Why do users need to see that?  Please, please, pretty please mighty developers: may we have a drag-and-drop facility to be able to pop pre-existing resources into newly-made folders?  This simple functionality would be *incredibly* useful.""",Improvement,Course
350804,"""There was a plugin submission that basically created a way to have per user debug modes.  This required a 'unpluginly' patch to the config.php to affect debug modes from the start.  Do we want to add an event trigger to propagate instant events to help setup related plugins (""""early plugins"""") that need to take effect in the early stages of the entire system?  - It seems this would cater to developers but i'm not sure if it would cater to any pedagogical needs.  - This sort of plugin would be prime candidates for existing within the plugins directory which would later be brought into core later if it was really popular.  - security concerns for such powerful early plugins could be evaluated as a plugin (and shown to be risky too) before they are considered to get into core.  - If we don't do this, the only way for such """"early plugins"""" would be to patch config.php and this not being a 'true' plugin. (Then it'd seem fair to allow these patches in the plugins dir)""",Improvement,"Admin,Libraries"
350892,"""I like the concept and direction 2.x has taken with the activity/resource descriptions being optionally viewable along with the item itself. Instructors have often used labels above or below items to provide the same functionality in the past. The description option really minimizes the effort to get this displayed.  I would value the ability to assign the location of the description field above the resource link as a way to provide an introduction to the content. I imaging this could be implemented as a pulldown menu with the values of [ above, below ] next to the currently available """"Display description on course page"""" checkbox. If the checkbox is selected the pulldown menu would become active for selection. Default value could remain at """"below"""".  Question to accessibility experts. Would having the description above the referred item have serious negative effects for the navigation standards used by screen readers?  It could also be slick to have a separate checkbox option to """"apply to all descriptions in this course"""" if someone was liking to shift all of their course content to this new location option.""",Improvement,"Accessibility,Usability"
350896,"""There's CSS in the base theme which is intended to color odd and even rows of the user list different colors by attaching classes called """"odd"""" and """"even"""". The bit of code that adds these classes has a bug, since it adds the class based on the """"rel"""" number (which might be the user id?) rather than on the position in the list. From the perspective of someone using Moodle this is effectively random.  I've used the following CSS instead: {code} .user-enroller-panel .uep-search-results .users .user:nth-child(odd) {code} This doesn't work in IE8 but works everywhere else and doesn't require you to add classes to each row.""",Bug,"Course,JavaScript"
350923,"""For the next academic year, we intend to automatically create all summative assignments in Moodle based on our student records system. As a result, we'll be setting a number of parameters by default for these assignments including: * name (to match what the students see on their transcripts) * due date * plagiarism detection settings  In order to do so, we need to alter the form for <USER>assign, and possibly <USER>turnitintool to prevent tutors from being able to change these settings.  I was pondering the best way of doing this, and I suspect that there are other situations where others may have a requirement to do the same - or to change the defaults for core forms without changing the core component itself, and the solution I've come up with is to allow local plugins to extend the <USER>{$module->name}_mod_form, override the definition, and change settings for the elements defined in the parent definition.  I've got a working PoC and would appreciate some feedback.""",Improvement,Forms Library
350945,"""- Admins always get full navigation trail in breadcrumbs. e.g. Master > 1.Semester > Chapter 1 > Learning how to learn  - Students get enroled into courses. These courses become """"my courses"""" their trail looks like this: My Courses > Learning how to learn    What I've seen in different moodle instances, is that students which are only enroled in a handfull of courses feel very pleased with the current """"My Courses"""" navigation. For them the reduced navigatioin trail is an improvement compared to the long path.     The situation is different with students who are enrolled in many different courses (including different categories).   - They get a long """"My Courses"""" list, including no categories  - E.g. when a student wants to go back one level to the 1. Semester and switch to the 2. Semester he needs to return to the main page or use additional site navigation blocks. He cannot use the breadcrumbs.      --> Would be gread if an additional configuration option was added, to allow each moodle admin to decide which kind of breadcrumbs trail is most suitable for his users.   (e.g. Siteadministration > Appearance > Navigation   Breadcrumbs   x Use Breadcrumbs using """"My courses"""" as root  x Always use full Breadcrumbs trail  )          """,Improvement,Usability
350986,"""Loginas is a great feature for admins to check permissiosn as a special user. In several environments its seen very critical because admins can change anything as the user who is logged in. Its not documented that the changes are not done by the user and the admin did them.  It makes lot of sense to document that an admin changed anything in the name of the user. I've two ideas how to do this: 1. <USER>all changes in the interface as """"changed by admin [name of admin] [date and time] of change"""". 2. write changes done by admin when logged in as a user into a seperate database table and create a report page that shows this database information sortable by date, user and admin who made changes. This report should include: - date and time of login as - date and time when login as was finished - did admin make changes in the name of the user? - who logged in as other user (name of admin) - login as which user [name of user] - where was change done [URL] - old entry from user - changed and stored new entry by admin  The report should only by accessible for admins. The report should be downloadable. If files were added or deleted or renamed the information should be included but not the file istself.""",New Feature,Admin
351007,"""I have brought in all of the user accounts from Active Directory through LDAP.   On examination of the accounts, it appears that there are a number of generic accounts that really shouldn't be able to sign in to Moodle.   I have segregated these accounts into a cohort and would like to be able to either suspend them or change their authentication to 'No login'.  I would like to do this as a bulk action.  I could do this myself by SQL by updating the column <USER>user.suspended  where <USER>user.id IN (SELECT M.[userid]              FROM [moodledb].[dbo].[<USER>cohort_members] M, [moodledb].[dbo].[<USER>cohort] C              WHERE M.cohortid = C.id                AND M.cohortid = 2 )   (where 2 is the number of the cohort id in question)  but I am not sure if this would have an affect on anything else.""",Improvement,"Admin,User management"
351044,"""Want to go 2.3 install but am told i need Inoodb and my host doesn't provide this.  Since Pg db is possible, tried using the migrate tool in Experimental and was able to get as far as the Pg table, etc. creation, but the data didn't make it.  The migration process ended with an error of """"Cannot read database."""" or something similiar after listing all of the tables, etc. which were created in the Pg db.  Have a great afternoon!  :)""",Bug,Admin
351228,"""steps to reproduce this error. create a repository. create a file in the repository named 'a.txt' with contents 'a' select a course add an activity or resource to the course. select file resource, click add, name = a, description = a content add, select file a.txt in repository select 'Create an alias/shortcut to the file' select this file. save and display ensure file a.txt shows file contents 'a'  now, edit the file a.txt to have contents 'b' in moodle, the file resource a still shows 'a', NOT 'b'  this is a serious error.  the way that moodle handles repositories has caused me countless hours of work as i have hundreds of references to a few files.  updating files is a total nightmare. i have upgraded to 2.3 (where this issues was meant to be fixed) only to find that the over-engineered backwards way of handling a link still does not work. all i need is to link to a file in the repository, then if i update that file, the new file gets linked to in the moodle browser. if i wanted a new version of the file, i would give the file a different name.    Question. why does my moodle LMS that has about 200mb of content have a moodle data folder that is 16GB?  because each instance of a file is re-copied to the system.  with content pdfs linked to each of the 2000+ questions as help, it just adds up.    """,Bug,Repository
351369,"""I've created an external video repository, where my repository list structure sets the 'hasauthor' and 'haslicense' properties set to false; but when I use the repository, select the 'View as list' mode and select a file the author and license fields are present.  This is not an issue when I use 'view as icon' mode.  Steps to reproduce:  * Create an external repository (The Youtube repository doesn't count because it lists it's video entries by bypassing the get_listing() hook by using the search() hook to list videos *cheater!*) * Define your video list structure such that the 'hasauthor' and 'haslicense' properties are set to *false* * Open the file picker and select your external repository * Select a file from the repository  Result:  * On the select file page, the license and author fields are present, even though you set the value to false in the listing structure  Expected result:  * The select file page only has the 'save as' text field visible  I was able to somewhat trace what and why it's happening...   In filepicker.js you have the following code {code}             scope.request({                 action:'list',                 client_id: scope.options.client_id,                 repository_id: scope.active_repo.id,                 path:'',                 page:page,                 callback: function(id, obj, args) {                     scope.parse_repository_options(obj);                     if (obj.login) {                         scope.viewbar.set('disabled', true);                         scope.print_login(obj);                         return;                     }                     var client_id = scope.options.client_id;                     var dynload = scope.active_repo.dynload;                     var list = obj.list;                     var panel_id = '#panel-'+client_id;                     scope.viewmode = 2;                     Y.one(panel_id).set('innerHTML', '');                      scope.print_header();                      var html = '<div class=""""fp-tree-panel"""" id=""""treeview-'+client_id+'"""">';                     if (list && list.length==0) {                         html += '<div class=""""fp-emptylist <USER>align"""">' +M.str.repository.nofilesavailable+'</div>';                     }                     html += '</div>';                      var tree = Y.Node.create(html);                     Y.one(panel_id).appendChild(tree);                     if (!list || list.length==0) {                         return;                     }                      scope.treeview = new YAHOO.widget.TreeView('treeview-'+client_id);                     if (dynload) {                         scope.treeview.setDynamicLoad(scope.treeview_dynload, 1);                     }                      for(k in list) {                         scope.build_tree(list[k], scope.treeview.getRoot());                     }                     scope.treeview.subscribe('clickEvent', function(e){                         if(e.node.isLeaf){                             alert(e.node.data.hasauthor);                             var fileinfo = {};                             fileinfo['title'] = e.node.data.filename;                             fileinfo['source'] = e.node.data.source;                             fileinfo['thumbnail'] = e.node.data.thumbnail;                             scope.select_file(fileinfo);                         }                     });                     scope.treeview.draw();                 }             }, true);         }, {code}   The video hasauthor and haslicense values are not passed to the select_file() function:  {code} fileinfo['title'] = e.node.data.filename; fileinfo['source'] = e.node.data.source; fileinfo['thumbnail'] = e.node.data.thumbnail; scope.select_file(fileinfo); {code}  In the select_file() function you have the following code  {code} select_file: function(args) {             var client_id = this.options.client_id;             var thumbnail = Y.one('#fp-grid-panel-'+client_id);             if(thumbnail){                 thumbnail.setStyle('display', 'none');             }             var header = Y.one('#fp-header-'+client_id);             if (header) {                 header.setStyle('display', 'none');             }             var footer = Y.one('#fp-footer-'+client_id);             if (footer) {                 footer.setStyle('display', 'none');             }             var path = Y.one('#path-'+client_id);             if(path){                 path.setStyle('display', 'none');             }             var panel = Y.one('#panel-'+client_id);             var form_id = 'fp-rename-form-'+client_id;             var html = '<div class=""""fp-rename-form"""" id=""""'+form_id+'"""">';             html += '<p><img src=""""'+args.thumbnail+'"""" /></p>';             html += '<table width=""""100%"""">';             html += '<tr><td class=""""<USER>right""""><label for=""""newname-'+client_id+'"""">'+M.str.repository.saveas+':</label></td>';             html += '<td class=""""<USER>left""""><input type=""""text"""" id=""""newname-'+client_id+'"""" value=""""'+args.title+'"""" /></td></tr>';              var le_checked = '';             var le_style = '';             if (this.options.repositories[this.active_repo.id].return_types == 1) {                 // support external links only                 le_checked = 'checked';                 le_style = ' style=""""display:none;""""';             } else if(this.options.repositories[this.active_repo.id].return_types == 2) {                 // support internal files only                 le_style = ' style=""""display:none;""""';             }             if ((this.options.externallink && this.options.env == 'editor' && this.options.return_types != 1)) {                 html += '<tr'+le_style+'><td></td><td class=""""<USER>left""""><input type=""""checkbox"""" id=""""linkexternal-'+client_id+'"""" value="""""""" '+le_checked+' />'+M.str.repository.linkexternal+'</td></tr>';             }              if (!args.hasauthor) {                 // the author of the file                 html += '<tr><td class=""""<USER>right""""><label for=""""text-author"""">'+M.str.repository.author+' :</label></td>';                 html += '<td class=""""<USER>left""""><input id=""""text-author-'+client_id+'"""" type=""""text"""" name=""""author"""" value=""""'+this.options.author+'"""" /></td>';                 html += '</tr>';             }              if (!args.haslicense) {                 // the license of the file                 var licenses = this.options.licenses;                 html += '<tr><td class=""""<USER>right""""><label for=""""select-license-'+client_id+'"""">'+M.str.repository.chooselicense+' :</label></td>';                 html += '<td class=""""<USER>left""""><select name=""""license"""" id=""""select-license-'+client_id+'"""">';                 var recentlicense = YAHOO.util.Cookie.get('recentlicense');                 if (recentlicense) {                     this.options.defaultlicense=recentlicense;                 }                 for (var i in licenses) {                     if (this.options.defaultlicense==licenses[i].shortname) {                         var selected = ' selected';                     } else {                         var selected = '';                     }                     html += '<option value=""""'+licenses[i].shortname+'""""'+selected+'>'+licenses[i].fullname+'</option>';                 }                 html += '</select></td></tr>';             }             html += '</table>'; {code}  But there is a problem with the logic when you are checking for 'hasauthor' and 'haslicense'.  Because neither of those properties is passed to the function, the are both """"undefined"""" and as a result your if condition results to true and prints the fields.  *To solve this you should check if it's false*.  Though the biggest problem is that the properties aren't passed to this function and repository plug-ins have no way of hiding those fields.""",Bug,Repository
351370,"""Just upgrade to version 2.3 for our integration platform. Seems like the theming and graphics have change quite a bit. All my images return a 404 error. It's not a permission issue. I want trough the image.php script and did not found anything that could cause the problem.   The image path has a timestamp though. *myserver/moodle/theme/image.php/afterburner/core/+1340729489+/i/navigationitem*  Have anyone experienced similar issue ?  Thank you""",Bug,Themes
351385,"""There are some places in code where maxbytes is set for filemanager as, for example, $course->maxbytes.  If user logs in as admin (or person with capability moodle/course:ignorefilesizelimits), filemanager form element tells me that file size is unlimited and I am allowed to upload a big file.  Although when user saves the form, the funciton file_save_draft_area_files() checks if filesize is not more than $options['maxbytes'] and does not save the big file.  I just fixed the maxbytes for <USER>forum in MDL-33948 but it does not seem possible to fix it everywhere where filemanager or textarea elements are created. Instead I would suggest that file_save_draft_area_files() checks the unlimited capability.  Examples of such forms: <USER>glossary, <USER>folder and probably many others""",Bug,Files API
351539,"""On test upgrade of 2.3 on moodle.org, logged in as admin the server runs out of memory when clicking on server files.  I'm pretty sure its doing something stupid there, previously have mentioned this issue in MDL-27236 and MDL-27236.  I did one 'blind' bugfix to improve the situation in the past, we need to resolve the problem properly.""",Bug,Repository
351771,"""We have found that for enrollment plugins that are also managing role assignments that adding a combined index on the fields component, itemid and userid on the role_assignments table can improve query performance.  We saw performance degradation when the role_assignments table had 1 million plus records.  Sorry, but I'm not sure if I have a core related query as an example for improvement, but we are querying against this table for an enrollment plugin to determine role assignment updates.  Just to avoid any confusion of the type of index, this is what we are suggesting: {code:sql} ALTER TABLE `<USER>role_assignments` ADD KEY `component_itemid_userid` (`component`,`itemid`,`userid`); {code}""",Improvement,Roles / Access
351788,"""Previously there was a checkbox that gave a marker the option to send feedback (or not) after grading an assignment.    This checkbox no longer appears.    To be honest, I don't think this option is necessary, but I'm recording its absence here for the sake of consistency.""",Improvement,Assignment
351809,"""Forgive me if this has been posted already. I'm not an IT person so some of the other posts don't make total sense to me. When viewing the course as a student, some images in the labels are not displaying, but others are. We are using 2.0 at present. I have uploaded all of them in .jpg format. I'm not sure what other information would be useful or even where to look to start resolving this, so please feel free to ask questions or direct me where to look. Thanks""",Bug,Course
352270,"""On fixing MDL-33053, I've noticed that the new nav panel has a different behaviour compared to the <Previous>/<Continue> buttons in 1.9. Here the user has '<<', '<', '^', '>', '>>' i.e. a set of tools to fully browse/traverse the tree but it seems to be inconsistent.  I'm working for a patch proposal to improve its features, to get the same functionality as in 1.9 as well as a consistent - at least for me ;) - tree browsing navigation.""",Improvement,SCORM
352302,"""If you drag/drop a file on a course, the Change Group Mode icon is shown, though not as a link. Upon refreshing the page, it disappears.  Please note, I'm aware that the toolbox icons don't work with JS immediately and have submitted a fix in MDL-33073""",Bug,"Course,JavaScript"
352325,"""Just been looking at a course in topic format with custom section names.  The course was set up with a general introduction in the general topic, followed by three general topics and then a topic for each week where weeks are numbered from 1 to 25. As a result, Week 1 appears in topic 4.  I think that it should be possible to remove the course section numbers in the topic format - though it may be desireable to either have it as an option, or include it in the section title.  I've included some screenshots to demonstrate the issues.""",Bug,Course
352338,"""When editing 'styles.css' in a course format, the styles are applied globally, they should only be applied when the format is being used.  Using Moodle 2.2.3 (Build: 20120514).  As an example, attached is a course format I've developed for iMoot 2012, which admittedly has css designed to affect the whole page, but it cleanly illustrates the point as is applied on the whole site.  To test, place the extracted 'imootopics' folder in '/course/format' and click on 'Notifications'.  There are no database modifications.  The course format appears as 'iMoot Topics' in the list of formats.  The workaround to this is a separate css file that is included by adding appropriate css sytle html tags to 'format.php'.""",Bug,"Themes,Course"
352403,"""The $COURSE variable is not being correctly set when viewing a repository in the FilePicker. I'm using the $COURSE->id and $COURSE->category parameters within the Repository lib.php (and some other lcode which is called from here) and the values of these parameters are always 1 and 0 respectively, no matter where the resource is actually located within the course structure. Under Moodle 2.0/2.1 these values were always correctly set.  Marking this as a minor security issue because while there may not be a direct impact on Moodle itself, this does have an impact on the security of my repository code. The Moodle categories are mapped to categories in the remote repository and it is vital that I am able to correctly map the categories using $COURSE->category to read the current category ID in Moodle.""",Bug,Repository
352409,"""In the discussion in MDL-32412, where it was proposed to use a percentage layout based on <USER><USER>Taylor's 'Perfect Holy Grail (Percentage) Layout', <USER>made a comment from which I would like to quote: {noformat} """"...Based on a quick read: if the issue here is that the base theme needs fixing to make it work better with RTL, then let's fix/replace the base theme. I don't think adding a new separate theme sounds like a good idea. It was already made confusing enough with the introduction of Canvas..."""" {noformat}  In line with <USER>s comments then, I would like to propose that BASE theme layout is change from PIXEL to PERCENTAGE for Moodle 2.4 and than all NEW themes can be based on this new layout.  The Perfect Holy Grail (Percentage) Layout uses less divs than BASE and is more versatile, it contains No CSS hacks, is SEO friendly, contains no images, no JavaScript and is Cross-browser & iPhone compatible.  Current CORE themes, whose parent is only BASE, could be converted to use the new layout. Whereas CANVAS theme could carry the old pagelayout.css from BASE while at the same time excluding the new new pagelayout.css from BASE theme, thus making those themes, whose parents are bother CANVAS and BASE, benefit from this and so will NOT need converting to the new layout since it will not affect them.""",Sub-task,Themes
352931,"""As an improvement I would like to see Anomaly with some basic custom settings this would include a 'tagline' MDL-29561 in the page header and also a Custom CSS textarea to add changes to the CSS.""",Sub-task,Themes
352997,"""This is true for both """"Include enrolled users"""" and """"Anonymize user information"""" on the Initial settings step of backup.  Default moodle install. Add a course and a user. Put that user in as a Teacher (editing teacher) (user w/o the moodle/backup:userinfo or moodle/backup:anonymise capabilities, but with other backup capabilities).  As that user, go to the first backup page, """"Include enrolled users"""" is properly Xed out, but """"Anonymize user information"""" is not, even thought the user doesn't have that capability.  Now go into (as an admin) Settings->Course->Backup->General Backup Defaults, and change backup_general_users to No. As the user, go back into the backup first page. Notice that """"Include enrolled users"""" is now unchecked and clickable (and if you click it all the children that should shouldn't have are clickable too). If you click it and continue you will get an exception (hence why this is not a security flaw).  Basically: Capability Yes and Default Yes = Checked and changeable Capability Yes and Default No = Unchecked and changeable Capability No and Default Yes = Xed out Capability No and Default No = Unchecked and changeable (Should be Xed out)   The same logic flaw applies to both """"Include enrolled users"""" (moodle/backup:userinfo) and """"Anonymize user information"""" (moodle/backup:anonymise)   I've already tracked it down and have a proposed patch, which I'll add here as soon as I have a MDL number.""",Bug,Backup
353009,"""It seems MDL-31086 introduces a very slow query in calendar, for large sites.  {code} $sql = """"SELECT DISTINCT c.* $select FROM {course} c JOIN {event} e ON e.courseid = c.id $join""""; {code}  In my case, after upgrading to 2.1.5 via CLI, I could not login to the site as an admin due to hitting 5 minute connection timeout. Checking postgres showed this query running for several minutes - it appears to try and sort by all columns in the query, then return the top 20 for the get_records_sql limit. (So, not a very good plan, but...)  Since we know course id is unique enough, it is much nicer on the database to advise it only to make the results unique by c.id i.e: {code} -        $sql = """"SELECT DISTINCT c.* $select +        $sql = """"SELECT DISTINCT ON (c.id) c.* $select {code}  This makes the query return in only 10ms for this site. I'm not sure if this is compatible enough syntax for core, however.  Github incoming.""",Bug,Performance
353046,"""I created a new account and confirmed it, then my login timed out and I could not log back in.  Got the message to contact the site administrator -- but there is no way to do that in """"guest"""" mode!!!!!  I was able to create a new account with the same password and I'm logged in again.  """,Improvement,Accessibility
353051,"""The date_time_selector has a javascript calendar popup, this works ok, unless the date_time_selector is one of the top elements of the form. The popup is constrained by the form area, but this is too restricted for some cases.  I've added a picture of the form constrain, and one where it has been replaced by the body element (simple fix).""",Bug,Forms Library
353070,"""When I enter as another user with the """"Login as"""" function and then I want to return to my role Moodle logsme out automatically and I have to reenter my credencials to the moodle site. In moodle 1.9 the behaviour was that you return to your user with all the capabilities and with your session. """,Bug,Authentication
353194,"""I feel like this has to have come up before, but I've looked through ~400 tickets and haven't found it.    It would be great if there was a pix store in dataroot that would just override base. Much the same way dataroot/lang overrides the base language translations.    E.G.: We have an icon that we want to replace a base icon in all themes (that don't override it themselves). Right now, as far as I can tell, we have to replace the icon in dirroot/pix/. We like to make as few changes in the dirroot as possible for hopefully obvious reasons.    I would be happy to work on the patch for this, but if it's too likely to get reject on some principle, I won't bother, so I want to run it by the people that know the most about this area.""",New Feature,Themes
353251,"""Steps to take to see issue: # In Front Page settings choose to show Course List on the front page with and without being logged in  # Log in as a user that is enrolled in courses.  You will see the course summary next to each course # Log in as a user that is NOT enrolled in a course.   The course summaries do not show next to each course.  Instead an information icon shows next to each course name and you must click on the icon to read the course summary.  I would expect to find the course summaries listed so users not enrolled can see summary of courses offered.    Please note that the course summaries do show by the courses before a user logs into the site.  It is only after the user logs into the site that they do not easily see the summaries. """,Bug,Course
353354,"""When managing my private files.  I'm trying to move files around.  When I click on a file and select """"move"""" the filepicker window shows.  The folder I'm trying to move it to has a LOT of files and folders.  To be able to move it to that folder you must click on the folder name which makes it expand.  The window then extends below the viewable area and when I scroll down to try and get to the """"move"""" button the window stays about 10px from the top in the window.  If I click the """"minus"""" button to close the expanded folder it essentially doesn't have that folder selected so I can choose """"move"""". Javascript is keeping the window there.  Either the window needs to be set to the height of the viewable window and can be scrollable or the window needs to stay at the very top even when scrolling.  *Replication steps:* # Have a file system with a large number of folders (20 or so). # Reduce the size of your browser window. # Click on the menu icon next to a file/folder and click Move. # Try to scroll to reach the """"move"""" button.""",Bug,Files API
353547,"""Howdy,  While working on an entry script today that had a mode parameter that has two known values I got thinking that it would be great if we had a function that worked like optional/required_param but also checked that the provided values was one of an array of expected values. So I've created a patch with a couple of functions optional_param_enum and required_param_enum which I'll push to the my github repo shortly so that you can all have look. I'd like to know if anyone has any thoughts on this, good or bad. If there's no interest and people can see reasons why we shouldn't add these then cool I'll forget about it. However if there is interest and people think it is a good idea then I'll polish the functions, find a few core areas to convert and then put this up for inclusion.  Cheers <USER>",New Feature,Libraries
353569,"""After logging in as a student and attempting a scorm package (one with an assessment), when the editing teacher looks at the results (Showing all students) any student after the student that takes the test has information entered in the columns 'Started on', 'Last accessed on' and 'Score'.  I've attached the scorm quiz that I've been using to test scorm activities with.""",Bug,SCORM
353628,"""While working on the new local_dev plugin, I realized that the current moodle.git does not contain all commits from CVS. The reason for this was that we kept just refs/heads/MOODLE_16_STABLE and higher. So all commits that led to MOODLE_13_STABLE, MOODLE_14_STABLE and MOODLE_15_STABLE were pruned by Git garbage collector. Therefore there was no way to re-tag legacy releases (which is what I need for local_dev).  Luckily there is still Catalyst's moodle-r2.git available so I was able to fetch missing commits and add them to my clone of moodle.git. I think it would be nice to have upstream moodle.git complete so I'm submitting this to upstream.  To get missing branches, commits and tags to integration.git, integrators may want to do something like this:  # git fetch --tags git://github.com/<USER>moodle.git MOODLE_13_STABLE:MOODLE_13_STABLE # git fetch --tags git://github.com/<USER>moodle.git MOODLE_14_STABLE:MOODLE_14_STABLE # git fetch --tags git://github.com/<USER>moodle.git MOODLE_15_STABLE:MOODLE_15_STABLE  When testing passes, just push the new branches and tags to moodle.git  See the testing instructions for how I think this could be double-checked.""",Bug,Admin
353768,"""Hello again, another bug unfortunately :(   This function should return the email of the user on some conditions, namely:  1) if the invoking user was a site admin, or 2) if the invoking user could check hidden user details (moodle/user:viewhiddendetails , although I'm not 100% certain on this one)  Neither of the conditions are met, because the underlying function to get user details, namely user_get_user_details() only returns email if those conditions are met:  1) if the capability 'moodle/course:useremail' is true, which will never be upon this webservice call 2) if the user has the email visible to everyone 3) or if the invoking user is in the same course of the requested user and the requested user is allowing email sharing only for members of his courses  So , the underlying function does not check if the calling user is admin, which I think it's wrong, because the admin should have access to the e-mail of all users, and also the users that have the capability moodle/user:viewhiddendetails should as well, but on this one I am not sure.  I was developing the implementation for get_users() when I stumbled upon this bug. This might be critical for some integrations I think (at least on mine is), because normally an email is an important key.  Edit: I forgot to mention that there is only one condition on which the e-mail is returned: when the invoking user is the same as the requested user.""",Bug,Web Services
354135,"""We noticed this when looking at an issue with timeouts in the rss_client block. The failure we were seeing was caused by the rss_client block calling lib/web.php->break_up_long_words($description, 30) where $description was a string of around 4,600 characters. break_up_long_words() instantiates a new textlib class and calls $textlib->substr on each character in turn; which in turn performs an iconv_substr on the supplied text.  Effectively, the code is calling: {code} $strlen = $textlib->strlen($text); for ($i = 0; $i < $strlen; $i++) {   $foo += $textlib->substr($text, $i, 1); } {code}  I'll attach some code which demonstrates the performance compared with mb_string()""",Bug,Libraries
354183,"""The SCORM course format allows a single SCORM instance to load as a course - I've often thought it would be good to make this more generic and allow other single modules to load as a course (had someone ask about facetoface this week)   The scormcloud plugin is another good candidate for this - they completely duplicate the existing scorm course format.""",New Feature,"Course,SCORM"
354199,"""1)i would like to upload files in <USER>site as admin but i can't open the upload files.  2)in how many ways we can upload files into moodles  3)i can't see floder (Or) file option in moodle   how to rectify all above prmbs  """,Bug,Libraries
354335,"""This may not be considered a true bug in the code, but it can become an issue if one tries to place the value of the summary field into the name field so that (A) the navigation block will use this title instead of Topic X and (B) a block such as topcoll will display the topic title properly.  What I did, was to execute the following query to populate the <USER>course_sections name field : update <USER>course_sections set name = summary  If there was any code preceding the text for the section summary, topcoll works fine but the navigation side block only shows ... for the section name.  Then the  """"error/setting_invalid_ui_label' error occurs when backing up those courses.  I think the issue is directly correlatable to the value in the name field in <USER>course_sections.  If the title in the summary field starts with any code (such as <span style=""""font-family: arial,helvetica,sans-serif;"""">Lecture 1) then the code (<span style=""""font-family: arial,helvetica,sans-serif;""""> breaks the ability to backup the course and causes the  """"error/setting_invalid_ui_label' to occur.  Unfortunately, i do not know of a clean way to strip out the code so that only the text is moved to the name fild from the summary field.  This is what I'm hoping can be resolved in a future update .... a way to populate the name field other than manually, so the Navigation block will show the proper name of the topic/section.  What first led me to this discovery was that courses with all properly named sections in the Navigation block backed up ok.  Those with ... in place of the section names in the side block would not.  Changing the section names to fewer characters allowed the title to show in the side block but the course would still receive the 'error/setting_invalid_ui_label' error.  The only thing that would eliminate the error for those courses was to revert to the default section title.  Why post this as a bug? ... This may be the cause of others receiving the  """"error/setting_invalid_ui_label' error message   As an aside, is there an easy way to set the default value for the course section names field for the courses that will not backup?  I have 43 of them and that's a lot of changing of this toggle when each course has 14-15 sections.  I use navicat to make backend changes to the database, so if there is a field that can be modified, it would be much easier and quicker.""",Bug,Backup
354454,"""Site Administration > Front Page > Front Page restore reports """"no files available"""" even when backups have been created.    Completing a Front Page backup takes the user to a list of available backups at /backup/restorefile.php?contextid=2, and from here the Front Page can be restored. However, the actual link to """"Front Page restore"""" points to /files/index.php?id=1&wdir=%2Fbackupdata, which always reports """"no files available"""".      Steps to reproduce  ==============  1. Login as Admin and click Site Administration > Front Page > Front Page backup  2. Complete the steps to create a backup of the Front Page  3. Verify that the new backup page is listed  4. Click Site Administration > Front Page > Front Page restore    Expected result  ===========  I would expect to see a list of backups of the Front Page, from which I could choose one to restore.    Actual result  =========  Message """"No files available"""" appears.""",Bug,Backup
354808,"""If the manager role is given to someone locally for certain category, the person can either create or edit a subcategory but neither hide nor delete it. There is no available interface for that. To delete or to hide the subcategory can only a global manager. As a local manager you can hide or delete only courses. I would expect that the local manager should be able not only to create the subcategories but also delete them.""",Bug,Admin
354901,"""We had a couple of cases recently of confusing bugs in our (non-core) code related to the way themes and page layout work.  In one case there were certain situations where the module ended up calling format_text before it had initialised the page. This basically causes everything to break. You do get a developer warning at this point that tells you that you haven't initialised page context, so this is fair enough.  But we have also had a similar problem where a page started displaying in the wrong page layout unexpectedly (it otherwise worked OK because in that case we had already set up the context) because we changed something that caused a call to format_text before setting the page layout.  Here are some independent suggestions for improvement:  1) Currently, when it initialises the theme it also fixes the page layout. This could be changed to a two-step process so that you can initialise the theme and the rest of the page object, but without fixing the actual page layout. The page layout should only be fixed when $OUTPUT->header is called. (See the example code in MDL-30329.) The reason this is important is that there are many unpredictable things you can do - not just directly calling format_text, but calling some function which happens to call some function which happens to call format_text, etc - which end up fixing the page layout unexpectedly.  2) If you call set_pagelayout after the page layout has already been fixed, this should print a developer debug warning (note: I would prefer to make it throw a coding_exception but I suspect there is existing broken code in this area).  3) Could add to format_text phpdoc to mention that you must not call it before setting page context and otherwise initialising the page.""",Improvement,Libraries
355051,"""When adding back a deleted peer, the peer should probably default to no longer being deleted  I'd argue that, if a peer is added and it already exists as a deleted peer, then that peer should be undeleted automatically""",Bug,Usability
355095,"""I'm noticing with several clients that there is not quite enough granularity available for """"Big A"""" admins who want to delegate most site management options to another user, but without giving them so much in terms of permissions that they can bring Moodle to its knees. This, I think, is a shortcoming in the granularity of the site:config role, which gives too much or not enough depending on who you talk to.  In particular, many of the settings within the 'Server' and probably 'Networking' areas (which you get access to when you have site:config capability) are things which should only be changed by a Sys Admin who understands what will happen if they start to mess with things like SMTP server settings (as an example).  Contrast this with the ability for a site configurator to switch activity modules on or off, which although may cause some surprises for end users when they see their plugins no longer available, is something which can be remedied with the flick of a switch, and is not something which is potentially going to have an impact on the performance or availability of the whole environment.  I'd love to see the site:config capability have some of the more 'dangerous' settings split out into another capability called site:systemadmin or similar - the former lets non-sysadmin types mess about with Moodle's configuration settings in the knowledge that they are not going to kill Moodle, the latter gives the 'serious' sysadmins the ability to change all the heavily technical stuff.  Thoughts anyone? """,Improvement,Roles / Access
355197,"""(10:31:52) <USER>Mudrak: JS """"enhancements"""" are useless if they in fact require me to click three times to actually edit something (10:32:43) <USER>Mudrak: When I press """"+Criterion"""", can we have a row with three or four blanks open, with points prepopulated to 0, 1, 2 and 3 ? (10:33:08) <USER>Mudrak: Whenever I want to add criterion or level, I have to click too much (10:33:43) <USER>Mudrak: when blank textareas are pre-opened, I can even jump using Tab  """,Sub-task,Grading methods
355230,"""Currently, an instructor can lock him or herself out of their course by using the """"Delete the contents of this course and then restore"""" option.  I've included a patch that allows for enrollments to persist when the option is taken. It also gives an admin the ability to """"purge enrollments"""" as a default behavior during the restore process.""",Improvement,Backup
355340,"""As a language teacher, I often need to give feedback to 50 students quickly.   I like to use the Online Text Assignment activity for this, because the student's answer is automatically copied into my text edit field and I can go to work immediately, and click """"Save and next"""" to continue with the next students assigment - wonderful! :-)  I high-light a grammar mistake with the mouse, then click the Text colour icon from the toolbar to open the colour menu, and finally choose the colour green from the frequent colour list in the pop-up window. For a spelling mistake I choose red and for general language improvement I choose pink. This has to be done several times per paragraph.  I would like a Text colour bar like the Table bar in the editor, because this would make feedback quicker by reducing the number of steps and removing the loading of the colour pop-up window. In short, you would brighten my day! :-)""",Improvement,Accessibility
355407,"""If you enter the manage blocks setting page in the administration menu, you do have the possibility to """"protect from delete"""" each block. The help says that """"Selected block instances will be protected from deletion from the site-wide context. This is primarily used to protect the navigation and settings blocks which can be very hard to get back if accidentally deleted."""".  BUT if you try to do this, some blocks act different, but I didn't find any block that acts like described within the help.  The course block, if added to the Default MyMoodle page appears for students until they click the """"Customise this page"""" button. The block disappears from the MyMoodle page, even if it was """"Protected from deleting"""".  The Calendar block has a different behaviour. If added to the Default MyMoodle page and protected from deleting, it appears on the students MyMoodle page but after clicking """"Customise this page"""" it is still deletable!!!!  So I've also tried to change the """"protect from delete"""" settings for the Navigation and Settings block, which come protected by default. If you deactivate the protect from delete option they are still NOT deleteable for students on the MyMoodle page.  So it seems like the """"protect from delete"""" button next to each block on the manage blocks setting page doesn't work at all. I was thinking of it as a replacement for the sticky blocks functionality.  Or did I miss the intended function of this button!?!?! """,Bug,Admin
355409,"""I really like the """"user override"""" feature for quizzes in Moodle 2+.  I had been wanting this for over 5 years.  Thanks.    I would like to see the same thing, however, for Assignments.  I looked all over for it, but could not find it.  This is why I am suggesting this as a new feature request.""",New Feature,Assignment
355598,"""user/editadvanced_form.php uses get_plugin_list() for the available auth options.  I'm wondering if this is by design or is incorrect as in it should instead be using something like get_enabled_auth_plugins()?  Since the login process has a fallback routine to other auth plugins (I'm pretty sure about this) maybe this is by design and allows the setting of auth plugins that may not yet be enabled.  Could cause end users some panic if they are actually using disabled auth plugins, not sure.""",Improvement,Authentication
355644,"""I noticed some inconsistency in the user interface of Moodle.  When I was adding a grade item, I noticed that making this grade item hidden, or not hidden, was a """"check box"""" (see attachment).  Then I recalled that in many other places in Moodle, for example making a URL visible, a combo box is used (see attachment 2).  This made me wonder about this inconsistent user interface.  I am wondering whether a check box should be used whenever switching between opinion and not hidden characteristics?  It seems to me that a check box is more appropriate when there are only two choices, in this case either on or off.  I don't know if anybody has addressed this issue in the past, so I thought I would open this issue as a suggested improvement to the overall Moodle interface. However, I also recognize that there may be some good reasons why a combo box is being used.  Also, I do like the way the little """"eye"""" icon is used to turn items hidden or not hidden.""",Improvement,Accessibility
355692,"""If courses have been created by an enrolment plugin which doesn't provide any summary, then the summary field is set to null.  Under Postgres (and probably other DBs) when the fullname and summary are concatenated, the presence of the NULL summary field means that the resulting concatenation is also null.  E.g.: If summary = 'foo', and fullname = 'bar', then summary || ' ' || fullname = 'foo bar' If summary IS NULL, and fullname = 'bar', then summary || ' ' || fulname = NULL  As a result, the search fails to match on valid fullname matches.  I'm unsure as to how compatible COALESCE is, but under Postgres, COALESCEing the summary, and an empty string ensures that we can search if the summary is NULL. For example: {code}   $concat = $DB->sql_concat('c.summary', """"' '"""", 'c.fullname', """"' '"""", 'c.idnumber', """"' '"""", 'c.shortname'); {code} becomes: {code}   $concat = $DB->sql_concat(""""COALESCE(c.summary, '')"""", """"' '"""", 'c.fullname', """"' '"""", 'c.idnumber', """"' '"""", 'c.shortname'); {code} """,Bug,"Course,Database SQL/XMLDB"
355826,"""Hello, I've done some searching on the forum and tracker and couldn't find this exact issue, so please excuse me if I have overlooked it.  We use the auth/db plugin to synchronize the user table with an external database. I have the 'Removed ext. user' setting set to 'Suspend internal user'. Note that the help message besides this setting says """"Only suspended users are automatically revived if they reappear in ext source.""""  My external database server went down for a bit last night, and this caused auth/db to set all users to auth='nologin' per my 'Removed ext. user' setting. (Note that their usernames remained intact.) This is fine and expected.   However, when it came back up, the external users were not restored to auth='db', they were left as is. Then, the auth/db sync_users proceeded to try to add all of my external users again, which caused it to encounter a database error on the first record since it tried to insert a duplicate user (violation of the unique index <USER>user_mneuse_uix which enforces unique username & mnethostid pairs).  So, my suspended users are not being revived.  Looking at the auth/db sync_users function (auth/db/auth.php) in my build, I can see why: (line numbers for my version are on the left) {code:title=auth/db/auth.php|borderStyle=solid} 255 } else if ($this->config->removeuser == AUTH_REMOVEUSER_SUSPEND) { 256 $updateuser = new stdClass(); 257 $updateuser->id   = $user->id; 258 $updateuser->auth = 'nologin'; 259 $DB->update_record('user', $updateuser); 260 echo """"\t""""; print_string('auth_dbsuspenduser', 'auth_db', array('name'=>$user->username, 'id'=>$user->id)); echo """"\n""""; 261 } {code}  According to this, all that is supposed to happen upon obeying the Suspend User setting is to update their ID with auth=nologin. This is happening.  However, this is what happens when it is adding users: {code:title=auth/db/auth.php|borderStyle=solid} 354 // maybe the user has been deleted before 355 if ($old_user = $DB->get_record('user', array('username'=>$user->username, 'deleted'=>1, 'mnethostid'=>$user->mnethostid))) { 356 $user->id = $old_user->id; 357 $DB->set_field('user', 'deleted', 0, array('username'=>$user->username)); 358 echo """"\t""""; print_string('auth_dbreviveduser', 'auth_db', array('name'=>$user->username, 'id'=>$user->id)); echo """"\n""""; 359  360 } else { 361 $id = $DB->insert_record ('user',$user); // it is truly a new user 362 echo """"\t""""; print_string('auth_dbinsertuser','auth_db',array('name'=>$user->username, 'id'=>$id)); echo """"\n""""; 363 // if relevant, tag for password generation 364 if ($this->is_internal()) { 365 set_user_preference('auth_forcepasswordchange', 1, $id); 366 set_user_preference('create_password',          1, $id); 367 } 368 } {code}  Accordign to this code, it is reviving users when the following 3 conditions hold: 1. username of new user is same as existing user -- this is true in my case 2. mnethostid of new user is same as existing user -- this is true in my case 3. deleted=1 -- this is not true in my case, in my case auth='nologin' and deleted attribute is not touched.  So it appears that by design, in my build, the revive users function is broken. In fact, I see no place in my sync_users() where it ever sets deleted=1 on any user.  Please advise. There were some patched versions of auth/db around, but I'm not sure if they are appropriate to use; I will gladly use a patched copy if that is the fix.  In the meantime, this means that whenever my external user database goes down, even if it is not for a long time (in this case I think it was a mere reboot), if auth/db tries to contact it to synchronize at the same time, all of my users will be de-activated and I will have to manually re-activate them when I happen to discover this all has occurred. It is true that is set to auth/db synchronizes very often, but even if it does so less frequent, this issue can still occur, and also, besides the case where users are not revived when the database is down, this also means my users won't be revived when I actually intend to suspend individual users and the database is still up to sync everyone else correctly.  Thanks!""",Bug,Authentication
355913,"""When viewing the message popup in a course and it's wide, then when there is a course completion icon behind it, the icon will bleed through.  This is because the icon has a z-index of 10.  I'm attaching two fixes: one incases the z-index of the popup and the other is to remove the z-index from the completion icon.  I'm in favor of the removal of the z-index from the icon because it causes problems with almost any modal you try to bring up in the course, but I don't know why the z-index is there in the first place (removing doesn't seem to break, but did not do comprehensive testing).  Maybe a hybrid approach would be better, as I think a z-index of 1 for the icon would be an improvement.""",Bug,Themes
356045,"""While playing with the enrolment UI (enrol/users.php) I've found various inconsistencies:  1) The whole """"roles"""" cell acts as a button for the add roles popup. 2) If you click 10 times in the """"roles"""" cell, without picking any role, and the 11th time you pick one role, it is added (at least visually) 10 times. (screenshot added) 3) The """"add"""" roles popup is not modal (aka does not dim the UI), it continues allowing other actions like """"remove role"""" to happen (screenshot added).""",Bug,JavaScript
356418,"""Authenticated users see the combined list view, i.e. they see a course category like """"Graduates of 2012"""" and then all ten courses - 1st through 10th grade. Below is the course category """"Graduates of 2013"""" and then all ten courses for those graduates, and so on.  As administrator upon login the combined view is very handy - you don't have to click, only scroll, and using the scroll wheel or scroll stroke on a touch pad you can move very quickly up and down to choose the category or course you want.  Unfortunately, students see the same thing - all categories and all courses within them.  I want students to   a) only see the category and the courses within it that they are enrolled to. b) be taken directly to their standard course so that they immediately see the week view. By standard course, I mean that students in 2nd grade belonging to the """"Graduates of 2012""""-category are taken to the 2nd grade course within the """"Graduates of 2012""""-category.  The students are put into cohorts with names like """"Graduates of 2012"""", """"Graduates of 2013"""" and so on, and then enrolled with the role of student to only one course within one category at the moment.""",Improvement,Accessibility
356542,"""The enrolment web service has a way through the manual plugin to enroll users, but it doesn't have a way to unenroll users.  I've been working on writing an unenrol user function in moodleDir/enrol/manual/externallib.php and I can post what I have if you all feel like this is something that should be implemented.""",New Feature,Web Services
356560,"""original description:  ------------------    Admins should see webservice tokens for all users, regardless of who created the token.  Currently the list is filtered by 'creatorid' == $USER->id in adminlib.php:admin_setting_managewebservicetokens->output_html()    I'm marking this as a security issue as it obscures external access routes from admin accounts.    ------------------  correction to the description from [~<USER>:  ------------------    I think that nobody should see others keys, it is like password. If you use loginas the system knows it is not the user, but if you steal the key or password nobody would know. My -10 for disclosing other normal user keys and to prevent disclosure when logged-in-as. Resetting does not make sense either if you can not gain access to the keys. There is one notable exception though, the webservice users can not login, so there must be a way for admin to setup and use the keys. My +1 to add capability to generate/reset/read keys for webservice users (because they can not use normal login/UI).""",Improvement,Web Services
356794,"""This would provide an easy way to create a set of related courses.  It appears that at the moment the only way to do this would be to put the resources/activities in the meta course, determine the link, and then manually put the link into ever child course. Why this is considered better that just uploading the file multiple times and avoiding the bother of creating a meta course isn't clear to me.   We're trying to avoid the practice of creating umbrella courses for multiple section courses.  Not only do this confuse faculty members and students, we've had a problem where a course starts with two sections, the instructor starts to build the course in the umbrella site, then one section is dropped.  As a single section course, the umbrella course is automatically deleted - along with the instructors work.  If we kept the umbrella they we'd have two course sites for one section which would just confuse thing more.   I want a meta course the instructor can use to administer all the section sites without the student being aware of what's happening.""",Improvement,Course
356828,"""alt+click/right+click is now captured by script and places godawful useless dropdown. I can no longer alt+click to copy image location for an image I'm going to embed in a Web page. I have to go through an insane workflow where I upload the image to the folder, add the image to the course as a resource, parse through the HTML for the resource page to get image url, and copy it there. Being able to get the URL of a file so it can be embedded in HTML is such a *basic* thing for instructional designers. Please, please fix this. At least give me the option in your godawful useless dropdown?  Replication instructions: # Upload an image or other file to file picker. # Right+click or alt+click. # View resulting useless dropdown.""",Bug,Files API
356841,"""This was an experimental hack designed for the first generation of japanese smart phones that did not support sessions in built-in browser.  Why remove? * it is a big security hole allowing session fixation attacks * all recent smart phones support sessions * buggy and unmaintained code * it was abused to work around site misconfiguration (Moodle 2.x actively prevents this now) * some people thought that it might help then with cookie privacy issues (wrong, session cookies are exception)  I would really like to get this removed from 2.2 asap...""",Task,Admin
356858,"""One of the most common issues I deal with with clients is that their learners sometimes do not have learner progress recorded. The scorm packages that I see associated with this kind of issue are all the same in the way they behave: they initialize and send some data on initial load, and then do not send any data until the last page view of the scorm content. I suspect that somewhere in there the learner (on hotel wireless, traveling, not the best 'net connection, whatever) has network dropped, and when data tries to save, it is not saved, but the SCORM object does not tell them at any time that network connection has been lost. Some vendors create content that only calls LMSCommit() once in a single 20m session, just before LMSFinish()... and according to the SCORM standard this is not incorrect.     It would be great if the Moodle scorm player could check the user's network connection at intervals, and notify the learner if network connection was intermittent or lost. This would preempt these issues with crappy vendor content. From what I can tell, if the network connection is lost, then LMSCommit() and other calls fail but do not return an error message. In my own testing, try/catch on these calls only catches in some browsers, not all.    Based on my own research, I've been able to determine that it is possible to test network failure by injecting an image into the content page with a same-domain src. This test works in webkit and ff. (not in IE). I also tested the .isonline status, but that is not reliable either. So it seems there isn't any reliable cross-browser way of checking the learner's network connection using javascript.     But I wanted to raise the issue here just in case there is a server-side way, that we could access from the scorm player code, of checking the network at intervals during an api session. I know this can add to bandwidth/load stuff, but it would be great, even as an option to turn on if a site had a lot of learners who weren't having their results saved.   """,New Feature,SCORM
356990,"""I received a question from a user that was confused by the current UI. There was an option to 'Choose a link' however because none of the enabled file repositories allowed for an external link to be returned they received the notice """"Sorry, none of your current repositories can return external files."""".   I would like to propose one of two options to possibly help improve the UI:  1) If there is no way for the user to choose a link (because none of those repositories are available) then do not show the option 2) Create a search engine repository that could be used to search various sites (yahoo.com, google.com, msn.com, bing, etc.)  My concern about option 1 is that it would cause confusion as to why the option sometimes appears for one set of users and not others. The current way is pretty transparent in that the option is always there and if there are no options the file picker box tells you so. It is simple and consistent which is a plug.   Option 2, should be pretty trivial to develop and may help folks get the idea of how to pull things in. I could see the initial box having a few options:  a) search term(s) b) search engine (Google, Yahoo, MSN, etc.) c) perhaps an advanced option for a site URL to search with in   Of course it could be argued that things are fine the way they are and folks simply need to learn to understand the behavior we have. If they want a search engine, go find what they are looking for an copy and paste the URL in rather than creating something within Moodle to do that. I can see arguments on both sides but figured it was worth throwing the idea out there to see if it might be helpful/useful.   Peace - Anthony  """,Improvement,Repository
357032,"""I just wanted to make sure that we might not have a potential leak of user information and if so think how we might avoid it. The case that got me thinking was that I logged in as site admin to qa.moodle.net and did a course backup. I noticed it was saved as a course backup and logged in as a teacher and was able to see the backup file created by the admin and download it. In that file was user information which the teacher should not have access to since it may contain user info about the admin. I'm not sure what the best way to handle this is. I would have expected the teacher not to see that particular file. Might it be better to the file in the user's private files? Peace - Anthony """,Task,Backup
357134,"""I was trying to write some simpletests to verify the correctness of our Moodbile Web Services when I realised that it was impossible.  External_api::validate_context method calls require_login function, which modifies $PAGE when """"Unit Testing"""" page printing has already started. So I get this error:  {noformat} Exception: local/moodbileserver/<USER>forum/simpletest/testforumexternal.php / ▶ forumexternal_test / ▶ test_get_forum_by_id Unexpected exception of type [coding_exception] with message [Coding error detected, it must be fixed by a programmer: The theme has already been set up for this page ready for output. Therefore, you can no longer change the theme, or anything that might affect what the current theme is, for example, the course.] in [/Users/jpiguillem/DFWikiLABS/projectes/Moodbile20/lib/pagelib.php line 1508]  Debug info:  Stack trace when the theme was set up:       line 1244 of /lib/setuplib.php: call to moodle_page->initialise_theme_and_output()     line ? of unknownfile: call to bootstrap_renderer->__call()     line 5664 of /lib/adminlib.php: call to bootstrap_renderer->single_button()     line 30 of /admin/report/unittest/index.php: call to admin_externalpage_setup()      line 727 of /lib/pagelib.php: call to moodle_page->ensure_theme_not_set()     line 799 of /lib/pagelib.php: call to moodle_page->set_course()     line 2347 of /lib/moodlelib.php: call to moodle_page->set_cm()     line 314 of /lib/externallib.php: call to require_login()     line 52 of /local/moodbileserver/<USER>forum/externallib.php: call to external_api::validate_context()     line 74 of /local/moodbileserver/<USER>forum/simpletest/testforumexternal.php: call to moodbileserver_forum_external::get_forum_by_id()     line ... of ...  {noformat}  Can it be considered a bug? An specific require_login function for Web Services might be implemented? I've seen that you haven't write any simpletest for externals, but you have written tests for SOAP, XML-RPC, etc. """,Bug,Web Services
357402,"""While working on MDL-27626 I am facing yet another issue with the order in which the handling methods are called for selected paths in the XML tree (moodle.xml in my case). The problem appears when there is a sub-branch in the middle of a branch and then other sub-branches follow at the end. This pattern is typical for structures like this (real example): {code}     <QUESTION_CATEGORIES>       <QUESTION_CATEGORY>         <ID>59</ID>         <NAME>Course level sub-sub-category</NAME>         <INFO></INFO>         <CONTEXT>           <LEVEL>course</LEVEL>         </CONTEXT>         <STAMP>glum+110531083817+9h2lta</STAMP>         <PARENT>0</PARENT>         <SORTORDER>999</SORTORDER>         <QUESTIONS>           <QUESTION>             <ID>7</ID>             ...             <MODIFIEDBY>2</MODIFIEDBY>           </QUESTION>         </QUESTIONS>       </QUESTION_CATEGORY>     </QUESTION_CATEGORIES> {code} Note that the QUESTION_CATEGORY element has a CONTEXT sub-path within its final elements and then another QUESTIONS sub-path after its final elements. So far, I've been dealing with the sub-path in the middle of final tags by relying on the processing method being called twice, with both halves of the final data being passed separately. That works well, but the order. See how notify_path_start(), notify_path_end() and dispatch_chunk() are called when parsing the structure above: {code} /MOODLE_BACKUP/COURSE/QUESTION_CATEGORIES start /MOODLE_BACKUP/COURSE/QUESTION_CATEGORIES/QUESTION_CATEGORY start /MOODLE_BACKUP/COURSE/QUESTION_CATEGORIES/QUESTION_CATEGORY process /MOODLE_BACKUP/COURSE/QUESTION_CATEGORIES/QUESTION_CATEGORY/CONTEXT start /MOODLE_BACKUP/COURSE/QUESTION_CATEGORIES/QUESTION_CATEGORY/CONTEXT process /MOODLE_BACKUP/COURSE/QUESTION_CATEGORIES/QUESTION_CATEGORY/CONTEXT end /MOODLE_BACKUP/COURSE/QUESTION_CATEGORIES/QUESTION_CATEGORY/QUESTIONS start /MOODLE_BACKUP/COURSE/QUESTION_CATEGORIES/QUESTION_CATEGORY/QUESTIONS/QUESTION start /MOODLE_BACKUP/COURSE/QUESTION_CATEGORIES/QUESTION_CATEGORY process /MOODLE_BACKUP/COURSE/QUESTION_CATEGORIES/QUESTION_CATEGORY/QUESTIONS/QUESTION process /MOODLE_BACKUP/COURSE/QUESTION_CATEGORIES/QUESTION_CATEGORY/QUESTIONS/QUESTION end /MOODLE_BACKUP/COURSE/QUESTION_CATEGORIES/QUESTION_CATEGORY/QUESTIONS end {code} As you can see, the """"QUESTIONS start"""" and """"QUESTIONS/QUESTION start"""" are triggered before the """"QUESTION_CATEGORY process"""" is dispatched with the second remaining half of the category data (STAMP, PARENT and SORTORDER).  I believe I will be able to hack the question bank conversion code to cope with that. I just want to be sure this is expected behaviour. Or is there a way how the second incarnation of """"QUESTION_CATEGORY process"""" could come before the questions starts?  For now, I'm setting this as a blocker as we must decide and specify the parser's behaviour to rely on.""",Bug,Backup
357601,"""I am using Moodle 2.0.1 (Build: 20101225) and FireFox 4.0.1.  In the """"Overlay"""" (three-column theme) I was customising the """"Default My Moodle Page"""".  Signed in as admin I did not notice anything displaying incorrectly.  When signed in as a student, the only column that displayed was the main / large column.  The two smaller columns containing the blocks no longer display.    I changed the theme to the two-columned """"Fusion"""" theme.  When signed in as admin, I see the theme as expected.  Still in """"Fusion"""", I signed in as student and the column containing blocks disappears.   What have I done?  I retraced my steps and don't see where I've made my error.  This does not appear to be a theme-specific error.  I want to use the """"Overlay"""" theme.  How can I get the two smaller columns to display again while signed in as a student?  Thanks in advance.""",Bug,Themes
357821,"""When I try to restore a course from the attached backup, I get the errors in the attached apache log extract  The fatal error seems to be the duplicate entry into grade letter table, but it may be as a result of the preceding ones, so I have included these  Please note that I can restore the backup to a fresh clean install of moodle 2.0.2, but not the production one, which was upgraded from 1.9.10 recently. I can also restore it twice to the fresh clean install (ie the first one is not fouling up the second one)  So I'm thinking that the problem is data related  I have tried restoring it to different categories within the production moodle, but I get the error each time """,Bug,"Course,Backup"
357829,"""Hi, While creating a custom grader report, I found a minor oversight in the M.gradereport_grader.classes.ajax.prototype.submission_outcome() function in grade\report\grader\module.js has a Javascript error in the catch block of the function: <snip>         message.replace(/\[1\]/, args.type);         message.replace(/\[2\]/, M.gradereport_grader.users[args.properties.userid]); </snip> The error is only visible when AJAX is enabled for the grader report and the ajax_callbacks.php has an error in it. The replace method doesn't assign the result its parent object(message in this case) - it needs to be assigned to a variable like: <snip>         message = message.replace(/\[1\]/, args.type);         message = message.replace(/\[2\]/, this.report.users[args.properties.userid]); </snip>  In addition, M.gradereport_grader.users[] is empty, so we need to use the """"this"""" reference.  I've attached a patch file that resolves the problem as shown above.  Cheers, <USER>",Bug,JavaScript
357857,"""These have not been marked as high priority, but they still require attention, so I'm listing them here for completeness.  1.  In FF various non-link items receive keyboard focus but they shouldn't because they are not links: label module instances (these each have two tab stops).  Recommendation: ensure non-link items do not receive keyboard focus.  2.  On Calendar block there is invisible tab stop between 'skip calendar' link and the month link. There is also invisible tab stop between month link and current date link.  Recommendation: ensure focus is visible, or remove focusable items.  3.  The Navigation block comes after the main content in the tab order. Recommendation: tab order should follow visual/logical order.  4.  'Your progress' tickboxes: when operate these with the keyboard, the focus returns to the top of the page.  The same issue applies for screenreader users.  Recommendation: if possible enable focus to remain on the button. (Another one for you <USER>)  5.  Link icon is labelled """"URL"""" which may not be meaningful for screenreader users.  Recommendation: change this to """"link"""" or """"web page"""" or similar.  6. A URL resource item is read as """"bullet link graphic URL link getting started URL"""" which seems unnecessarily long.  This is due to 'URL' being repeated and there being two separate links.  The same repetition occurs on the other items.  Recommendations: remove the repetition of URL (and similar labels) and if possible remove one of the links.  Perhaps the two links can be combined into one? (Note - I'm not sure what Chetz means by two links here, as the html clearly only has one).  7. The 'hide' block buttons are announced as graphics, but not as links so a screenreader user would not know they could interact with it is a button.  Recommendation: enable this to be announced as a link or a button so users know they can interact with it.  8  Resource icons: in OU-Moodle 1.9 there was clever programming to indicate icon types without repeating the name of the item, e.g. if the link to a forum was 'activity forum' the forum icon label would not be included, but if the link was 'café' then the 'forum' icon label would be included.  Recommendation: this function should be instated in standard Moodle.  9  File icon: this has the alt 'file' but this does not indicate the type of the file and this could be significant information for screenreader users.  Recommendation: change this to indicate the type of file, e.g. PDF, Word etc.  """,Bug,Accessibility
357901,"""Scenario:  I have a course which I want to let teachers edit, but there is also one resource in the course which I'd like to lock from editing, as it is a standard resource which teachers should not be changing within the course. I (as an admin) edit the resource, and go to the Permissions screen for this resource so that I can override the role for the teacher to prevent them editing it, but there's no capability in there to say 'stop this role editing this resource'.  Proposed change - let me have a way of blocking editing of this one resource for a given role, whether it be by adding another capability (if that's possible) or through another mechanism.  Possible?""",Improvement,Roles / Access
357911,"""in MDL-26580 Petr has described concern for performance problems with upgrade_set_timeout() as well as adding a no-timeout option for cli mode upgrade:  (16:38:10) <EMAIL>: arrggh, I might have found a bug in my upgrade_set_timeout() (16:38:23) <EMAIL>: could you please review it for me? (16:38:27) apu: really?? sure (16:39:11) <EMAIL>: the question is - does it execute get_config() too often? (16:39:59) apu: !isset($CFG->upgraderunning) or $CFG->upgraderunning < time()  (16:40:00) apu: hm (16:40:06) <EMAIL>: yes (16:40:53) <EMAIL>: if (!isset($CFG->upgraderunning) or $CFG->upgraderunning + XXX < time()) { (16:41:44) <EMAIL>: I suppose 10 could work there (16:42:11) apu: tricky, how much to use is how much u save (16:42:47) <EMAIL>: it would be critical if you use it in large loops like those you added (16:44:02) <EMAIL>: well, not really critical (16:44:22) apu: yes, it would help, what is the risk.. that between 0- 10 we skip get_config and assume running (16:44:44) <EMAIL>: no big deal, if anybody interrupts upgrade it may create problems anyway (16:45:16) apu: ahh.. yea, interrupt as in cli ctrl-c ? (16:45:37) <EMAIL>: hmmmmmmmmmmmmmm (16:45:53) <EMAIL>: I think can remove the timeouts completely in CLI mode (16:46:30) apu: yes, cli mode should be much more interactive (or it can be) (16:47:02) <EMAIL>: the problem is when you stop web loading the upgrade has to continue until the next step to get consistent data, on the other hand ppl using CLI do not ctrl-C when it runs (16:50:53) <EMAIL>: did you plan to fix some other upgrade issues next week? (16:51:35) apu: i don't have any, not even planned the next 'sprint' ... i can work on it still if there are any (16:52:33) <EMAIL>: the problem is that the MDL-26580 should be resolved by the PULL, but I think it might not be (16:53:11) apu: going with reports so far, it is, but there is no other trace given (from Luis) (16:53:26) <EMAIL>: did he test it? (16:53:32) apu: no (16:53:48) apu: no one did yet (16:53:59) <EMAIL>: that is not good (16:54:25) apu: yea.. i know.. i'll ask in tracker (16:54:57) apu: grr, MDL-17344 for MOODLE_20_STABLE keeps conflicting.. (16:55:11) apu: espcially version.php (16:55:24) <EMAIL>: yeah, tricky (16:57:40) <EMAIL>: I am a bit scared to commit your patch without fixing the upgrade_set_timeout(), but at the same time I am not 100% sure the 10s is good there (16:58:15) <EMAIL>: also it would be great to get some testing from the reporter (16:58:43) apu: yea it would <USER>we get him to test the pull or test in tracker? (16:58:56) <EMAIL>: maybe we could add the 10s there and let him use your branch for testing""",Bug,Admin
357999,"""Hello I'm not sure this is the place for a suggestion bur here it is.  I think it would be interesting to have a block only for teachers into each subject block.  I usually work with moodle to support my presential courses, and I think would be interesting to have an area in each content section to which the students don't have access (only for teachers). It could be as """"complements for teachers block"""" with some complements, exercises, exercise solutions, extra content to project or to use with the interactive white-board.   I've attached a photo to show an example. In the photo the block is called """"RPOFESORES""""  I know i can change the permissions directly to the content but i think there is enought space in the right side of each content block to put it there and is really useful for teachers that use moodle as a complement to their presential courses.  """,New Feature,Course
358041,"""I've got the setting block as a sticky running through the site but..clicking some of the my profile settings sub-menu links whilst in context on the course pages bring up coding errors as per the stack trace debugging below..The block is set for anywhere on the site any page. Though tried it with block set in the context of course only and still the same errors: Beyond my php coding experience at the moment, and pretty sure permissions and configurations are ok, done cron/purges just in case but still the bugs persist. Although I have customized the strings to re-label the portfolio links and setting block name the rest are as per moodle default...  The sub-menus in the MY profile settings that do work in the course page context are: Change password Security keys blogs Preferences External blogs Register an external blog  The problem sub-menus are Edit profile Portfolio Messaging + edit icon on settings block    Edit profile Stack trace: line 3972 of /lib/accesslib.php: coding_exception thrown line 3089 of /lib/accesslib.php: call to get_course_context() line 77 of /blocks/online_users/block_online_users.php: call to get_enrolled_sql() line 279 of /blocks/moodleblock.class.php: call to block_online_users->get_content() line 232 of /blocks/moodleblock.class.php: call to block_base->formatted_contents() line 895 of /lib/blocklib.php: call to block_base->get_content_for_output() line 947 of /lib/blocklib.php: call to block_manager->create_block_contents() line 342 of /lib/blocklib.php: call to block_manager->ensure_content_created() line 5 of /theme/magazine/layout/general.php: call to block_manager->region_has_content() line 647 of /lib/outputrenderers.php: call to include() line 605 of /lib/outputrenderers.php: call to core_renderer->render_page_layout() line 270 of /user/editadvanced.php: call to core_renderer->header()   Portfolio configure line 3972 of /lib/accesslib.php: coding_exception thrown line 3089 of /lib/accesslib.php: call to get_course_context() line 77 of /blocks/online_users/block_online_users.php: call to get_enrolled_sql() line 279 of /blocks/moodleblock.class.php: call to block_online_users->get_content() line 232 of /blocks/moodleblock.class.php: call to block_base->formatted_contents() line 895 of /lib/blocklib.php: call to block_base->get_content_for_output() line 947 of /lib/blocklib.php: call to block_manager->create_block_contents() line 342 of /lib/blocklib.php: call to block_manager->ensure_content_created() line 6 of /theme/magazine/layout/general.php: call to block_manager->region_has_content() line 647 of /lib/outputrenderers.php: call to include() line 605 of /lib/outputrenderers.php: call to core_renderer->render_page_layout() line ? of unknownfile: call to core_renderer->header() line 1200 of /lib/setuplib.php: call to call_user_func_array() line ? of unknownfile: call to bootstrap_renderer->__call() line 68 of /user/portfolio.php: call to bootstrap_renderer->header()  portfolio logs line 3972 of /lib/accesslib.php: coding_exception thrown line 3089 of /lib/accesslib.php: call to get_course_context() line 77 of /blocks/online_users/block_online_users.php: call to get_enrolled_sql() line 279 of /blocks/moodleblock.class.php: call to block_online_users->get_content() line 232 of /blocks/moodleblock.class.php: call to block_base->formatted_contents() line 895 of /lib/blocklib.php: call to block_base->get_content_for_output() line 947 of /lib/blocklib.php: call to block_manager->create_block_contents() line 342 of /lib/blocklib.php: call to block_manager->ensure_content_created() line 6 of /theme/magazine/layout/general.php: call to block_manager->region_has_content() line 647 of /lib/outputrenderers.php: call to include() line 605 of /lib/outputrenderers.php: call to core_renderer->render_page_layout() line ? of unknownfile: call to core_renderer->header() line 1200 of /lib/setuplib.php: call to call_user_func_array() line ? of unknownfile: call to bootstrap_renderer->__call() line 64 of /user/portfoliologs.php: call to bootstrap_renderer->header()  My Profile settings messaging configuration messaging configure  Stack trace: line 3972 of /lib/accesslib.php: coding_exception thrown line 3089 of /lib/accesslib.php: call to get_course_context() line 77 of /blocks/online_users/block_online_users.php: call to get_enrolled_sql() line 279 of /blocks/moodleblock.class.php: call to block_online_users->get_content() line 232 of /blocks/moodleblock.class.php: call to block_base->formatted_contents() line 895 of /lib/blocklib.php: call to block_base->get_content_for_output() line 947 of /lib/blocklib.php: call to block_manager->create_block_contents() line 342 of /lib/blocklib.php: call to block_manager->ensure_content_created() line 6 of /theme/magazine/layout/general.php: call to block_manager->region_has_content() line 647 of /lib/outputrenderers.php: call to include() line 605 of /lib/outputrenderers.php: call to core_renderer->render_page_layout() line ? of unknownfile: call to core_renderer->header() line 1200 of /lib/setuplib.php: call to call_user_func_array() line ? of unknownfile: call to bootstrap_renderer->__call() line 206 of /message/edit.php: call to bootstrap_renderer->header()  Also this debugging info comes up clicking on the edit block icon (again in the context of the course pages): line 3968 of /lib/accesslib.php: coding_exception thrown line 3089 of /lib/accesslib.php: call to get_course_context() line 77 of /blocks/online_users/block_online_users.php: call to get_enrolled_sql() line 279 of /blocks/moodleblock.class.php: call to block_online_users->get_content() line 232 of /blocks/moodleblock.class.php: call to block_base->formatted_contents() line 895 of /lib/blocklib.php: call to block_base->get_content_for_output() line 947 of /lib/blocklib.php: call to block_manager->create_block_contents() line 342 of /lib/blocklib.php: call to block_manager->ensure_content_created() line 5 of /theme/magazine/layout/general.php: call to block_manager->region_has_content() line 647 of /lib/outputrenderers.php: call to include() line 605 of /lib/outputrenderers.php: call to core_renderer->render_page_layout() line 1271 of /lib/blocklib.php: call to core_renderer->header() line 1040 of /lib/blocklib.php: call to block_manager->process_url_edit() line 1178 of /lib/pagelib.php: call to block_manager->process_url_actions() line 702 of /lib/pagelib.php: call to moodle_page->starting_output() line 600 of /lib/outputrenderers.php: call to moodle_page->set_state() line 196 of /course/view.php: call to core_renderer->header()  """,Bug,Course
358100,"""I want to use the dropbox repository for every user of my moodle.  I am admin and I configurated the dropbox plugin with my dropbox key and secret. When I want upload a file I can choose the dropbox for this action. (see dropbox-admin.png)  When I am logged in as a second user with the teacher role or as a third user with the student role I can't use the dropbox repository of the second or third user. (see dropbox-another-user.png) I could not find a documentation for this problem. """,Bug,"Repository,Usability,Documentation"
358161,"""As an external service, I want to enrol users in courses.  But I don't know the id of the user, or the contextid of the course.  Solution: expose something like the following:  {code}enrol_user(array $user, array $course, array $role){code}  For example, any of the following should work:  {code} enrol_user(array('username'  => '<USER>) ,             array('shortname' => 'PHP 101'),            array('shortname' => 'student') );  enrol_user(array('idnumber'  => '0123456'),             array('idnumber'  => '2010-2012-PHP101'),             array('id'        => '3') );  enrol_user(array('email'     => '<EMAIL>'),            array('fullname'  => 'Introduction to PHP programming'),            array('name'      => 'Student') ); {code}  Looks a bit like MDL-26250 and MDL-26251""",Improvement,Web Services
358199,"""In the question code, I would like to be able to delete a files en-masse, where the itemid comes from a subquery.  <USER>has a similar requirement to bulk delete in ForumNG.  Therefore, I have implemented this method.""",Improvement,Files API
358312,"""Customizing file picker UI is either difficult or non-existent.  The attached patch is a very very basic start to adding additional hooks for customization by firing off an event.  This is just a start though, it would be better if the repository API had a hook for plugins to register their JS event handlers (EG: $PAGE->requires->js_init_call) and then an easier way for the JS handlers to identify if they are indeed the active repository or not.  What I mean by the last part is that I don't think knowing the repository ID would necessarily work unless all repository ID's were given at the time of the js_init_call.  Otherwise, I would suggest that the active_repo object in filepicker.js has a plugin name or component that can be passed along with the JS events.""",Improvement,Repository
358386,"""It should be possible to upload a new version of an image you have made for (e.g.) a forum post without renaming it. There may actually be two issues here...  To reproduce:  1) Create new forum post. Add an image called image.jpg (uploaded from your hard disk). Save the post.  2) Edit the forum post. Delete the existing image (e.g. select it, press Del).  3) Immediately add a new image, again using upload. On your hard disk, select a different file that is also called image.jpg (imagine this as a person who has made the image, but now changes it slightly and saves changes).  4) When you click the button to upload the new file, you get a popup error 'File name already being used, please use another name'  This error is incorrect; the file name is not being used any more, because it was deleted.  5) Cancel out of the image dialogs but save the edited post, so that it now does not contain any image at all (and this change has been saved).  6) Edit the post again.  7) Add the image again as in step 3.  The same error appears (now it is even more incorrect).    So there may be two issues here:  * (step 3) file api code doesn't notice that somebody has used various tinymce UI to delete the image. I can imagine this is probably difficult to fix.  * (step 7) after saving the post, the file api (save_draft_area_files or something?) does not notice that the previously-in-use images are no longer present in the html. Ideally it should detect this as part of the replacing it does, and remove the file references.  It would also be possible to workaround all such cases by making it automatically change the filename e.g. to add _1 or _(n+1) if it already has _n at the end before extension. This might be a better solution.  Anyway - not sure any action is needed but our testers reported this so I thought I would check and file it.""",Bug,Files API
358387,"""This was caused by the code in MDL-16592. The date picker used to be basic, but usable and fast to operate. Now it is hard to operate and looks broken. Our technical testing staff reported it as a bug as follows:    <<  For each of the date fields for the above labels i.e. 'allow editing from' and 'prevent editing from' the drop-down and the calender get displayed at the same time. This looks untidy and only one of the 2 options is necessary to make the selection and there is no need to have them both displayed at the same time.  >>    My suggestion would be:    1) As the easiest fix to leave it in a professional, working state with reasonable UI, we could just remove the popup.    2) Alternatively I suggest adding a separate button/icon (only added in JavaScript) which appears just to the right of the dropdowns, and opens the popup. So it doesn't interfere with you using the dropdowns at all. When clicking that button, the popup appears and the dropdown is disabled until you close the popup (which updates the dropdown dates).    If anybody wants us to implement #1 we certainly can. I think we might have time to implement #2 as well. Alternatively of course we're happy to leave this to somebody else :) This isn't on our critical list, we can leave it sucking if we have to. But it does suck really hard, I'm surprised nobody else has complained.    On which note - I did try to use Jira search to find if anyone had already reported this, I searched several times but didn't spot it? Very odd? Probably this is a duplicate and I just couldn't find it?""",Bug,"JavaScript,Forms Library"
358662,"""During an upgrade from 1.9.10 to Moodle 2.0 I get an error during the enrol_meta upgrade process that says """"Can not find data record in database table course"""" (see screen shot).  If I flush out the table <USER>meta prior to the upgrade it works fine but obviously I have no child courses or enrolments. If I ignore the error then the upgrade continues but meta functionality does not work even though it's enabled in the Moodle 2.0 enrolment settings.  I can then not select this as a plugin.  At this stage I'm not sure where it's failing or what the error message means.  My upgrade has over 3000 meta courses, could this be the issue? """,Bug,Course
358813,"""Hi, I'm generally impressed with a number of the new features in Moodle 2. However, I've found this bug:  1. I added a """"File"""" resource to my course. 2. I uploaded MP3 and OGG audio files to the File resource. 3. I wanted to embed an audio/ media player in a database activity (a comment), and right-clicked on the MP3 link to display my browser's context menu. 4. Instead I found the Moodle Javascript replacement for the browser default, which did not have a """"Copy Link Location"""" (see screenshot).  There may be an alternative way to achieve what I want, but that is not the point - I'm transitioning from 1.9 like lots of people, and after experimenting this was the first way I found.  This is a serious usability bug, and I seriously question whether it is appropriate to replace the right-click menu in this way. There are typically lots of useful commands in the browser right-click context menu, including ones added by browser extensions/ addons - these may be important for an individual's work/ learning.  While I as a developer can do a view page-source to get the URL, a teacher can not. """,Bug,"Usability,Files API"
358897,"""I'm marking usability aspects now as critical. I think if users don't understand features and can't find any help or explanation the feature won't work for them. That means the feature is not working and this is really a big critical problem.  The settings for blocks are not user friendly. Terms and functionalities are not explained. Also search at docs don't give any better information:  Where this block appears: missing help information. You can define where the block appears as a default block. Depending from the context where you are adding the block and your own role the options may be different.  Page context: missing help information Display on actual page Display on actual page and all subpages Display on all page types Display on My own MyMoodle page Display on all MyMoodle subpages Display on ...  Restrict to these pages types: human unfriendly cryptic description of page types. Really a mess. Some of the types have descriptions. Others not. If the cryptic terms are described in the brackets, the cryptic terms are only relevant for developers and can be deleted here.  Make descriptions understandable. delete: course-view-topics   write: Any course main page in topics format delete: course-view-topics-* write: Any course page in topics course format (Correct?) delete: course-view-*  write: Any course main page (format independent) (Correct?) delete: course-*  write: Any course page (format independent) (correct?) delete: *  write: Any page everywhere delete: my-index write: Users 'My' home page delete: my-index-* write ?  What does it mean? delete: my-* write: Any 'My' home pages from any user (Correct?) delete: * write: ?  What does it mean?  Specific sub page: missing help information Any page matching the above  This specific page (page 7)  ? what menans 'page 7'  Default region missing help information Defines where the block is placed: - left left column - right right column - middle/center area of the page (only if the theme allows this location)   Default weight Missing help information Defines the ordering of the block in the defined region -1 on top -2 second .. 0 middle 10 last  On this page missing help information What are these settings for? Why can I define visibility on this page if it is predefined that a block is shown at all page types? Why is there a different region/weight setting? What happens if the definitions differ?    Link to documentation page don't contain any of the information for the 2.0 page.   """,Bug,"Usability,Documentation"
358959,"""I have a test  install of Moodle 2.01+using the Base theme.  I've just started creating a test set up but have come up with the following error whilst creating categories.  The attached error message only shows in Internet Explorer and won't let me progress to a course that I've created under the last category of the screen shot - works OK in Firefox, Chrome and Opera.    My initial test site install was using Moodle 2.0 when I came across this issue. Only changes I'd made was to the Theme """"Splash"""" by adding the path to a graphic for a logo plus added some entries for the top drop down menu. I then upgraded to 2.01+ and still had the error as detailed. What I did notice after the upgrade was that the theme Spash seemed to have lost it's formatting even after a cache clear. So I changed to Base theme to explore from there. Interestingly I was able to navigate to my test course using IE 8 via the Navigation menu in the side bar OK <USER>""",Bug,Admin
358988,"""We have a number of people Blogging on a site. When someone adds an external Blog feed, next day (after Cron) any entries made on the site (rather than imported from the external site) are deleted! I've seen this 'disappearing Blog entries' happen twice now, and on both occasions it was just after someone added an external Blog feed. I've suggesting setting the Priority of this bug to a Blocker - because if sites are using Blogs as an assessment tool (e.g. a reflective diary) this loss of data could have a major impact. I'm happy to give developers Admin access to the site affected, or phpMyAdmin access to the database. <USER>:-)""",Bug,Blog
359048,"""Logged in as a new user to Moodle 2.0.1 authenticating via SSO. I'm attempting to edit my profile and trying to upload a profile picture.  I click on 'Choose a file...' next to the New Picture label. The File Picker window pops up, as does a small dialog box in front of it. The message is:  *** error (in the top bar) Unsupported redirect detected, script execution terminated. ***  I click on OK, which closes this box but leaves the File Picker in the background. The 'Loading...' circle continues to go round.   Clicking on 'Upload a file' from the File Picker brings up the same error again. """,Bug,Files API
359061,"""While peer-reviewing a patch I stumbled upon a bug with the file picker / mforms library. I was on a form where I was required to upload a file, however I found I could submit the form and proceed without the file. Originally I thought someone had forgot to add a required rule, however upon inspecting the form I see that it does indeed have a required rule on the file picker element.  To reproduce: # Log in as an admin # Enter a course and click _grades_ in the settings block for that course. # In the settings block again under _Grade administration_ expand _Import_ and click _CSV file_ # On the form you see immediately click _Upload grades_  Outcome: You will be taken to the next form in the series, however because the file picker has a required rule I would expect to get an error and see the original form again. I tested this by adding a validate method and manually validating the field, doing this worked fine, however it isn't a fix its a work around.  Yell out if there is anything more I can help with.  Cheers <USER>",Bug,"Forms Library,Files API"
359169,"""During performance testing -- with HTMLPurifier enabled, we witnessed Moodle 2.x re-wrote the HTMLPurifier cached serialised definition file for every request which used the purifier; this thoroughly destroyed performance of the entire application.  This is a major point of contention performance-wise, as all other pages being purified will block until completion of this write. This is heavily exaggerated when used in an environment with shared storage or a clustered filesystem used across multiple webservers and is essentially the worst use-case for a shared filesystem (multiple writes to the same file in the same directory, concurrently from multiple nodes).  I'm not sure if this is """"correct"""" behaviour from HTMLPurifier (i.e. does it need to re-write this file frequently?), but I suggest this needs to be repaired to not re-write the serialised definition file and use the already cached file (if possible), otherwise alternate config options would need to be provided so that these cached serialised definition files can be written to alternate directories (e.g. one for each webserver perhaps).  The poor performing call stack is:  lib/weblib.php: purify_html() => lib/htmlpurifier/HTMLPurifier.php: HTMLPurifier->purify()   => lib/htmlpurifier/HTMLPurifier/Generator.php: HTMLPurifier_Generator->__construct()     => lib/htmlpurifier/HTMLPurifier/Config.php: HTMLPurifier_Config->getHTMLDefinition()       => lib/htmlpurifier/HTMLPurifier/Config.php: HTMLPurifier_Config->getDefinition()         => lib/htmlpurifier/HTMLPurifier/DefinitionCache/Decorator/Cleanup.php: HTMLPurifier_DefinitionCache_Decorator_Cleanup->set()           => lib/htmlpurifier/HTMLPurifier/DefinitionCache/Decorator.php: HTMLPurifier_DefinitionCache_Decorator->set()             => lib/htmlpurifier/HTMLPurifier/DefinitionCache/Serializer.php: HTMLPurifier_DefinitionCache_Serializer->set()               => lib/htmlpurifier/HTMLPurifier/DefinitionCache/Serializer.php: HTMLPurifier_DefinitionCache_Serializer->_write() """,Bug,Performance
359207,"""I have tried to find this bug in the tracker, but I'have seen nothing like it.   To reproduce  1.log in as a teacher 2. click on """"my courses"""" in the navigation block 3. click on one of your course  4.click on participants 5. click on """"blogs""""   6. the following message appears: Coding error detected, it must be fixed by a programmer: Attempt to re-define already required string 'cancel' from lang file 'moodle'. Did you already ask for it with a different $a? Annuler !== Cancel""",Bug,Blog
359246,"""When logged in as an administrator I continuously received a """"stream not found"""" error if I embedded a flv file in a page with other text.  If I removed the other text then the video was """"found"""" and played correctly.  When a student logs in they still see the """"stream not found"""" error, although it works when I'm logged in as an admin.  If I open the link directly while logged in as the admin, I am prompted to download the video.  If I attempt to link directly while logged in as a student I receive a """"invalid user id"""" error.""",Bug,Files API
359370,"""Having added RSS feeds at the site level, on trying to add them to an RSS block, there's an error (shown below).  The issue is that title is an ntext block, and it can't be used with the order by clause. Locally, i've just cast title to nvarchar in the query, around   line 56 of \blocks\rss_client\edit_form.php   but that's not a fix-- I don't yet understand the db stuff enough to offer a real fix.   Message: [Microsoft][SQL Server Native Client 10.0][SQL Server]The text, ntext, and image data types cannot be compared or sorted, except when using IS NULL or LIKE operator.<br>  SELECT id, CASE WHEN preferredtitle = N'' THEN title ELSE preferredtitle END AS acutaltitle FROM bhi_block_rss_client WHERE userid = '2' OR shared = 1 ORDER BY acutaltitle [array ( 0 => '', 1 => '2', )] Stack trace: line 391 of \lib\dml\moodle_database.php: dml_read_exception thrown line 252 of \lib\dml\sqlsrv_native_moodle_database.php: call to moodle_database->query_end() line 363 of \lib\dml\sqlsrv_native_moodle_database.php: call to sqlsrv_native_moodle_database->query_end() line 761 of \lib\dml\sqlsrv_native_moodle_database.php: call to sqlsrv_native_moodle_database->do_query() line 838 of \lib\dml\sqlsrv_native_moodle_database.php: call to sqlsrv_native_moodle_database->get_recordset_sql() line 1183 of \lib\dml\moodle_database.php: call to sqlsrv_native_moodle_database->get_records_sql() line 56 of \blocks\rss_client\edit_form.php: call to moodle_database->get_records_sql_menu() line 65 of \blocks\edit_form.php: call to block_rss_client_edit_form->specific_definition() line 152 of \lib\formslib.php: call to block_edit_form->definition() line 58 of \blocks\edit_form.php: call to moodleform->moodleform() line 1166 of \lib\blocklib.php: call to block_edit_form->__construct() line 1027 of \lib\blocklib.php: call to block_manager->process_url_edit() line 1178 of \lib\pagelib.php: call to block_manager->process_url_actions() line 702 of \lib\pagelib.php: call to moodle_page->starting_output() line 580 of \lib\outputrenderers.php: call to moodle_page->set_state() line ? of unknownfile: call to core_renderer->header() line 1200 of \lib\setuplib.php: call to call_user_func_array() line ? of unknownfile: call to bootstrap_renderer->__call() line 89 of \index.php: call to bootstrap_renderer->heade""",Sub-task,Database SQL/XMLDB
359565,"""I encountered this problem when I started using the free moodle hosting site: learnbymoodle.com. I was able to upload around 55 students using the """"Upload users"""" function. A few days later, when I wanted to upload another group of students, only a blank page appears. Please help. This is the fatal error message:  Fatal error: Call to undefined method MoodleQuickForm_hidden::MoodleQuickForm_hidden() in /var/www/www.learnbymoodle.com/htdocs/users/encinas/lib/pear/HTML/QuickForm/element.php on line 363 """,Bug,Admin
359585,"""When backing up an activity, step 5. Complete has a continue button which leads to a restore page for the activity with the sections:  * Import a backup file * Activity backup area (with wrong help popup!) * Course backup area * User private backup area  I find it very confusing to end up on a restore page with so many sections.  Would it be possible for the backup process to finish with a page with only a link to download the backup file?  Also, I'm wondering whether it is necessary for an activity to have a restore link in the activity settings? Surely a restore link in the course administration settings is sufficient?""",Bug,Backup
359661,"""In Moodle 2, the capabilities such as   <USER>page:view  work correctly to prevent a user who does not have that capability from viewing the page. However:  a) They do not work on the course page or in navigation b) These capabilities do not exist for all modules  This is definitely not a bad enough problem to delay Moodle 2.0... However it is still a bug (at least part a).  II propose that, in Moodle 2.1, we should do the following:  a) Make these capabilities work (to hide the activity) on the course page and in navigation b) Implement these capabilities for all other modules that don't have them yet  As well as correcting existing behaviour and making the system more consistent, this also means you can do something which is only sort of possible at the moment: you can use roles to control who accesses a page.   For example if I want a forum which only people with the 'moderator' role can see. I don't want to mess about creating new groups and groupings just for this, I want to use the actual role to control access to the page. And if they don't have access to it, it's important that the link doesn't show on course page as I don't want a link which gives an error. Right now it is not possible to do this except by making the forum invisible and overriding the 'view hidden activities' permission. That's sort of an okay way to do it, but it is a bit hacky and doesn't allow for 'negative' cases (ie if I want everyone to view the forum *except* moderators).  ** I am very happy to implement these changes **  (Pretty much as soon as Moodle 2 branches for 2.1 dev to start, I think. Or can even code them in 2.0.x if somebody thinks that's a good idea.)  The reason for this is that we need this facility locally as we are moving to using more standard moodle features and fewer weird customisations. As a consequence we need the ability to control access to activities using roles. You can do that now for everything except 'view', but 'view' is important. So I have to implement this here, hence I'm very happy to do it in core moodle as well.""",Bug,Roles / Access
359685,"""regarding emulate bounds hack in MDL-24863 : Eloy <USER> """"the emulate bounds hack .... we did something similar for sqlsrv/mysqli and there was some unit tests related to that. And my mssql test server was ALREADY passing them, so the tests perhaps aren't valid? Or perhaps it doesn't affect ALL mssql (freetds) drivers?....""""  I'll look around to see what these other tests were (hopefully committed against MDL-23997 or the mysqli bug) and check why they were passing.  """,Sub-task,"Unit tests,Database SQL/XMLDB"
359895,"""With large Moodle installations containing lots of users, the assign roles page (/admin/roles/assign.php?contextid=1) takes increasingly longer times to load. At our institution this was around 30 seconds with 40000 users.   On investigation I found a constant defined in /admin/roles/assign.php called MAX_USERS_PER_PAGE set at 5000 which then isn't used.  Later in the code an SQL query counts the number of users but this is also not used. I believe that at this point the code should make a decision as to whether it is going to make the SQL call to get all registered users which it then uses to populate the list of users on the page.  It is this second SQL call which is costly and causes the slow down of the page.  Attached is a small patch which adds this logic and displays the 'too many users, please search instead'. As you can see, the patch is very small as most of the functionality is already contained in the page but isn't being used. For this reason I've marked this up as a bug.   For us, this reduced the initial page load from 30 seconds down to 3.""",Bug,"Admin,Usability"
360030,"""Was reviewing some old bugs, when I found this, by Anthony: MDLSITE-949  Apart for the contrib issue itself, that caused me to review a bit more what is the situation of those plugins under 2.0. Here it's the summary:  - They continue being supported, as far as get_plugin_types() continues returning them as available. - Under 1.9 there were 2 plugins: -- groups import: That has been moved, if I'm not wrong, to the """"manage groups"""" page. -- import of activities: That used to call restore under 1.9 and now hasn't sense at all (have to be deleted). - There is no frontend to show those plugins at al within 2.0  So the question is... to be or not to be. Do we need to keep them working, two alternatives:  1) To be (yes), then we need: - delete the """"import of activities"""" one. It's a different beast under 2.0. - re-link from somewhere to the course/import.php script (make the frontend visible).  2) Not to be (no), then we need - delete the """"import of activities"""" one. It's a different beast under 2.0. - delete them from get_plugin_types() - delete the whole course/import dir and the course/import.php script  I feel myself inclined to keep them working, just in case people has been using them in previous versions. In the other side, I don't know if anybody has used them really. Anyway my +1 goes to keep them  For deciding ASAP. Please comment. Ciao :-)""",Bug,Libraries
360182,"""The matter of having forum posts treated as messages has been really badly executed. This is particularly noticeable on the (admittedly high traffic) moodle.org site.   Some points...  1. The barrage of popups saying """"you have n new messages"""" is, at best, an annoyance.   2. If you mean forum posts you should say forum posts to avoid confusion - """"you have n new forum posts"""" AND/OR """"you have n new messages"""" would be better. It would be *even* better if it could be restricted to forum threads you actually participated in (a long time coming).   3. Having accepted the link to the new 'messages' all you get is a big list of usernames. I'm not sure what purpose this is supposed to serve or what the intention was. It seems the most useless option (for forum posts). You actually need to know what the discussion was and/or what forum it was in.   4. Having accepted one of the 'messages' (if you know to select the mysterious icon that is) you get the recycled message UI which implies a conversation between yourself and the message poster. This is clearly just wrong - surely you should go the the actual forum post or the UI should be much more clear that this is a forum post not a proper message.   Sorry to sound so negative but this seem thoroughly broken to me.""",Bug,Usability
360193,"""Since CLI maintenance mode was implemented (MDL-24723), if you have $CFG->debug set high enough to display notices and $CFG->displaydebug set to true (both in your config.php), theme/image.php gives the following (and therefore doesn' serve images properly): Notice: Constant CLI_MAINTENANCE already defined in C:\xampp\htdocs\moodle2\lib\setup.php on line 171  This is because lib/setup.php gets included twice by theme/image.php - once via config.php with ABORT_AFTER_CONFIG set, and then again with ABORT_AFTER_CONFIG_CANCEL set - but the new CLI_MAINTENANCE code happens before the ABORT_AFTER_CONFIG trigger.  It can be fixed by shifting the CLI_MAINTENANCE code block down to below the ABORT_AFTER_CONFIG block, though I'm not sure if this has any implications for CLI_MAINTENANCE.  Another option would be to define/set NO_DEBUG_DISPLAY in theme/image.php, but that might obscure any other issues which may arise in theme/image.php...""",Bug,"Themes,Admin,Libraries"
360364,"""I was told in the forum that this would make a good item for the tracker as there may be others interested in this feature. When I set up groups in the offline world, I can have those groups submit a single copy of an assignment and when I grade that assignment, the feedback and grade can be shared by the whole group.  I would like to be able to do the same thing in the online Moodle world.  Yes, there are times that I want to place students in groups but still have them submit assignments individually that the whole group can then view, view feedback and even see grades on.  However, for some assignments, since the groups are working collaboratively, it would be a waste of time (and online disk space) to have students submit the same assignment 4 or 5 times so that it could be graded 4 or 5 times, assigned feedback 4 or 5 times and assigned a numerical score 4 or 5 times.  It would be much more beneficial to both students (so they don't have to log out one student so another can log on and upload the file) and me as a teacher to be able to have an assignment (whether online text or upload) checked as being not just separate groups but a Shared Group Assignment so that a submission by one person in the group would count for everyone.  The person who replied in the forum <USER>the only way to simulate such a thing now would be to setup a """"dummy"""" student where all students in the group shared the password and could upload the information.  However, that doesn't allow each person's individual grade book to be updated when the assignment is submitted and graded.  It seems that this would be fairly simple except that it does involve more than just a single submission...it also involves the grade books, etc.""",New Feature,Assignment
360367,"""I set my site to require agreement with the Site Terms and Conditions statement.  It works fine for enrolling new accounts.  However, if I log in as a Guest, it also requires the guest to check the agreement check-box.  But, it does not present the agreement popup for the Guest to read (see attached screenshot).  I don't think the guest user should be required to agree with the terms statement.  They should not even see the terms statement at login.  The guest will have limited access to the site anyway and the requirement to read and agree with the Terms statement will just cause my guests to leave the site.  I want guests to have access to a demonstration course.  All other courses and functions are blocked anyway by the role feature.  Thanks, Clarence""",Bug,Authentication
360415,"""I'm hoping this is fixed in 2.0, but figured I would make an issue in part as a reminder to myself to follow up. I was trying to show someone how to import courses from another teacher's course. They were a non-editing teacher and I wanted them to be able to pull those resources in from the other teacher; however, the capabilities required were manage activities and course update which of course makes them the equivalent of a teacher and make the whole non-editing part obsolete. In the spirit of sharing, I would think that the non-editing teacher role would allow a teacher to see the resources and activities being used in one course and allow teachers to get them. Perhaps we need a new capability to handle this scenario or perhaps it is already handled in 2.0. Peace - Anthony """,Improvement,Backup
360536,"""I went to upload a system file (an image to be used by a theme) to the legacy files area. I clicked add, went through the upgrade, and saw the image listed on the list of legacy files and thought I was done. I did not realize that it saved it into a draft mode and that when I left the page my work evaporated. I'm not sure what the purpose/benefit of this draft mode is but it seems that requiring the user to hit save changes without much of a visual warning that they are in draft mode is going to cause some confusion. Usually when you upload a file and see it listed as an attachment then you are finished so this seems like strange behavior compared to the rest of Moodle file attachments. Peace - Anthony """,Sub-task,Files API
360537,"""<USER>- I created a custom menu with a submenu. From the initial drop down the item for the submenu is not visible because the background of the site is white (#ffffff) and the text for that item type is also white (#ffffff) so it is not visible until hovering over. See attached screen shots to demonstrate.   This appears to be the affected css: #custommenu .yui3-menu-horizontal .yui3-menu-content .yui3-menu-label { color: #ffffff;  When I remove the color tag using Firebug - the text appears fine in black. Not sure what the best way to resolve this but figured I would raise it as an issue so that we don't have white text on a white background. Peace - Anthony """,Improvement,Themes
360569,"""The tag_compute_correlations() function in """"tag/lib.php"""" is not computing tags correctly. This is the query it's currently using to compute the correlated tags for a specified tag:  {code}$query = """"SELECT tb.tagid """".     """"FROM {$CFG->prefix}tag_instance ta INNER JOIN {$CFG->prefix}tag_instance tb ON ta.itemid = tb.itemid """".     """"WHERE ta.tagid = {$tag->id} AND tb.tagid != {$tag->id} """".     """"GROUP BY tb.tagid """".     """"HAVING COUNT(*) > $min_correlation """".     """"ORDER BY COUNT(*) DESC"""";{code}  However, in the 'tag_instance' table, different items can have the same 'itemid' but different 'itemtype'. The query above does not account for that. To fix only this query, you could change it to this:  {code}$query = """"SELECT tb.tagid """".     """"FROM {$CFG->prefix}tag_instance ta INNER JOIN {$CFG->prefix}tag_instance tb ON (ta.itemtype = tb.itemtype AND ta.itemid = tb.itemid) """".     """"WHERE ta.tagid = {$tag->id} AND tb.tagid != {$tag->id} """".     """"GROUP BY tb.tagid """".     """"HAVING COUNT(*) > $min_correlation """".     """"ORDER BY COUNT(*) DESC"""";{code}  The only difference is the join condition (the part after """"ON"""" in the 2nd line).  However, there's another problem with the tag_compute_correlations() function. It runs a minimum of 2*N+1 queries where N is the number of rows in the 'tag' table. 1 query to get a list of all the tags, 1 query per tag to get a list of correlated tags, and 1 query per tag to get the cached correlated tags from the 'tag_correlation' table.  I work for Remote-Learner and many of our clients have at least hundreds of rows in that table, and some have thousands. The particular client that caused me to discover this currently has 9649 rows in their 'tag' table. So every time that function gets called for their site, it runs a minimum of 19299 queries. I know that it's only supposed to run 20% of the times that 'admin/cron.php' runs, but still, that's a lot of queries! Especially when the equivalent can be done with 1 single query. I've attached a rewrite of the tag_compute_correlations() function that accomplishes this. I've thoroughly tested it on a few different sites. Let me know if you have any questions about it.  Sorry I didn't upload a patch file (if that's what you prefer). I thought it wouldn't really make sense as a patch since I just rewrote the whole function.""",Bug,"Performance,Database SQL/XMLDB"
360571,"""The follow query is being run all the time on all of our clients' Moodle sites. On one particular site, the query is running average of 1-2 times per second. Here's the query:  {code}SELECT g.id, g.name FROM <USER>glossary g,      <USER>course_modules cm,      <USER>modules m WHERE m.name = 'glossary' AND       cm.module = m.id AND       cm.visible = 1 AND       g.id = cm.instance AND       g.usedynalink != 0 AND       (g.course = 'XXX' OR g.globalglossary = 1) ORDER BY g.globalglossary, g.id;{code}  Where 'XXX' is a course ID number, of course (pun intended). This query is being run from the glossary_filter() function in '<USER>glossary/filter.php'.  MySQL reports this query as a 'Select_full_join' which is basically a join that's not properly using indexes. When you run the query with 'EXPLAIN' in front of it, MySQL gives you something like this:  || id || select_type || table || type || possible_keys || key || key_len || ref || rows || Extra || | 1 | SIMPLE | m | ref | PRIMARY,<USER>modu_nam_ix | <USER>modu_nam_ix | 62 | const | 1 | Using where; Using index; Using temporary; Using filesort | | 1 | SIMPLE | g | ALL | PRIMARY,<USER>glos_cou_ix | NULL | NULL | NULL | 307 | Using where | | 1 | SIMPLE | cm | ref | <USER>courmodu_vis_ix,<USER>courmodu_mod_ix,<USER>courmodu_ins_ix | <USER>courmodu_ins_ix | 8 | moodlel_classes.g.id | 1 | Using where |  Notice where it says 'ALL' in the 'type' column. This is a waste of memory. Especially when the query doesn't even return any rows (the one that produced that explain output didn't). If you add an index to the 'globalglossary' column of the 'glossary' table, the explain output turns into this:  || id || select_type || table || type || possible_keys || key || key_len || ref || rows || Extra || | 1 | SIMPLE | m | ref | PRIMARY,<USER>modu_nam_ix | <USER>modu_nam_ix | 62 | const | 1 | Using where; Using index; Using temporary; Using filesort | | 1 | SIMPLE | g | index_merge | PRIMARY,<USER>glos_cou_ix,<USER>glos_gloglo_ix | <USER>glos_cou_ix,<USER>glos_gloglo_ix | 8,1 | NULL | 4 | Using union(<USER>glos_cou_ix,<USER>glos_gloglo_ix); Using where | | 1 | SIMPLE | cm | ref | <USER>courmodu_vis_ix,<USER>courmodu_mod_ix,<USER>courmodu_ins_ix | <USER>courmodu_ins_ix | 8 | moodlel_classes.g.id | 1 | Using where |  So in this particular instance, this query is scanning 98.7% less rows each time it runs. Which when it runs 1-2 times per second, can be a big improvement.  I've confirmed that this 'bug' is present in versions 1.9.3 to the latest 2.0.""",Bug,Database SQL/XMLDB
360701,"""Hi <USER> while using the backup/restore UI here, I've found these things that should be fixed/improved. They are unrelared but as far as they are tiny, I put them here:  1) Logged as admin, any time I click on the """"backup"""" link. I get the initial page with the restore settings. And, using Safari, I clearly see how the """"Include user files"""" is disabled (by javascript) immediately after initial page rendering. So it causes user files to not being included in backups ever. Not sure if it's one Safari problem or happens with all them.  2) One improvement: When restoring one course, the 3-forms page is showed, with these options: - Restore into """"this"""" course - Restore as a """"new"""" course - Restore into an """"existing"""" course  I'd do this:   a) change the order to be """"new"""", """"this"""", """"existing"""" (or """"this"""", """"existing"""", """"new""""). The """"logical"""" point is to have """"existing"""" and """"this"""" together. Not really important. b) More important, in the """"existing"""" option, skip showing the """"this"""" course. It sounds silly but I've ended deleting my test course 2 times by mistake when I wanted to restore it into another course. If somebody wants to restore into """"this"""" he should use that option, and not the """"existing"""" one. I think I'd also keep the """"SITE"""" course (frontpage) out from the list. Overwriting it is really dangerous.  Ciao :-)""",Bug,Backup
360713,"""Dongsheng - I am using moodle.jesuitscholar.com as a test server and tried to setup Dropbox repository when navigating to select a file, I receive a message about an invalid  JSON string:   Notice:  Undefined property: stdClass::$path in /home1/jesuits1/public_html/moodle/repository/dropbox/lib.php on line 134  Notice:  Undefined property: stdClass::$contents in /home1/jesuits1/public_html/moodle/repository/dropbox/lib.php on line 163  Warning:  Invalid argument supplied for foreach() in /home1/jesuits1/public_html/moodle/repository/dropbox/lib.php on line 164 {""""list"""":[],""""path"""":[{""""name"""":""""Dropbox Sandbox"""",""""path"""":""""\/""""}],""""manage"""":false,""""dynload"""":true,""""nosearch"""":true,""""repo_id"""":5}  I took a quick look and the logic about testing for whether $result->path is set seems like it could use a little attention. It seems to be used without being checked for on line 134, then it is checked for being empty and sets a current path. It looks like a general review of the get_listing function could be helpful in cleaning up some whitespacing issues as well.   I'm hoping I have everything entered for this to work. I did enter the Dropbox API key on the Moodle end.   Peace - Anthony """,Bug,Repository
360784,"""Hi <USER> I attached few screenshot so you can see the problems. One of the screenshot is with IE7, I let you guess which one :)  To reproduce: 1. connect as admin 2. add community block to the front page 3. select search operation 4. on the search page, select Mooch hub, and select 'Course I want to download', select 'search' operation 5. on one of the result click on the comment button/icon 6. look at the problem (different on each screenshots caused by IE or the theme) """,Improvement,JavaScript
360813,"""Hi <USER>  just read somewhere (I think it was in the forums) that uploading one 1.9 backup file leads to one cryptic error (exception) in the early stages of restore. So I've uploaded one 1.9 backup file and have confirmed that this is thrown:  Backup is missing XML file: /Users/<USER>Sites/moodle_data_git_head/temp/backup/XXXXXXXXXXXXXX line 119 of /backup/util/helper/backup_general_helper.class.php: backup_helper_exception thrown line 172 of /backup/util/ui/restore_ui_stage.class.php: call to backup_general_helper::get_backup_information() line 56 of /backup/restore.php: call to restore_ui_stage_confirm->display()  The key here is that we are trying to execute backup_general_helper::get_backup_information() assuming that the directory contains one Moodle 2.0 backup. IMO, before calling that we must know which type of file and show the corresponding information (surely one nice message saying that 1.9 => 2.0 isn't implemented yet) as far a as a lot of users are going to try 1.9 backups for sure.  There is the static backup_general_helper::detect_backup_format($tempdir) method that will return you some information. You can execute it freely (without instantiating the whole restore controller).  Once we start working on conversions we'll have to organize all this again, to see where exactly detection/conversion happen, because right now the final flow isn't properly defined IMO. So let's talk about that later (here or in another bug). Surely we'll end instantiating the controller earlier (without courseid) to make it to perform the needed format checks and conversions... but we aren't going to do that yet.  But, as commented we need that detection running and displaying a nice message in early stages ASAP.  Ciao :-)""",Bug,Backup
360818,"""In bulk_user_download the user profile data is loaded by profile_load_data().  Each custom profile field is then iterated over and sent to $worksheet->write() from lib/excellib.class.php which does some manipulation of datatypes with regular expressions before actually writing to the spreadsheet file.  The problem is that if the custom profile field is a multi-select one, and the user has made multiple selections, the data loaded in the $user variable is stored in an array rather than as a string.  The regular expression then issues a warning message and fails to write anything into the spreadsheet for that user & field cell.  I'm not sure if this affects the other bulk export formats.  For Excel it can be fixed by simply doing              $worksheet[0]->write($row, $col, explode(',',$user->$field));  rather than              $worksheet[0]->write($row, $col, $user->$field);   on line 136. """,Bug,Admin
360859,"""I wanted to do full backup of a course before clearing it for new year. I logged in as admin and tried to do full backup with user data, but backup failed due to several assignments (last on the list). Then totally strange things started:  after failed backup I could click on one link - it will work. Any click later leads to the login page with message about session time-out. However, after that there is no login for some time: when loggin in with any correct login/password (it still recognizes incorrect ones) you are redirected to the front page ... without actually loggin in. If you click on something on the front page, then you are redirected to the login page with session time-out message once more etc... This behavour is independent of browser, location etc.  I first encounter this last night. The site seems to work today (after several hours), but when I tried to repeat my steps I ended up in this problem once more. It seems like session time droped to zero temporarily.""",Bug,Backup
360873,"""I have a database activity on the frontpage (hidden in the main menu box).  What I require is that of moodle.org theme database. No logon required to view or download entries and associated files.  Currently a visitor can view entries but when clicking on the attached file are sent to the logon page. They need to be able to download the file without being logged in.  If the database is part of a course allowing guest access, then guests can view and download the files as desired without prompting as I have the auto login guests set to yes.  But as the database in on the frontpage, there is no option to """"allow guest access"""" as there is with courses. Therefore visitors are not automatically logged in as a guest. They have the option to login as a guest on the login screen which then allows them download, but this is assuming the visitor knows about the guest option or could be bothered.  I need visitors to be able to view databse entries and download attached files, either remaining as a visitor or being auto logged in as a guest, just like I can without being logged in on moodle.org (themes database).  I have been able to replicate the problem on the QA site by adding a databse activity to the frontpage label. """,Bug,Roles / Access
360876,"""If users try to install a 1.9 module in 2.0, this might not be nicely detected. Potentially the user could get the module halfway installed and then see horrible errors.  We considered using the 'requires' version (if 'requires' is a 1.9 version then assume it won't work) but this is not a good solution because 'requires' is actually optional. Many core code modules have incorrect requires value, by the way (they should probably not have it at all, actually, but).  A better solution would probably to add to version.php (etc) a new flag which can be used as well as requires, specifically for cases where backward compatibility is broken:  $version->apilevel = 2;  I was going to code this for the 'requires' version solution but given that a solution is now unclear, I'm just going to put this on record as a tracker issue, maybe this is something for moodle 2.0.1 for instance.""",Improvement,"Admin,Usability"
360906,"""I have two users assigned as a Site Administrator.  I was logged in as Admin-2 and switched roles to the Student role.  The next time I logged in as Admin-2 I was stuck in the Student role and the Site Administration block was not visible.  I'm not sure, but I must have closed the browser while Admin-2 was still switched to the Student role.  I logged in as Admin-1 and opened """"Check System Permissions"""" under User > Permissions for Admin-2.  The permissions showed Admin-2 as an administrator.  But when I logged in as Admin-2 I could only function as a Student (the Site Administration block is not visible).  So Admin-2 could not switch back to the regular role.  To fix this, I had to log in as Admin-1 and delete the user account for the Admin-2 user.  Then add a new account and recreate the Admin-2 user account and reassign this user as a Site Administrator.  Then when I log in as Admin-2, the role is correct.  Thanks, Clarence """,Bug,Roles / Access
360916,"""We're trying to integrate Moodle into our organisations IT systems -- which means using MS-SQL. I'm using the new SQL Server driver for PHP version 2.  We use student id numbers as usernames. They are of the form YYnumber Currently, unless a student id begins with 0, there is an error as follows (developer level error reporting):  Debug info: SQLState: 22018<br> Error Code: 245<br> Message: [Microsoft][SQL Server Native Client 10.0][SQL Server]Conversion failed when converting the nvarchar value 'guest' to data type int.<br>  SELECT * FROM bhi_user WHERE username = 9841370 AND deleted <> 1 AND mnethostid = 1 [array ( 0 => '9841370', 1 => '1', )] Stack trace:  line 380 of \lib\dml\moodle_database.php: dml_read_exception thrown line 261 of \lib\dml\sqlsrv_native_moodle_database.php: call to moodle_database->query_end() line 371 of \lib\dml\sqlsrv_native_moodle_database.php: call to sqlsrv_native_moodle_database->query_end() line 786 of \lib\dml\sqlsrv_native_moodle_database.php: call to sqlsrv_native_moodle_database->do_query() line 864 of \lib\dml\sqlsrv_native_moodle_database.php: call to sqlsrv_native_moodle_database->get_recordset_sql() line 1218 of \lib\dml\moodle_database.php: call to sqlsrv_native_moodle_database->get_records_sql() line 1190 of \lib\dml\moodle_database.php: call to moodle_database->get_record_sql() line 3812 of \lib\moodlelib.php: call to moodle_database->get_record_select() line 3549 of \lib\moodlelib.php: call to get_complete_user_data() line 122 of \login\index.php: call to authenticate_user_login()  I've spent some time looking into this. Originally I was confused, because the parameter array holds username as a string value.  It turns out that in lib/dlm/sqlsrv_native_moodle_database there's a call in  do_query to emulate_bound_params (line 362) The function (line 695) seems to insert the parameters into the sql, using php type-testing functions to determine the syntax as follows ....  if ($this->is_number($param)) { // we can not use is_numeric() because it eats leading zeros from strings like 0045646    $return .= $param; } else if (is_float($param)) {      $return .= $param; } else {      $param = str_replace(""""'"""", """"''"""", $param);      $return .= """"N'$param'"""";  So if a parameter can be cast to a number it will be.  There is a note above do_query      /***     * Bound variables *are* supported. Until I can get it to work, emulate the bindings     * The challenge/problem/bug is that although they work, doing a SELECT SCOPE_IDENTITY()     * doesn't return a value (no result set)     */  but I don't really know what it means :-(   I did try changing the driver to free_tds, but couldn't get free_tds to work. Is that the direction I should be moving?   """,Bug,Database SQL/XMLDB
360941,"""Based on what I've read, the recommendation for web design and online content is to use sans serif fonts. Since this practice is a fairly well accepted standard and based on research that indicates that sans serif fonts are easier to read on screen, I would propose that all of the themes included with a standard Moodle install use sans serif fonts for text in menus and on the course page. Of course, if someone is aware of new research that says differently, please let me know.  In reviewing the standard themes, I found that anomaly, serenity, and brick all have serif fonts. I viewed these themes in both Firefox and Internet Explorer. The magazine theme has a mix of serif and sans serif fonts, but since the text in the menus and links on the course page are sans serif, I think it's probably okay.  On a related note, the font size in the formal white theme is significantly smaller than the font size in other themes. I think it should be sized to match the other themes; at this point, it's really too small to be friendly to a majority of users. Again, viewed this theme in Firefox on Mac and Windows and in IE.""",Improvement,Themes
360949,"""See attached screenshot. I'm restoring a backup and want to restore as a new course. my categories are listed. I only have one. """"Miscellaneous"""" There are two related problems.  1) If I click """"Continue"""" without selecting a category I am returned to this page with no message or other indication of why.  2) It doesn't default to selecting a category. Maybe we don't want to do that once there's some sort of visual indication that the category selection is mandatory however having to pick a category when I only have one is a bit silly.""",Bug,Backup
360972,"""In lib/moodlelib.php in the get_complete_user_data() function around line 3777 below the /// Get various settings and preferences comment there's a call to $DB->get_records('course_display', array('userid'=>$user->id)  This function gets called every time a new session starts.  On OpenLearn, there are many guest sessions started frequently, so this particular query gets called a lot.  There's no index on course_display for just userid, though there is one for course,userid.  This query is appearing rather too often in my slow queries log, so I was wondering whether adding an index just for user would help?  I could do this just as an OU customisation, but I guess it must affect some other big sites as well, and since its in Moodle 1.9 and Moodle 2.0 perhaps this is a good time to get the change into core?  Happy to code it myself if someone would just give me a thumbs up?  (Note all these code references are for Moodle 2.0 but the same problem exists in Moodle 1.9 which is where I've actually seen the problem on my live system).""",Improvement,Database SQL/XMLDB
360982,"""I am trying to setup a wiki which will allow guests to view pages and comments, similar to the moodledocs here on moodle.org.  Following a recent fix to the guest auto login (MDL-23833) guests can certainly view pages and comments once the guest login is done.  If a wiki is part of a course which allows guest access, then the visitor is changed to guest upon entering the course (auto login guest set to yes), then the wiki within the course is fully viewable as per permission set.  The problem is I that I wanted to put the wiki off the frontapge (hidden in the main menu block). As there is no trigger to say allow guest access when setting up the wiki, visitors do not become guests when clicking on the wiki activity or any associated page links. I enter the wiki activity not logged in (nor as a guest) and I get an error message under the view tab:  """"Coding error detected, it must be fixed by a programmer: PHP catchable fatal error."""" This error is reproduceable across all themes with 2.0.  Strangely enough there is no error when trying to view the comments of the wiki page.  If I enter the course and become a guest, then go back to the front page, I remain a guest and can click on the front page wiki and view pages as desired.  So the guest setting works but I simply need to be able to trigger visitors to become guests when clicking on activities on the front page.""",Bug,Roles / Access
361044,"""Error as above when web server is IIS.  It's just a coding error... it is (at line 35):  if (!$_SERVER['REQUEST_URI']) {  but I think he meant...  if (!isset($_SERVER['REQUEST_URI'])) { """,Bug,Authentication
361109,"""When I login as a site administrator and I go to the Security Overview Report (Site administration/Reports/Security overview) in Moodle 2.0 Preview 4+ (Build: 20100813) (2010080901) I get a """"Error reading from database"""" message and the report is not displayed.  When the PHP error reporting is set to Max (Developer) PHP shows the following error:  Debug info: The ntext data type cannot be selected as DISTINCT because it is not comparable. SELECT DISTINCT r.* FROM <USER>role r JOIN <USER>role_capabilities rc ON rc.roleid = r.id WHERE rc.capability = ? AND rc.contextid = ? AND rc.permission = ? [array ( 0 => 'moodle/backup:userinfo', 1 => '1', 2 => 1, )]  Stack trace:  line 380 of \lib\dml\moodle_database.php: dml_read_exception thrown line 247 of \lib\dml\mssql_native_moodle_database.php: call to moodle_database->query_end() line 694 of \lib\dml\mssql_native_moodle_database.php: call to mssql_native_moodle_database->query_end() line 723 of \lib\dml\mssql_native_moodle_database.php: call to mssql_native_moodle_database->get_recordset_sql() line 793 of \admin\report\security\lib.php: call to mssql_native_moodle_database->get_records_sql() line 112 of \admin\report\security\index.php: call to report_security_check_riskbackup()  I'm connecting to MSSQL 2005 using sqlsrv in Moodle and the Microsoft SQL Server Driver for PHP (MDL-15093). """,Bug,Database SQL/XMLDB
361149,"""I tried to wrote an Authentication plugin, that authenticates against the field <USER>user.idnumber. At the same time this auth-plugin had a fallback to the manual-auth-plugin. So iff authentication via username and """"idnumber"""" could not be done, a passed password would be evaluated by the manual-plugin to authenticate. This means when authentication via """"idnumber"""" was done, there is no value in $user->password.  Moodle by default updates the internal password, when user authentication was successful. In my case, when I authenticated via """"idnumber"""" and the $password thus is empty, Moodle will store an empty string (as hash) in <USER>user.password. But of course I want to keep the original password, so I can always use it as a fallback! The method """"prevent_local_passwords"""" is no help either, because it overwrites the internal password with """"not cached"""" as you know.  The method in question is """"update_internal_user_password"""" in """"lib/moodlelib.php"""". The statement says:  return set_field ('user', 'password', $hashedpassword, 'id', $user->id);  Wouldn't it be possible to have some kind of switch here, so I can turn that off - either as plugin developer or in general as Moodle Admin in the Moodle configuration? """,Improvement,Authentication
361206,"""I hope you guys don't hate me now, with 3 bug reports in 2 days...  I have exported a course with 11 weeks and an instructor guide from <USER>  <USER>did not order them on export. :-(  On import, the first week to appear in XML order, Week 10, was presumed by Moodle to be the course description etc, so is now an unmovable block at the top when I go to re-arrange the weeks in order.  On the one hand, <USER>IS just being daft to not order it, and by not including the stuff you expected in that first child node to be the course info,  I suppose.  On the other hand, the cartridge DOES validate, so I guess Moodle needs to accept a course with just the Weeks info, and no course description etc?  Or maybe that stuff is in there somewhere else???  Anyway, would it be terribly tricky to add a button to """"demote"""" the unmovable block to just another movable week?  That would allow me to fix this manually, at least, as a work-around.  The LAST week in XML order is completely missing.  :-( No real theory on what that got turned into...  Perhaps I'll find it later. Moodle is probably assuming some kind of """"footer"""" element that <USER>does not include... This may warrant a separate bug report, but it """"feels"""" like it's in the same code/category as first XML week becoming Course Description type stuff. I can just add a fake week in the manifest and probably get this pulled in.  I will attempt to copy the course and sanitize the proprietary content to provide a repro cartridge, but that's going to take a long time, due to me not being <USER>admin here... """,Bug,Backup
361295,"""Hi :)  Teresa Gibson found an error while testing the user activity reports in Moodle 2.0 that appears to be related to the use of hard coded CONCAT within the log_display table when using a pg database.  The first patch I've attached fixes this issue within Moodle 2.0 and seems fine. The second patch attempts to fix this issue within 1.9 however it hasn't dealt with the install.xml file yet which contains these entries. 2 The solution I have used for all other fixes here is to simply replace the CONCAT with the result of the proper sql_concat functions in each version, however because of install.xml perhaps this needs to be fixed in a better way. As I'm no expert on either the install of the logging I'll leave that up to the expert who gets assigned this issue ;)  I've also marked this as a blocker as it is now holding up the QA tests :(  Cheers <USER>",Bug,Database SQL/XMLDB
361404,"""Summary:  Ability for auth plugins to specify a URL for users to edit their profiles, similar to the change password url.  Explanation: We have an auth plugin that does SSO from our main site, which pulls in all the needed profile information.  We currently have the permission to edit your own profile turned off for all users, but it would be nice if auth plugins could specify a url for users to edit their profiles from an external site.  I see this behaving in much the same way as the change password url that is currently available.  If this is something that sounds reasonable and has a good chance at being accepted, I would be willing to write this and submit a patch (but may not go through the trouble otherwise).""",New Feature,Authentication
361565,"""Some branches of the Navigation menu require JavaScript to access, as they are not generated in the page but rather loaded in by YUI via an AJAX request.  The ones I've found so far (there may be more) are: -""""Site pages"""" - when viewing a course (it's generated in-page on site-wide pages) -Any course other than the current course (therefore all courses on home page)  The courses are clickable links in all cases, so you can click through to the course, which will then display that course's branch - but that doesn't seem ideal (though each course branch is admittedly VERY large, so the page would get incredibly long if you're part of several courses).  The """"Site pages"""" node is not a link, so you have to manually navigate to the homepage (or any other non-course page) in order to get to it.  While it is, strictly speaking, possible to get to everything if you know which page you need to go to in order to find it, this still seems like a pretty major accessibility / usability issue to me (especially """"Site pages"""" as it just appears as a non-clickable piece of text with no JS).""",Bug,"Accessibility,JavaScript,Usability"
362133,"""Just cvs updated now (I was running 24h old code), then pointed to root page, logged as admin and redirecting to root page took ages, showing this information once the page was loaded:  31.184108 secs RAM: 27.2MB RAM peak: 27.5MB Included 649 files Contexts for which filters were loaded: 1 Filters created: 1 Pieces of content filtered: 3 Strings filtered: 0 get_string calls: 1679 strings mem cache hits: 1554 strings disk cache hits: 128 Log DB writes 1 DB reads/writes: 144/1 ticks: 3119 user: 92 sys: 25 cuser: 0 csys: 0 Load average: 0.30  Next executions of the same process got memory reduced (and stabilised) to 15.3M and 0.6 seconds (with the rest of performance bits being the same).  It seems that 30 secs/15M is too much for styles/caches to be rebuilt, isn't it? At least we could show some info in perf. debug when that happens if it's impossible to reduce those numbers.  I'd say never in the past I've had to spend that time regenerating styles/caches.  Ciao :-)""",Bug,Performance
362207,"""I was able to create a questionnaire, but never add any question. I've updated Moodle by cvs (now 2.0 version 3) and at a maximum level of debbugging support, this is the description I get:  Coding error detected, it must be fixed by a programmer: Attempt to re-define already required string 'cancel' from lang file 'moodle'. Did you already ask for it with a different $a?   Stack trace:      * line 647 of /lib/outputrequirementslib.php: coding_exception thrown     * line 250 of /lib/outputrequirementslib.php: call to page_requirements_manager->string_for_js()     * line 863 of /lib/outputrequirementslib.php: call to page_requirements_manager->init_requirements_data()     * line 317 of /lib/outputrenderers.php: call to page_requirements_manager->get_head_code()     * line 23 of /theme/boxxie/layout/general.php: call to core_renderer->standard_head_html()     * line 612 of /lib/outputrenderers.php: call to include()     * line 570 of /lib/outputrenderers.php: call to core_renderer->render_page_layout()     * line 448 of /<USER>quiz/edit.php: call to core_renderer->header()  (It was on my first attempt to create the questionnaire, an the questions table is empty) """,Bug,Language
362302,"""Here there are some critical assignment issues found while playing with it:  - New installed site - Create one online assignment - logged as student, perform one online submission (adding one image from the html editor). Save it. - PROBLEM A: Visually, as student, the submission is ok and the image too. But, in DB, the image remains as """"user_draft"""". Should be """"assignment_submission"""" or whatever.   Same assignment, as admin:  - go to """"view xx submitted assignments"""", admin see the file of PROBLEM A incorrectly (broken). - check """"allow quick grading"""" and save preferences. - Both the comments and the """"scale"""" elements are shown. Introduce anything on them and click """"save all my feedback"""". - PROBLEM B: Nothing is saved in DB. On refresh, the page shows nothing.  In the same """"view xx submitted assignments"""", as admin, click on """"grade"""" link: - one popup is opened. - introduce one grade and one comment. - PROBLEM C: It's impossible to add images in the comments editor. - PROBLEM D: The comments field has not its corresponding """"submissioncommentformat"""" field. - Save changes - PROBLEM E: The popup remains with """"horrible"""" page with blocks and message """"Changes saved. This window should close automatically. If not, please close it now"""" - Close the popup manually. Reload the """"view xx submitted assignments"""" pages. Both the comment and the grade have been saved properly. - PROBLEM F: The comment in that page shows plain text so you can see the HTML (if quick grading is enabled).  Edited:  - PROBLEM G: Create one 1-file upload assignment. As student upload one file. Seems ok, but the """"itemid"""" in the files table is the submission->userid. It should be the submission->id instead.  That's all I've found related to the online assignment. Ciao :-)""",Bug,Files API
362492,"""Hello,  I am working with blocks to have on a side bar for my courses. I can create a new block and edit who can see the blocks and who can manage the blocks by assigning roles to a paticular block. However, only specifying whether people can view the block or manage the block is not enough for me. I want to have site wide blocks that are on every course page as a course description block that can be EDITED by teachers but NOT DELETED OR HIDDEN. I have only found a way to let the teacher manage the block (edit, delete, etc...) but not where I can only let them edit it. I'm thinking there is an easy way to do this using permissions and roles, but have not figured out how yet.   Thanks""",Task,Roles / Access
362560,"""It looks like the Quiz module has been completely rewritten, but their is no style for me to use as a template in the Standard theme. I could just style this how I think it should be styled, but I'd prefer to have a template for elements I may not find in my testing.  With that <USER> the developer responsible for recoding the quiz module probably had a design in mind. It would be useful to see that design, if nothing else.  Was it <USER>""",Sub-task,Themes
362586,"""Having been playing with getting Mahara working on Firebird, I've dropped back to Moodle and run the same exercise. Attached are the files needed to allow the installer to create the database. ACTUALLY getting the code running is a bit ore complex since field names are not correctly managed for wrapping in double quotes. Firebird should be able to run identical to Oracle, but at present there is a problem with a couple of reserved words which needs addressing. Adodb driver replaces the current version, and the class drops into the xmldb library.  To go with this there are obviously a few additions to add 'Firebird' as a database option, and if there are no objections I would like to be able to add those to the code base. However the field name wrapping is a little more difficult to address. I am used to the standard approach with ADOdb to add 'backtick' to all the scripts which is then translated ( or removed ) as required before running a query ...""",New Feature,Database SQL/XMLDB
362608,"""I believe I enabled Statistics; however, at first it locked up gathering info (classes were held as far back as July 2009), then it just kept giving me an error message: error/moodle/nostatstodisplay From MoodleDocs Jump to: navigation, search  Stats  This page can appear if you have not enabled statistics. To enable statistics, log into your Moodle as an administrator. Then, in the Site Administration menu, under Server --> Statistics ensure that they are enabled.  If you have enabled Statistics recently you should note that statistics are collected by a background task and take some time to become available. Try again later, but do note the time for statistics job to be run in the Site Administration page noted above.   I'm hoping to get tracking data through Statistics but it isn't working properly...""",Task,Performance
362740,"""Hi Dong,  just was editing group settings here, both adding images with the html editor in the description of the group and also, adding images as attachments.  At the end, looking at DB records (files table), I get:  - A lot of user_draft files (I would say that more than the times I've edited the group but not sure). - All those draf files have /undefined as path. - One """"course_group_description"""" file that looks correct (corresponds with the image I added in the editor). - NO file for the attached image. It seems that it's going to moodledata/groups instead of the files storage. Not sure if that is the expected behaviour for groups. In any case, note it's generating a lot of draft file (see 1st point).  Ciao :-)""",Bug,Files API
362957,"""Our site (OpenLearn) is deliberately quite open, encouraging sharing of ideas and materials. A user recently pointed out that an item of material he had uploaded to the site required several attempts to access through the url he had sent to his (not registered user) friends.  The issue is that file.php requires a login as guest for these users without the wantsurl being set, so these users get redirected to site home page as a guest, and only then if they try the url again will they get the image required.  To reproduce this from a default 1.9+ stable installation with a user with only default permissions: 1) as admin  ensure $CFG->autologinguests is enabled (part of an open site) (under User policies) 2) as user create a blog entry with an attached image, and note the url of the image (something like <$CFG->wwwroot>/file.php/blog/attachments/1/flower.jpg) 3) using a fresh browser (or ensuring logged out from site) go to the url  The result is to be logged in as guest and to see the site home page rather than the url.  This is really a consequence of the good work done on MDL-14495, and suitable notes are in file.php (line 60) explaining that this will happen. Of course if the site has autologinguests set to the default not enabled, we would indeed expect to and do find the login page. Our site encourages open sharing by auto logging guests in.  I have considered the issue covered in MDL-14495 and this issue and have a possible solution.  Line 66 of file.php is:  require_login(0, true, null, false);  If this is adjusted like this:  require_login(0, true, null, true); unset($SESSION->wantsurl);  Then both MDL-14495 and this issue are solved. The reasoning goes like this:  For a first fresh request to file.php setting the $setwantsurltome to true sets the $SESSION->wantsurl and gets the user logged in as guest, then redirected to the site home page, but as the wantsurl is set it is used for a second request to file.php. Now the user is logged in the wantsurl should be removed to prevent the old issue, and all is fixed (I think!).  While I have tried to test this out as fully as I can, I would appreciate some better minds than my own to double check the logic.""",Bug,Files API
362986,"""At present, the site policy is only shown until a user agrees. This is stored in the users table and they are never prompted to agree with the site policy again.  We've had a feature request to allow admins to show the site policy every time a user logs in, but only once for that session.  I've written this as a user-configurable option in admin/settings/security.php """,New Feature,Admin
363016,"""Just installed one Moodle 2.0 from scratch.  After all the tables are created, I arrive to the create admin user (/user/editadvanced.php) page. These happen:  * The """"description"""" field isn't able to show the html editor. * There is one big rectangle (iframe?) below the that field, where the """"/user/editadvanced.php"""" page is shown again.  After finishing installation, I go to the edit profile page as admin (/user/editadvanced.php) again. These happen:  * The """"description"""" field isn't able to show the html editor. * There is one big rectangle (iframe?) below the that field, where one """"add"""" button is shown.  Note I've javascript enabled and other pages like the """"front page settings"""" one, on install, properly show the html editor.  Ciao :-)""",Bug,"Admin,Forms Library"
363132,"""When we designed the web service we agreed that external function would return anything. That the reason why we need to clean the return values against the web service description into the server layer.  I recently implemented the cleaning into our XML-RPC server. I'm currently working on the SOAP cleaning. SOAP is much more complex as I need to clean a xml variable. PHP soap don't let me hand an array/object response at anytime, and the xml result seems quite difficult to serialize (I didn't find yet any function manipulating it).  When I talked about it with <USER> he though that in fact we should have the cleaning into every external functions. It does make my life easier (even if it was a waste of time: cleaning XMLRPC, as <USER>did too with Amf, would not be useful anymore). Petr what do you think?""",Bug,Web Services
363295,"""If a scorm package is set with the skipview parameter to """"Always"""" or """"First time"""", the view is skipped for everyone including someone with """"<USER>scorm:viewreport"""" capability. The problem is that reports can't be read by this person.  In locallib.php, function scorm_simple_play, the following condition can be modified : original :  if ($scoes) { proposal :  if ($scoes && has_capability('<USER>scorm:viewreport', $context)===false) { for doing this, the $context must be passed as a parameter to the scorm_simple_play function  Another thing : if the view is skipped, the student is redirect to the first sco retrieved by the sql query : $scoes = get_records_select('scorm_scoes','scorm='.$scorm->id.' AND launch<>\''.sql_empty().'\'');  I think, an order can be added to the sql query to ensure that the taken sco is really the first in the package : $scoes = get_records_select('scorm_scoes','scorm='.$scorm->id.' AND launch<>\''.sql_empty().'\'', 'id', 'id');  I've attached a patch for this.""",Bug,SCORM
363472,"""I have been struggling with this issue for the last two months and I'm starting to think it's actually an issue with login/index.php.  I have set up CAS authentication (v. 3.3.3) to run over LDAP. I have tried different versions of the phpCAS library, currently experimenting with 1.1.0RC4, What happens is straightforward:  - if I try CAS authentication using _any_ phpCAS version, and a small php script written by me, I manage to authenticate: this proves the CAS setting is right  - if I try using CAS authentication on moodle    1) I get a redirect loop on login/index.php    2) the login window is NOT shown  Basically index.php keeps reloading itself, without any authentication happening. My suspect is that there is some variable in $_SESSION that is misinterpreted, causing a redirection before even authentication is tried. I attach the logs of phpCAS, maybe they can be useful.  We have a development install of moodle and I would be very happy of cooperating with you to test any solution or show the problem.""",Bug,Authentication
363476,"""Sorry for the long description... but I don't see any other way ;-)  See attached zip file for package, complete logs, and text versions of 'track detail report'...  Here are the steps to reproduce this issue...  As an instructor - deploy the scorm package -- Now as a student log in and run through the package... max score is 40 in this case.. answering everything correctly results in score of 40 and status of 'completed' just as it should. -- Instructor can verify this using the 'track detail report' (see report in zip) The report correctly indicates 'lesson_location' of 4... score 40 and status complete... -- Go back in as a student and enter the LO again... The LO correctly initializes and jumps to 'lesson location' of '4' - which is the quiz results summary showing 40 points and completed status... Just as it should... Do nothing at this point except exit the LO...  Upon exiting, the debug output correctly shows the following....  Sat, 09 Jan 2010 15:55:03 GMT: LMSSetValue(""""cmi.core.score.raw"""", """"40"""") => 0 Sat, 09 Jan 2010 15:55:03 GMT: LMSSetValue(""""cmi.core.lesson_status"""", """"completed"""") => 0 Sat, 09 Jan 2010 15:55:03 GMT: LMSSetValue(""""cmi.core.lesson_location"""", """"4"""") => 0 Sat, 09 Jan 2010 15:55:03 GMT: LMSSetValue(""""cmi.core.session_time"""", """"00:02:11"""") => 0 Sat, 09 Jan 2010 15:55:03 GMT: LMSSetValue(""""cmi.suspend_data"""", """"E0Enone$nP1Enone$nP1Enone$nP1Enone$nP1Enone$nP1Enone$nP001BA001EAA~$YS*qdzwT110BEB0H3;2;1;0DnilDnilDnilBDB1BCB0BBB0BAB0BB~$YS*ED7wT110BCB1DnilDnilDnilDnilBBB0BAB1CC~$YS*LB_wT110BBB2DnilDnilDnilDnilB0FsmartDD~$YS*wKBxT110BDB4DnilDnilDnilDnilB1BbB0BaB2BcA"""") => 0 Sat, 09 Jan 2010 15:55:04 GMT: Commit("""""""", """""""") => 0 Sat, 09 Jan 2010 15:55:04 GMT: Commit("""""""", """""""") => 0 Sat, 09 Jan 2010 15:55:04 GMT: LMSFinish("""""""", """""""") => 0  At this point everything still looks good to the student.. the summary page shows the score as 40 and status is completed.. -- Now go back in as the instructor and pay close attention to the 'lesson location' line of the track detail... It shows as follows...  Completed 00:04:07.00 Score: 40 Raw score 40 Status completed Time 00:04:07.00 cmi.core.lesson_location 0  << WHOOPS - the prior attempt clearly shows we should have set this to '4' - somehow that didn't happen.  -- Now if you go back in as a student and enter the LO again, it initializes incorrectly and upon moving to 'lesson location 0' this particular LO (made with captivate) sets your status to incomplete and your score back to zero... Thus 'undoing' your prior work... --  I've had similar results with LOs made in softchalk as well.  We would really like to get this resolved and I think this may fix some of the other posts I see in the forums about captivate LOs losing their status on subsequent attempts.  Thanks - and let me know what I can do to help! <USER>  """,Bug,SCORM
363489,"""I can't remove a course and get the error 'Can not set parent for category or course item!' and when I click continue it kicks me out.  I don't know if this is a bug or something I caused.  I'd like to know a manual way of removing it (can I just remove the folder number in moodle?)  two reasons the error could be me and not a bug - EDIT: I just fixed these so unlikey causes now 1. I imported this course from a course in my other moodle system. It has a pdf upload activity that this new moodle does not have in it.  EDIT: I just added the upload pdf module here too and tried to delete the course and still cannot.  I'm leaning towards this being a bug now. 2. I have notifications errors I have never resolved that may be affecting it.  I had unsuccessfully tried to install and uninstall a group selection module.  EDIT:  I just fixed these errors  by removing the 3 folders - a temporary fix.  Plugin """"courseoverview"""" (2007101503) could not be installed. It requires a newer version of Moodle (currently you are using 2007101509, you need 2007101532).Scroll to next warning Scroll to previous warningPlugin """"security"""" (2007101500) could not be installed. It requires a newer version of Moodle (currently you are using 2007101509, you need 2007101533).Scroll to next warning Scroll to previous warningPlugin """"unittest"""" (2007101501) could not be installed. It requires a newer version of Moodle (currently you are using 2007101509, you need 2007101532).Scroll to continue button  Thanks <USER>",Bug,Course
363506,"""In course/import/activities/<USER>php, the following code can cause a crash:      $taught_courses = array();     if (!empty($tcourseids)) {         $tcourseids = substr($tcourseids,0,-1);         $taught_courses = get_records_list('course', 'id', $tcourseids, 'sortorder');     }  In Moodle 1.9.7, the query against the database is:          SELECT * FROM <USER>course WHERE id IN ($tcourseids[0], $tcourseids[1], ..., $tcourseids[n])   In Moodle 2.0, the query against the database is slightly different:          SELECT * FROM <USER>course WHERE id = $tcourseids[0] OR id = $tcourseids[1] ... OR id = $tcourseids[n]  While a simple query for an instructor, for an administrator in a large system, the request actually exceeds the maximum query length allowed by MySQL (and presumably other database systems as well). So what is an alternative? What we're actually trying to determine is what courses a user has moodle/course:update permission in. Essentially, then, we'd want to find a list of all courses contained within contexts where they have such a role assignment.  Here is my proposal for a new query that uses get_records_sql() instead of get_records_list():          SELECT crs.*          FROM <USER>context ctx          JOIN (SELECT DISTINCT ctx.path                  FROM <USER>role_assignments ra                  JOIN <USER>context ctx                          ON ctx.id = ra.contextid                  JOIN <USER>role_capabilities rc                          ON rc.roleid = ra.roleid                  WHERE ra.userid = {USERID}                          AND (rc.capability = 'moodle/course:update' OR rc.capability = 'moodle/site:doanything')) priv              ON ctx.path LIKE concat(priv.path, '%')          JOIN <USER>course crs              ON crs.id = ctx.instanceid          WHERE ctx.contextlevel = 50;   This is a fairly heavy query, but as an explain shows, it's fairly well-optimized:        +----+-------------+------------+--------+---------------------+---------+-----------------------+-------+-----------------+          | id | select_type | table | type | key | key_len | ref | rows | Extra |          +----+-------------+------------+--------+---------------------+---------+-----------------------+-------+-----------------+          | 1 | PRIMARY | <derived2> | ALL | NULL | NULL | NULL | 3 | |          | 1 | PRIMARY | ctx | ref | <USER>cont_conins_uix | 8 | const | 12078 | Using where |          | 1 | PRIMARY | crs | eq_ref | PRIMARY | 8 | moodle.ctx.instanceid | 1 | Using index |          | 2 | DERIVED | ra | ref | <USER>roleassi_use_ix | 8 | | 4 | Using temporary |          | 2 | DERIVED | rc | ref | <USER>rolecapa_cap_ix | 767 | | 19 | Using where |          | 2 | DERIVED | ctx | eq_ref | PRIMARY | 8 | moodle.ra.contextid | 1 | |          +----+-------------+------------+--------+---------------------+---------+-----------------------+-------+-----------------+   It becomes more expensive as the number of context paths where the privilege exists increase.  I would be interested to know if other large institutions have had this problem, or if there are any other potential (and lighter?) solutions people have come up with. This has been a show-stopper for our administrators, who have had to use login as to loose privileges before making Imports in the meantime.""",Bug,Course
363705,"""While working on MDL-20735 I encountered the issue that when require_course_login is called from a module on the front page the default page objects are not set up. This was an issue because the navigation relies on the PAGE objects primarily context, course, and cm to work out what to generate. I'm not 100% sure whether this is a bug, or intended functionality but I have attached a bug that addresses the issue. If it isn't an bug I'd be keen to know why as I have to find a solution for MDL-20735 and if page objects can be relied on then the fix is easy.""",Bug,Libraries
363983,"""Currently in HEAD print_scale_menu_helpbutton() & help_button::make_scale_menu() are throwing exceptions.  print_scale_menu_helpbutton() is in deprecated lib and is simply used as follows: $OUTPUT->help_button(help_button::make_scale_menu($courseid, $scale));  2 immediate problems are  * OUTPUT doesn't have a help_button method.... it should probably be help_icon * The help_button class doesn't exist... it should probably be moodle_help_icon  However even after those have been corrected you still get mountains of exceptions because of different things not being set on the classes. I'm not 100% sure but in this case scale menu is a fringe case as it has a special help page (Am I thinking of the right thing here). I would imagine that the whole implementation of that needs to be re-thought.  Also to whoever takes on this issue, grap for '::make_scale_menu' there are several implementation that need correcting. """,Bug,Libraries
364031,"""I come across a situation yesterday eve where in one of the course a participant has three roles assigned. I know, this should not happen but it did SO I am posting a blog here for note purpose.  So, in the situation above, the participant in that course has the role: students, editing ta, and head ta. Now, in most authorization code I've written or experience, the resulting permission is the most restrictive (or less access permission set) gets applied to the login user. In this case, it should have been the student roll permission that should have applied. BUT that's not the case. The user actually has all the permission set of the head ta and the two other permission set were simply ignored it seems.  Question 1: Is there a sum total permission that gets calculated when users has different roles in a given course?  For security purpose, the participant should have <USER>the most restrictive (student) permissioin applied just in case the instructor or administrator """"acidentally"""" added the student to a more privileged role (ie editing ta, and/or head ta).""",Improvement,Roles / Access
364035,"""We had this problem already before in unittests, some people incorrectly put print_error() into library functions. At present I need to use different default exception handler in web service protocols and instead of HTML output+exit I need to construct error XML and send it back to client - please note it does not make sense to use output renderers here because sometimes the error indicating XML is created in external libraries.  * require_login() was already converted to use exceptions instead of print error. * $DB->get_record() with MUST_EXIST throws exception too (replaces very many print_error()s)  Unfortunately the drawback here is that somebody might incorrectly use try {} catch {} and discard security access exceptions detected in code. We have to make sure exception catching is used only when really necessary and definitely not around the access control related code.""",Sub-task,Admin
364064,"""Currently, there does not appear to be a way for a Teacher, Course Creator, or Administrator to receive an email notification when a student enrolls in a course. For example, we authenticate to our AD/LDAP and then the student enrolls in a course (some with and some without access keys). Because of increasing volume, we have a need for a more passive way to allow students to self-enroll without some pre-approval or application process, BUT still be able to have """"awareness"""" of who and when a student enrolls. Also checked reports, but did not appear to be any meaningful report to provide this information at a summary level (could only dig down at a specific user level and see a course enrol action). A report with more meaningful and batch enrollment information might also be desireable, but would definitely like to see the email notification feature, perhaps as a check box under course settings. Another notification approach might be an RSS feed that would show enrollment activity. I'm not a developer, but would offer to help with spec, testing, and documentation.""",New Feature,Course
364183,"""When testing changes I was making to the auth language files I found that the `curlcache` string in de_utf8/admin.php is causing multiple notices to be shown.  Notice: Trying to get property of non-object in /var/www/m20.dev1/moodle/lib/moodlelib.php(5960) : eval()'d code on line 117 Notice: Trying to get property of non-object in /var/www/m20.dev1/moodle/lib/moodlelib.php(5960) : eval()'d code on line 118 Notice: Undefined variable: CFG in /var/www/m20.dev1/moodle/lib/moodlelib.php(5960) : eval()'d code on line 378 Notice: Trying to get property of non-object in /var/www/m20.dev1/moodle/lib/moodlelib.php(5960) : eval()'d code on line 378 + several more (note the CFG one)  The error can be replicated by simply logging in, changing your language to `de` and then browsing to the admin section.  The error is (I believe) caused because $a is used several time throughout the string sometimes as an object, sometime just '$a'. I'm not too sure what $a is meant to be and as I don't speak that language I really have no clue what is going on. If someone who understands de this could please check it and make a decision about what it should be a fix it that would be fantastic :)""",Bug,Language
364397,"""Login 'forgot password' should tell users if they have a non-working email (esp fake emails created by admin) Goal is to reduce user frustation  I get many frustrated calls from users who forgot their password and keep trying to submit info to 'forgot password' but Moodle CAN'T email them for one of three reasons below.    SOLUTION DESIRED: a message telling the user,  ******************************************************************* """"You do not have a working email"""" contact your administrator. ********************************************************************  Instead, they get a generic message (below) that leads them to believe that it might work if they keep trying by checking different emails addresses, usernames, and by waiting.    If you supplied a correct username or email address then an email should have been sent to you.    Three reasons why users might have nonworking emails: 1. disabled (I assume moodle has fixed this as a saw a separate tracker issue indicating that.  If not, fix it 2. non-working email by accident (mistyped, expired, etc) ***3. non-working email by DESIGN of admin. By default all my users are auto created with <EMAIL> until they change and enable it.   I'd really like moodle to search for this wording and alert users my message above : you do not have a working email. Contact your administrator.  Please help with number 3 especially as 4000 of my users have fake emails.  I know what you're thinking, If users are autocreated then they should know that the system does not have a working email.  However, they fill out so many forms that they assume their email might be attached for them to their account.  I don't have time to go through those forms to find those with working emails and attach them.  I train them to change it  but some users forget and think they might have edited their profile in the past.  They keep trying and email me only after very frustrated.  Thanks <USER>Tinley  """,Improvement,"Authentication,Usability"
364519,"""When I create a label and then input text. I want to color the background so I use the color background icon.  The label then only applies the background behind the text. Becuase my topic section has a different color to the label background, i get grey lines through the text.  I just want the whole label to be one white block.""",Bug,Course
364767,"""Here's how to reproduce...  1.  go to any image on your site and copy it's url 2.  log out of your site and kill all cookies. 3.  paste the image url into your browser  What happens is that you just end up at your site's home page.  This is because require_login is being called from file.php - because you've got no guest access set up yet.  But as of MDL-14495 an extra parameter was added so require_login here didn't maintain $SESSION->wantsurl.  I'd be tempted to switch back to the original code, but suspect the original bug is worse than this one.  The trouble is that some of my users have a nasty tendancy to copy urls of uploaded assets in emails.  I'd like to be able to support that use, but I'm not entirely sure what to do.  Suggestions please?""",Bug,Authentication
364811,"""IMHO Backup and Restore does not  justify its purpose in presently the way they are working,  Am the administrator , and teachers in my site has access to create or delete a course and its contents, I am scheduling backup so as to something drastic happens (teacher deletes a course by mistake) in order to restore.  Here comes the play,  Actual Behaviour :  1. If i store the backups in default folder when the teacher deletes the course , the backups would have gone along with it , as moodle deletes the entire course id'd folder from moodledata if am right  2. To counter the above scenario i stored the backups in /moodledata/1/backupdata/ so that it will be accessible by admin when he goes into Site Admin -> Frontpage -> Site Files-> Backupdata.  The problem with the above process is i cannot restore courses from their backups from site files itself (i mean the outer course ie. id =1) because the restore option says. Restore to 1. New course. 2. New course by deleting the present course . 3. Add components to the existing course.  (Have not typed the exact words above which moodle option offers please bare with me)  option 1 works absolutely fine , whereas if you select option 2 it will delete all the configurations and files stored in the main course (courseid=1), and obviously option 3 wont work.  The admin could have stored the backups in different place in the disk, but it will not be accessible using moodle api, i wanted to establish that connectivity so that without the intervention of network admin the admin of the site can do a backups and restore.  Expected Behaviour :  There should be a place where the backups are stored and accessed using moodle api with ease so that the admin can restore if above situations are met.  To make it easier it would be nice to have 2 more options in the restore process obtained when the restore link is clicked besides the backup which are stored in site files ,  1. Delete an existing course and add new. 2. Add contents to existing course.  on selecting any of above the user should be provided with the list of courses which he needs to work with, if the course has been already deleted and hes going to create a new one then he can go for """"Add new"""" option which is presently working fine.   Derived Solution :  This is the work around i performed to solve this problem temporarily, i store the backups in site files, so if a teacher deletes a course by mistake, he/she mails the admin requesting for the dated backup, and the admin mails the backup (though i hate this part- backups being distributed to teachers to handle) and then the teacher uses the backup to create a new course by uploading the backup in some existing course.  """,Bug,Backup
364927,"""I'm running 2.0 dev (Build: 20090707).  After installation, when I go to Site Administration -> Users -> Accounts -> Add a new user, I receive this error message:  Coding error detected, it must be fixed by a programmer: The theme has already been set up for this page ready for output. Therefore, you can no longer change the theme, or anything that might affect what the current theme is, for example, the course.  More information about this error Stack trace when the theme was set up:      * line 793 of /lib/setuplib.php: call to moodle_page->initialise_theme_and_output()     * line ? of unknownfile: call to bootstrap_renderer->__call()     * line 1091 of /lib/formslib.php: call to bootstrap_renderer->old_icon_url()     * line 144 of /lib/formslib.php: call to MoodleQuickForm->MoodleQuickForm()     * line 81 of /user/editadvanced.php: call to moodleform->moodleform()  Stack trace:      * line 1132 of /lib/pagelib.php: coding_exception thrown     * line 522 of /lib/pagelib.php: call to moodle_page->ensure_theme_not_set()     * line 1996 of /lib/moodlelib.php: call to moodle_page->set_course()     * line 5005 of /lib/adminlib.php: call to require_login()     * line 190 of /user/editadvanced.php: call to admin_externalpage_setup()  I tried this on multiple servers and go the same error.  This is a showstopper for our clients who want to trial 2.0 (they will be great testers for you!).  Thanks""",Bug,Admin
365028,"""When restoring a course to an """"existing course, deleting it first"""", I was asked to click on the course I wanted to restore to from a list of 800+ courses, which appeared to be in no particular order.   The order may have been ascending course id, but that wasn't included in the list. Only the course's name was in the list but it was not in alphabetical order and I had to find the course I wanted to restore to by using Firefox's 'find' feature  Ordering the courses in alphabetical order would simplify this task, or including the course ID (if indeed the courses were in increasing numerical order) would at least make sense of the apparently random order of the courses.""",Improvement,"Backup,Admin"
365231,"""I would imaging that for a lot of instructors, they would probably prefer to have """"Grade export display type"""" set to """"Real"""", but have a option to export their final gradebook to Excel spreadsheet which has an option to include the """"Final letter grade"""". (See my screenshot).  Under the 1.9.x gradebook (Grader report) pull-down menu """"Choose an action"""", instructors could edit/view the range of the letter grades already. It would be nice to also have an option for the """"final"""" letter grade to be exported to the Excel spreadsheet.  Just my $0.02.  """,Improvement,Usability
365349,"""Hi all,  I have been been able to reproduce the following issue on a MOODLE_19_WEEKLY CVS build updated every Thursday as well as another installation of Moodle hosted by Articulate technical support.  SCORMs are generated using Articulate Presenter '09 Update 4 (most recent released version) and consist of 10-15 slides with an embedded Quizmaker quiz on the last slide.  Articulate Presenter/Quizmaker attempts to set cmi.core.exit immediately upon starting the SCORM to """"suspend"""". If cmi.core.exit is not already set to suspend (from stopping and restarting the SCORM), then any new attempts to set the value of cmi.core.exit will not be recorded by Moodle.  Viewing the SCORM in debug mode shows that the SCORM is sending the right value and that Moodle is not returning an error, however the database is not updated and the user is presented with a """"suspended"""" graphic icon.  Altering the SCORM to not set cmi.core.exit to suspend upon initialization appears to have corrected the issue of SCORMs that are played from start to finish without exiting getting stuck in suspended mode. Comments in the <USER>SCORMFunctions.js included with Articulate SCORMs indicate that they are setting the value initially to suspend as some LMSs will not store suspend_data otherwise.  This appears to be an implementation issue with Articulate's software, however as this is a popular software package, I'd recommend investigating this issue within Moodle to see if cmi.core.exit can be set to a changed value during playback of the SCORM.  I'm not familiar enough with the SCORM guidelines to know whether or not Moodle's response is correct--at the very least, Moodle should report an error in debug mode to let SCORM developers know that attempting to change the value during playback will not work.  Best, <USER>",Bug,SCORM
365706,"""In testing the new 1.9.5 gradebook version I discovered that when I cursor over an item in the gradebook (cell) a pop-up message stating the activity and student name appears.  For usability/accessibility purposes - I'm posing the question, """"can this feature be toggled on/off """" as a user preference?  I ask because I know my faculty will ask...  Thanks for all the work on the new gradebook!""",Improvement,"Accessibility,Usability"
365720,"""I recently made a mistake and hit the green light thinking that I was adding someone to my contact list. Perhaps this will be addressed by the new Moodalis tango icons as part of  MDL-14361. Since a green light (at least in the U.S. indicates go and something positive, it strikes me as a bit of a counter-intuitive thing to use it to block. I would rather see something red or something with some type of block symbol - like the circle with the line through it. """,Improvement,Usability
365828,"""To recreate the error:  1. Go into any moodle course as an admin and click on reset. 2. Then scroll down to the bottom of the page and click on """"Select Default"""" 3. Then that's when you get the error: Fatal error: Unsupported operand types in /web location of moodle/course/reset_form.php on line 114  I've been able to recreate this issue in our production environment which is 2 servers with LVS as the load balancers going to 2 web servers using NFS as the moodle data share, and also houses the moodle session information, and our single web server test environment with everything local. Both solutions use a seperate SQL server.""",Bug,"Course,Usability"
365904,"""When clicking the """"Add a New Entry"""" link in the blog block from within a course, the user is directed to a URL like /blog/edit.php?action=add&courseid=4  The form has a hidden field """"courseid"""" but it never gets set, so the resulting post ends up with courseid = 0.  I've attached a patch which sets the courseid field.""",Bug,Blog
365960,"""I would like to propose an improvement to the way Moodle handles popup windows. Instead of opening in a new browser window, resources (and other activities) could also be displayed in an embedded window (inline popup). A major advantage of embedded windows over new windows is that embedded windows do not need to make use of the standard navigation bar at the top. The resource just appears as an overlay over the main window. Simply by clicking on """"close window"""", users will get back to the main page. Besides being nice to look at and avoiding cluttering of windows, this will make Moodle much more accessible to children and adults struggling with """"going back"""" through the Moodle navigation instead of using the browser """"back""""-button.  From a technical point of view, this improvement may be realized as a third option in each resource/activity setting. You can choose between """"same window"""", """"new window"""" and """"embedded window"""". No changes to the database have to be made. An open source javascript library (Greybox) can be used for handling these windows in a cross-browser compatible way. I have programmed a prototype, which demonstrates the use of these embedded windows (also see screenshots).""",Improvement,"JavaScript,Usability"
366063,"""Sometimes it is helpful to know how long someone has been registered. We currently do not save when the user account was created. I was thinking of using first access; however, it appears that sometimes that data does not necessarily get populated. Where I would find this helpful is in reviewing CVS write applications it would give me an idea of how long someone has been around in the community. With the logs not being stored beyond a couple of weeks it is hard to see if someone is a quiet person that has been around for a while or a new person. I generally assume new and can use the user id as an estimate but figured it might not be a bad idea to suggest having a 'Member since' type field that would be set when the user account is created. Peace - Anthony""",Improvement,Admin
366084,"""When users would try to sort assignment submissions by the Comment field, all of the assignment submissions would disappear. You have to actually logout and log back in again to get the assignments back.  The issue appears to be that MSSQL doesn't allow you to sort records using text or ntext columns -- and since the submissioncomment column is ntext, the query fails.   What I've done is modified the flexible_table class in /lib/tablelib.php to have an additional variable, $column_properties, and method, column_properties, which allow you to store arbitrary metadata about columns. I then changed the flexible_table::get_sql_sort() method to check if a column had a property associated with it called 'type'. If a column has a 'type' property, and that property is set to 'text', then the generated $sortstring calls sql_order_by_text() on that column, to generate any DB-specific SQL necessary for sorting an ntext or text column.  I also modified /<USER>assignment/lib.php to make use of the new flexible_table::column_properties() method so that the submissioncomment column has a 'type' property associated with it.""",Bug,Database SQL/XMLDB
366142,"""If a logged-in user clicks on a course on which they are not yet enrolled, and that course allows guest access, then the user enters the course immediately as a guest. However, it is far from obvious that they are there as a guest until, for example, they try to post a message and cannot do so. It is easy to miss the """"enrol on this course"""" link in the Admin block if you are not looking for it.  This causes problems for our students who cannot understand why certain courses seem to give them restricted access.  Maybe it would be better if, when a logged-in user first visits a course, they are asked whether they want to enrol or enter as a guest. Choosing guest would admit them as guest on subsequent visits, until they choose to enrol using the link on the Admin panel.  I'm not sure if that is the best solution, but I don't think the status quo is ideal.""",Improvement,Course
366203,"""I was working with some contrib code which was sending an empty value to html2text.php. As a result, the following notice was produced:  Notice: Uninitialized string offset: 0 in /home/arborrow/Moodle/code/19stable/lib/html2text.php on line 39  I am attaching a diff file that gets around the notice; however, I'm not sure if setting $chr to an empty string is the best thing. But I think verifying that there is something there to work with in $badStr{0} is a good idea.   Peace - Anthony""",Bug,Libraries
366217,"""Because of the current architecture of the gradebook, whenever a graded activity is created and then changed to a non-graded activity, it still appears as an item in the gradebook.  We changed the gradebook to ignore activities that are """"not graded."""" If data was entered for a graded activity and then the activity is modified to """"no grade"""", then the data will be retained should the activity switch back to a graded activity, but the item will not be displayed in the gradebook. Please see the attached diff file for more details.  Basically it changed the:  $items = get_records('grade_items', 'courseid', $this->courseid);   to  $items = get_records_select('grade_items', """"courseid = $this->courseid AND gradetype <> 0"""");  I also tested with the latest 1.9.4+ codes (build 20090223), still behave the same. So I'm sure that this problem also exist in all major 1.9.x builds right now. Not sure if this is something that we should maintain in our own customization or people would be interested to see this in the core. The fixed behavior would make better sense though, IMHO.  Would love to hear some comments about this fix, and if <USER>OK'ed with it I could go ahead to commit it into MOODLE19_STABLE and HEAD""",Bug,Usability
366221,"""I found this as a really annoying problem now that I'm teaching again this semester. For courses that has a lot of course materials (especially if you have a lot of materials under section 0 area), if you selected """"show this week(topic) only"""" for a certain week (let's say week4) and have the """"Section LInks block"""" enabled, somehow if you click into a resource/activity in week4, then use """"breadcrumb"""" to quickly go back to the course, moodle will automatically bring you to the """"TOP' of the course page. Then if you want to continue with the next activity of week4 (with """"show week4 only"""" enabled), if you click on the """"4"""" in your """"Section links"""" block,  it won't bring you right to week4 area! This has created some navigation usability issues (especially when you have a lot of course materials in section 0 area and has chosen to enable """"Show only weekx"""" optoin . Please try the attached backup file to understand what I was talking about.  One solution is to add """"0"""" for topic/week 0 in the """"Section Links block"""" and treat topic/week 0 as a """"regular"""" topic/week. Also implement the """"Section Links"""" block in the way that no mater you have """"Show only weekx"""" or """"Show all weeks"""" enabled, the section link block will behave the same. This issue is also related to my suggestion of MDL-18237. Thanks!   Shouldn't be difficult to implement, if no one is taking this one, I could take a look when I'm less busy.""",Improvement,Usability
366267,"""Helen,  Currently defined as - $string['requires_date_before']='Not available from $a.';  It displays something like:  Activity conditionally restricted: 'Not available until 17 March 2009.Not available from 20 March 2009.'  Because of the phrasing of it not being available I think 'from' would read better if it were 'after' which is ironic considering the name of the string. That would produce:  Activity conditionally restricted: 'Not available until 17 March 2009.Not available after 20 March 2009.'  which I think makes more sense. I'll attach  a patch.  Looking at the string names, I wonder if we might want to consider making all of the Not available statements into the more positive available statements. That could produce something like:  Activity conditionally restricted: 'Available beginning 17 March 2009. Available until 20 March 2009.' or  Activity conditionally restricted: 'Available beginning 17 March 2009. Available before 20 March 2009.'  I also noticed that the get_full_information function in conditionlib.php is not putting a space between the different conditions so we may want to end each of those with a space. This started out as a simple request to change a word but the more I looked at it the more ideas that came to mind. I'll stop for now ;-)   Peace - Anthony""",Improvement,Language
366283,"""This is probably more of a Files issue but I noticed it when trying to restore a course from 1.9 into a fresh install of HEAD (as of 02-16-09). I am attaching two screenshots - one from 1.9 which is what I would expect to see - namely options for restoring, unzipping, etc. As a result, I am unable to restore a course in HEAD. """,Bug,"Backup,Files API"
366325,"""OK, as I'm teaching (as an university instructor) this semester, I'm starting to experience some """"pain"""" that our instructors were experiencing. For the course that I'm teaching (Multimedia authoring with Flash), I have a lot of resources/activities for each week/topic. This created a big problem for navigation. Each time when student click into a resource or take a quick, then come back to the course home page using the breadcrumb. The will be taken to the top of the page. This could be partially resolved by adding the """"section link"""" block into my course (and I put it on the upper-left corner on my course), however, if they are currently on week3, and want to jump to week7 resources (and remember, I have 15 - 20 resources for each week), then they will have to do mouse scrolling a lot. Currently there is no way for them to quickly jump from week3 to week7. They can't jump from week7 to week 0 area (where usually people put the """"syllabus"""" and other helpful resources, etc.). Just thought that it would be a good idea to implement a optional """"Jump to..."""" pull-down menu for each section (the option can be controlled in the """"course setting"""" area).  Just my 2 cents (see screenshot for more info).""",New Feature,"Course,Usability"
366412,"""I have yet to work out why the items in the gradebook have no id numbers until one is added. This serves as a barrier to the use of calculations as it's an extra step to do, it's non-obvious what the purpose is and it could easily be automated.  I would suggest having an id number added automatically, with the option of altering it later if needs be.""",Improvement,Usability
366427,"""Steps to Reproduce: A) 1. Add a resource & Select 'Link to a file/website'     2. Upload a file & choose it     3. Save B) 1. Update the Resource     2. Choose another file     3. Save C) 1. Edit & Save the external file (e.g. Word/Excel File) you uploaded originally     2. Update the Resource from A & B     3. Upload and Choose the edited file (C.1.)     4. Save     5. View the resource.  It is still the same original file you uploaded in A)2!  Expected Behaviour: I would think if you deleted a file that if you made an offline edit and reuploaded it that Moodle would treat it as a new file.  Miraculously it revives the old file and disregards the new file.  Workaround: When editing the file, rename it and follow steps C)2 to C)5.  Only now has your editing taken affect and the invisible 'deleted' file finally 'gone'.  MIght this bug be similar to MDL-7660?  Apologies if this is a clone of MDL-2661(very old) but it seems the issue was more about the warning message rather than fixing this bug. I also couldn't find it as a fixed bug for 1.9.4""",Bug,Usability
366507,"""Although Moodle has lots of great pop-up help, and even better moodledocs pages, there is definiely a usability barrier in that it takes time and effort to find out what all the items in a long form actually do. Taking the quiz setup screen as an example, there are 26 separate help pop-ups, so to find out if any of them is useful for a novice takes 26 clicks!  What I think is needed is a small (1sentence) bit of optional text below some of the options to explain what it does or why it might be needed. This text could be unobtrusive and greyed out, like in the example I've attached from drupal.""",Improvement,"Forms Library,Documentation"
366670,"""the file admin/uploadpicture.php has a function called my_mktempdir - can we please rename this to something like create_temp_dir and put it in one of the main lib files?  I want to use it as part of MDL-7206  feel free to assign it back to me to do the work if you approve :-)""",Improvement,Files API
366723,"""Hello  In some countries, for example Germany where I am from, it's necessary to include the users lastname during correspondence - for example mails. The maintainers of the German language pack have included the necessary variables in the mail text ( $string['emailpasswordconfirmation'] = 'Guten Tag $a->firstname $a->lastname ). As this did not solve the problem and still only the first name was included into the mails, I looked up the code that is responsible for sending the mails. There I found the reason and was able to fix it for me.   I'd suggest to include this in the main code of Moodle. The fix would be in the function send_password_change_confirmation_mail() in file /lib/moodlelib.php at line 4469. Here it would be fine to include the line:  $data->lastname = $user->lastname;  At least for me it would be fine to have this in the main-code so I do not have to patch the code after every update :-).  Andreas""",Improvement,"Language,Authentication"
366764,"""The current program logic for getting parent language strings is:  1) check location for existence of parent language 2) check parent language for string in that parent language location  The problem is that if the site has a parent language defined one would expect that parent language to be checked for the block; however, if  the block does not have the parent language defined there is no langconfig.php for that location and a parent language for that location never gets checked. To re-create the issue with a fresh install:  1) Install the de_utf8 language pack 2) Install the de_du_utf8 language pack 3) Install a 3rd party block (for this example mrbs which has a lang folder for de_utf8 but not de_du_utf8) 4) Set your language to de_utf8 5) You should notice that all the regular blocks get translated to German; however, the mrbs block does not and is shown in English because the one location that defines the parent language /lang/de_du_utf8/langconfig.php then looks in that location for the string /lang/de_utf8/block_mrbs.php but the file is not there. Instead it is in /blocks/mrbs/lang/de_utf8/block_mrbs.php. But when the location /blocks/mrbs/lang/de_du_utf8 is checked there is no folder and thus no langconfig.php and thus no parent language is detected. However, I would still want and I think most users would expect that /blocks/mrbs/lang/de_utf8/block_mrbs.php would be checked.   I wonder if having a separate function for parent_lang would help defined along the lines of:  parent_lang($lang, $locations)  $lang is the current language (say de_du_utf8) $locations are the various locations defined by get_string if there is a parent language defined it will return it as a string (i.e. de_utf8) if not it will return false  That way we can simply check  if ($parentlanguage=parent_lang($lang, $locations)) { then if the $location/$module file does not exist ($CFG->dirroot/blocks/mrbs/lang/de_du_utf8/block_mrbs.php) and there is a $parentlanguage defined we can check for $CFG->dirroot/blocks/mrbs/lang/de_utf8/block_mrbs.php  One problem is that if different langconfig.php files point to different parent languages. I cannot think of a case when this would practically happen but theoretically that could confuse things but for the sake of this issue I think it can be presumed that each child language has one and only one parent language.   CONTRIB-957 has more information about what brought about this issue. Let me know if there are any questions. I will try to work on a patch.   Peace - Anthony""",Bug,Libraries
367111,"""I'm not sure if the 'components' I have selected above are correct but here goes.  The 'Maximum upload size' menu (when creating a new course and in other places) is a little inconsistent. Currently the choices I have are 10k, 50k, 100k, 500k and so on. Here in the UK and I think Australia (however I am not sure elsewhere) the currency runs 1p/¢, 2p, 5p, 10p, 20p, 50p, £1, £2, £5, and so on, the sequence 1-2-5 is repeated and I thought it would make more sense to have that type of sequence as a choice. Example (with other option too, based loosely on the USA's 'quarters'):  10KB 20KB (or perhaps 25KB) 50KB 100KB 200KB (or perhaps 250KB) 500KB 1MB 2MB (or perhaps 2.5Mb) 5MB 10MB 20MB (or perhaps 25Mb) ... and so on.  Obviously this is open to interpretation and I'm happy to be ignored. :)""",Improvement,"Accessibility,Usability,Files API"
367288,"""As a corporate user, it would be extremely beneficial for me to be able to do an export of all the training data for our online courses at one time.  I have over 20 online SCORM courses and I want to be able to create custom reports OUTSIDE of Moodle.  It would be great if there a way to export all the participation data of all my SCORM courses, including student name, date online module taken, score, etc to a CSV or XLS file so I could do manipulation outside of Moodle.  Right now I have to go course by course to get the data out of the system.  I want to be able to export all data at once!  I was quoted a price of $8,700 to do this from our partner, but unfortunately, the money doesn't exist this year in our budget.  """,New Feature,SCORM
367305,"""When No authentication is selected Self Registration is disabled.  On log in page you get  Is this your first time here? Hi! For full access to courses you'll need to create yourself an account. All you need to do is make up a username and password and use it in the form on this page! If someone else has already chosen your username then you'll have to try again using a different username.   BUT there are no links  I've not provided a URL but test this on 3 different site - all same result""",Bug,Authentication
367429,"""This came out of a conversation over lots of pasta with Skodak.....  In order to recover lost administration rights, implement something similar to Gallery. The outline is as follows... * Create a """"hidden"""" (but documented) script in the admin folder..... recoveradmin.php or somesuch * It invites you to create a file in the moodle root with a given name and a randomly generated key therein and then refresh the page * The rest of the process is locked out until this file is found to be correct - obviously, this needs to be locked down properly (I'll look/steal what Gallery does) * Once you're in there it will do this... *** Allow a new manual user to be created and assigned to the administrator role *** Reset the administrator role (and assignment rights) to default *** Reset the User Policy settings to default (a source of locked out admins) * The above should be options probably * The Moodle site will NOT run with the key file in place as an additional security feature. """,New Feature,Admin
367440,"""It would provide much enhanced usability, discoverability and user reassurance if grade items that have been dropped as a result of droplow or keephigh category settings would give a visual cue in grade reports.  I think this information should probably be stored in the database after grades have been recalculated.  I think lib/grade/grade_category.php apply_limit_rules function will have to be altered to accept a grade_item array and alter an attribute on each object in the array which contains display hints.  This data would then be committed to the db and displayed in the grader and user report by way of a cell class attribute that highlights the cell to show it was not used in the category calculation.  I've done some of this work, but I got bogged down in the DB storage and retrieval portion (plus this would require a DB alteration.  Help, pointers or affirmation would be appreciated.""",Improvement,Usability
367482,"""While looking at MDL-15187 (assigning roles via CSV upload), I noticed that we are making an assumption that the idnumber field can be used as a unique record identifier for the user and course tables. I also noticed that we do not have an idnumber field for the course_category or groups tables which I would advocate for adding to Moodle 2.0 for increased interaction between Moodle and other student information systems (SISs) which may have unique internal identifiers for course categories and groups. I do not see a need for an idnumber field for the course_modules and block_instance tables. If the idnumber fields are assumed to be unique identifiers and are used as such for importing records, I wonder if we ought to enforce that either via an index or some checking similar to how we handle checking for duplicate usernames.  I figured now is a good time to take a look at these issues and see if we wanted to add the idnumber field to course_categories and groups tables. Peace - Anthony""",Task,Admin
367511,"""I was about to write a new check, and I realised I would have to copy and paste a lot, so I did a refactor. Eloy, please could you review my changes. If they look OK to you, assign this issue back to me and I'll check them in.  I have tested, and they return the same results before and after my changes.""",Improvement,Database SQL/XMLDB
367567,"""I'm getting an error when I import a .csv file containing quiz results into a course. Moodle shows a message saying: """"Grade import success"""" but then it also gives me this warning:  Warning: unlink(D:\MoodleData/temp/gradeimport/cvs/2/1223556950) [function.unlink]: Permission denied in D:\Moodle\grade\import\csv\index.php on line 481  See the attached screenshot.  If I do a select on the <USER>grade_grades_history table (or <USER>grade_grades) the quiz results from the .csv file have not been added.  The first thing I did was to give both the moodle installation folder and the moodledata folders various different NTFS permissions to try and resolve the permission denied problem e.g.:  IUSR_myserver : modify, read, write, read & execute IWAM_myserver : modify, read, write, read & execute NETWORK SERVICE : modify, read, write, read & execute  I even gave the 'Everyone' group full control of both my ..\moodle and ..\moodledata folders, but the problem persists - I still get the permission denied error and the grades are not imported into Moodle.  I had a look at the code in line 481 of the ..\grade\import\csv\index.php file and it contains the unlink() function:  unlink($filename);  I think that this command tries to delete the file contained in $filename, but it is not able to and I don't know why. Could it be because the file is still open, or is it genuinely a NTFS permission error?  I then tried to disable line 481 by commenting out the unlink($filename); command. Then I tried to import the .csv files into Moodle again and this time I did NOT get any permission denied errors. Moodle successfully (?) imported the .csv files and I was able to see the quiz results in the Grader Report.  But I'm a bit concerned about the implications of disabling the unlink($filename); line. Will it leave a bunch of unnecessary temporary files on the server? Will it cause some other regression?  Finally, when I had a closer look at the error message that Moodle was giving me (see screenshot) I noticed that there are forward AND back slashes in the path of the MoodleData folder that is passed to the unlink() function:  Warning: unlink(D:\MoodleData/temp/gradeimport/cvs/2/1223556950)   Does this have any meaning? Shouldn't they all be backslashes? I don't think the above path is valid - at least, not in Windows!   If it helps debug the problem, the .csv file that I used to import the users has this structure:  id,username,password,email,firstname,lastname,idnumber,institution,department,phone1,phone2,city,url,icq,skype,aim,yahoo,msn,country,deleted,course1,course2 1,student1,changeme,<EMAIL>,Student 1,User 1,7501020304085,ABC.com,ABC,0123454567,0123334545,SomeVille,N/A,N/A,N/A,N/A,N/A,N/A,ZA,0,CRS_TEST1,CRS_TEST2  And the .csv file that I used to import quiz data into the QZ_TEST1 quiz has this structure:  username,Quiz: QZ_TEST1 student1,9.00 student2,2.00  Moodle is able to import both these .csv files only when the unlink($filename); line is disabled. """,Bug,Course
367706,"""One regular criticism of Moodle (and course management systems in general) is that the student's work is locked away behind a login screen.   If the student loses access to the course, his or her work and the grades it received become inaccessible. This can happen if the course has an enrollment time limit, or if there's a third-party auth module used and the student is no longer enrolled at the institution.  That's not the case with old-school work done on paper -- graded assignments are handed back to the student with the teacher's comments, and the student is free to keep them forever (for example, I still have some term papers I wrote as an undergraduate, and I'm pretty sure my mother still has some school work that I did as an elementary student :-)).  I propose a feature which allows a student to export all of his or her work in one bundle -- perhaps as a zip file consisting of a set of HTML pages.  Certainly, there's nothing stopping the student from doing this now (by copying and pasting all the messages, for instance), but it's not very convenient to do it by hand.  I wonder if the backup module could be hacked to do this, e.g. take a student ID as a parameter and make a backup consisting of only that student's work?   """,New Feature,Usability
367946,"""There is a missing piece of code in:  moodle / course / lib.php  Near line 1788, I have added the following to correct the missing icons, however this is a temporary fix and should be permanently corrected.  ************************ *   FIND near line 1788 ************************  echo '<td valign=""""top"""" class=""""category name"""">';   ************************ *   INSERT THE FOLLOWING ABOVE THAT LINE SO THAT IT LOOKS LIKE THIS: ************************  // ZZZ Add Images - Eventually correct this by adding an object code snippet and an include          $catimage = '<img src=""""'.$CFG->pixpath.'/i/course.gif"""" alt="""""""" />';   echo '<td valign=""""top"""" class=""""category image"""">'.$catimage.'</td>';  //ZZZ Added to test images on front course page  // ZZZ End of add images to the front page.  The entire course categories section logic should be turned into a reusable """"printed"""" code snippet          echo '<td valign=""""top"""" class=""""category name"""">';  ~~~~~~~~~~~~~~~~~~  That second location for the images does not contain the reference to the theme images and should be corrected.  However I have not had the time to properly code all of it so I put in a hard fix for my site.  ~~~~~~~~~~~~~~~~~~  SUGGESTION:  Going forward it would be ideal to have all display / block / code sections that are common to users logged in and those that are not logged in to be created as """"include"""" objects and the print option used (unless performance becomes a problem).  OR, possibly define it as a separate function?  I'm not a coder so I really don't know.  I know enough PHP / HTML / JavaScript to be dangerous.  However, to make maintenance / coding easier, and more consistent, all commonly rendered sections of code whether the user is logged in or not might need adjustment to a re-usable method?""",Bug,"Themes,Course"
368165,"""As a result of some previous efforts dragmath is now offered in Moodle CVS,  but relies on a version of adminlib.php that includes minimal setting for dragmath  'insertdragmath' => 'em.icon.dragmath.gif'  The problem is that as new releases of moodle are made available the dragmath cvs faces either having to provide a separate version for each release, or face possible inconsistencies in adminlib.php.  I don't think requiring or expecting that that all users have the most recent release is reasonable.  As Dragmath is expected to be included in core 2 moodle anyway and could be actually move to core 1.9 Moodle with very minimal work,  I would like to see the core adminlib.php for version 1.8.4 on be altered in Stabel to include the additional code so that adminlib.php can be deleted from the dragmath cvs.""",Improvement,Libraries
368192,"""This is very weird. 'm logged in as administrator. I'm restoring 1.6.5+ courses without users (nor userdata). Backup file in Front Page - Site files. Moodle 1.9.2+ ((Build: 20080806) has to create a new course in an empty category. The restore process tells me the (26) categories I made are not available. If I continue the restore process it ruines my Moodle frontpage: apparently the restored course overwrites parts of it.  This restored course doesn't show up in the course list.  I then tried the blank course solution. Uploaded backup file in Course files. The zip file is OK: Moodle lists all files. Roles assigned: editingteacher, teacher and Student The backup fails at forums telling me """"the default teacher role was not found"""". After that I get a """"No files found"""" message and I get transfered to the front page. Categories indicates there was a course created but inside the category there's nothing. To delete the whole thing I have to delete the categorie :-(  Now, if I opt for a restore without the forums the backup executes correctly. Is this a bug? Is this somekind of a role issue?""",Bug,Backup
368487,"""I sometimes need to hide blocks from students and have never had a problem overriding roles/permissions until the latest version of 1.9.2+. When logged in as admin, and you get to the stage where you should be able to set permissions and say hide from students you don't get any choices but a messages that says  """"no capabilities available in this context"""". I may be missing something simply, but I've never touched """"roles"""" in either install except in the manner I've described. Two screenshots attached.""",Bug,Roles / Access
368659,"""For accessibility reasons it is important to ensure correct heading nesting. Unfortunately the HTML editor does not support this practice because it always offers users the choice of any heading level, so that you can choose <h1> within a forum post for example, which will spoil the page's heading tree.   It is possible to work around this by disabling headings within the editor but obviously we would like to allow users to create headings. (For accessibility reasons we have of course disabled the font/size options.)  Desired behaviour would be that for each specific instance of the html editor, for example the course summary box in the course settings form, or the main box for a forum post, the developer who writes the code instantiating the editor could specify a maximum level of permitted headings (eg h3). It would also be possible to specify no headings. This should be possible both in 'hard-coded' use of editor by calling the function, and in Moodle forms (as an attribute I guess).  So if you specified h3 the dropdown would read 'Heading 3, Heading 4, ...' , simply omitting H1 and H2.   In addition, form validation might also automatically replace any <h1> or <h2> with <h3> on processing the form input, thus supporting manual/source code input.  (Note I am reporting this because I'm closing an in-house bug as 'won't fix' and I had a discussion with Petr about this some months back but I don't think I filed it then. Unfortunately I am not personally able to do anything on this at the moment, I just wanted to record it as a desirable accessibility improvement.) """,Improvement,Accessibility
368900,"""Something in what is done in config.php has stopped the progress bar html rendering progressively in the browser. The js and the html that is output by the server is not being displayed in the browser until the page has completed. So the js to update the progress bar is not getting run until the last moment when the full page is displayed.  I attach a copy of a test script that shows the problem.  1. Put the script into folder pb/ in the top of your Moodle install. 2. Go to the url for pb/ex1.php with your browser. 3. The progress bar won't update until the whole page has completed. Then it executes all the js at once, so it updates in a flash.  This is not happening in the 1.9 branch. If I copy the code for the progress bar into adminlib there and do the same steps as above the progress bar works OK.  I want to use this function for regrading questions in the quiz. Hope it can be fixed.""",Bug,"Admin,Libraries"
369040,"""i am trying (as a teacher) to switch to a specific student in the course but i am unable to do so. as Administrator i can (obviously) i tried to manipulate(override) the roles but with no success :-( shouldn't this be a default role right for the teacher ?  btw. i want to do so in order to set a private event in the specific student's calendar. is there a better way ?""",Bug,Admin
369330,"""[Eloy from MDL-14698] some comments after looking to user/index.php code:   1) The whole drop-down menu is only showed if the user has 'moodle/course:bulkmessaging' capability. Not sure if that """"individual"""" capability is the correct one to decide about the WHOLE drop-down.  2) Messaging system status isn't ever check and it should ($CFG->messaging) together with the 'moodle/course:bulkmessaging' to decide properly about that menu item.  3) Notes are ok, AFAIK, being checked by 'moodle/notes:manage' capability (I don't know it there is any $CF->switch to disable notes (haven't been able to find it).  4) Extend enrol options happens without control at all! 'moodle/course:update' should be checked if I'm not wrong. Also, extended enrolment options should be ONLY available if the course has some sort of enrol limit defined.  5) With all particular permissions calculated in 2) , 3) and 4)... I'd buid the drop-down menu IF the menu has any content. Only if that (instead of 1)) """,Bug,Course
369457,"""When editing is turned on, each block on a course page has an assign roles icon (next to show/hide, delete and move icons). Why not provide the same assign roles icon next to each activity module, to enable quick access to the assign roles page for the module, rather than having to update the activity then click the """"Locally assigned roles"""" tab.  (I'm guessing I'm not the first to suggest this, however a quick search didn't turn up a similar suggestion for improvement.)""",Improvement,Roles / Access
369655,"""I would like the ability for users to be able to change/edit there own usernames. This feature was available in 1.6, but since upgrading to 1.9 it does not seem to be available even as an option.  It could be added as an ability to roles, probably default would be Allow for Admins, and Not Set for all other roles.  Also, in the meantime, is possible to know which files would be relevant so I could look at a hack solution for this?""",Improvement,Roles / Access
369714,"""I just found that there is no override permissions tab in the add a file or website setup page. I was thinking of making a file (with answers in it) available to only some roles, but this was not possible. I don't want to use hide/show as assigning permission to view hidden stuff is overkill - its meant for some files, wheras others I want to keep hidden.  Could an override permissions and locally assigned roles bit be put here with just view permissions there?""",Improvement,Roles / Access
369790,"""I've just noticed this strange behavior, new to 1.9 and 2.0, whereas functionality worked OK in previous Moodle versions. ------------------ PROBLEM ------------------ Checked with 2.0, but same problem exists in 1.9 too. Logged in as Admin, make a partial backup of any course (other than site front page), e.g. Course 101. Save backup file locally. Go to Course 102 and go to Restore. Upload previously saved backup file to Files -> backupdata. Click on Restore. Message: Later in this process you will have a choice of adding this backup to an existing course or creating a completely new course. Click Continue. # Creating temporary structures # Deleting old data # Copying zip file etc. Click Continue. Restore to -> Select *Existing course, adding data to it* On the next page, as per moodle 1.8, you should be offered the option to ***Choose a course*** from the list of existing courses on your moodle site... but you are not presented with this option.  You only see the Restore this course now button. Click on it, the restore process seems to work: # Using existing course     * From: Questionnaire module Demo (QUEST_DEMO)     * To: moodle20 (moodle20)     * Adding data to existing # Creating groups.  etc.  but the restored data can be found nowhere on the site!!! ------------------ DIAGNOSTIC ------------------ The problem arises from file backup/restore_check.html lines 247 & sqq. where we strangely have a commented out line, and conditions which are never met for the display of the """"choosecourse"""" message...  ------------------ SOLUTION ------------------ I do not understand enough of the workings of the restore functions to suggest a solution but I do hope one will be found, and I am really surprised that this bug has gone un-noticed (in 1.9) so far..  Joseph""",Bug,Backup
370045,"""This was reported to me by our accessibility team. I haven't actually asked them why it is an accessibility problem, but it's certainly not ideal if all the help documents are just titled 'Help' (as they are).  The attached patch changes help to work as follows:  1) If you include in your help file, <title>something</title>, then the HTML title becomes 'Help - something'.  2) If your help file has any heading e.g. <h1>Whatever</h1>, then the HTML title becomes 'Help - Whatever' (for the first heading)  3) If your help file has no headings either then the title is still just 'Help'.  This patch could be applied to 1.9 but I'm not particularly recommending that, HEAD would be ok...""",Improvement,Accessibility
370046,"""I was browsing the web on my Nintendo DS the other day, using the Opera Browser. The browser is somewhere between a novelty and actually useful due to the underpowered status of the device. As a result browsing without images is much more useable.  Testing Moodle.org to see how it fared with this browser I was surprised to read the text """"<USER>playing a contrabassoon"""" in a list of forum topics (i.e. a page with a url like <USER>forum/view.php?id=1234).  It turns out that this was what a user had entered as a description of their user picture.  I'm not sure where this text would be useful, but in my experience as a user browsing without images on a portable device it was initially confusing and then unecessary in this particular location. I'm guessing that it might be similarly so for users with screenreaders, but I don't have any easy way to find out if that's true.""",Bug,Accessibility
370229,"""Automated backups has a problem. Since  upgraded our system to 1.9+ the automated backup files are very large in size. A normal 3MB course generates a 197MB bacup file. After invistigating the matter, we found out that Moodle backup all user files (not only course user files), that are 8000+ in each course, resulting on the huge file size.  I've tried to perform a manual backup, and although it displayed the total number of site users (8000+), it performed backup for course users only, resulting in a 3.9 MB file.  We are disabling the site files option in the backup.  Regards,  <USER>Al-Quds University""",Bug,Backup
370266,"""download_file_content has a $timeout parameter. When in emulation (Snoopy) mode, this is the 'read timeout', but in Curl mode it only sets CURLOPT_CONNECTTIMEOUT, the connection timeout, and not CURLOPT_TIMEOUT, the hard timeout.  I have made a patch. Some notes about it:  1) I made the current timeout parameter control the read timeout. This makes Snoopy/curl behaviour consistent (changing curl to do what snoopy did).  2) I changed the default to 5 minutes (300) in case people are using it to download largeish files.  3) I added an extra 'connect timeout' which defaults to 20.  The reason for making the normal, first timeout parameter the read timeout one is that (i) you probably won't need to change the connect timeout unless you want to make things really short e.g. link checker, (ii) current code probably expects the timeout parameter to work, which it won't in many cases at present.  Connect timeout doesn't do anything if using a proxy (well unless your proxy is down), this is documented. [Another reason for focusing on 'real' timeout.]  I'd suggest fixing this in 1.9x if possible.""",Improvement,Libraries
370487,"""I'm not sure if this is a bug or by design, I cannot find any one else referring to it.  The Administrator role does not have moodle/course:view set to Allow on a default installation. This seems odd to me as an administrator at course category level should be able to enter courses without having to enrol on them for example.  """,Bug,Roles / Access
370543,"""The log table url field is 100 characters. This can be a bit limiting in some instances where log entries may include text strings, such as filenames. (The easiest way to cause a problem with this is to make a wiki page with a 100-character name.)  There are three related problems:  1) Calling add_to_log with something longer than the url field results in an SQL error, and sends an email to the admin suggesting the database might have run out of disk space. While this behaviour is pretty cool, I don't think 'some stupid module tried to log a really long url' is serious enough for an admin email :) My suggested fix is:  a) Check length is under the limited for 'url' and 'info' parameters (only ones which are likely to potentially depend on user input) and trim to limited length, with '...', if needed.  b) If this trim happens, show debugging() info so that developers might notice they are logging something that can get too long.  c) In log view, if url field ends with ..., don't treat it as a link (as it won't work!).   2) The phpdoc for add_to_log is unclear as to the value of the URL field. It might seem obvious for developers to assume that it is, in fact, a fully qualified URL; however in fact it doesn't have to be, and the precise value for it depends on the type of plugin (sigh). It's difficult to track this down without following through a variety of code in log view, and when you do find out how it works, it's complicated.  a) Change make_log_url in course/lib.php so that it supports block_, format_ prefixes in $module (if you passed in block_whatever and url frog.php at present, it would change the URL to /<USER>block_whatever/frog.php - not helpful) and so that [apart from the existing exceptions] it is possible to begin your URL with / to mean 'start at $CFG->wwwroot' whatever the $module. The intent would be to make this function return exactly the same as it does now for all uses of the current function that return a correct url, but make it more consistent for future use and support blocks, course formats.  b) Make phpdoc for add_to_log reflect this.   3) 100 characters is too short in some cases especially if full URLs are logged. (are they supposed to be? maybe it's relative to $CFG->wwwroot, but this isn't clear in the phpdoc, this should also be clarified). I think the limit should be increased to 255. Could this cause problems for some databases, or is that change ok?    I haven't done the code for this yet, if it is ok I will do it for moodle 1.9.1 or just for 2.0, let me know which. (I'll remember to <USER>only the relevant version fixed as per martin's request in developer meeting!)""",Bug,Libraries
370624,"""Was testing a bit Moodle 1.9 under Oracle, when I got this error:  ORA-00932: inconsistent datatypes: expected CLOB got CHAR  SELECT r.id, COALESCE(rn.text, r.name) AS name FROM m_role r JOIN ( SELECT DISTINCT allowassign as allowedrole FROM m_role_allow_assign raa WHERE raa.roleid IN (3,1) ) ar ON r.id=ar.allowedrole JOIN m_role_capabilities rc ON (r.id = rc.roleid AND rc.capability = 'moodle/course:view' AND rc.capability != 'moodle/site:doanything') LEFT OUTER JOIN m_role_names rn ON (rn.roleid = r.id AND rn.contextid = 23) ORDER BY sortorder ASC  It seems that the role_names->text field is of type TEXT (CLOB) and that prevents it to be used within COALESCE and other functions.  If I'm not wrong that table, role_names, is exclusively used to name roles in a different way per context. If that is the only usage I would recommend to:  1) create one new role_names->name char(255) not null field. 2) copy current role_names->text contents to role_names->name 3) drop role_names->text 4) Change usages from the old field to the new one.  That way the field will be better and his behaviour cross-db.  Ciao :-)  P.S.: Raising this to critical... to alarm about it (prevents Oracle usage completely)""",Bug,Roles / Access
370902,"""As a Moodle newbie I find it very strange that there's no batch or automated function for changing dates in a course. Especially when you're restoring a backup into a new course, but also making changes to a current course is very unfriendly and time consuming. More technically: I would like to see a function on course-level to shift all activity-dates with x days (and times by x minutes?).""",New Feature,Course
370937,"""1) Log into main page as admin 2) Turn editing on 3) Turn editing off button missing  I've looked through MDL-9044 and MDL-9172 but those workarounds do not apply to the main page""",Bug,Admin
371044,"""I have created a new feature that makes RSS feeds for all the courses on the site, and also feeds for each categories.  I believe <USER>of The Open University had asked you if we could commit this feature to core.  There is a new setting in the RSS admin area that controls the amount of items within all these feeds, and also in each category settings you have to enable the RSS feeds.  I have also changed it so that if you make a course visible or hidden it changes the timemodified <USER>in the database as it uses this field to see if the feed needs updating.  There is a couple of issues that I do not know how to solve these are:-  a) I currently use the timemodified field in the course table to work out if the course has change so that I can move it to the top of the RSS field. Unfortunately this only seems to change when you change the course settings. Should I use another method to work out if the course has changed? i.e. loop through all the course modules and get the time <USER>those modules where updated. This would also suffer from a similar problem because the time <USER>of those modules gets changed when the module settings are changed and that shouldn't really be classed as an update.  b) Any sites with a whole bunch of courses could suffer a performance hit as I get all the courses from the database and place them into an array for later use. This could be tided up but I did not have time to re-factor that code. Would someone be able to look at this in the future, it should not cause any issues to normal sized sites.  <USER>( either one :) You both know Moodle well! ) could you please cast you eye over the patch attached. This is the first time I have ever committed to core and I would like an experts opinion on the code before I commit. I have created the RSS stuff as an object rather than a lose collection of functions as I believe that is the way  you wanted to go with Moodle I hope this was the correct way :)  Currently the code has only been tested on the PostgreSQL database as that's all we have here at the OU.  There will also be another new feature I will be working on which is the searching and filtering system we have on openlearn.open.ac.uk/course and labspace.open.ac.uk/course which will also display links to the feeds. I haven't put in any links in the normal core code as I wasn't sure where they should reside.  If you are happy with me committing this patch, wish me to change any of the code, or just have questions please contact me on j [dot] e [dot] c [dot] brisland [at] open [dot] ac [dot] uk  Thanks,  <USER>""",New Feature,Course
371214,"""I would love to see a course-specific setting, """"week begins on"""" - to set the day-of-the-week start of a course.  I'll often have two sections of a course, sharing the same course page, in which the first of the two sections meets on e.g. Thursday, and the second section on Monday; (because Monday was a holiday) - so that the Monday section, has the same materials as Thursday of the week preceding it.  Of course one can use the Topics layout of the page; but the Weeks layout highlights the current week, which is a great feature (esp. for my chronically disoriented students). """,Improvement,Course
371311,"""I'm running CVS top-of-tree for 19_STABLE (updated yesterday).  When I am logged in as an admin and use the """"switch roles to"""" drop-down menu, and choose """"course creator"""", I enter an infinite loop of redirects.  Both Firefox and Safari eventually say """"sorry this will never complete"""" and give an error page.""",Bug,Roles / Access
371352,"""There is a problem in the restore component of backup that causes the dates on forum posts to be wildly off (7+ years) when a course with forums is restored by a teacher. If a course is restored by an admin, the forum post dates come in correctly.  What's happening is that /backup/restore_form.html has logic that checks to see if a course has a start date, and if it does, it gives the user the ability to modify that start date.  However the problem is that this start-date-checking logic is tied to the """"course creator"""" capability -- if you don't have the coursecreator capability (and teachers don't) then it doesn't do the check, and assigns the start date to 0, which triggers some date offset calculations, which results in the wonky dates.  I've created a patch that fixes the problem by moving this logic out of the """"coursecreator"""" portion of """"restore_form.html"""". As a result, teachers will now see the """"course startdate"""" field as well, and everything works as it should. The patch is attached to this bug report. """,Bug,Backup
371555,"""See the screenshot.  As administrator, using site Administration -> Security -> Site policies, I can set Blog visibility to any one of five levels, the highest being """"world"""" (let's call this level 5) and the lowest being """"users can only see their own blog"""" (let's call this level 1). Suppose I select """"all users on this site"""" (i.e., level 4).  When a user creates a Blog entry, he also has some control over who gets to see it, obviously constrained by the site policy. For example, if the site policy specifies level 4, the user should be allowed to choose any level less than or equal to 4. If the site policy is level 3, the user should be allowed to choose any level less than or equal to 3, and so-on. However in the current implementation, the user is only given two choices: """"anyone on this site"""" (level 4) and """"yourself"""" (level 1) -- see screenshot.   Users cannot specify an intermediate level.  Here is a use case for being able to specify intermediate blog levels.  Suppose I wanted to Blog on moodle.org some comments that might be construed as critical of Moodle.  I would not Blog these comments if they were visible to World (the current site policy).  However if I can Blog them so that only the group of """"Particularly Helpful Moodlers"""" can see them, that would be a different story. """,New Feature,Blog
371927,"""We are generating SCORMs 2004 with Articulate Presenter 5.0 in Moodle 1.8 and we are suffering problems in order to give visibility to the students about their progress and results.  The grades are set properly, because as a Teacher I can get an overview of the progresses, with the limitations of Articulate, but the students can't see anything.  I've tried several options when I'm attaching the scorm but this also doesn't work.   I attach two screenshot, maybe it will be helpful.  Thanks in advanced for your support.""",Bug,SCORM
372222,"""Hello,  I'm working at a school for visually impaired and blind people in Germany and we are using Moodle. We did some modifications to core-modules like quiz.  Improvements / changes in our quiz-version: - Question number gets marked as headline   If navigating with screenreader software (tested with JAWS6/7 and Webformator 2.3) you have build-in features like """"jump to text"""" or """"jump to next headline"""". So if you <USER>the question number as headline, a screenreader user presses a shortkey (offered by the screenreader) and is on the right place to answer the question.  - short keys for navigating for and back on question pages:   We are only using 1 question per page to make quiz usable especially for our blind participants. They know 1 question and at the end 2 switches. As a result, you get many question pages. There are currently links presented for back and next, but it is not easy to get with screenreader.  We implemented 2 shortkeys (Alt + . / Alt + ,) you can press for back and next, which makes it pretty easy for our blind people.  - True/false, shortanswer and multichoice questions: The question is displayed as Alt-tag for input fields or radio buttons  It would be nice to see some of my mentioned improvements in the next versions of Moodle. Maybe there has to be a new debate about Moodle and support of Accesskeys?  Kind regards, <USER>",Improvement,Accessibility
372273,"""Hello!  We are not very new to moodle, we use it for about two years, but this year is the first we've started a wide usage of moodle in studying process. We have started using tests in studying process since september 1, so we have some little expirence in testing of students.  Today is the first day we've used moodle for testing at unit control. This is a very important test; the <USER>student got at this test is a half of all cource <USER> All teaters MUST <USER>students.  We've got a problem today. The main difference between tests we've used before and the test we've used today is that all previous tests contained maximum 10-15 questions for 10-15 minutes to pass. The test we've used today contained more then 50 questions and the time to answer the test was about 1.5 hour.  We find it not a confortable to split questions on many pages, so in all tests, including today's test we've showed all questions at a single page. The result was that when students completed answering ant when thay was trying to save-and-finish the test, they've got a 'session timout' error, and all the data was lost.  That may wouldn't be a big problem for us, but 100 students have lost their answers today. We've lost more then 3 hours testing them. And now it's seems we need to repeat the test, to geather all that students again and to make them to pass the test again. This is really a big serious problem.  We understand that this problem occured because of our low expirence in using moodle. Now we known ways we could make this problem not to occur: we could split questions to many pages, we could increase session lifetime, we could ask students to save w/o submitting and so on. We did not know about this problem today's morning, we didn't do anything to prevent it and so now we have a great headache.  If've searched your moodle.org forums and i've find that many people had a headache like ours.  Some simple technical features could prevent that headache for us, and, i think, for many people:  1. The main, and the most simple. Save all POST variables in a separate log (stripping passwords, of course). Saving togather with all GET variables would be useful. That log may not be accessible via web-interface, but is SHOULD exist. I'm a web-programmer, and it is unconditionally more simple task for me to write a script that would restore test results from that log, then to geather 100 students and to trifle away more 3 hours re-testing them. This simple log could solve many other problems, and could be useful itself: this may help to restore ANY data lost because of session timeout, not only quiz; this log may help you to get additional information while fixing bugs. I've implemented this log in my CMS and i've used it many times to investigate hacker's and spammer's activities; it has helped me to find real persons flooded at my site anonimously. I think this very simple log is absolutely useful, even without any additional web-interface or any other featers around it. I think database is the best place for that log; using gzcompress(serialize(array($_GET,$_POST))) would be the best choice, IMHO; automatic cleaning of old items will prevent the log from repletion, but i think this is trivial.  2. Making simple empty ajax-queries to server every 5 minutes will prevent sessions from timeout until browser is closed. Without regarding of server sessions configuration. This will lead to that we'll not be required to split questions to many pages anymore, users who write messages in a forum for a long time, will not be required to rewrite their messages if a session is timed out; this will not make some people using session.gc_probability=0 or a very long session.gc_maxlifetime - these two last features may easily lead to lack of free disk space.  3. I think that would be a good feature, if moodle would write a special warning message, showed to teachers, if a quiz timeout is greater then the session timeout. A separate system warning about possible problems with sessions and an additional notification to use `Save without submitting` button, showed to all students, if they have JavaScript switched off (leading to that empty ajax queries will not be executed and session still may time out). Teachers may have great expirence using moodle, not students. Teachers may notify students to use that button if they are near the studens. But if a student is far from the teacher, this becomes impossible, and the moodle becomes the only one who may tell students about possible problems.  4. If a session is timed out at a POST-query, it would be a good practice to store all POST-variables in a NEW session and to get users back to their form filled with data they've posted after a user re-logged-in. For example, re-filled quiz or re-filled forum reply form; data for this form may be got from current session. This will allow users to save their data even if session is timed out. You may even implement a question i've seen at some mobile phones: """"You have not saved a previous message. Would you like to continue with prevous message or to create a new empty message?""""  Thank you for attention. Hope my four feature requests will be implemented and will prevent many peoples from long hours of headache.""",New Feature,Authentication
372349,"""Clicking the +/- icon next to items in the SCORM package list does not expand/contract the items and throws an error.  This is originates from the """"scorm_get_toc"""" function, within the files scorm_13lib.php and scorm_12lib.php.  scorm_13lib.php line 117 (creating a link using javascript:expandCollide) The second parameter needs to be passed as a string, but isn't surrounded by single quotes which makes the browser think it's a undeclared variable. $result->toc .= '<a href=""""javascript:expandCollide(img'.$sublist.',\'s'.$sublist.'\','.$nextsco->id.');"""">'.  Same as above but for scorm_12lib.php line 266.  Then in both files, the JavaScript function expandCollide is incorrect and badly written (switches between trying to find a suitable method for getting an element by ID, but then just uses document.getElementById anyway for changing the image).  I updated both to the following (since I didn't know what level of browser Moodle wants to support when it comes to JavaScript).                function expandCollide(which,list,item) {       var el = document.ids ? document.ids[list] : document.getElementById ? document.getElementById(list) : document.all[list];                   which = which.substring(0,(which.length));       var el2 = document.ids ? document.ids[which] : document.getElementById ? document.getElementById(which) : document.all[which];                   if (el.style.display != """"none"""") {                       el2.src = """"'.$scormpixdir.'/plus.gif"""";                       el.style.display=\'none\';                       new cookie(""""hide:SCORMitem"""" + item, 1, 356, """"/"""").set();                   } else {                       el2.src = """"'.$scormpixdir.'/minus.gif"""";                       el.style.display=\'block\';                       new cookie(""""hide:SCORMitem"""" + item, 1, -1, """"/"""").set();                   }               }    I've attached two updated files for inspection (updated files from 1.8.3+ 2007-10-24). """,Bug,SCORM
372476,"""After setting up all the course pages within their categories, I started adding the staff to their subject/ course pages as the Teacher role, so they could begin editing their pages, then on the homepage once they was assigned to their courses in the My Courses block it listed all the course page titles they was responsable for. Great!  However if a Teacher of Science for example then clicked on all courses and attempted to view a Maths course page it prompts them to enrol?So if they click enrol it then displays that Maths course page in their My Courses block on their home page. Not Great!  To avoid having the teachers enrol in any other courses I set a global role for all teachers as Non-editing teacher so they can view any course pages, the problem with this method is the My courses block on their homepage then lists every course on the site.  So is there a way of letting teachers view all courses without enrolling and without the courses being listed in their My Courses block, all I want listed in their My courses block for them is the pages they will indeed be responsable for editing.""",Task,Roles / Access
372508,"""In the ext db auth, in the password setting, these values are available:  - plain text - md5 - sha1 - internal  that """"internal"""" setting is used by the is_internal() method to return true/false.  And, in moodlelib, if one auth method is internal, then update_user_record() is 100% prevented, so nothing is refreshed from ext db.  Just guessing if that """"internal"""" value in the db auth plugin has another hidden sense. I cannot find it. Everything gets refreshed properly without it and the ext db, by definition, seems to be a not internal auth plugin.  I would propose to kill such """"internal"""" option, unless somebody have any explanation for current behaviour (I've take a look to the ldap auth - really similar in concep) and it haven't such """"internal"""" posibility at all.  So:  - IMO the db auth plugin should return always false. - The plugin should be able to run together with the """"cron.php"""" passwords and """"auth_db_sync_users.php """"  for new users and so on but not using the """"internal"""" concept for that.   Just one opinion. Ciao :-)""",Bug,Authentication
372696,"""I suspect this is another issue related to MDL-11347 which only seems to be occuring on an upgrade. But I'll create the issue so I remember to check it.  I've upgraded a site which has a complicated course category layout, simplified:  - X  --- Y [hidden] ------ Z  [hidden]  I am an admin,  I can browse to sub cat fine, but when I try and browse to sub sub cat I get 'That is not currently available' at line 47 of course/category.php: call to error().  Which means as an admin i'm failing to receive has_capability('moodle/course:create', $context) at that category context   (To make it more hazy there are some roles assigned at context Z!!)""",Sub-task,Roles / Access
372751,"""Trying to find out whats causing this..   I've just tried upgrading from 1.8 and there is weirdness when logged in as admin with get_assignable_roles:  Warning: Invalid argument supplied for foreach() in /var/www/moodle/lib/accesslib.php on line 3898  This means the admin can't currently assign roles, or switch role on courses.""",Sub-task,Roles / Access
372871,"""I'd like to be able to reset courses in bulk (in particular deleting the students), for some course categories, at the end of the academic year, rather than delegate this responsibility to each member of the register staff who has a lot of courses to manage. This global feature could be added to the Site Administration / Courses menu, just after """"Backups"""".""",Improvement,"Course,Admin"
373398,"""Warning:  Wrong parameter count for array_slice() in /Library/WebServer/Documents/moodle/lib/datalib.php on line 837  According to the PHP docs, the fourth parameter to array_slice was added in PHP 5.0.2.  I'm running PHP 4.  This is in Moodle 1.8.2 latest (not sure which date it was released, but the bug is confirmed by someone with a different version);  Moodle 1.8.2 stable (as of today anyway) is OK.  To reproduce:  1. As admin, browser the list of all users (Users/Accounts/Browsel list of users in Site Administration tab) 2. Click on the name of a user to visit user/view.php  I may be understating the severity of the error;  with debugging off I don't see a problem, but I spoke to someone who I believe encountered a blank page.""",Bug,Libraries
373513,"""This is on the Wiltshire College site, where they have been using Moodle since 2003.  We recently (about a week ago) upgraded their site from Moodle 1.6.4 to 1.8.2+.  They have a large site, with ~1800 courses and many thousands of users.  When using 1.6.x, they had 153 users that were set up with the course creator right.  Upon upgrade to 1.8.2+, all of these users now have the course creator legacy role assigned globally (this was done automatically by the upgrade).  This seems to cause an IMMENSE slowdown of the login process, so that the login for a single user with this global role assigned takes between fourteen and fifteen minutes.  That is not a typo - 15 MINUTES, not seconds.  I sat there and watched.  All that time, mysqld was hitting between 97% and 99% on the CPU.  This is on a brand new rack-mount IBM server with two hyper-threading 3GHz Xeon processors, 4GB RAM and a SCSI RAID system.  And with no other users online.  On top of that, if you log on as an administrator and try to remove the global role from just ONE of those 153 users, Moodle takes about ten MINUTES and loads of maxed-out mysqld cpu before it finishes and you get control back.  Once that has been done, the user can log in as normal.  Add the role back and you get another bout of maxed-out cpu for ten minutes or so.  I was going to try this with one of the teacher roles assigned as a global role, to see if it was any global role (other than admin) or just the Course Creator one, but everything had taken so long that we ran out of time.  Logging in as an administrator takes about two seconds.  So I'm guessing that something is seriously wrong here - either a problem with the database, or a bug somewhere.  <USER>and <USER>Thompson at Wiltshire College would be happy to allow remote access if someone needed to take a look at the system.  cheers  Sean K""",Bug,Roles / Access
373567,"""I would like to open this issue regarding the Front Page accessibility that was previously closed in the MDL-5983 issue.  I would like people who are not logged in to have a different stylized Front Page from what the logged in users would get.  Presently this is impossible to have if one wants to put a Login button or other site access information - as that does not make sense for someone already logged in to see - unless we are forced to use My Moodle which will not work if the focus is on creating a standardized learning environment that is not cluttered or made more complex by the ability of students to click around and play with various block type functionality - and thus likely resulting in more admin type of work.  The concern with using Moodle blocks is that I do not want to give my students the ability to create or use blocks - I want a tightened down standardized learning environment across all students that just focuses for logged in students on the courses to take and the actual courses themselves.  As a result, I am hoping that this can be an enhancement request to be able to better define the Front Page by role to eliminate the above issues.  Thanks in advance and keep on Moodling!  Best Regards,  Bob""",Improvement,Accessibility
373700,"""We've had some useability testing work done recently on OpenLearn.  We received some feedback about the way that Help functions, which I'd like to share.  I don't personally see this as a very big issue and so am not in a position to change the code right now, but I'm logging the issue here in case other Moodle users think it's important and maybe a core solution could be made?  Essentially its been suggested that a link to the help index page should be made available more widely on the site - which would be a content issue and trivial to do.  But then the following suggestions are that the help texts displayed should depend on the permissions of the user browsing them i.e. you should only see help pages about assigning roles or creating a quiz (for example) if you are able to do that at some point within the system.  Additionally it was suggested that there should be a search facility within the help pages. """,Improvement,Usability
373885,"""This issue has been reported as an """"improvement"""" in MDL-6591 but I think it's a really serious (functional) bug and be treated with great priority.  The question of notifying grades by email can be discussed and considered an improvement, but I think it just doesn't make sense that students automatically get a copy of their obtained grade in an assignment if you've <USER>NO to """"show grades to students"""" in the course configuration.  It's happened to me and other teachers as I've seen in the forum and I think this is still happening in Moodle. It is a really, really big annoyance, especially because I don't know if this feature is remarked in the documentation.  Not being a technical bug, it is a serious behavior bug that can lead to really undesirable situations between teachers and students. Isn't it?   """,Bug,Course
374006,"""I've been hours trying to find out why, under certain circumstances, a link like """"/file.php/263/onefile.htm"""" would return a blank page while """"/file.php/263/otherfile.htm"""" does not. Tracing took me through file.php->filelib.php (send_file function) -> filter_text function and finally to mediaplugin_filter function, where I found that preg_replace() was the one returning a blank $text.   The problem arose when upgraded the server to PHP5, so I suspect preg_replace() is returning an empty $text when the original $text is not well-formed UTF-8. This is very probable, since the files in moodledata not necessarily are in UTF-8 (this particular installation was a 1.4, then 1.5, and now 1.7.2).""",Bug,Language
374167,"""I want to intergate moodle in my school to the Yale CAS system which I modified the loging process using MySql,but I found that moodle has a CAS module upon LDAP, I  don't want to use LDAP, so can anyone help me to achieve this? I thought to modify the source code to achieve this, but I couldn't understand the logining  process in details, so any one can help me ? I apericate you very much!                                                                                                         a middle school student from china                                                                                                                    my E-mail :<EMAIL>",Improvement,Authentication
374203,"""Good afternoon, I am having a problem with the scorm module on Moodle, version 1,6+. The problem appears on the """"update scorm"""" form. When I try to modify the name of the scorm on the """"update scorm"""" form, a window appears stating that if I accept the changes, the tracking data will be lost. Yet, when I choose to not accept the changes, the tracking data is lost regardless.   I would very much appreciate someone who has already solved this bug to give me a hand in resolving this problem. Very much appreciated.  """,Bug,SCORM
374280,"""Following on from bug MDL-7861 """"Strict XHTML 1.0"""", I am building up a patch to fix stray ampersands, poor semantics, etc. in help HTML files (lang/en_utf8/help/*). This is what I'm tackling:  1 Well-formed XML     - Stray ampersands, regular expression /&[^a^l^g^q^n^#]/ (Not perfect, but a start). Specific searches &file, &bug, &<USER>  2 Semantics     - Each */index.html should start with a <h2> </h2>, except help/index.html which should start <h1>  </h1>     - Other headings, regular expression /<p><b>(.*?)</b></p>/, should be replaced variously with <h2>$1</h2>, <h3>$1</h3> as appropriate - a headache!  NOTES,  - I want to be pragmatic. - should we aim for Strict DTD for help? Would Transitional be sufficient?  - I'm ONLY fixing English language, en_utf8 - how can we systematically fix other languages.""",Bug,Accessibility
374330,"""I've been unable to verify if this problem persists in newer versions that 1.6.5 - but I should be able to soon :-)  If a course has a large (but not ridiculous) number of quiz attempts (order of 1000 for a quiz with about 25 questions) backup and/or restore is severely compromised. It seems that building the XML for those attempts burns up crazy amounts of memory (we went up to half a gig before giving up). A similar problem is exhibited trying to restore the file - this has been verified as a problem in 1.8 STABLE.  IMHO, backup and restore shouldn't ever use that amount of memory, although I appreciate the solution will be complex.""",Bug,Backup
374362,"""Change the existing capability check in myMoodle from moodle/site:manageblocks to moodle/my:manageblocks so that the users of myMoodle don't see the """"Turn editing on"""" button on the site home page when you give them this capability.  What I did was add this capability to the default role for all users so that anyone could see the editing blocks on the myMoodle page. As I <USER>above this had the unfortunate side affect of allowing them to see the """"Turn editing on"""" button on the site home page, even though if this is clicked on nothing happens.  So all I would like is to have a new capability and change the check, it's that simple :)""",New Feature,Roles / Access
374406,"""I work for a multinational organization that is looking at deploying Moodle as our Enterprise-wide LMS. But in order to do so we need the ability of being able to have groups created from additional attributes within LDAP. I have made some modifications and additions to pull these additional attributes from LDAP for the creation of the GROUPS. These GROUPS reside outside of the course. The structure of GROUPS will be as follows; Within each Region there are a number of countries. Within each Country there are Branchs (offices) Within each Branch are users. I was able to add within /moodle/auth/ldap.php the following additional containers to which data mapping to LDAP was enabled. Once I created these fields I then added two new records for storage of the attributes from LDAP. Once the data mapping was completed and I verified that it (<USER>user) populated to the table. I then added the labels to the moodle.php file within the English language module. I then modified the /admin/user.php file which once completed would be renamed groups.php. I removed the email element from the form. I then added the Region and Branch elements. The only item that I am missing is the data from <USER>user for the two attributes above. It is within the table but I will not display on the form for the users that have been updated. I would like to ask which file would I need to modify inorder to retrieve the data from the table to display it on the form???? I have posted several times within the forums but have not recieved any responses, I have had to pretty much figure this out on my own. It has been fun for me but now I am running into a snag and need to present the groups page to management. Any assistance would be greatly appreciated. Apart from working within this organization I am also an Adjunct Professor for a local college here and am in the process of recommending our move from eCollege to Moodle. I currently use it for my onground classes. Thank you all.""",Improvement,"Database SQL/XMLDB,Authentication"
374477,"""The backup (see attachment) created empty quizes despite the fact that there were about 120 questions and it even says so in the restore.  This is quite a problem for me since I rely on back up and restore of quizes.    In more detail.   I have three courses Monday, Wednesday and Thursday with lots of quizes. Normally I created the quizes in monday's course and then move them to Wed and then Fri.  There was a Monday bank holiday recently, so I created the categories and questions in Monday, making them """"public"""" as usual, since I wanted to keep the home of all the categories in one place.  I then created the quizes in Wednesday, backed them up and moved them to friday. All was fine.   Then this morning I backed up the quizes again in Wednesay to move to Monday. I first of all restored them in a dummy course because I wanted to move the week (the bank holiday meant that the Monday week is the 3rd not the 2nd). This works okay but takes AGES - about three minutes even though I am only restoring one week (8 quizes, 125 questions 50 categories). The questions are there.   Then I backup this dummy course and restore it to Monday.   The quizes are created but there are NO questions inside them.   Also when I try and add questions to the quizes thus created, I press the """"add a random question"""" button and nothing happens. No question is added.   Bear in mind that the categories and questions were created and remain extant in Monday and I guess that this is the cause of the error.   In other words, this bug is PERHAPS """"quizes using publically available questions can not be reimported into the quiz from which the questions and categories came. """"NOOOO!  Hold on! Looking inside the backup it seems that the questions are not even there only the quizes. So it is not surprising that that the questions did not appear.   I have no idea how I managed to backup quizes without their questions.... """,Bug,Backup
374514,"""Does it make sense to have a new class moodle_url in weblib.php to handle constructing a url. It would internally stores params as an array and have methods to manipulate the array. The class could output the params as hidden fields in a form or as a url with get params appended. This is functionality that is needed all over moodle and some code encapsulated in a class might help? Could be also passed to the constructor of formslib.  I'm updating the question code to get rid of the dependence on session vars to store params used in the page and I'd like to write a class like this and put it in weblib.php. Was thinking of moving methods like below out of listlib.php into a class that could then be used as a param for methods of the listlib class and elsewhere. Will also add a method to ouput the params as hidden form elements for buttons. And also to process a url string and break it down into an array of params and a url.  Also could we have a global var. $FULLMEOBJ which is $FULLME but as moodle_url object with params so you can use methods to output $ME or $FULLME and easily manipulate the params for the page to override some of them, add params, remove params etc.      /**      * Add an array of params to the params for this page.      *      * @param array $params      */     function add_page_params($params){         $this->pageparams = $params + $this->pageparams;     }      /**      * Get url and query string for an action on this page (get_url() + sesskey)      *      * @param array $overrideparams an array of params which override $this->pageparams      * @return string      */     function get_action_url($overrideparams = array()){         global $USER;          $arr = array();         $paramarray = $overrideparams + $this->pageparams + array('sesskey'=>$USER->sesskey);         foreach ($paramarray as $key => $val){            $arr[] = urlencode($key).""""="""".urlencode($val);         }         $params = implode($arr, """"&amp;"""");          return $this->pageurl.'?'.$params;     }      /**      * Get url and query string for this page      *      * @param array $overrideparams an array of params which override $this->pageparams      * @return string      */     function get_url($overrideparams = array()){          $arr = array();         $paramarray = $overrideparams + $this->pageparams;         foreach ($paramarray as $key => $val){            $arr[] = urlencode($key).""""="""".urlencode($val);         }         $params = implode($arr, """"&amp;"""");          return $this->pageurl.'?'.$params;     } """,New Feature,Libraries
374520,"""I've been getting Notices when addslashes_object encounters an object property that is an array. This happens with MNet sometimes: PHP Notice:  Array to string conversion in /var/www/cvshead.moodle.local.com/htdocs/lib/datalib.php on line 30  I tested the function with an object that had another object as a property, and got a fatal error.  I'll attach a patch for the function... I think it works exactly like the current version for all scalars AND for nulls and resources (which fail the is_scalar test). For objects and arrays it recursively adds slashes to all elements.""",Improvement,Libraries
374543,""" There's a problem related with Javascript and Internet Explorer 7 (and maybe 6) that affects Moodle 1.7.2 I'm responsible for several Moodle instalations and version 1.6.4 was ok. After I upgraded to 1.7.2 I received reports that """"opening files returns error when using IE"""". You can see the error in the attachment.  So, I did tests: - Firefox and Opera no problem at all - Haven't tested IE6 but got reports saying to have the same problem of IE7 (not sure though) - With IE7 I could successfully open JPG, GIF, XML files but couldn't DOC, XLS, PDF, ZIP (didn't try other types) - Still, for all file types I was able, with IE7, to right click and: """"open in new tab"""", """"open in new window"""" and """"save target"""" with complete success.  So, the problem was with onclick that calls javascript function openpopup and I found out from Internet search that there are some issues with IE/XP/Vista. The problem is with """"return false"""" and fixes can be tried with changing it to """"return true"""" or playing with """"href="""" and avoiding """"onclick"""".    I have chosen to """"return true"""". The other browsers still behave nicely and IE stops giving error but only displays the window with message """"you should receive the document in a new window, if it doesn't...."""". It doesn't, so people have to click the next link and it works.  I also tried to put before """"return false"""" the following line location.href=fullurl; but IE7 gives the download file and also the error :D.  Then putting """"return true"""" works fine on IE7 but the problem is that Firefox and Opera do double open: in new window and in current moodle window.....  so is there better solution?   Thanks    """,Improvement,Libraries
374547,"""During install, when creating the admin account, the user object has a null mnet_host_id (because $CFG->mnet_localhost_id is null). MySQL refuses to insert the user row into the user table, as mnet_host_id is a NOT NULL field. I've attached the file adminlib.php.patch. It initializes a mnet_environment object so that $CFG->mnet_localhost_id gets set.""",Bug,Database SQL/XMLDB
374582,"""Hi - the $CFG->restrictusers feature was removed from Moodle because the roles system was supposed to cover this. However it doesn't: there seems to be no way to prevent a user changing their password.  I'm filing this as a bug rather than a feature request because there may well be sites who were using the $CFG->restrictusers feature to provide """"demo"""" accounts (like moodle.org used to do), and who are presumably now unable to do this. A site that I'm working on needs to have a certain class of user unable to edit their password or other details, for example.  I'd guess that there should be a capability for can-edit-own-password, default on.  (Removing of $CFG->restrictusers was a tracker item but I can't seem to find it, sorry.)""",Bug,Roles / Access
374730,"""""""Login as"""" doesn't work.  I'm logged in as admin, then I go into a course, in participants I choose a student, click """"login as"""", get the following message: """"You are logged in as ...."""" When I click on """"continue"""" I get following message: """"Sorry, but you can not enter this course as ...""""""",Bug,Roles / Access
374786,"""I've defined one external db auth server. All the """"mapped"""" fields have been defined as """"locked"""". As admin I get:  - username & password are editable by admin. Trying to change username --> error, trying to change password --> does nothing. IMO, both fields should be non-editable ALWAYS. - the rest of mapped fields (firstname, conutry....) are editable, no matter if they have been defined as locked. IMO the """"locked"""" flag should have high priority than other flags (update external) or capabilities (doanything, editadvanced...) and such fields should be also non-editable if locked or not marked as """"update external db"""".""",Bug,Authentication
374977,"""There seems to be quite a few changes to formslib in 1.8 that were NOT ported to 1.7.  Has anyone got any objections to me copying the WHOLE 1.8 formslib back to 1.7?   I don't think anyone's really using it yet, and it would mean that any new 1.7 modules that use it would work better in 1.8.  Are there any problems with this that I'm not seeing?""",Bug,Forms Library
375208,"""If you make a comment on a database table that has a ' in you get errors, at least in Postgres but it may apply to other database types as well.  For example, I made the following change in my copy of lib/xmldb/generators/postgres7.class.php   from          $comment.= """" IS '"""" . substr($xmldb_table->getComment(), 0, 250) . """"'"""";  to          $comment.= """" IS '"""" . addslashes(substr($xmldb_table->getComment(), 0, 250)) . """"'"""";  This fixed it but I'm not sure it's the right approach, also it probably affects other databases most of which I can't test on, so I think somebody who has a clue how that code works should fix it, not me.   For now I have changed my comment to not include a ' symbol. I think this solution may be ok for 1.8. :)  """,Bug,Database SQL/XMLDB
375233,"""I would like to be able to disable a """"text"""" field depending on the value of an associated """"select"""" field.  Here is some sample code to use for testing.       - locate the following line """"<USER>chat/<USER>form.php"""" (around line 48)         $this->standard_coursemodule_elements();      - insert the following code just before the above line         $mform->addElement('header', 'testsection', 'Test Section');         $HOTPOT_FEEDBACK = array (             0 => get_string(""""feedbacknone"""", """"hotpot""""),             1 => get_string(""""feedbackwebpage"""",  """"hotpot""""),             2 => get_string(""""feedbackformmail"""", """"hotpot""""),             3 => get_string(""""feedbackmoodleforum"""", """"hotpot""""),             4 => get_string(""""feedbackmoodlemessaging"""", """"hotpot"""")         );         $mform->addElement('select', 'studentfeedback', 'Student Feedback', $HOTPOT_FEEDBACK);         $mform->addElement('text', 'studentfeedbackurl', 'Student Feedback URL');         $mform->disabledIf('studentfeedbackurl', 'studentfeedback', 'in', '0,3,4');      - locate the following lines in """"lib/javascript-static.js"""" (around line 99)                   case 'noitemselected':                       lock = master.selectedIndex==-1; break;      - insert the following code just after the above lines                   case 'in':                       lock = false;                       var values = value.split(',');                       for (var i in values) {                          if (values[i]==master.options[master.selectedIndex].value) {                             lock = true;                          }                       }                       break;      - login to Moodle as a teacher/admin     - add a """"Chat"""" activity in the course and view the fields on the """"Adding a Chat"""" page        (you may need to """"force-refresh"""" the page, Ctrl+F5, to get the new javascript file to load)  This allows the text field to be used to take extra information for certain values of the select field, and I believe it would prove useful in other situation beyond the example above which is taken from the HotPot module.  thanks in advance for considering this option <USER>",Improvement,Forms Library
375260,"""I wanted certain teachers to be able to see hidden courses in hidden categories.  I tried two methods using roles.  The two methods should have yielded equivalent results, but only one worked.  Method 1: modify the legacy role Teacher (this works)       login as admin      Site administration -> Users -> Permissions -> Define roles      edit the Teacher role           allow """"See hidden categories""""            allow """"View hidden courses""""      any Teacher who now logs in can see hidden categories and courses  Method 2: create a new role that inherits from Teacher (this does not work)       login as admin      create a new role CanSee inheriting from legacy role Teacher (editing)           allow CanSee to see hidden categories           allow CanSee to see hidden courses      in site context, assign user X to role CanSee      login as user X -- cannot see hidden categories and courses""",Bug,Roles / Access
375270,"""When importing a course to a new course, the """"Summary"""" part of each week does not get imported into the new course.  I've include an image with a screenshot of the """"source"""" course on the left and the """"destination"""" course on the right.   == Note to others who find this to be a problem == The work around for this is to not use the """"summary"""" but include the """"summary"""" part as a label at the top of each lesson.""",Sub-task,Course
375419,"""Hi jamie, think this is one for you - hopefully i'll have a patch for you to review! (And, sorry i've left it so late to report this - I was aware of changes to this, and there are now problems...)  There is a image of a red asterisk in the <label> against required form fields, but it is hidden with:   """" .req { display:none } """"  Issue 1 is that display:none means """"expunge me from the DOM tree"""" and for some browser/ screen-reader combinations this means don't speak me. The work around is illustrated by the Standard theme class 'accesshide':  """" .accesshide { position:absolute; top: -1000px }  """"  So, that's one way to fix it _BUT_ issue 2 is that the only visible cue is dark red text in a <label> for a required field - if you're colour blind this may appear black, if you override with high-contrast settings...etc. - basic accessibility says do not convey information with colour alone.  So I propose we show the red asterisk icon, perhaps add a title and help cursor, and keep the red text if you wish. Cheers <USER>""",Bug,"Accessibility,Forms Library"
375522,"""We're in the process of testing out the upgrade to 1.7.1. After doing the upgrade from 1.7.0, I'm seeing a new error in debugging mode when I log in as a student who has more than one role (specifically the """"student"""" role and a custom """"student assistant"""" role we created.   Here's the SQL error:  --------------- Error: Turn off debugging to hide this error.  SELECT * FROM <USER>role_assignments WHERE contextid = '153' AND userid = '35'(with limits 0, 100)  Found more than one record in get_record_sql !  Array ( [421] => Array ( [roleid] => 7 [contextid] => 153 [userid] => 35 [hidden] => 0 [timestart] => 0 [timeend] => 0 [timemodified] => 1169486964 [modifierid] => 4 [enrol] => manual [sortorder] => 0 )  [422] => Array ( [roleid] => 5 [contextid] => 153 [userid] => 35 [hidden] => 0 [timestart] => 0 [timeend] => 0 [timemodified] => 1169487058 [modifierid] => 35 [enrol] => manual [sortorder] => 0 ) )  ---------------  So far, it does not seem like its having any adverse effect other than generating the error code, but I'm willing to bet that's just a happy accident based on how the fact that the """"student assistant"""" role is the first result returned by the query (roleid #7 is the student assistant; roleid #5 is the default student one). Had their roles been reversed, I'm guessing the student wouldn't have the right permissions in the course.  Is anyone else seeing this error?  """,Bug,"Database SQL/XMLDB,Authentication"
375558,""" Using the Latest CVS checkout (2/16/2007 - 12PM EST) as a clean install  Data Mappings containing Capital letters in the field names fail but field names containing only lowercase letters work correctly  Steps to reproduce the problem:  Configure multi-authentication to use an LDAP server first then manual accounts only Configure LDAP to authenticate against a MS Active Directory 2003 server Use the following Data mapping fields:  First Name ->givenName Surname ->sn Email ->userPrincipalName Description ->description Department ->company  Try to log in using an LDAP user   The Surname, description and department fields are correctly populated with data The Firstname and Email Address fields are left blank.  I've verified my LDAP settings, they work on both my Moodle 1.6 and 1.7 test installations """,Bug,Authentication
375625,"""When using db authentication, by default Moodle connects under iso-8859-1 charset. This causes that retrieved data from external DB is iso-8859-1. And such data (fistname, lastname...) is discarded on insert if Moodle is running under UTF-8.  So the actual situation is that DB authentication with non-ASCII characters is only working if Moodle is running in NON-UNICODE mode (then such retrieved data is perfectly inserted).  With more and more Moodle sites running under Unicode, this is really an ugly problem. Luckly it has a SIMPLE solution:  1) Always stabilise the authdb channel to be UTF-8. This will cause retrieved data to arrive always in UTF-8 (no matter of the authdb real encoding, both drivers will perform the needed conversions automatically). (That's the attached patch). 2) Some lines below, data will be converted back to ISO-8859-1 for Moodle running in NON-UNICODE mode (this part of code currently exists).  Note that this will fix the problem under MySQL and PostgreSQL (where we can stabilise the communication channel). MSSQL and Oracle auth-dbs are required to run under UTF-8.  Please, take a look to the simple patch. I've tested it both against iso-8859-1 and utf-8 autdbs and under iso-8859-1 and utf-8 moodle dbs. Seems to support all the combinations.  Ciao :-)""",Bug,Authentication
375700,"""I'm site admin. Then I've created one course and I've assigned myself as admin in that course. Now, I'm trying to undo such assignment from the course roles page and I'm not allowed to do so. I press the """"right arrow"""" button but admin never disappears from the left list.  Ciao :-)""",Bug,Roles / Access
375904,"""I was playing with one old module (certificate) that works perfectly under Moodle 1.6. Then, I've tried to perform one backup from one course containing one certificate.  I was surprised when no certificate was available to restore. Looking at code it seems that the """"individual selection of activities"""" developed for 1.6 supported to """"fall back"""" to all activities if the module didn't implemented the xxx_backup_one_mod() function (these was discussed in order to allow 3rd part modules to continue working without problems).  After looking to code, I've found that, in backuplib.php in the backup_fetch_prefs_from_request() function, the line 1987 (16_STABLE):  if (!count_records('course_modules','course',$course->id,'module',$<USER>>id) || !function_exists($modbackupone))   prevents activities without the """"one"""" function to be registered for backup.  The question is what to do:  1) Try to fix that, in order to allow old modules not implementing the """"one"""" function to work (from 1.6 to 1.8). I imagine this will affect backup in other steps and perhaps restore too. 2) Ignore it, assuming that only modules implementing the """"one"""" function will work (current approach). This will allow us to delete a bunch of code that is currently in backuplib and modules in order to support """"old"""" backup.  Just to think a bit about it... it's important but it seems that it hasn't caused too much problems in the past...""",Sub-task,Backup
375957,"""From .../lib/xmldb/classes/generators/oci8po/oci9po.class.php:      // To define the default to set for NOT NULLs CHARs without default (null=do nothing)     // Using this whitespace here because Oracle doesn't distinguish empty and null! :-(  This means courses get a default value for the (interactive) enrolment plugin that is a white space, and _not_ and empty string, when we use the 'Site Default' option. Later, in .../enrol/enrol.class.php we do:      function factory($enrol = '') {         global $CFG;         if (!$enrol) {             $enrol = $CFG->enrol;         }  So we test whether $enrol is empty or not, to use the default interactive enrolment plugin. But this breaks with Oracle, as $enrol is not empty  but has a single white space character. So we end up using an enrolment plugin called ' ', which doesn't exist and generates a run-time error when we do the require_once() a few lines below.  I'd have a cursory look, and I'd say there are other places where such assuption is made (emtpy field === default value), which totally breaks when using Oracle.  (I'd like to thank Mikel Kortabarria, our local Oracle guru, for tracking down this bug :-).  Saludos. Iñaki.""",Bug,Database SQL/XMLDB
375996,"""We created a new moodle installation and added two users and assigned them course creator permissions to the main site. First course creator user logs in and creates a course. Upon creation, we allowed enrollment and enrolled the creator into the course.   Issue 1 - Course Creator enrolled as a student  Issue 2 - Next we went to the main page and the course was listed with two course creators. In fact if we added another course creator to the main site, he would be automatically listed in each and every course.  Issue 3 - Next course creator logs out and the next course creator logs in. He cannot see a course creation button.  Issue 4 - We forced a course creation by using url and it worked, however this time we disabled enrollment. Guess what the course was created but the course creator was locked out of his own course. Only an administrator could assing the user to the course.  Issue 5 - The course created in Issue 4 was still marked with all the course creators assigned to the site.  I've watched the thread on this and it shows that this might have been solved many times, but the problem still exists in the latest download today on 6th January. :(  Anyone with resolutions, please help.  <USER>",Bug,"Admin,Roles / Access"
376179,"""In our environment backing up empty courses halted for quite a while on the """"calculating items to backup"""" page, so I investigated:  Currently the function user_files_check_backup() gets the list of user directories and then queries the backup_ids table to see if this user is needed in the backup. In our environment this generates _thousands_ of database queries even if the course is empty or has just a couple of users in it.  In the attached patch I'm querying all of the users that are needed in the backup with one query to a PHP array, then instead of going to the database for _every user_ that has a profile picture, I'm just checking against that array. Instead of ~3000 queries I get just one.  Also, I didn't see a need for the strtok() as the directories are the userids already... Or is there a """"/"""" there in some filesystems? I tested this both in Windows and Linux, no problems.  The function in question seems to be unchanged in 1.7 so this patch might apply there too. The users are fetched from the backup_ids table and not from the user_students and user_teachers tables.""",Sub-task,Backup
376181,"""The block_instance table has a field called pagetype set to varchar(20).  While developing my own module, I've run into a truncation error, as my module is called """"lifecyclemgr"""". This name is too large to fit into 20 chars when you add a page identifier.  Perhaps this could be increased to varchar(30) or varchar(50)?""",Bug,Database SQL/XMLDB
376458,"""The accesslib.php uses get_record() a lot, and often it only fetches a single field of that record. For example:                 $group = get_record('groups','id',$context->instanceid);                 $courseinstance = get_context_instance(CONTEXT_COURSE, $group->courseid); could be                 $courseid = get_field('groups','courseid','id',$context->instanceid);                 $courseinstance = get_context_instance(CONTEXT_COURSE, $courseid);  In the attached patch I've replaced the appropriate get_record() calls with corresponding get_field() calls. In some places I've left the get_record() there, but have limited the columns to those that are actually read anywhere.  Another, smaller, optimization in this patch is that I added a parameter to create_context(): $allowexisting (defaults to false). This means that with $allowexisting=true we don't want an error message for a possibly existing context, just return the freshly/previously created context. This saves one database query per get_context_instance() (those that are not in the context cache).  My test box with 42,000 enrolments ran the moodle_install_roles() over 66% faster, from 278 minutes to 91 minutes, so there are some real savings here! I'd think that all of the capability queries will benefit from these optimizations also, meaning a bit faster Moodle in overall.  Ps. There wasn't any options between Minor and Major, so I marked this as a Major improvement, but this (being Major) applies to the moodle_install_roles() mostly...""",Improvement,Roles / Access
376541,"""1. Set up a moodle site in which the default role has moodle/legacy:guest.  2. Create a course that doesn't allow guest access.  3. Try to create a role which has access to view all courses by giving it moodle/course:view.   4. Create a new user and assign it this role at site level.  5. Log in as that user and try to view the course. You get asked to enrol in the course (if enrolment is allowed, otherwise it fails with a message).  The expected result was that, since moodle/course:view was explicitly allowed, this user should be able to view all courses (whether or not they allow guest access) without having to enrol.   The reason this happens is as follows:  1. Remember that users can have multiple roles.  2. All users have the 'default user role' at site level (this is correct - it's so that e.g. making somebody a student on course X doesn't somehow reduce their permissions on course Y below what they would have if they weren't assigned to any course at all).  3. There is this code in accesslib, load_defaultuser_role:           // SPECIAL EXCEPTION:  If the default user role is actually a guest role, then         // remove some capabilities so this user doesn't get confused with a REAL guest         if (isset($USER->capabilities[$sitecontext->id]['moodle/legacy:guest']) and $USER->username != 'guest') {             unset($USER->capabilities[$sitecontext->id]['moodle/legacy:guest']);             unset($USER->capabilities[$sitecontext->id]['moodle/course:view']);  // No access to courses by default         }  I agree with unsetting legacy:guest (mostly) but why are we unsetting course:view? 'No access to courses by default' as the comment says, sure, but that will apply anyway unless somebody has intentionally added moodle/course:view to the role.   The unset applies to capabilities set by the user's 'real' role as well as those included in the default one. I would suggest moving a version of this code into the for loop above so that it only applies to the default role and leaves all capabilities set by the 'real' role alone, but probably still worth removing the bit about course:view as I don't see what that has to do with anything here.   It could be there is some complicated reason for this though so I didn't want to change anything personally, but here's a patch that seems to work (tested in the above situation to (a) check that my <USER>can now get into the site, (b) check that clicking the 'login as guest' button still doesn't get you into the site; not tested beyond that):  Index: lib/accesslib.php =================================================================== RCS file: /cvsroot/moodle/moodle/lib/accesslib.php,v retrieving revision 1.190 diff -u -r1.190 accesslib.php --- lib/accesslib.php 12 Nov 2006 08:55:13 -0000 1.190 +++ lib/accesslib.php 13 Nov 2006 12:45:29 -0000 @@ -130,17 +130,14 @@      if ($capabilities = get_records_select('role_capabilities',                                       """"roleid = $CFG->defaultuserroleid AND contextid = $sitecontext->id AND permission <> 0"""")) {          foreach ($capabilities as $capability) { -            if (!isset($USER->capabilities[$sitecontext->id][$capability->capability])) {  // Don't overwrite +            // Don't overwrite capabilities from real role... +            if (!isset($USER->capabilities[$sitecontext->id][$capability->capability])  +                // ...and if the default role is a guest role, then don't copy legacy:guest, +                // otherwise this user could get confused with a REAL guest +                && ($capability->capability!='moodle/legacy:guest' || $USER->username=='guest')) {                   $USER->capabilities[$sitecontext->id][$capability->capability] = $capability->permission;              }          } - -        // SPECIAL EXCEPTION:  If the default user role is actually a guest role, then -        // remove some capabilities so this user doesn't get confused with a REAL guest -        if (isset($USER->capabilities[$sitecontext->id]['moodle/legacy:guest']) and $USER->username != 'guest') { -            unset($USER->capabilities[$sitecontext->id]['moodle/legacy:guest']); -            unset($USER->capabilities[$sitecontext->id]['moodle/course:view']);  // No access to courses by default -        }      }        return true;   """,Bug,Roles / Access
376564,"""These ideas are related so I'll mention them together.  # 1 ==== It would be nice to have the current lesson at the top of the page, with the previous lesson following it and then the lesson previous to that.  In other words, add the option of being able to reverse the lesson order, having the oldest (first lesson) at the bottom and the newest (highest number lesson) at the top.  (Teachers could manually hide upcoming lessons.)  # 2 ==== If you add the above suggested improvement, then this is an additional improvement that could be made.  In """"weekly mode"""", have Moodle automatically hide upcoming lessons until the date indicated. This would mean that the current lesson would always be displayed at the top and that teachers wouldn't need to manually hide upcoming lessons.  #3 ==== If you add the above suggested improvement, then this could be done, too.  In """"weekly mode"""", Moodle could also automatically hide older lessons after a given time set by the teacher.  Some teachers might want to allow students to only see last week's lesson, other teachers might want to allow more lessons to be shown.   NOTE: I have noticed that some of our teachers are doing something similar to this using """"topics mode"""", but are manually moving everything each week, which is quite time-consuming.""",Improvement,Course
377071,"""I've found two issues with the sitewide student role (defaultuserroleid = Student).  The first is that when you browse the site with guest access (ie. $USER=>username='guest') you can post in forums.  To get round this you can add specific guests for username=='guest' with each has_capability test but there may be a more elegant solution you want to consider.  The second is that as a registered user you never get any course enrolment process - because the user inherits course:view from the default role, and so has access to every course without going through, for example, the """"do you want to enrol"""" screen for manual enrolments.  Now that might be what you intended, but I really want the manual auth to work as well, and its taking quite a bit of hacking at the moment to get it back!!  I'd be happy to share the hacks if you want - but they're definitely not a sensible way forward.""",Task,Roles / Access
377216,"""Currently all the DDL functions are normal functions, but require an object as a parameter. Some of the time this is not as elegant as it could be.  $table = new XMLDBTable('user_students'); drop_table($table);  I filed this bug because I was starting to use them and wasn't expecting to  have to create objects for things that are defined by one name,   eg I would prefer:      drop_table('user_students');  because it makes the code in upgrade.php (which is where developers will mostly be using DDL) look so much sweeter .  :-  Can we add a little hack to each function to allow $table to optioanlly  be a string instead, and if it is then the object is created automatically?  if (is_string($table)) {     $table = new XMLDBTable($table); }""",Improvement,Admin
377238,"""I'm trying to get the stats to work on a 1.6.1+ install (also 1.6.2). It seems to run away and never return. I set the stats settings so that it only went back a week, but it still never seems to end.  Info: 1. Site has about 125 courses. 2. 800 users. 3. Stats settings are set to look one week back and run no longer than an hour. 4. After running for more than an hour, the 'stats_monthly' table has 2,344,571 records in it (it started empty), 'stats_daily', 'stats_user_daily' and 'stats_user_monthly' are empty, 'stats_user_weekly' has 195 records, and 'stats_weekly' has 136 records. 5. If I access the stats functions, I get the message """"There is no available data to display, sorry"""".  After running some tests we discovered that the run ends up in an infinite loop in the 'stats_cron_monthly' function. In particular, the iterator of the loop, function stats_get_next_monthend, never increments the value. It stays the same.  The function stats_get_next_monthend is doing some funky stuff with timestamps. The code reads:  function stats_get_next_monthend($lastmonth) {     return stats_getmidnight(strtotime(date('1-M-Y',$lastmonth).' +1 month')); } """,Bug,Admin
377403,"""I'm not entirely sure whether this is related to roles, but it might be. Here's how to reproduce:  Logged into a site as Admin, and looking at user/index.php for a particular course.  I see a list """"my courses""""  I select a different course from the one I am in from the list, but it just reverts back to the current course.  """,Bug,Roles / Access
377405,"""The themedir configuration option seems to be quite broken from a few perspectives.  In: lib/moodlelib.php, functio  get_list_of_themes():  if (!file_exists(""""$CFG->dirroot/theme/$theme/config.php"""")) {   // bad folder  Should be: if (!file_exists(""""$CFG->themedir/theme/$theme/config.php"""")) {   // bad folder  same goes for:  include(""""$CFG->dirroot/theme/$theme/config.php""""); to: include(""""$CFG->themedir/$theme/config.php"""");  In order to use theme/index.php to select a theme from this directory, one would have to change the get_list_of_plugins_line.  As a trivial example in theme/index.php, i'd need to change:  $themes = get_list_of_plugins(""""theme"""");  to: $themes = get_list_of_plugins(""""local_theme_dir"""");  Where local_theme_dir would be under my dirroot. This is what i've found skimming the surface.  (Configurable theme directory would be quite powerful for me)""",Bug,Libraries
377414,"""If a user (authenticated against our Novell eDirectory LDAP server) with an apostrophe in their surname (e.g. O'Connor) attempts to update their own profile they get the error e.g. 'Could not update the user record (8481)'. (With PHP error reporting on: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'Connor'...etc.)  I get the above if I'm logged in as the user, but when logged in as an admin there's no problem updating the profile.  Also, if I escape the apostrophe with a backslash the user is then able to update their profile *once only* (although the backslash is displayed in certain places as in bug report MDL-6266) but then the surname reverts to its original unescaped form and the problem is back.""",Bug,Authentication
377458,"""I would like to suggest that the new code to amend and log dates that recently has been added to the """"restorelib.php"""" scripts for all Moodle modules be moved to a separate function.  This would have several advantages:     - you can check the validity of the date fields. For example, it makes no sense to increment the date field if it is empty     - improve the readability of the """"restorelib.php"""" scripts, which have become much harder to read-at-a-glance due to the plethora of recenlty inserted """"fwrite"""" statements.      - allow the message to be printed in the admin's own language      - the Moodle core programmers can control and therefore standardize how the $restorelog is accessed and how the messages in it are formatted  I would like to propose a new function for the restore library. It needs to be called only once for each record, before the record is restored. When passed a list of datefields and the XML tree, it would modify the date fields in the XML tree and write to the restore log.   For example to modify the dates in Assignment records:     backup_dates('Assignment', $restore, $info['MOD']['#'], array('TIMEDUE', 'TIMEAVAILABLE'));  Or for Hotpot records:     backup_dates(         'Hotpot', $restore, $xml, array('TIMEOPEN', 'TIMECLOSE', 'TIMECREATED', 'TIMEMODIFIED')     );  Or for workshop records:     backup_dates(         'Workshop', $restore, $info['MOD']['#'],          array('SUBMISSIONSTART', 'ASSESSMENTSTART', 'SUBMISSIONEND', 'ASSESSMENTEND', 'RELEASEGRADES')     );  Or for workshop submissions records :     backup_dates('Workshop Submission', $restore, $sub_info['#'], array('TIMECREATED'));  The function requires the following strings in lang/en_utf8/moodle.php     $string['backupdaterecordtype'] = """"<br>$a->recordtype - $a->recordname<br>\n"""";     $string['backupdateold'] = """"$a->TAG was $a->weekday, $a->mday $a->month $a->year\n"""";     $string['backupdatenew'] = """" &nbsp; $msg->TAG is now $a->weekday, $a->mday $a->month $a->year<br>\n"""";   A  candidate for the new function appears below.  ============================================== function restore_dates($recordtype, &$restore, &$xml, $TAGS, $NAMETAG='NAME') {     global $CFG;      // if necessary, adjust date/time fields and write to restorelog     if (!empty($restore->course_startdateoffset)) {          // loop through time fields         $openlog = false;         foreach ($TAGS as $TAG) {              // check $TAG has a sensible value             if (!empty($xml[$TAG][0]['#']) && is_string($xml[$TAG][0]['#']) && is_numeric($xml[$TAG][0]['#'])) {                  if ($openlog) {                     $openlog = false; // so we don't come through here again                      // check course_directory exists                     $course_dir = """"$CFG->dataroot/$restore->course_id"""";                     check_dir_exists($course_dir, true);                      // open $restorelog                     $restorelog = fopen(""""$course_dir/restorelog.html"""", """"a"""");                      // start output for this record                     $msg = new stdClass();                     $msg->recordtype = $recordtype;                     $msg->recordname = $xml[$NAMETAG][0]['#'];                     fwrite ($restorelog, get_string(""""backupdaterecordtype"""", """"moodle"""", $msg));                 }                  // write old date to $restorelog                 $value = $xml[$TAG][0]['#'];                 $date = usergetdate($value);                  $msg = new stdClass();                 $msg->TAG     = $TAG;                 $msg->weekday = $date['weekday'];                 $msg->mday    = $date['mday'];                 $msg->month   = $date['month'];                 $msg->year    = $date['year'];                 fwrite ($restorelog, get_string(""""backupdateold"""", """"moodle"""", $msg));                  // write new date to $restorelog                 $value += $restore->course_startdateoffset;                 $date = usergetdate($value);                  $msg = new stdClass();                 $msg->TAG     = $TAG;                 $msg->weekday = $date['weekday'];                 $msg->mday    = $date['mday'];                 $msg->month   = $date['month'];                 $msg->year    = $date['year'];                 fwrite ($restorelog, get_string(""""backupdatenew"""", """"moodle"""", $msg));                  // update $value in $xml tree                 $xml[$TAG][0]['#'] = $value;             }         }     } } ============================================== """,Improvement,Backup
377633,"""When I am assigning sitewide roles (from the admin screen I want to assign a new site administrator), or when I'm assigning a teacher for a course, the first thing I'm shown is a dropdown box with the text """"Current role:"""" This is extremely obscure - as a user, I think, """"Does this mean my personal role? The role that the other user already has? Or the role I want to assign to the other user?""""  Strongly suggest changing it to something like """"Role to assign:"""" which I think will make more sense.""",Improvement,Roles / Access
377741,"""I am using Moodle 1.6.1+ (2006050512) on WIMP.    We have been attempting to lock the last name, first name, and email fields by setting them to locked in the authentication screen.    When logging in as a student, the fields are clearly not locked.      This feature was working in 1.5.3+, which was a LAMP server with manual authentication.    I am now using IMAP authentication on the WIMP server, but I wouldn't think that would make any difference.    Thank you.""",Bug,Authentication
377789,"""I already logged into moodle.org as a guest, I can't see a log out button, and the password recovery for my real account doesn't work since I'm logged in already :(""",Bug,Authentication
377948,"""If, as a course creator, I backup a course and restore it on to an existing course, deleting it first, my role as 'teacher' is lost by the end of the restore and an error is thrown.  When I re-login the course is restored on to the other course as requested, and only problem is that I am no longer teacher of the course.    As an aside I would prefer if teachers could do this without being course creators as I do not wish them to create new courses, but I do with them to be able to copy their courses on to their next year occurrence! I hope this will be possible in Moodle1.7 with the new focus on roles.""",Sub-task,Backup
378061,"""You can reproduce this problem only if you are using a custom language that has a parent language different from English in a non standard module (such as feedback activity module).    Let's make an example:  I'm using it_elis. Its parent language is it_utf8.  I install feedback activity module (non standard) so that its language files are under moodle/<USER>feedback/lang. I install en_utf8 and it_utf8 languages for this activity module.    When Moodle want to resolve a string in feedback.php language file, it try in some locations: moodle/lang, moodle/<USER>feedback/lang, moodledate/lang, ...    First of all it try for it_elis, it'll find this folder only under moodle/lang, but here there will be not feedback.php (because it's only in a it_utf8 or en_utf8 folder).    So it try for a parent language. For each location it look for an it_elis folder in which is defined a parent language and after it scan for feedback.php in that parent language folder. It try in moodle/lang location where it find an it_elis folder, but its parent language folder doesn't contain feedback.php. So it try in moodle/<USER>feedback/lang location where there isn't it_elis folder, so nothing happend.    After all Moodle try for en_utf8 and it find feedback.php in moodle/<USER>feedback/lang location.    I know that in Moodle 1.6 there is it_utf8_local solution, but I think that is important solving this bug for backward compatibility.""",Bug,Language
378263,"""When I am logged in as Admin or Teacher, I quite often log in as student (double log-in). This was very useful in 1.5, not so useful in 1.6 because of the Student View facility.  However, in 1.5 when I was Admin logged in as Student, I could click on the [User Admin] link in the headermenu/logininfo section of a course, and I was restored as full Admin user.    In 1.6 when I do this I get an Course ID was incorrect error message and I'm sent directly to the Moodle site main page... Why is that?    Joseph""",Bug,Authentication
378309,"""I created a link resource on my main course page (it's actually hidden in Chardelle's resource block). The link however links to a file inside a particular course. Even if I am not logged in, neither as a regular user or even a guest, it still displays that file in the link (in this case a video file). Shouldn't files inside courses only be accessible when one is logged into that course, either as a user or using guest access? I've placed a link to the URL of the page in question so you can see what I am talking about.""",Bug,Authentication
378403,"""The MyMoodle / My courses redirect fails after a bad login attempt    Steps to recreate the problem on Moodle 1.6 beta 5 (I've also been able to recreate this in moodle 1.5.3+)    1. Visit the Moodle login page  2. try to log in as admin with a BAD password  3. You should get an error Invalid login, please try again  4. log in again as admin this time with the correct password  5. You should see something like the enclosed screenshot    Environment:  Windows Server 2003  PHP 5.0.5  MySQL 4.1.16""",Bug,Authentication
378495,"""I'm not sure what the developer preference would be. I prefer that the admin user have full rights to do and change everything within Moodle.     I do not want my users to be able to change their user image so I utilized the CFG->disableuserimages=true option in the config.php; however, as admin I want to be able to go in and change images without opening this up to the entire site. So I changed the /user/edit.html around line 278 from:        if (!empty($CFG->gdversion) and $maxbytes and empty($CFG->disableuserimages)) {    to:         if (!empty($CFG->gdversion) and $maxbytes and (empty($CFG->disableuserimages) // isadmin())) {    to slightly change the logic to allow for the admin to be able to change the images. This is the behavior that I would see as optimal but I'm not sure how the developers feel. I am trying to minimize my site's customizations. This minor change would help in that if it is consistent with the intended logic of the developers.    I am filing this under administration; however, I wonder if there should not be a user component since it is in its own separate directory. Thanks for your consideration of this feature request.""",New Feature,Admin
378778,"""One of the problems I've had with large courses is that they can become very long to scroll down. The focus on topic button helps, but is it possible to present this as a series of tabs instead?""",New Feature,Course
378959,"""In hidden sections (using the hide eye icon in the top right corner) each resource/activity still has a 'show' icon that appears to do nothing.    If a check is added for this, note that you can still create visible resources due to another possible bug I'm about to enter, so it might be wise to leave the ability to hide visible resources in hidden sections.""",Bug,Course
378985,"""Using the standard white theme, the > search submit button is very big. I'm using Firefox 1.0.7 in the screenshot which I'll attach.    In IE 6, the button text is very large, but with the added text Search.    Also, notice how the breadcrumb trail is oddly misaligned (it's lower down than it should be). In IE6 the breadcrumb trail displays as a bullet list. I assume this is being fixed in the CSS changes.""",Bug,Accessibility
379183,"""I just wanted to make a note here of something weird that happened to me and that could be a potential problem (I have no idea).  I had created a new block and called it (unfortunately) course_section.  I went to the admin/blocks page and deleted this block, then clicked on the Administration link in the breadcrumbs and my installation begain installing moodle again (of course there were errors since everthing was already installed). Then, when I went to my courses, everything was gone from the course sections--the stuff is still there somewhere, for example, you can go to the activities/resources from the links in the log tables, but it does not show up on the course homepage or the index pages.  (Luckily this was a test site).  After looking at my db, the only thing I could find was that the the last two fields in the course_sections table were gone (available and deadline).       Is it possible that because this block had a file called block_course_section.php, that when it was deleted it also deleted fields from the course_sections table?  If so, then I'm wondering if this is a potential problem for other mods or blocks, or was this just a fluke?""",Bug,Admin
379397,"""Hello,     I've made this change for my site, but think it would be good to have as a standard moodle feature.     When a user chooses a social format for his/her course, the only way to add an assignment or delete an assignment (AFAIK) is to enable the social activities block and turn on editing.     Instead, why not just use the Assignments link in the normal Activities block to get to the assignment index page, and have the ability to edit, add, or delete the assignments from there? I've placed the delete.gif image next to each assignment name in the assignment listing, and added an Add New Assignment Link to the bottom of the page after the table printout. My user seems pleased with that, and it enabled him to completely do away with the Social Activities block, since he's only running a very simple course (though it has about 200 users).""",New Feature,Course
379641,"""I've activated the option mymoodleredirect  and when I access as a normal user(student) an press teh button Edit this page I obtain a warning like this:    Warning: array_merge() [function.array-merge]: Argument #1 is not an array in C:\AppServ\www\moodle\lib\pagelib.php on line 244""",Bug,Course
380002,"""I'd like to see the transmission of ALL the users in the default Assign-Creators page curtailed unless requested via clicking on show all.  I have made a minimal change that seems to work well for my case of 6000+ users which will grow to 16000+ when all courses are moved from Blackboard.  What I did was to print Too many users to show in the right-hand box, when the list count was greater than MAX_USERS_PER_PAGE. When I issue a search, the matching set must follow the same rule.  Thus I am able to access and admin Moodle via a slow dialup, and not have to wait while thousands of user names are sent to my browser!  The previous behaviour is restored by clicking on Show All.  How about something like this in a future Moodle?  As an alternative, I suppose a pagination scheme could be used, but often I just want to do a search on a relatively unique ID, and make the matching user a creator.""",New Feature,Admin
380267,"""I've got a user whose email account is no loger valid (he seems to have graduated and lost his college account). He is subscribed to a number of forums and every message sent to him generates a message to me that his account doesn't exist anymore.Just as there is a prompt user to change password checkbox, it would be nice if there was a checkbox that could remind a user to change their email address upon login.""",Improvement,"Authentication,Usability"
380333,"""The CSS in the standard theme has many !important declarations, particularly for Calendar. The !important declaration means that CSS further down the cascade (e.g. themes based on the standard theme, user style sheets specified in browsers) can't override these declarations unless they themselves are also declared !important.    I'm guessing there was a reason to add these in the first place (maybe to override styles encoded in the HTML?). If that reason is still valid then that original problem should be fixed, if it is already fixed and the !important declarations are uneccesary then I'd prefer if they were taken out.""",Bug,Themes
380372,"""the Formal_white theme has a bad character somewhere - causes images to have a carriage return at the top of the file! - took me a while to figure out where the problem was! - I'll keep looking & try to figure out which file.....""",Bug,Themes
380993,"""Having used moodle now for about 15 classes, i've created a fair number of activities, tests, etc... When doing these things you get a sense of the small nuisances of repetative tasks. I'd put forth that whereever there is a drop-down (aka pull-down) menu with 3 or less items it be put out as radio buttons instead. For example, on a page like the forums, one might need to create 15 (one for each week) and all need the option each person posts one discussion. As a drop-down it take several actions to select a choice. As a radion button it's move and click. Tabs tend to be cumbersome because each help icon is tabbed, as is each toolbar item... so doing setups without the mouse is tough. As such a change like this would reduce the numbers of steps in setting up certain activities significantly.  Let me know if a mockup up of a page would be useful (like the activities page) to illustrate this.  d.i.""",Improvement,Usability
381113,"""We are having a groups problem that I cannot pin down.  Here goes, email me if this doesn't make sense:    I tried configuring groups in Charlie&#8217;s Test Course.  I created 3 groups and was able to add users to the groups, able to &#8220;get info&#8221; on them by clicking the button, click on the group and see the right side list update on the fly, etc.    So then I tried going to the  &#8220;8th Grade Test&#8221; course.  I get into groups and click on a group name -- but no refresh on the right column to show me the users in that group.  I click on the &#8220;Get info on group&#8221;  and I get a &#8220;Must turn Javascript On&#8221;  error.  I try to add people to groups, no luck.  (These things JUST WORKED on the Charlie&#8217;s Test Course groups and gave me no error. Same browser, same login.  I&#8217;m a teacher in both courses, etc.)        It seems to be a course-related problem and is not reproducible in all courses.  It&#8217;s almost like something is getting corrupted somewhere.    Our Moodle is running on Fedora Core 2 on linux mysql on backend.  I've tried using IE 6 on Windows, IE 5.2 on Mac OS X and Firefox on both for the browser.""",Bug,Course
381334,"""On our institution site (French university) the administrator has set French as default language for the whole site.  In my English courses, as a teacher, I want everything to be in English, so I have set the Force language parameter to English.  It works OK, except for a number of places where some messages continue to be displayed in French (the site default language) rather than in English (my forced course language).  Among those are:  #1 CHAT MESSAGES  Joseph R&#233;zeau vient d'arriver dans ce chat  Joseph R&#233;zeau vient de vous appeler!  #2 FORUM messages display :   r&#233;ponses emboit&#233;es / r&#233;ponses en ligne / etc.  plus a few others here and there...    Of course, if the student has chosen English from the dropdown list of languages before connecting to our university Moodle site, everything is OK, but I'd rather have the Force language to English settings work for the English courses...  Thanks,  Joseph    PS This bug has been corrected in version 1.5DEV for #1 but not for #2 above.""",Bug,Admin
381477,"""Looged as admin, I go to Administration/Edit Users and clicking over any student name, I get this message: 'The details of this user are not available to you'. Curiosly I'm able to click over some 'teachers' but not oves some 'students'.    Perhaps related to some recent isteacherinanycourse()  calls added.""",Bug,Admin
381521,"""Next days I'm going to check everything as intensively as I was able. Here I'll write my progress...""",Bug,Backup
381699,"""A functionality that would allow adding a course to multiple categories.    Detailed description of problem and feature below:    At start I would like to write, that it is nice to be here, thanks for possibility to use Moodle as an excellent e-learning tool. You all did a great job.    I think I have one suggestion for Moodle. I am not sure if someone already wrote few words about it, sorry in case of I am doubling the topic. Let me explain the problem.    As a school of higher education we do have various types of studies. For example We offer to our students three-year bachelor courses, two-year supplementary master courses, five-year uniform master courses or postgraduate courses. Additionally some of those types can be realized in full-time or part-time. Now about subjects - some of subjects (which are courses on Moodle) do fall under all types of courses, some do not. And we do have years.. one subject can be taught in the first year of full-time studies and third year of part-time studies. One more thing.. we do have various majors.    So as You can see we do have a lot of categories one Moodle course should fall under. Now I can assign a course to one category only and I do not really know how to solve my problem. One solution is to name courses using a particular schema and use description field to add information about who is recipient of the course. Another solution is to create for example 4 copies of one course and assign it to few main categories.. but then a teacher will have to publish a new document in four courses. Hmm.. those solutions are still a half-solutions and seems that in the nearest future those can couse a lot of additional problems.    I think many other schools will have to face (or already did?) this problem. What do You think about the problem ? Is it important ?    Finally, how about adding such functionality to Moodle ? A functionality that would allow to assign a course to multiple categories (and a course can still remain in one category only).     From my point of view it seems rather easy, but probably it is not. Some interfaces should be changed allowing to assign course to multiple categories and database structure should be changed too by adding new table which stores data about courseIDs and categoryIDs.    Dear <USER>what do You think ?    with best regards,  Tomasz Jankowski""",New Feature,Course
381932,"""As admin I can not assign teachers to any course. This error message comes up:    Can't open file: 'user_teachers.MYI'. (errno: 145)    SELECT u.id, u.username, u.firstname, u.lastname, u.picture, u.lastaccess, s.timeaccess FROM user u, user_teachers s WHERE u.id = s.userid AND (s.timeaccess > 1096006254 OR u.lastaccess > 1096006254) ORDER BY s.timeaccess DESC""",Bug,Admin
382073,"""Many pages write to the log before they have checked whether the user is allowed onto the page. So I find a lot of logs which appear to show that someone accessed a certain page even though they turn out not to have the authority. I would like to be able to see this in the logs somehow.    My idea: a new function notice_with_log() as an alternative to notice() which is called with the name of the language string rather than with its translation and which logs the referring page and the notice. This could then be used where appropriate, for example when a user fails the isteacher or isstudent tests.""",New Feature,"Admin,Logging"
382265,"""I'm using the Topics Format for my courses.    When a student navigates through the various activities, the drop-down list in the upper right corner displays available activities under the ----Topic 1----, ----Topic 2---- headings.    I would like (as a Teacher) to be able to rename those pretty meaningless headings to headings of my own choice.     Suggestion: add an edit Topic name/title button (similar to the existing edit summary button). Then that new topic title would appear in the drop-down navigation list instead of ----Topic 1----, etc.    Thanks!    Joseph""",New Feature,Course
382320,"""I'm not sure if this is anew feature request or merely one I don't know about yet.  I would like my people to have a remember me check box so theyd on't have to re-type password and login information everytime they want to venture onto out Learning center just like you do on this log in page and I was wondering if someone could direct me to what (I am sure) has already been written and perfected.  After all...why reinvent the wheel eh? Oh yes and I apologize for the run-on sentence above.""",Bug,Admin
382361,"""This is a wish not a bug  The upload and delete profile image is causing me some grief. As I manage a high school extranet the students get a degree of fun out of having a psuedo image for their profile. After spending a few hours associating a correct image to their profile it is frustrating that it can be changed.  What I would like is that the Administrator can view and employ the upload and delete profile image with the capacity to hide this feature from the students. A toggle of show and hide would be fantastic.  The reason is that I have been promoting our Moodle Extranet as a community based environment where transparency is the key ingredient. Knowing that the image is who the person is, is extemely important in my High School environment.   Yours <USER>Bennett""",New Feature,Admin
382730,"""I don't know if this can cause problems or not, but in the user table one of the columns is named password.  I'm fairly new at php but I thought this was a no-no. I was using a script called AppGini and it couldn't handle the name.""",Bug,Admin
382825,"""I've hidden a sub-category (one level *below* the top level) and visited my website once as a simple visitor and once logged in as a guest.    In both cases the category is listed in the sub-category area. In the dropdown menu above it, it is correctly hidden.    See the  _DEMO Department of Mathematics, University of Antartica sub category which is hidden. (I also checked it with category names not beginning with an underscore and with an empty and a non empty sub-category. The problem is the same).    One level above (course categories on the top level) it works fine.    Greetings  Andoo""",Bug,Admin
382830,"""On installation, I accept the default method (using PHP) for email. Admin user is set up and everything is running okay. Logged in and created a new user account (teacher1), and was told email was sent. No email arrived. Logged in as admin, listed users, and it says Displaying Users 0 to 3. It lists only two users, admin and a blank account. If the new user (teacher1) tries to log in, he is told that he needs to use the email directions. As admin in 1.1.1, I was able to delete this user in this case. As admin now I cannot even see this user.    Of course, I've added an smtp server, and all is well, except that Teacher1 can not be removed... It would be nice to have an option to resend the email.""",New Feature,Admin
382921,"""Whenever adding a resource (text, file, anything) after selecting type, then submitting the actual data on the next page, I would always get an error message that was just a number (that would increment 1 every subsequent time I did it).  From looking at the code under the add case in course/<USER>php it seemed like when it would test a return value to see if it was an actual value or an error message (by using is_string()) it would always return true even if it was a number, so naturally it ALWAYS thought it was an error.      Now I'm guessing this is not a common problem or it would have come up by now, but the only way I could get around it was to test if the return to is if it was NOT a number ( !is_numeric() ) instead of seeing if it WAS a string.  here's my diff:      Index: <USER>php  ===================================================================  RCS file: /cvsroot/moodle/moodle/course/<USER>php,v  retrieving revision 1.46  diff -r1.46 <USER>php  74c74  <                 if (is_string($return)) {  ---  >                 if (!is_numeric($return)) {""",Bug,Admin
383369,"""Here's something I'd like. Our university has a site-wide IMAP server, but I only want students in our department to get onto our moodle server. If I use IMAP authentication then any students could get in. They wont be able to get onto any courses since they'll have enrolment keys, but they'll still be able to create account records.    What I'm considering is that I would fill the <USER>user database with usernames of our students, use IMAP authentication, but only allow those students already in <USER>user to get in. I cant see a way to do that with the current authentication schemes. I could use an external database, but I'd rather the students used the IMAP authentication since thats the same username/pass as they use for most everything else on campus.    I'll have a bash at hacking this on to moodle later.""",New Feature,Authentication
383579,"""Assignments and Quizzes allow for points to be alloted from 1-100.  I have a variety of assignments and quizzes that have different weights against the final grade.   Having the 1-100 score without a weighting coefficient means I end up doing things like =sum(c$:d$)*1.6667 in the excel exports to actually get the right distributions for grades.    The alternative (which I'm considering again) is yes fix the max points to the appropriate final grade distribution.  I had avoided that because giving the student a 67 our of 84 points seems more troublesome than just leaving things as a max 100 scale.  It turned out to be a pain in the arse for grading this time around, as I had alot of differently weighted assignments.    I'm going to look at the moodle/course/grade{s}.php and see how a weighted distribution could be hacked on those pages.    I think I'd discussed this in a different bug but generally a field in the quiz and assignment tables for final weight would be handy.    Thoughts?""",New Feature,Course
384567,"""As an API developer, I would like to ensure that generated APIkit projects have the components listed in the following order:    1) APIkit router  2) API implementation flows  3) API exception strategy    So that I may be able to more easily understand the logic of the API's implementation.   """,Bug,REST
384570,"""As an API developer, I would like to have access to RAML specific meta-data associated with the message at hand (e.g. the traits associated with the current message) so that I may perform content based routing using that information. """,Enhancement Request,REST
385659,"""As a Support Engineer i want to be able to see all the system properties values configured on mule engine.    it will be nice to have a place/way to see al the system properties and the values for the mule engine (enterprise or community) so it can help for troubleshooting.""",Enhancement Request,Core
389039,"""As a user I want to set a specified timezone for the cron expression that I want to use.  Right now I have to set the cron in the timezone that I am, if i am using mule local I have to set the cron in my timezone. So I have to change the cron expression every time I change the environment.  Quartz Scheduler allow you to do this, but in java code. This is going to be useful if we can set the timezone in the transport like an attribute.""",Enhancement Request,Transport
389656,"""I have an application using OAuth provider. It gives out access tokens to users when they login, they get a new access token for each device they login with.   I need a way to revoke all tokens granted to a user.   Mule does allow the revocation of tokens via <oauth2-provider:revoke-token /> but using it requires customer to maintain a list of access token given out against each user.   I checked the OAuth code, but couldn't find we expose the following 2 functions:   revoke all tokens granted to a user.   return all tokens granted to a user.   It would be nice if we do provide the above functions, as all the information are there in the objectstore. """,Enhancement Request,Core
389824,"""As a user, if I've changed my default expression language, enricher should still work.""",Story,Core
389825,"""As a user, I want to be able to configure the default expression language for my project so that I can use Weave as my expression language.""",Story,Core
390361,"""AS A  user    I WANT  I want to have documentation around the features of this transport    SO THAT  I can learn how to use it and to have a reference  """,Story,Core
390362,"""AS A user  I WANT I want the transport to be tested against a real broker and a real application.  SO THAT I can be sure the transport works on real world usage. """,Story,Core
390363,"""AS A  user    I WANT  I want to be able consume instantly amqp messages    SO THAT  I can consume messages at any point of the flow  """,Story,Core
390364,"""AS A  user    I WANT  I want to be able consume continuously amqp messages    SO THAT  a MuleEvent is created and delivered to a flow when a messages arrives to a queue and is comsumed  """,Story,Core
390365,"""AS A  user    I WANT  I want to be able to configure the transport in a single point    SO THAT  I can configure the common transport attributes in a single reusable place  """,Story,Core
390367,"""AS A  user    I WANT  I want to be able to rely on a defined schema to write my AMQP integrations     SO THAT  I have exact knowledge of how I’m syntactically supposed to use the transport and to have help from my IDE to autocomplete and detect errors.  """,Story,Core
390368,"""AS A  product owner or community member    I WANT  to be able to review the specs     SO THAT  I can and contribute with my feedback  """,Story,Core
390755,"""The class   org.mule.transport.polling.schedule.FixedFrequencyScheduler    has all it properties private and no accessor methods for most of them.  In particular I care about the property:     private ExecutorService executor;      This makes impossible for me to extends and change just and small portion of the class.    Could you please make this property protected.      The cause for me asking you this is that I'm trying to do tests that execute a poll in a flow.  Now I would like to make this execution synchronous in such a way that the thread executing the test, waits for the poll to finish run and then keep executing thus avoiding to make the test to check wether or not the poll has finished.     Then I could just extends this class and override the schedule in this way:     @Override   public void schedule() throws MuleException {    Future future = executor.submit(job);    try {       future.get();    } catch (Exception e) {     throw new DefaultMuleException(e);    }   }""",Enhancement Request,Core
390825,"""As a user, I would like to use the payload of an API response, obtained by using the outbound endpoint, no matter which compression was used.  Nowadays if you want to uncompress the payload, you have to create a choice with every possible compression.    Motivation/Context:  It is a very common use case when doing a proxy to need a compoment to decompress the payload, making it available for modifications. Mule already has one different component for each compression type, but it would be better to have one compoment to decompress everything.    Use case:  As a developer, I want to modify the payload that i recieved from an outbound endpoint, no matter which compression they used in it.    What happens now:  If I receive a compressed payload, I need to use a big switch to ask which compression did the server use in the payload (gzip, compress, etc) and then decompress it using the appropriate decompressor.  This steps could be avoided if we used one compoment with all this information.    Syntax:  <decompress type=""""gzip"""">  <decompress type=""""compress"""">""",Enhancement Request,Transport
390926,"""As a user I want to set a specified timezone for the cron expression that I want to use.    Right now I have to set the cron in the timezone that I am, if i am using mule local I have to set the cron in my timezone, if I am using Cloudhub I have to set the cron in UTC. So I have to change the cron expression every time I change the environment.     Quartz Scheduler allow you to do this, but in java code. This is going to be useful if we can set the timezone in the transport like an attribute.  """,Enhancement Request,Transport
391056,"""As an user, I want to catch exceptions at a record level in a for-each/batch element so I can continue processing and put my individual records on a DLQ.""",Enhancement Request,Core
391275,"""I'm trying to set a default reconnection strategy with reconnect-forever and blocking=false:    {code:title=Non working reconnection strategy|borderStyle=solid}  <configuration>      <reconnect-forever frequency=""""5000"""" blocking=""""false""""/>  </configuration>  {code}    {quote}  org.mule.api.config.ConfigurationException: Error creating bean with name '_muleNotificationManager': FactoryBean threw exception on object creation; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name '_muleConfiguration' defined in URL [jar:file:/Users/victor/.m2/repository/org/mule/modules/mule-module-spring-config/3.3.1/mule-module-spring-config-3.3.1.jar!/default-mule-config.xml]: Error setting property values; nested exception is org.springframework.beans.NotWritablePropertyException: Invalid property 'retryPolicyTemplate' of bean class [org.mule.config.spring.MuleConfigurationConfigurator]: Bean property 'retryPolicyTemplate' is not writable or has an invalid setter method. Does the parameter type of the setter match the return type of the getter? (org.mule.api.lifecycle.InitialisationException)   at org.mule.config.builders.AbstractConfigurationBuilder.configure(AbstractConfigurationBuilder.java:52)  {quote}    Trying the same but with *blocking=""""true"""" works perfectly*.  When *applying to connectors both true and false will work*:    {code:title=Non working reconnection strategy|borderStyle=solid}  Working reconnection strategy  <jms:activemq-connector name=""""jmsInboundConnector"""">      <reconnect-forever  frequency=""""1000"""" blocking=""""false""""/>  </jms:activemq-connector>    <jms:activemq-connector name=""""jmsOutboundConnector"""">      <reconnect count=""""5"""" frequency=""""1000"""" blocking=""""true""""/>  </jms:activemq-connector>  {code}""",Bug,Core
391368,"""As a component developer I want to be able to share an open result set inside a flow and have Mule closing it (free up resources) when the flow ends (same lifecycle than a transaction)  """,Enhancement Request,Core
391492,"""As a user I would like to be able to re-throw exceptions from within an exception strategy block.     A common scenario for this use case is when I need to customize the exception thrown to return a business exception.     This comes very handy in CXF where customizing the SOAP Fault right now requires handling the SOAP envelope which is not easy on.  """,Enhancement Request,Core
391542,"""As a user I need more guidance when coding Mule Expressions.    Acceptance Criteria:    Have mule reduce the need to use so many expressions to code simple cases. Make Studio more friendly for typing expressions. (more code completion, more syntax highlighting, suggest properties already defined, etc.)""",Enhancement Request,"Core,Documentation"
391543,"""As a starter user I need my life easy not complicated.    Time consuming ramp up, you spend 3 quarters of the time trying to figure out how to use mule, instead of using it, there is no easy and clear starting guide""",Enhancement Request,Documentation
391544,"""As a user I would like to have a way to know what is inside the payload (Tooling may help)    Acceptance Criteria:  - The user should have a way to know what is inside the payload as well as the Java type  - One tip can be added as how is the expression to be used inside a Flow to retrieve that data""",Enhancement Request,Tools
391545,"""Users need updated and correct documentation for new and old features and as a new user I would like to have an easy way to learn Mule     Acceptance Criteria:  - Take new users and take them through the new documentation, give him some task from easy to complex.   - The user should be able to complete all the path only learning with the docs.   - The docs should teach the user not just tell him to set values. """,Enhancement Request,Documentation
391690,"""Add support for add/remove attachments in MuleMessage.     Further analysis is required:  - Support for message attachments modification while processing message and while doing message enrichment  - Studie most common uses cases in order to simplify configuration    Use case 1:  During error handling I want to put the actual payload as an attachment and send it through email  """,Enhancement Request,Core
391734,"""Even if Mule seems to accept a pooled DataSource on a JDBC connector, none of them (xapool, DBCP, ...) implements the interface _javax.sql.XADataSource_. So the DataSource is never wrapped in an instance of _org.mule.transport.jdbc.xa.DataSourceWrapper_ when the DataDource is set on a _org.mule.transport.jdbc.JdbcConnector_.    {code:title=org.mule.transport.jdbc.JdbcConnector|borderStyle=solid}  ...     public void setDataSource(DataSource dataSource)     {       if ((dataSource instanceof XADataSource))       {          this.dataSource = new DataSourceWrapper((XADataSource)dataSource);       }       else       {          this.dataSource = dataSource;       }     }  ...  {code}     As you should know, a _org.mule.transport.jdbc.xa.DataSourceWrapper_ will create _org.mule.transport.jdbc.xa.ConnectionWrapper_ instances and in particular, _org.mule.transaction.XaTransaction.MuleXaObject_ instances.    It's not really an issue until you use this connector on an endpoint with a XA transaction. As soon as you use this connector on a endpoint with a XA transaction, Mule will never enlist nor delist the corresponding resource in/from the Transaction because the resource doesn't not implement _org.mule.transaction.XaTransaction.MuleXaObject_ nor _javax.transaction.xa.XAResource_. As a matter of fact, none of the pooled DataSources creates resource instances implementing directly theses interfaces (that's also what happens when you use Mule embedded in a web application and obtain a connection from JNDI through a pooled DataSource setup on the application server).    {code:title=org.mule.transaction.XaTransaction|borderStyle=solid}  ...      public synchronized void bindResource(Object key, Object resource) throws TransactionException      {        if (this.resources.containsKey(key))        {          throw new IllegalTransactionStateException(CoreMessages.transactionResourceAlreadyListedForKey(key));        }          this.resources.put(key, resource);          if (key == null)        {          this.logger.error(""""Key for bound resource """" + resource + """" is null"""");        }          if ((resource instanceof MuleXaObject))        {          MuleXaObject xaObject = (MuleXaObject)resource;          xaObject.enlist();        }        else if ((resource instanceof XAResource))        {          enlistResource((XAResource)resource);        }        else        {          this.logger.error(""""Bound resource """" + resource + """" is neither a MuleXaObject nor XAResource"""");        }     }  ...  {code}     As a consequence, only an error log message is written without throwing any exception (?!?!):  """"Bound resource xxx is neither a MuleXaObject nor XAResource""""    Moreover, the corresponding resource is never returned back to the pool because only resource implementing _org.mule.transaction.XaTransaction.MuleXaObject_ are closed on a rollback or a commit on a XA transaction.    {code:title=org.mule.transaction.XaTransaction|borderStyle=solid}  ...     private Map resources = new HashMap();       protected void closeResources()     {       Iterator i = this.resources.entrySet().iterator();       while (i.hasNext())       {         Map.Entry entry = (Map.Entry)i.next();         Object value = entry.getValue();         if ((value instanceof MuleXaObject))         {           MuleXaObject xaObject = (MuleXaObject)value;           if (!xaObject.isReuseObject())           {             try             {               xaObject.close();               i.remove();             }             catch (Exception e)             {               this.logger.error(""""Failed to close resource """" + xaObject, e);             }           }         }       }     }  ...  {code}     As a consequence you will finally got an exception when you pool is full because idle connections are not properly returned back to the pool. And these resources are never removed from the Map as well.    For completeness... I've also found a unit test for this usecase in Mule distribution under MULE_HOME/src/mule-3.1.2-src.zip/org/mule/test/integration/transaction/XABridgeJmsJdbcTestCase.java. When you execute this test, you will have a successful result instead of a failure because no exception is thrown ! But if you look at the log messages, you will find that the connection has never been enlisted nor delisted in/from the transaction because the resource is neither a MuleXaObject nor XAResource !""",Bug,Core
391923,"""In Jetty: from Greg W:    When you pass localhost, we look that up,  so it is whatever the host operating system has that mapped to, which  is normally 127.0.0.1 (or ::1).    If you don't set anything, then  that is all hosts, and the only special handling we have is for a set  of 0.0.0.0 is equivalent to setting nothing.    We picked 0.0.0.0  because that is what the JVM sockets report in toString if you bind to  the null host.    In Mule localhost is binds to all network interfaces on the local machine. The impact of this is seen on iON where we use localhost as the host that URLs should bind to, but when using Jetty or Ajax (which  uses Jetty) you must use 0.0.0.0 to explicitly get the same socket binding.    I want to to put a check in the Jetty code that localhost is used mean all network interfaces.  The only impact here is that local host will  not just bind to 127.0.0.1 but will still work for clients that connect using 127.0.0.1    I would like to fix this for Mule 3.2 since it has a pretty big impact on iON users and shouldn't affect Mule users  """,Enhancement Request,Transport
392344,"""With the use of the header expression evaluator its easy to put things in the outbound headers of message (which is default) when all you really want is to add a variable using an enricher maybe for use later on in the flow.    RM:  I wonder if it would be better to introduce a 'value' expression  evaluator that always stored and retrieved data in the INVOCATION  properties scope.    The nice thing about this is that you just store 'values' by name  without needing to know where on the message and which scope.  It is  still valid to use 'header' but the user can use it deliberately, not  as a temporary store.  Also, putting values on the INVOCATION scope  means they will live with the flow by not pollute the message.  Thoughts?    DF:    Sounds like a good idea, properties are the only place to put these values but there is a risk as you say of polluting message and generating confusion with scopes.  It would need to be accompanied with an ValueExpressionEnricher too.  Couple of comments/alternatives:  - Alternative could be to use 'invocation' as the default property/header scope and not outbound.. and then you can just use #[header:name] directly.  A scope would be required if you need to do anything that will affect outgoing message (i'm not convinced about this though,  there is also value in outbound being default).  - Not sure about 'value', with header/attachment its very clear that they are part of the message that is flowing along, value doesn't quite convey that in the same was I don't think.  Ok if no better alternative though.  - Message """"property"""" could work where inbound/outbound scope are called message headers and invocation scope becomes properties.    RM:  > Alternative could be to use 'invocation' as the default property/header scope and not outbound.  I thought about this but to the user it may seem odd that we set headers for enrichment data. Also I don't like the Idea of switching default scopes in different contexts. Sometimes it is outbound sometimes it's invocation.    Maybe 'variable'. Something that says this is a variable that will be available for this flow.     Sample syntax:  #[variable:myTempVar] instead of #[header:INVOCATION:myTempVar]""",Enhancement Request,Core
392372,"""This may be categorized as """"Don't do that!"""" and """"Will not fix"""" (or maybe even duplicate since I'm not sure how to search for this), but I thought I would raise the issue.    When using embedded Mule with Spring and """"Spring-first"""" configuration, org.springframework.web.context.ContextLoaderListener is typically loaded via web.xml and creates a bean context.  org.mule.config.builders.MuleXmlBuilderContextListener is then loaded, which checks to see if there is a org.springframework.web.context.WebApplicationContext available, and sets it as a parent context to the Mule context if so.  Finally, the mule.context attribute is set in the ServletContext.    Because bean visibility in the parent context can be obscured by the child context, it's easy for bean namespace collisions to generate spurious errors.  For instance a flow with the same name as a component bean in the parent context will be substituted in a <component><spring-object/></component> construction.  When Mule (in my case, the Jersey module) tries to initialize the flow as if it were a component, all heck breaks loose, and the errors that Jersey throws out are completely unrelated to the cause.    Is it possible that some strategically-placed type checking could help users more easily find these kind of configuration gaffes?  The user impact is very high when the problem presents itself (i.e. for new users), otherwise not very important.  """,Bug,Core
392537,"""The URIBuilder is an internal object that has a fixed set of properties, these are used to configure the endpoint URI from Spring. The problem with the current impl is that you cannot associate other properties to the URI other than using query parameters.  To work around this I added a new property to the URIBuilder called 'property1', and then set an alias in the spring config to what I want users to provide.      A better solution to this is to replace the current EndpointBuilder implementations with transport specific URIBuilders that have all properties injected as bean properties.  Alas we've missed the boat for Mule 3.0 for this one.""",Enhancement Request,Core
392568,"""There are several cases in which exceptions are being handled twice by exception listeners. I worked on MULE-4924 and MULE-4927, but the issue is more general. Issue EE-1968 shows this.    For example:  We have this when the exception occurs inside a transformer on the outbound endpoint and we are inside an xa transaction.    In that use case these two calls are in the stack when the exception is thrown:  * WebSphereMQMessageDispatcher(AbstractMessageDispatcher).send(MuleEvent) line: 165  ** Exception gets handled and rethrown here at:  *** WebSphereMQMessageDispatcher(AbstractMessageDispatcher).send(MuleEvent) line: 200  * TransactionTemplate.execute(TransactionCallback) line: 114  ** Exception gets handled and not rethrown here:  *** TransactionTemplate.execute(TransactionCallback) line: 138    The problem is that we cannot avoid the throwing of the exception inside the {{AbstractMessageDispatcher}} because we need that exception for the {{TransactionTemplate}} to know there was an exception (for example, to rollback a transaction).  And in the {{TransactionTemplate}} we have no way of telling it the exception was handled or not.    I think we need some marker on the Exceptions that would tell us when they have been handled.    *The proposed solution:*    The idea is to have some marker on the {{MuleException}} and {{MuleRuntimeException}} that would let us know if an exception was already handled or not. Probably by making both of them implement a new interface called something like {{MuleHandleStatusException}} that would have a {{isAlreadyHandled()}} and a {{setAlreadyHandled(boolean handled)}}.    We would also have a {{MuleExceptionHandlingUtil}} with a method that could be called {{boolean handledExceptionIfNeeded(ExceptionListener listener, Exception e)}} that would take care of detecting if somewhere in the causes of that exception there is a {{MuleHandleStatusException}} and check its status. It could return {{true}} if the exception is handled and {{false}} if it was already handled.    We can then change all the lines similar to this:  {{exceptionListener.exceptionThrown(e);}}  to   {{MuleExceptionHandlingUtil.handledExceptionIfNeeded(exceptionListener, e);}}    Potential Problems:    Some decisions made:    * What should we do if we mark an exception as handled and later someone else in the stack encapsulates it in another exception?  ** We will only handle it once. It won't be rehandled even if encapsulated  * What do we do with exceptions that are neither {{MuleException}} nor {{MuleRuntimeException}} (ie, that don't implement {{MuleHandleStatusException}})?  ** This case should be very rare (I've seen a lot of places where we catch {{Exception}} and we encapsulate it with some {{MuleException}} or another). As we don't have a way of know if it was already handled or not, we will handle it everytime.""",Bug,Core
392644,"""Currently inbound TCP endpoints open server sockets on the localhost and listen to incoming connections. Customer requested inbound endpoint that opens client connection to the remote TCP server.    ===  I want to configure a socket based inbound-endpoint (tcp, ssl) to connect to a server as a client, and just wait for incoming messages. The problem is, inbound-endpoints always seem to want to open ServerSockets - even if the IP/host I put in isn't local, which causes a binding error.  ===""",Enhancement Request,Transport
392904,"""When working with cxf endpoint configured as proxies, Mule is carrying XmlStreamReaders object. The object is built by cxf and usually read by cxf aswell. Such a stream is not (at the moment) readable more than once so, for multicasting purpose (as an example), user may need to transform it to a duplicable object (dom document or string since xml is the message).  When using the XML module to perform this transformation, soap request containing comments (like most of the requests from soapui do) mule will fail throwing a """"com.ctc.wstx.exc.WstxUnexpectedCharException: Illegal character (NULL, unicode 0) encountered: not valid in any content"""" exception.    A correction in org.mule.module.xml.stax.XMLStreamReaderToContentHandler did solve the problem :   from   protected void handleComment() throws XMLStreamException      {          int textLength = staxStreamReader.getTextLength();          int textStart = staxStreamReader.getTextStart();          char[] chars = new char[textLength];            staxStreamReader.getTextCharacters(textStart, chars, 0, textLength);            try          {              filter.comment(chars, 0, textLength);          }          catch (SAXException e)          {              throw new XMLStreamException(e);          }      }  changing the getText function to :   protected void handleComment() throws XMLStreamException      {         char[] chars = staxStreamReader.getText().toCharArray();                 try          {              filter.comment(chars, 0, chars.length);          }          catch (SAXException e)          {              throw new XMLStreamException(e);          }      }    enables """"filter"""" to write the comment fully.    I'm not sure whether the problem came from a bug in the handleComment or in getTextCharacters which could have been loaded in a wrong version by the classloader.  """,Bug,Modules
393249,"""The JDBC connector has a property called """"transactionPerMessage"""" which, when set to false, enables batch processing, i.e., an entire ResultSet is processed by Mule in a single transaction.  Since there is no documentation around this property, I'm not sure what it was originally intended to be used for.  If its only use case is for batching, perhaps we should rename it to something which alludes to batch processing? """,Enhancement Request,Transport
393271,"""MuleMessage has a mechanism where by when a consumable payload type (one that can only be read once) is read the message payload gets updated with a new payload so that the message can be read again by filters/transformers/dispatchers etc.    This mechanism works well, but when transformers are called directly which often occurs from things like filters this mechanism is bypassed and problems occur.    Only workaround is to manually insert transformers that transform to a non-consumable type just before the filter for example with the issue.  This is undesirable though but in terms of configuration and in some cases possibly performance.    One example is jaxen-filter, I'm sure there are others too.""",Bug,Core
393313,"""As a user, I would like the TCP connector to have more intelligent defaults to improve it's performance/scalability.   Note: The TCP Connector Hard codes some of the properties, for example the client pool property maxThreads (set to 1?). These properties could be allowed to be set using the configuration file just like the other transports or another approach as suggested by Ross could be used. The preference is not to add another set of configuration options to address this, if possible.  """,Enhancement Request,Transport
393408,"""I am receiving an inbound file using a FileConnector. When a file is recieved, I want to use the LogComponent to log a simple message before sending the file outbound to a separate outbound folder (Or a JMS Queue). The following service works as long as I dont uncomment the <log-component/>.  Once I uncomment out the <log-component> i get an error from the file connector that the output file path is invalid. The problem is that the log component returns null, which prevents the original MuleMessage from passing through to the outbound transport. As a result,  it is unable to find the #[header:originalFilename] property in the MuleMessage. Shouldn't the LogComponent return the original message untouched?  {noformat}         <service name=""""FileTest"""">             <inbound>           <file:inbound-endpoint connector-ref=""""aFileConnector"""" pollingFrequency=""""30000""""         transformer-refs=""""ByteArrayToString"""" address=""""file:////test/inbound"""">           </file:inbound-endpoint>    </inbound>     <!--<log-component/>-->              <outbound>                 <pass-through-router>                  <file:outbound-endpoint  connector-ref=""""aFileConnector""""                   address=""""file:////test/archive"""" outputPattern=""""#[header:originalFilename]""""/>                 </pass-through-router>    </outbound>         </service> {noformat}""",Bug,Core
393564,"""User Story:  As a user, I want to configure a retry policy for the JNDI lookup of a resource, i.e, if the JNDI server is off-line I want some way to automatically wait for it to come back on-line and not simply fail.  """,Bug,"Transport,Transport"
393674,"""I've configured a nested router to communicate via JMS. In case a timeout occurs, the JMS transport returns a null value as a reply (the message itself is null, not the payload). This is given back by the NestedInvocationHandler to the caller. I would expect that NIH instead would throw a runtime exception, instead of forcing my application code to check for null pointers and throwing the exception myself.""",Bug,"Core,Transport"
393920,"""I started looking at the top nav bar last night, and I would recommend that we delete the Developer and Cookbook buttons and add a Community button, which can have a landing page that includes links to the cookbook, how to become a committer, the download page, user lists, muleforge, FAQs, public mulecdev stuff, etc. I also recommend that we delete the About button and move the info to Community and Getting Started as appropriate.  I recommend we change the User Guide button to Documentation, and have the landing page have links to all of the product documentation (currently home3). We should add the Javadoc links to the Extending Mule section of this page as well as the Reference section.  The Getting Started button should display the wiki-based version of the Overview guide for the current release (which includes a link to the download page) plus links to PDFs of the Overview guide for each major release. """,Enhancement Request,Documentation
393960,"""The MuleProtocolHeadersOutInterceptor is not being added to the fault chain, meaning the protocol headers such as the Content-Type are not being written.  From the mailing list: I'm using Mule 2.0.1 with CXF transport, Aegis databinding (frontend=""""simple"""").  Exception strategy for each service = org.mule.transport.cxf.CxfComponentExceptionStrategy  Trying to migrate services from XFire. I'm having a problem when CXF throws a Fault message- the encoding is set to plain/text rather than plain/xml. Normal soap responses are fine, only Faults have this problem.  My client application is using Jaxws and as a result of this cannot catch exceptions, it reports:  Could not access remote service at [snip]  nested exception is com.sun.xml.ws.server.UnsupportedMediaException: Unsupported Content-Type: text/plain Supported ones are: [text/xml] Caused by: com.sun.xml.ws.server.UnsupportedMediaException: Unsupported Content-Type: text/plain Supported ones are: [text/xml]  I have used the SoapUI tool to check this and it confirms that Faults have text/plain encoding.  Not sure whether this is a CXF or Mule problem, or whether it's a known issue, can anyone advise?""",Bug,Modules
394060,"""Here is the email I submitted to the user list where Ross asked me to raise this as a bug: ------------------------------------------------------ I recently added two new components to my Mule deployment.  The first component monitors an FTP folder and downloads files to a local directory as they become available.  The second component monitors that local directory for new files and extracts the necessary information from them.  One thing I did that I hadn't done before was add the <exception-strategy> handler for the second component.  Unfortunately, what happens now is that I get an error in my log file whenever there is no file to download from the FTP server.   Two questions: -          What do I need to do to fix the error from being thrown when there is no file to process? -          Why is the error occurring on the FileDownloader component when I added the <exception-strategy> handler to the FileParser component?  I can tell this because the error messages in the log file are in 5 minute increments; that's the same polling frequency as the FileDownloader inbound endpoint.""",Bug,Core
394072,"""In 1.x, the ConnectionStrategy was configured on the connector.  In 2.0 it can only be configured on the endpoint.  I'm not sure whether this was a conscious decision or should it also be configurable on the connector?""",Enhancement Request,Core
394083,"""Some routers such as Aggregators, ChainingRouter and possibly RecipientList and MessageSplitter potentially can return multiple messages. Right now we return the last or in the case of aggregator the aggregated message.  I think I'll implement a MessageCollection that maintains a list of MuleMesssge objects but implements MuleMessage itself.""",Enhancement Request,Core
394319,"""A customer has raised to us that, messages are stuck in the queue after being consumed and committed by Mule. This problem is reproducible on 1.4.3 and 1.4.4-SNAPSHOT (latest code from trunk).  Currently, the only workaround is to limit the number of receivers to one via threading-profile, e.g.  {code:xml} <connector .../>   <threading-profile id=""""receiver"""" maxThreadsActive=""""1""""/> </connector> {code}  Looking into the code, I found that when {{JmsMessageReceiver}} creates the consumer (at {{doConnect()}}), the same session will always be assigned and used across other receivers. This could potentially cause race condition while messages are being consumed and another receiver is trying to commit the session. And it could be the cause contributing to the Tibco EMS problem.  I have also tested similar settings on ActiveMQ but it's not replicable. However, I discovered another strange thing which is, while Mule tries to commit the session, the first {{commit()}} call commits all messages, for example, if there are 3 messages being consumed (and there'll be 3 {{commit()}} calls), you will notice that all messages are committed in the admin console after the first {{commit()}}. If Mule is integrated with Tibco EMS, it has a slightly different behaviour, you'll see each {{commit()}} call commits one or two messages instead of all. I'm not sure if this is relevant to the issue or it is an expected behaviour.  More information can be found in case 00001339.""",Bug,Transport
394399,"""There is currently no central place to get Mule properties safely and consistently from. You will have to search them in several places and may end up using the wrong approach or worse assemble info yourself. I found the following places with redundant implementations:    MuleConfiguration   MuleProperties   MuleManifest (formerly in MuleConfiguration but deprecated now)   As system properties, e.g. mule.home  I came across this when trying to find the working directory and log folder properties as APIs centralized and accessible within Mule. I could create my own path to the 'logs' folder but this leads to real problems at the very moment when a customer wants to have the logs in a different place, e.g. changing wrapper.conf->wrapper.logfile. I am not sure what a good consolidation would be but seeing different tactics, e.g. statics in MuleProperties interface and API calls in MuleConfiguration makes me shrug ;) Probably, having MuleProperties as a util class, acting as the main entry point for any Mule property is what I would have expected. This e.g. would allow me to return the MuleManifest, get the Mule home or Mule log folder.""",Bug,Core
394568,"""Ross: """"Since the new endpoint changes there is a class of test that is now failing. Since endpoint typses are now more clearly defined, using an endpoint as a sender and receiver interchangeably no longer works (which is better in the long run). The AbstractConnector caches UMOMessageDispatcher objects based on its endpoint but now and inbound ep and outbound ep with the same address are no longer equal. This highlights a wrinkle in the UMODispatcher API that has always bugged me, but now we must do something about. Basically, the receive() method on the UMOMessageDispatcher is on the wrong class. Historically, it was there since doing a receive() call directly was a client task and the UMOMessageDispatcher represents the client implementation of a transport, conversely UMOMessageReceiver represents the server implementation. I think there needs to be a 3rd interface called UMOMessageRequestor that has a single receive() method on it. The AbstractConnector can cache these UMOMessageRequestor objects separately from dispatchers so that UMOMessageDispatchers are always cached with outbound endpoints and UMOMessageRequestors are always cached with inbound endpoints"""".  Andrew C: """"I'd suggest changing receive() to request() on the requestor interface""""  Andrew P: """"Do you want to apply the same threading profile to both requestors and dispatchers? Should it be shared or configurable separately (with meaningful defaults)?""""""",Enhancement Request,Core
394610,"""When tring to create a QName for a method on a soap axis binding,  the URI's are not properly parsed.   This affects any URN that is used as a namespace.  Example Configuration: <property name=""""soapAction"""" value=""""urn:www-callcommand-com:web-services.CallImporter/${method}""""/> <map name=""""soapMethods""""> <list name=""""qname{ImportFile:urn:www-callcommand-com:web-services.CallImporter}"""">  The code is parsing based on Colons and assumes that the extra colon means there is a prefix.  In this case there is not a prefix, just a method name and a URI.  The following code parses the string, from the configuration and then improperly hands it to the Axis QName object.    I tested the use of the URN above with the QName object and it handles it correctly.  The following code parses it incorrectly.      protected QName parseQName(String val)     {         StringTokenizer st = new StringTokenizer(val, """":"""");         List elements = new ArrayList();          while (st.hasMoreTokens())         {             elements.add(st.nextToken());         }          switch (elements.size())         {             case 1 :                 return new QName((String) elements.get(0));             case 2 :                 return new QName((String) elements.get(0), (String) elements.get(1));             case 3 :                 return new QName((String) elements.get(1) + """":"""" + (String) elements.get(2),                     (String) elements.get(0));             case 4 :                 return new QName((String) elements.get(2) + """":"""" + (String) elements.get(3),                     (String) elements.get(1), (String) elements.get(0));             default :                 return null;         }     }   Unfortunately this is blocking a deployment. I've tried getting around this with escape characters, but to no success.  There is no way to change the target WSDL or namespace, because of existing customers. """,Bug,Core
394730,"""I am attempting to configure Mule to use a XA transaction across a VM queue and JMS queue.  A file connector reads files from a directory and puts the contents on the VM queue (this is just used as a means to bootstrap the test).  A second component picks up the messages from the VM queue and attempts to send them to the JMS queue using an XA transaction so that a failure will roll back the transaction and leave the message on the original VM queue to be reprocessed.  Mule starts fine with no errors and appears to connect to the JMS queue ok.  However, when it attempts to send a message to the queue it fails with an illegal state exception.  Removing the transactional settings from the endpoints and removing the transaction manager factory the messages end up on the queue fine.  I'm new to Mule so it may be a basic mistake I'm making but I've looked through the documentation and debugged through the code and can't seem to find the issue.  Rgds Dave""",Bug,"Transport,Core"
394988,"""ActiveMQJmsTransactionFunctionalTestCase (and at least one other, I think) is failing both on the 1564 branch and on the trunk.  This may only be a problem with Linux - it's been seen by Andreas and myself.  I am excluding ActiveMQJmsTransactionFunctionalTestCase on 1564 branch as I need to see what is OK and what is not.  If other tests fail, I will add those here too (assuming they're related).""",Bug,Transport
395228,"""Two related issues here.  First, I don't understand why this has an """"inbound"""" attribute since it is used in a context like: {{noformat}}         <mule:service name=""""relay"""" implementation=""""bridge"""">             <mule:inbound-router>                 <mule:endpoint address=""""pop3://bob%40example.com:password@localhost:65432"""">                     <mule:retry-connection-strategy inbound=""""true"""" retryCount=""""5"""" retryFrequency=""""1000""""/>                 </mule:endpoint>             </mule:inbound-router>             <mule:outbound-router>                 <mule:pass-through-router>                     <mule:endpoint address=""""vm://receive""""/>                 </mule:pass-through-router>             </mule:outbound-router>         </mule:service> {{noformat}} where inbound is implicit (inbound-router).  Second, if I add  {{noformat}}         //Retry strategies         registerBeanDefinitionParser(""""retry-connection-strategy"""", new SimpleChildDefinitionParser(""""connectionStrategy"""", SimpleRetryConnectionStrategy.class)); {{noformat}} to MuleNamespaceHandler, there is no inbound setter anyway.  Seems like either I'm misunderstanding, we need a dedicated definition parser, or we need two different elements (one for inbound and one for outbound).""",Bug,Core
395524,"""I am very new to mule. i have been playing with the HelloWorld example. no i want to save the name that the user enters in a SQL database. How to i make a connection to a database?  Also, as an alternative, if i have a java class that connects to a sql db, and also have a seperate mule app like HelloWorld, how do i get mule to use the java app to select\insert\update the db?  Assistance will be greatly appreciated.""",Task,Transport
395593,"""User Story:  As a user, I want to be able to configure a retry policy on an endpoint which overrides the retry policy  I have configured on the connector. One reason for doing this may be that I am using local transactions can can't create different connectors.  Acceptance Criteria # Can configure for inbound endpoint # Can configure for outbound endpoint ------------------------------------------------------------------------------------------------------------------------------  It makes more sense in almost all cases to have a ConnectionStrategy assigned to the endpoint rather than the connector since it is usually the MessageReceiver or MessageDispatcher that makes the connection to the underlying resource.""",Enhancement Request,"Core,Core"
395990,"""I'm trying to perform an insert on an Oracle Database. The problem I have is with null parameters to be passed to the statement.     The jdbc dispatcher performs a:   int nbRows = new QueryRunner().update(con, writeStmt, paramValues);        Looking at the QueryRunner code I can see that the update method invokes the fillStatement operation:     protected void fillStatement(PreparedStatement stmt, Object[] params) throws SQLException {      if (params == null) {          return;      }         for (int i = 0; i < params.length; i++) {          if (params[i] != null) {              stmt.setObject(i + 1, params[i]);          } else {              stmt.setNull(i + 1, Types.OTHER);          }      }  }       When a null value is found, the stmt.setNull(i + 1, Types.OTHER); is called and the following exception is thrown:       java.sql.SQLException: Invalid column type Query: INSERT INTO FPA.ESPECIES (rest of the query here)...        at org.apache.commons.dbutils.QueryRunner.rethrow(QueryRunner.java:330)        at org.apache.commons.dbutils.QueryRunner.update(QueryRunner.java:399)        at org.mule.providers.jdbc.JdbcMessageDispatcher.doDispatch(JdbcMessageDispatcher.java:98)        at com.lumina.integration.mule.providers.jdbc.JdbcMessageDispatcher.doSend(JdbcMessageDispatcher.java:61)        at org.mule.providers.AbstractMessageDispatcher.send(AbstractMessageDispatcher.java:196)       Apparently the error is because of the Types.OTHER sql type. I know that this is an issue involving Jakarta dbutils package but maybe someone has already got this problem and found a workaround to this.    At the Jakarta mailing list a workaround is mentioned by subclassing the QueryRunner class. In order to do that, I'll need to subclass the JDBC provider completely (the dispatcher and factory at least). Perhaps a custom QueryRunner could be injected to the JdbcMessageDispatcher in a future version.       Regards,         Juan Manuel   """,Bug,Transport
396039,"""Some use them, some don't. I think it would be natural to support w3c DOM anywhere we offer DOM4J, after all, it is the """"standard"""" way of storing XML as a structure in memory.  Also, if you wish to use Message-style web services with Axis, this requires Document (or similar) support.  If no-one objects, I'll add utility functionality to create JAXP Source and Result objects for given input/output Java types, and plug that into the transformers.""",Enhancement Request,Core
396121,"""Argument type checking is bypassed when a method name is specified as a property of the endpoint .  For example, given the following object:  public class MyComponent {     public String myMethod(Integer myArg) {         return """"myReturnValueForInteger"""";     }          public String myMethod(String myArg) {         return """"myReturnValueForString"""";     }          public String myMethodTwo(String myArgTwo) {         return """"someOtherReturnValue"""";     } }  If the component previous produces messages with String payloads then I need to specify whether MyComponent should invoke myMethod or myMethodTwo since they both accept String arguments.  But when I set the method property as follows:  <mule-descriptor name=""""myComponent"""" implementation=""""foo.bar.MyComponent"""">     <inbound-router>         <endpoint address=""""myInboundEndpoint"""">             <properties>                 <property name=""""method"""" value=""""myMethod""""/>             </properties>         </endpoint>     </inbound-router>     <outbound-router>         <router className=""""org.mule.routing.outbound.OutboundPassThroughRouter"""">             <endpoint address=""""myOutboundEndpoint""""/>         </router>     </outbound-router> </mule-descriptor>  Mule doesn't check the argument type, but just invokes the first method it finds that matches the specified method name.  Thus myMethod(Integer myArg) gets invoked and an exception is thrown because the argument is a String but the method being invoked accepts an Integer.  This is because ClassHelper.getMethod() (called from onCall() in DefaultLifecycleAdapter) only accepts the method name as an argument and just returns the first method that matches the name.""",Enhancement Request,Core
396247,"""A couple of issues due to the way URI's are constructed using the following : """"axis:jms://topic:testTopic"""".  In AxisConnector.registerReceiverWithMuleService(), on the following line:      String endpoint = receiver.getEndpointURI().getAddress() + """"/"""" + serviceName;  the address in the URI is not set with topics, so endpoint would be """"/EchoTest"""" instead of """"jms://SampleQ/EchoTest"""" for example.  I was able to get the correct endpoint value for topics using the following after the scheme is set:      if( scheme.equals(""""jms"""") && endpoint.indexOf(""""/"""") == 0 )  {                    endpoint = scheme + """":"""" + receiver.getEndpointURI().getUri().getSchemeSpecificPart() + """"/"""" + serviceName;     }  This registers everything correctly on the Axis side, etc, but becomes problematic (with topics only) in the JmsMessageReceiver.createConsumer() method,  where the following call:      Destination dest = jmsSupport.createDestination(session, endpoint.getEndpointURI().getAddress(), topic);  doesn't have the right address value due to how java.net.URI parses the resourceInfo part using topics (e.g. """"jms://topic:testTopic"""". There is a way to get this working that doesn't break either the axis/jms queue address or other jms endpoint addresses, but its not pretty ;-):       ...       UMOEndpointURI epUri = endpoint.getEndpointURI();               String address = epUri.getAddress();               String epPath = epUri.getPath();                         if( epPath != null && epPath.length() > 0 ) {                    if( !address.equals(epPath) && epPath.indexOf(""""/"""") == 0 ) {                         address = epUri.getHost();                         // if no host, then parse out the authority             if( address == null ) {                              String authority = epUri.getAuthority();                              if( authority != null ) {                                  if( authority.indexOf("""":"""") > -1 ) {                                       address = authority.substring( authority.indexOf("""":"""") + 1, authority.length());                                       topic = """"topic"""".equals(authority.substring(0, authority.indexOf("""":"""")));                    }                    else {                                       address = authority;                    }                                             }                // should else this with an exception throw             }         }     }      Destination dest = jmsSupport.createDestination(session, address, topic);   """,Bug,Modules
396361,"""I think non-UMO components (Transformers, Routers, Filters, etc. that I'll call NUMOs here) should be configurable as Spring Beans, much like UMOs.  They'd differ from UMOs in that they'd be required to implement their NUMO interfaces (enforced at runtime) and reflection-based invocation would/should not be supported.  This new feature has a few advantages:  * Allows user-defined NUMO attributes to be configured on instances using Spring IoC.  * With the use of AOP mixins, this can decouple NUMOs from the Mule framework.  I'm currently doing this with UMOs and ServiceMixin, but without this feature, my NUMOs must directly inherit from Mule interfaces and classes.  * Non-Spring configuration for NUMOs would still work for backwards compatibility, much like UMO implementations work if a class name is provided for implementation rather than an existing Spring bean. """,Enhancement Request,Core
396390,"""Currently when working with transactions and JMS endpoints, only one receiver thread is created for the endpoint. It would be helpful, if the TransactedMessageReceiver could be extended to manage a number of receiver threads based on the messageReceiverThreadingProfile settings on the connector.  This is Ross's answer on the user mailing list for the problem:   This is a problem with the current JmsMessageReceiver.  The problem is that only one receiver thread is created for the endpoint when running with transactions. Thus when a receiver gets a message it blocks that thread until it's finished processing, which is the behaviour you are seeing.  One solution is to just register multiple inbound endpoints on the same destination, but currently the Jms connector will complain that there is already a listener on the endpoint, so this will not work.  I think we need to change the TransactedMessageReceiver to manage a number of receiver threads based on the messageReceiverThreadingProfile settings on the connector.  Can you raise a Jira for this.  Cheers,  Ross  <USER>wrote:  >Hi > >I'm currently helping to build a system with Mule, which consumes a message from queue A, transforms and enriches that message and then put it back in a queue B on the same WebSphere MQ instance. The whole system needs to be scalable and reliable. So I basically have a component with two endpoints (inbound and endbound), where both are a queue. > >When I'm setting the inbound endpoint as transactional (ALWAYS_BEGIN): > >      <mule-descriptor name=""""Enricher"""" implementation=""""ch.raiffeisen.om.component.Enricher""""> >          <inbound-router> >             <endpoint address=""""jms:/InQueue"""" transformers=""""JMSMessageToObject XMLToDom""""> >                  <transaction action=""""ALWAYS_BEGIN"""" factory=""""org.mule.providers.jms.JmsTransactionFactory""""/> >             </endpoint> >          </inbound-router> >          <outbound-router> >             <router className=""""org.mule.routing.outbound.OutboundPassThroughRouter""""> >                 <endpoint address=""""jms://OutQueue"""" transformers=""""DomToXML ObjectToJMSMessage""""/> >             </router> >          </outbound-router> >      </mule-descriptor> > >then everything seems to work synchronous and only by one thread/component. In the documentation I found the following > > Transactions can only (currently) be managed over synchronous endpoints. Thus if an endpoint has been configured for transactions it will automatically be synchronous. When dispatching and a transaction is in progress the dispatch will also happen in the current thread. > >So i understand that both the reading and dispatching will be done in the current thread. But what about the pooling of components. I was assuming, that if I use the correct pooling profile, that i would get multiple instances, each one working synchronously in one thread. But on the """"printHTMLSummary"""" that compontent always shows the component pool size as 1 and the Pool Max-Size as 10/1 for example, it will never go above 1, independent of the profile settings. > >If I remove the transactional behaviour, then everything works as expected, with a component pool size > 1 for the Enrichter component, dependent on the pooling profile. > >Is that the correct behaviour ? What can I do to make this processing scalable, i.e. to be able to process the messages in parallel. Do I have to declare different components based on the same implementation, each working on the same queues ? Did I miss something (maybe in the documentation) ? > >Thanks a lot for you help, I'm still kind of a beginner with Mule, but I have to say it's very impressive ! Thanks a lot for all your work. > >Guido > > >I will be able to send you a working example for a WebsphereMQ connector configuration, if you still require that for the documentation. >  >    """,Enhancement Request,Transport
396406,"""The MuleEndpointURI class does not appear to handle the equals method correctly. Not sure why that is as it just delegates to the internal uri object, but this does not work.  MuleEndpointURI uri = new MuleEndpointURI(""""tcp://localhost:8900""""); //this returns false uri.equals(uri);  Where as attemping the same thing with URI object does work  URI uri = new URI(""""tcp://localhost:8900""""); //this returns true uri.equals(uri);  This probably doesn't mean much to most people who are using either spring or mule to configure and handle everything, but when working with MULE programatically it becomes a bit more of a pain. Especially as I need to allow users to add and remove endpoints while the system is running.""",Bug,Core
396599,"""I've been thinking about configuration of the Mule server and I've come up with some requirements that I would like -  1. To have other configurations methods so that I can easily configure Mule from an XML file, from script such as beanshell/JS, or from some third-party container framework such as Spring or Pico.  2. control over how objects are created, i.e. I want to be able to intercept and override the default method of creating some or all objects in the system.  3. To be able to configure certain objects from JNDI (such as global providers, transformers, global endpoints and such). To a certain degree this is possible but not uniform for all administrative objects.  4. To override the default MuleServer implementation. This was in the original design, but has been overlooked and is currently not cleanly achievable.  5. Sensitive information in the config file should be secure, such as passwords.  6. To have the ability to separate my config file into multiple composite files.  This is how these issues could be solved -  1a. Use ConfigurationBuilders to determine how the server should be configured.  The builder could be specified as a CL arg with the default one being the MuleXmlConfigurationBuilder. Though you may also have a SpringConfigurationBuilder or JavaScriptConfigurationBuilder.  2a. I propose introducing a central MuleObjectFactory that is used to construct the different types of objects in the system and that can be overridden by custom factory methods.  This would giver more control over the creation of all objects in Mule and allow developers to customise as much as they needed.  Right now I think this would only apply to the Mule Xml configuration.  3a. A JNDIObjectFactory could be used instead of the default MuleObjectFactory to lookup objects in a JNDI store.  4a. Even the MuleManager would be created via the MuleObjectFactory so it would be easy to override the default implementation.  Also, I would provide a default Manager factory implementation that would perform a discovery-style search on the classpath for the UMOManager implementation to use, meaning that the developer need only have their UMOManager implementation on the classpath to use it.  5a. Pass all config files through an expression language such as OGNL (www.ognl.org) so that encrypted passwords and any expressions embedded in the file can be evaluated when the file is loaded and before it is processed.  6a.  This really only applies to the Xml Mule config and could be achieved simply by introducing an 'import' element where you can specify a file to import.  Though this will impact the mule-configuration.dtd. """,Enhancement Request,Core
396641,"""h5.Description    When debugging an application that has a space on iOS in the name the debugger fails to connect and subsequently breakpoints fail to get hit.    I see the following in the log    {code}  !ENTRY com.aptana.core 4 0 2019-07-16 17:31:49.482  !MESSAGE (Build 5.1.3.201907112159) [ERROR]  Unable to find our application in the listing of applications.  !STACK 0  com.axway.inspector.protocol.InspectorException: Unable to find our application in the listing of applications.   at com.appcelerator.titanium.ios.internal.core.debug.JSCInspector.handleConnectedApplicationList(JSCInspector.java:266)   at com.appcelerator.titanium.ios.internal.core.debug.JSCInspector.messageReceived(JSCInspector.java:115)   at com.appcelerator.titanium.ios.internal.core.debug.JSCInspectorReader.run(JSCInspectorReader.java:53)   at java.base/java.lang.Thread.run(Thread.java:844)  {code}    Unsure if this is related, but on the vscode debugger as the debugger protocol reports source paths as a file URI the app name is {{Hello%20World}} so Studio might need to also handle this    I'm able to debug an app called HelloWorld with now issues    h5.Steps to reproduce    1. Create an app with a space in the name  2. Debug the app on iOS    h5.Actual    Debugger appears to fail to connect and breakpoints are not hit    h5.Expected    Debugger should connect an breakpoints should be hit """,Bug,"Debugging,iOS"
396729,"""h5. Issue Description     An associate of Amerisource was trying to get hyperloop to work.  He didn't require the 6.2.2 CLI fix that was made for me because his project is relatively new.  As a result, he spent 3+ hours of his time researching the problem only to discover that LiveView was still on.    Apparently, Hyperlook will NOT work when LiveView is on.  However, he did NOT notice that LiveView was on.    Perhaps the Appcelerator Studio should incorporate a better indicator for LiveView other than a small 16x16 icon with a slightly different look?  I would recommend changing the entire button menu background a different color to match the LiveView icon or perhaps changing the border color to something striking.      Or at least show a more specific error log if Is compiling a project with hyperloop and Live view on.     """,Improvement,Hyperloop
397548,"""After installing a fresh copy of Studio 4.1.0 Beta on a fresh copy of Windows 10, Studio prompts me to install (code analyzer 1.1.1-beta and) version 3.5.1.RC of the SDK.    As a user I'd expect it to prompt for the 4.1.0 Beta of the SDK (4.1.0.v20150626223333).""",Bug,Windows
397623,"""When I go to _Help > Install Appcelerator SDK.._ and then select _Install from Update Stream:_ the dropdown is empty so I cannot install older SDKs.    See attached screenshots.    As a side-note, it is confusing that the menu option has _Appcelerator SDK_ and the dialog itself _Titanium SDK_. I would suggest using the latter.""",Bug,Mobile
398180,"""Many of our customers would like to make it much easier to create and share content, in particular base templates to make starting projects much easier. For example, I could create a project that has our standard login widget, and corporate theme. Additionally, it may have specific connection widgets/modules to my backend data sources. This would act as my standard template whenever i want to start a new project.     We should provide a way to save an existing project as an Alloy or TiClassic Template (based on the nature of the project), which would then be added as part of a new ruble bundle or added to an existing bundle.     Additionally, I should be able to create a new Grouping for my Templates so when i open the New Mobile App dialog, the template would show up as  part of one or more categories.    When saving the Template, we should do the following:  - Provide an interface to:       - Give the Template a Name       - Select an existing Category or create a new category       - Add to existing bundle or create a new bundle       - Option to add this template to source control or store it locally  - Clean the Project so its prepared as a template       - Strip out project specific entries from the TiApp.XML (guid, acs keys, apm and soasta information etc.)       - remove any files / directories that are generated or non essential to the template       - We should make sure not to delete alloy plugins / hooks etc if we detect that custom ones exist.    """,New Feature,Templates
399253,"""I use Node.ACS and build with Titanium. In the last version of studio I was able to dock the Terminal window next to the console at the bottom - this was useful as the terminal is where Node.ACS output is displayed.    In the latest verison, the terminal window is only allowed to sit in the top as a tab along with the code I'm editing. This makes it more time consuming to follow along log output in the terminal with my code.    I would love it if we could dock it to the bottom like in the previous version. (Marking as a bug since the previous version had this feature, so it appears to be a regression).""",Bug,Terminal
399358,"""- As a Studio developer, I want to debug node.acs locally using Studio  - As a developer, I want to debug Node.ACS apps remotely in the cloud    4 person-weeks estimated time""",New Feature,"Debugging,Node"
399600,"""If a user has a pre-commit hook that may fail the commit (or there's some other issue that causes a commit to fail) we used to ignore it. Now we pop an error dialog after the commit dialog closes and the commit fails. In my case, I have a pre-commit hook that enforces updated copyrights. I need it to not close the commit dialog (so I don't have to re-open), retain my commit message, and refresh it's file listings (because the hook also modifies files).""",Improvement,Usability
399747,"""iOS iTunes Packaging: success toast links to studio application directory:    ~/Desktop/tistud/tistud-rc/titanium-studio/TitaniumStudio.app/Contents/MacOS    I would expect it to open Xcode and launch the archive perspective as it does using Titanium Studio 3.0.2 Stable.    This is verified as a regression.    iOS Ad-Hoc Packaging opens correct directory, as specified in the run configuration.""",Bug,iOS
400069,"""I started up Titanium Studio in a dev workspace of eclipse. On startup I was prompted to install a number of things including NodeJS. On the second page I was asked to Install or set the path manually. I already had Node installed in it's standard {{/usr/local/bin/node}} location. My installed version is 0.8.3, the new version is 0.8.17. I'm guessing it's trying to update Node, but if that is the case it shouldn't be prompting """"Install/Set path manually"""" in a second page.""",Bug,Node
400481,"""As a developer I want to debug my Alloy projects using the Studio debugger""",Epic,Debugging
401120,"""I am using Titanium as a university student, by sharing a provisioning portal for my institution.  This allows for educational use without paying the $99/year for a full iOS Developer account.    The problem I have is that Titanium Studio is too knowledgeable of the commercial iOS developer program web site, and automates a few steps that I need to perform manually.    I need to be able to input a developer certificate from the filesystem, as well as a provisioning profile also from the filesystem.  The wizards should not prevent me from doing this, even though Titanium's test for me being in the iOS developer program fails (wizard error """"Please sign up for the iOS Developer Program"""").  I can't even get by with editing the Launch Configuration.    Any advice would be much appreciated, including dirty tricks like editing Titanium Studio config files, etc.  """,Improvement,iOS
401385,"""As a Ti user, I want to be able to package an iOS app without using MacOS. Unless we have a standardized shell we can drop a web view into, it's unclear how feasible this.""",New Feature,Publishing
401387,"""As a Titanium user, I would like to be able to view my GitHub repos inside Studio. I want to be able to download and commit my changes easily    * Register my GitHub account.  * When importing a Git project, show a list of the current GitHub projects in a drop down, and allow me to pick one to clone.  * If publishing a new project, allow me to upload the new project to GitHub without leaving the IDE.""",Story,Publishing
401388,"""As a Ti user, i want to be able to modify the font size and style of every view""",Improvement,Editor
401694,"""When you get a crash (platform dependent), we need to do a better job of telling / helping the user.  Maybe you get a better stack trace or at least a dialog or something.  This will likely be platform specific behavior on detection and unified dialog, messaging, etc.      For example, in iOS when it crashes, you'll get a nice crash log.  It would be nice to show that to them and potentially allow them to submit to us the crash log details (and potentially the app itself to reproduce) as a checkbox -- sort of like apple does.  I'd like to draw a distinction between this and crash detection libraries built into the app.  I'm thinking this is more a wrapper inside studio code that's smart enough to detect a failure and then provide some level of developer help.""",Improvement,Debugging
402014,"""When a run/debug configuration is created, we need the ability to pass additional command-line arguments to the android emulator.    As an example, the -scale argument in order to use skins whose full scale causes them to be larger than the screen you are running on. For example, I am using the Motorola MD860 skin for the Atrix 4G whose screen is 960 pixels high. The resulting emulator window is some 1200 pixels high but my screen is only 900 pixels high. If you run the AVD from the AVD Manager, it offers the option to run with a particular screen size which uses the -scale argument. We need the capability to do this in Studio. I would recommend simply adding an extensible option to specify any additional command line arguments. The standard Eclipse launch configurator allows this on the configuration's Target tab but that appears to be disabled in Studio.    Please re-add the tab and allow those arguments to be passed to the emulator.    Note: This can only be allowed on SDK versions that support the feature...i.e this won't work on 1.8, but should work on 2.0  """,Improvement,Android
402024,"""First scenario:  1) Create a new project  2) Import a large file    Results:  The whole system slows down and TiStudio becomes very slow to respond.  A lot of the code does not have its normal color.      Second scenario:  1) Create a new project  2) Copy the win1 variable and begin pasting it many times (I did it until my .js file was about 40k lines long).  3) Press undo repetitively    Results: Once again the system slows down, the CPU usage spikes, and many of the variables are not colorized.     Notes:  I've tested on a few different systems, and how large a file needs to be to slow cause this depends on each system configuration.  Faster computers need larger files.  For testing purposes I have attached a 20k, 40k, and 160k line file.    """,Bug,Editor
403599,"""We use a *lot* modules (~ 40, about half/half iOS/Android) in our project - many of them are maintained by ourselves which is fine. We love the performance of them, but find it nearly impossible to debug them in a proper way.    *iOS*    For iOS, we used to create a demo project that includes a pre-built version of module, open the generated project, drag the app.js to the {{Resources}} group, drag the module project into the Xcode project, link the module product into the main target and add the module project as a build dependency aaaand build. Sounds complicated? It is! Unfortunately, this is not even possible since 7.5.0 because of the new """"ti.main"""" file that is not part of the project so it either needs to be copied from the SDK source to the Xcode project or just be renamed in the kroll core to """"app.js"""" so it directly accesses the app.js. In SDK 8, this is also not possible anymore, because the TitaniumKit.framework manages the kroll-core, so it's precompiled already and cannot be changed during a generated project (except ripping it out again and re-referencing the project again). Still can follow? Ok, here is the suggestion: Fix the generated project (!). This has been discussed in numerous other tickets, Slack threads and StackOverflow posts. People should be able to just open  the generated project and do whatever they want to with it. Practically, this means that all source files should be linked as resources *or* be copied to the product with a simple build-phase script. This would solve most of our iOS issues regarding developer-experience already.    *Android*    For Android, we still need to trial-and-error every module change, means to change something in the *.java files, compile it, add it to a project, clean the project, do a full build and see what comes out of it. I heard of a way to integrate modules into Android Studio, but there is zero documentation about that. As a developer, I'd love to just open the module directory in Android studio and Android studio would link all necessary libraries to it, so the module runs.    Bonus: The best would be to just pick up the {{example/app.js}} that is available in every module project and link that one to a """"module demo project"""". Maybe a new ti command or npm package could be """"ti build -p ios --test-module"""" from the module project and it would be smart enough to a) check the directories for ios/android and b) check the example/app.js for an execution point and then just run the project. Something similar (I think) is already done when using """"ti build"""" alone, which uses the app.js as well, but it doesn't link anything, so debugging is not possible either.    Sorry for the long text, but I hope I get can something started with this! Maybe it's not too hard to do, as many of the required components are already in place and """"just"""" need to be combined. Thx!""",Story,"iOS,Android"
403600,"""I'm experiencing what I believe is a bug in the Hyperloop in Android only. We have multiple apps that share a similar codebase. We have a lot of this code in one directory and we   symlink it to all of the other projects. If I use Hyperloop in a file that lives inside this symlinked directory to require a class it throws an error during build. If I run the project that contains the actual directory, not just a symlink, it works just fine.     The error I'm seeing.     {code:title error}  [ERROR] :  TiExceptionHandler: (main) [228,228] ti:/module.js:303  [ERROR] :  TiExceptionHandler:  throw new Error('Requested module not found: ' + request); // TODO Set 'code' property to 'MODULE_NOT_FOUND' to match Node?  [ERROR] :  TiExceptionHandler:  ^  [ERROR] :  TiExceptionHandler: Error: Requested module not found: android.view.View  [ERROR] :  TiExceptionHandler:     at Module.require (ti:/module.js:303:8)  [ERROR] :  TiExceptionHandler:     at Module.global.Module.require (/ti.internal/extensions/binding.js:33:34)  [ERROR] :  TiExceptionHandler:     at require (ti:/module.js:570:15)  [ERROR] :  TiExceptionHandler:     at /shared/Example.js:1:102  [ERROR] :  TiExceptionHandler:     at Module._runScript (ti:/module.js:612:9)  [ERROR] :  TiExceptionHandler:     at Module.load (ti:/module.js:108:7)  [ERROR] :  TiExceptionHandler:     at Module.loadJavascriptText (ti:/module.js:457:9)  [ERROR] :  TiExceptionHandler:     at Module.loadAsFile (ti:/module.js:512:15)  [ERROR] :  TiExceptionHandler:     at Module.loadAsFileOrDirectory (ti:/module.js:429:20)  [ERROR] :  TiExceptionHandler:     at Module.require (ti:/module.js:262:17)  [ERROR] :  TiExceptionHandler:  [ERROR] :  TiExceptionHandler:     org.<USER>kroll.runtime.v8.V8Runtime.nativeRunModule(Native Method)  [ERROR] :  TiExceptionHandler:     org.<USER>kroll.runtime.v8.V8Runtime.doRunModule(V8Runtime.java:162)  [ERROR] :  TiExceptionHandler:     org.<USER>kroll.KrollRuntime.runModule(KrollRuntime.java:207)  [ERROR] :  TiExceptionHandler:     org.<USER>titanium.TiLaunchActivity.loadScript(TiLaunchActivity.java:97)  [ERROR] :  TiExceptionHandler:     org.<USER>titanium.TiRootActivity.loadScript(TiRootActivity.java:414)  [ERROR] :  TiExceptionHandler:     org.<USER>titanium.TiLaunchActivity.windowCreated(TiLaunchActivity.java:174)  [ERROR] :  TiExceptionHandler:     org.<USER>titanium.TiRootActivity.windowCreated(TiRootActivity.java:283)  [ERROR] :  TiExceptionHandler:     org.<USER>titanium.TiBaseActivity.onCreate(TiBaseActivity.java:767)  [ERROR] :  TiExceptionHandler:     org.<USER>titanium.TiLaunchActivity.onCreate(TiLaunchActivity.java:167)  [ERROR] :  TiExceptionHandler:     org.<USER>titanium.TiRootActivity.onCreate(TiRootActivity.java:260)  [ERROR] :  V8Exception: Exception occurred at ti:/module.js:303: Uncaught Error: Requested module not found: android.view.View  {code}    The class I'm requiring in the call stack is a third-party AAR that is in my platform/android directory, but it still throws an error when requiring a default android class like android.view.View.     I also noticed that in build/hyperloop/android/js, it builds a corresponding js file for every needed android class, but when the require is inside a symlinked directory the necessary file doesn't get generated. I figured as a workaround I could require the   class in a file that is not symlinked so it would generate the bindings needed and then I would be able to require it again in the symlinked directory, but this was not the case and it still threw an error.     Here's roughly what our directory structure looks like:     App1   /Resources   /app1code   /shared     App2   /Resources   /app2code   /shared <- this is a symlink to App1/shared, requiring an android class in here causes an error     *Steps To Reproduce:*  1. Just create two new projects manually.   2. Pase this code in app.js file for both App.  {code}  var tabGroup = Ti.UI.createTabGroup();  var Example = require('/shared/Example');  {code}    3. Then create a directory called """"shared"""" in one app and take the attached """"Example.js"""" file  and put it in the """"shared"""" directory.   4. Then symlink that directory in the  other project using 'ln -s /path/to/shared /path/to/symlink'. That's all I  did.   5. Now run those Apps.  6. First App is working perfectly.  7. Second App is throwing an error.      Again, this does not seem to be an issue on iOS, only Android.""",Bug,"Android,Hyperloop"
403633,"""I'm in the <USER>of updating our test suite to use the latest {{should.js}} library, which now uses a stricter equality check. As a result, the tests are now flagging a number of windows UI tests as failing due to comparisons against expected numeric values. (the comparison used to do {{==}} but now do {{===}} so won't coerce to Number)    Turns out we're reporting string values for top/left/right/bottom always on Windows where we report Numbers on Android/iOS. The properties themselves are Number or String, so Strings can be valid but would typically be reported when followed by the """"unit"""" (i.e. """"px"""", """"em"""", """"%"""").    For parity's sake, I think it'd be good to report the values as Numbers when they're purely numeric/have no units explicitly attached.""",Bug,Windows
403807,"""We've achieved parity between iOS and Android with the various Ti.Blob imageAs methods, and the results are much more consistent and make more sense visually.    The unit tests written as a result of that work now fail for Windows, which means we likely have parity issues there. I've since skipped the tests on windows.""",Improvement,Windows
403987,"""Since 7.3.0.GA+ I'm seeing an error when calling setRemoteBackup(bool) on an individual File object. """"Could not configure remote backup: The file “mydatabase.db” couldn’t be opened.""""  (Setting the flag on a folder works fine,)    Looking at the code in TiFilesystemFileProxy.m around line 570, it appears that the object is setting the flag correctly (on the 'URL'), but then continues to treat the object as a folder, get the contents then output (not throw) an error if no contents are found. (Which a single file will always fail on).    It's not a fatal error, but the code just shouldn't have got that far to output the ERROR message.    A quick fix would be to change the setRemoteBackup method to :     {code:objc}  - (void)setRemoteBackup:(id)value  {    ENSURE_TYPE(value, NSNumber);    BOOL isExcluded = ![TiUtils boolValue:value def:YES];      NSNumber *isDirectory;      BOOL success = [[NSURL fileURLWithPath:[self path]] getResourceValue:&isDirectory forKey:NSURLIsDirectoryKey error:nil];      if (success && [isDirectory boolValue]) {          [self addSkipBackupAttributeToFolder:[NSURL fileURLWithPath:[self path]] withFlag:isExcluded];      } else {          [self addSkipBackupAttributeToItemAtURL:[NSURL fileURLWithPath:[self path]] withFlag:isExcluded];      }  }  {code}          """,Bug,iOS
404087,"""I have a module where I want to configure an SDK by passing the current activity to it automatically  on app start. We have the {{onCreate}} method via the {{@Kroll.onCreate}} annotation, but neither of the following methods worked there:  - {{getActivity()}} (because of the static context)  - {{TiApplication.getAppRootOrCurrentActivity()}} errors:   {quote}  No valid root or current activity found for application instance  {quote}  - TiApplication.getInstance().getCurrentActivity(): Same as above    I also tried invoking them in the constructor of the module but that did not work either. I am trying to prevent a manually {{initialize()}} call from the module, so I am wondering if there is a better approach here.""",Bug,Android
404252,"""I'm launching Google Fit permissions window but once the user has accepted there's no way know the result, onActivityResult is never called.    Here is my code    {code:java}     var windowActivity = new Activity(activity);        windowActivity.onActivityResult = function(requestCode, resultCode, intent) {          console.log(""""result: """" + requestCode);      };        if (!GoogleSignIn.hasPermissions(GoogleSignIn.getLastSignedInAccount(windowActivity), fitnessOptions)) {          GoogleSignIn.requestPermissions(              windowActivity,              REQUEST_OAUTH_REQUEST_CODE,              GoogleSignIn.getLastSignedInAccount(windowActivity),              fitnessOptions          );      } else {          //google fit ready to access      }  {code}    No error happens, it's just being ignored    Environment: SDK7.0.1, Hyperloop 3.0.1/3.0.2. Google Play Fitness API 11.8.0. Testing in Google Nexus 5X running Android 8.1.0 let me know if you need more details  """,Bug,Hyperloop
404332,"""One major task resulting of TIMOB-25927 is that we want the framework to be added as a target dependency of the main Xcode project. This is possible via Xcode > Build Phases > Target Dependencies. The problem is that we cannot just generate a .framework that works will all configurations.     Precompiler flags like IS_XCODE_XXX or USE_TI_XXX are necessary in situations where we want to rip out portions of the SDK for unsupported Xcode versions and unused API's, primarily to save file size and prevent apps from being rejected from the App Store.     Thats why the CLI has to detect certain project changes and trigger a new build if necessary. In theory, this could already be done by Xcode, detecting changes in the framework and recompiling its parts automatically. But this should be validated, so we can ensure the build pipeline is not broken for recurring and clean builds.""",Story,"CLI,iOS"
404566,"""I'm using a Toolbar as an ActionBar and I want to show the left drawer menu when clicking on the navigation icon, but the problem is that nothing happens when I click it.    Check my code below.    view/xml:  {code:xml}  <Alloy>   <Window class=""""container"""" id=""""window"""" onOpen=""""open"""" title=""""Directions List"""" >        <DrawerLayout id=""""drawerLayout"""">          <LeftView>           </LeftView>          <CenterView>      <View>       <Toolbar        title=""""Directions List""""        width=""""Ti.UI.FILL""""        elevation=""""10""""        top=""""0""""        barColor=""""#db4337""""        titleTextColor=""""white""""        displayHomeAsUp=""""true""""        homeButtonEnabled=""""true""""         id=""""actionBar""""        navigationIcon=""""/icons/nav_menu_white.png"""" / >                 <Label text=""""test"""" />             </View>       </CenterView>    </DrawerLayout>     </Window>  </Alloy>  {code}    style/tss:  {code:css}  """"Window"""":{   theme : 'AppTheme.TransBtns.Drawer',   windowSoftInputMode: Ti.UI.Android.SOFT_INPUT_STATE_HIDDEN  }  {code}    android theme:  {code:xml}  <style name=""""AppTheme.TransBtns.Drawer"""" parent=""""Theme.AppCompat.Light.DarkActionBar"""">       <item name=""""windowActionBar"""">false</item>          <item name=""""windowNoTitle"""">true</item>   <item name=""""android:buttonStyle"""">    @style/Widget.AppCompat.Button.Borderless.Colored   </item>  </style>  {code}    controller/js:   {code:javascript}  function open() {   var activity = $.window.getActivity();      if (activity) {    var actionBar = activity.getActionBar();    if (actionBar) {     actionBar.displayHomeAsUp = true;     actionBar.onHomeIconItemSelected = function() {      $.drawerLayout.toggleLeft();     };    }   }  }  {code}    I tried showing on console the value of activity.getActionBar() and apparently it shows the value of the original ActionBar, the one which isn't visible, instead of showing the Toolbar.    I also tried this but nothing happens:  {code:javascript}  // $.actionBar is the id of the Toolbar  $.actionBar.onHomeIconItemSelected = function() {   $.drawerLayout.toggleLeft();  };  {code}""",Bug,Android
404823,"""I'm using Titanium SDK 6.2.2.GA with Xcode 9 and iOS 11.    I'm testing the app with the new SDK and Xcode and I noticed that my app started crashing a lot. After some debugging I was able to see that the crash was happening when I close a Window that has a TableView with data binding.    I ran the app using Instruments with the Zombie profile and the crash is happening because is finding a zombie in the   {code:Objc}  TiUITableView dealloc  {code}   on this line:    {code:Objc}  [headerViewProxy setProxyObserver:nil];  {code}      I'm also getting crashes on this line:  {code:Objc}  [footerViewProxy setProxyObserver:nil];  {code}    My app crashes at least 1 of 3 times when closing windows that have TableViews.      """,Bug,iOS
404899,"""This is similar to the one I reported earlier this year, related to iOS 11: When running an app for the iPhone X (maybe others as well, not tested so far), the build succeeds, the app is installed, but not launched. Instead, the CLI session is terminated and the console stops.     Also, when deploying to the iPhone X simulator while it is already running, it get's killed and relaunched, after it fails launching again. This is not only a major UX regression, but also makes it impossible to debug any iOS app.    I need to find the other ticket again, but it looks like the Sim-status indicates it is shutting down and therefore we kill it. Xcode 9 hopefully has a different signal-system to handle this.""",Bug,iOS
404906,"""There are some utility API's in the iOS 11 GM seed that we might want to add to the SDK:  - {{prefersHomeIndicatorAutoHidden}}    - Returns a Boolean indicating whether the system displays a visual indicator for returning to the Home screen.    - Could be exposed as a Ti.UI.Window boolean-property """"homeIndicatorAutoHidden""""    - {{setNeedsUpdateOfHomeIndicatorAutoHidden}}    - Notifies UIKit that your view controller updated its preference regarding the visual indicator for returning to the Home screen.    - Could be exposed as a Ti.UI.Window event """"homeindicatorupdate""""  """,New Feature,iOS
405031,"""One of my apps uses the Ti.ICloud module, which works great, if you have the SystemCapabilities properly configured. The Xcode project generated by Titanium doesn't have the proper capabilities turned on. So I have to `--build-only`, run some regular expressions to fix the SystemCapabilities, and then `fastlane gym` my project to build and upload it as a part of my CI <USER> Outside of CI, I'd have to build from Xcode directly, if I want iCloud to work right.    If I run my regular expressions, and then do another `appc ti build`, my changes to project.pbxproj are wiped out, so that doesn't work either.    Is there any way to customize the capabilities in the generated project?    {code}  SystemCapabilities = {   com.apple.iCloud = {    enabled = 1;   };   com.apple.InAppPurchase = {    enabled = 1;   };  };  {code}""",New Feature,iOS
405412,"""I am not providing a testcase here since i think its time that <USER>brings forward a working testcase for the custom URL schemes because there have been so many broken examples.    There have been various issues created and (supposedly) fixed in JIRA, like TIMOB-15253, TIMOB-20490 and TIMOB-20459, but issues still seem to exist in the current build (6.1.0.GA as of speaking).    Please provide:  - The absolute minimum setup for _tiapp.xml_ in order to start receiving intents through custom URL schemes  - The usage and workings of """"intent-filter-new-task"""" versus """"android:launchMode"""".  - Test this case with the app being """"resumed"""" from background and """"newly created"""".    If this can be created into a page for the Titanium Documentation than that would be great.    The end result should feature an application that is always a single instance, that is:  - Opening an intent from the browser like _<a href=""""myap://somedatafortheapp"""">Link</a>_ should +always+ re-use the existing instance if there is one. If not, it should open a new instance.  - A splash screen should only be shown when the app is completely shut down.  - In absolutely no case should there be multiple instances of the same app. In my tests i've come across multiple instances of the app after opening a custom URL. I was able to identify this due to the logging of events.    By looking at the comments of the TIMOB's mentioned above and by my own experience, I know there are still bugs regarding this functionality. Issues seem to be closed for sake of progression rather than actually fixing the issue.""",Bug,Android
405428,"""In SDK 6.0.0, we updated the default minimum iOS target to 8.0 as a result of the Xcode 8.x version that is now supported. Doing that, our iOS SDK core shows some deprecation-warnings regarding API's that should be replaced in iOS 8 and later.  Replace ABPeoplePickerNavigationController (in Ti.Contacts, replacement straight forward)    This is subset of ticket TIMOB-24335.""",Story,iOS
405437,"""In SDK 6.0.0, we updated the default minimum iOS target to 8.0 as a result of the Xcode 8.x version that is now supported. Doing that, our iOS SDK core shows some deprecation-warnings regarding API's that should be replaced in iOS 8 and later.  Remove old UIAlertView and UIActionSheet API's.  This is subset of ticket TIMOB-24335.""",Story,iOS
405438,"""In SDK 6.0.0, we updated the default minimum iOS target to 8.0 as a result of the Xcode 8.x version that is now supported. Doing that, our iOS SDK core shows some deprecation-warnings regarding API's that should be replaced in iOS 8 and later.  Replace NSDayCalendarUnit with NSCalendarUnitDay etc. (search and replace, easy ones)  This is subset of ticket TIMOB-24335.""",Story,iOS
405590,"""{quote}As a developer,  I want to know which iOS simulator device type is being used,  So that I can use this information for app logic and testing.{quote}    I would prefer that Platform.model returned the device type and have a Platform.isSimulator property.   I realize this would be a breaking change so adding an additional property such as Platform.simulatorModel might be a safer option.      """,New Feature,iOS
405659,"""{quote}As a developer,  I want to be able to publish/execute the last successful build to my simulator/device without rebuilding,  So that I can more quickly test apps and features.{quote}    A lot of times, developers have multiple apps that they might be testing (with or without LiveView) and they want a way to quickly test or start an app without having to rebuild.       I think having a command like """"run"""" and an options like """"--skip-build"""" would get us much closer to this.      """,New Feature,iOS
405762,"""When including an external AAR to an Titanium Application and trying to use it, you may face issues if it has a dependency on the v4 and v7 libs.    *Steps to reproduce*:  1. Create an empty Android module. This can be done on Android Studio.  2. In this module, include the support v7 library in the build.gradle file:    {code:java}  compile 'com.android.support:appcompat-v7:23.4.0'  {code}    3. Create a new class that extends from *AppCompatActivity*.  4. Define this activity in your module's *AndroidManifest.xml* file.  5. Build the module into an *.aar* and place it inside an Titanium project with Hyperloop.  6. Inside your Titanium app, create a simple window with a button.    {code:java}  var testactivity = require('com.test.ActivityTest');    $.index.open();    function onButtonClick(e) {      var intent = Ti.Android.createIntent({          className: 'com.test.ActivityTest',      });        Ti.Android.currentActivity.startActivity(intent);  }  {code}    7. Execute the project and try launching your external activity. The error below occurs because the JS wrapper of your external activity at some point requires the *AppCompatActivity* class and can't find it.    {code:java}  Uncaught Error: Requested module not found: ./android.support.v7.app.AppCompatActivity  {code}    Launching an external activity with this method works as long as it doesn't depend on an external lib (like v4 or v7).    I've tried manually adding the v4 and v7 libraries to my Titanium project, but doing so results in several errors just like this:      {code:java}  [ERROR] Failed to package application:  [ERROR] /var/folders/1j/7_wc2wx52yzggclqvnw1wpjh0000gn/T/117029-2405-16cw5bj/res/values/attrs.xml:123: error: Attribute """"actionBarSize"""" already defined with incompatible format.  [ERROR] /Users/Rodolfo/Documents/Workspace/app-sandbox/build/hyperloop/android/appcompat-v7-23.4.0/res/values/values.xml:82: O  riginal attribute defined here.  {code}      Example Titanium project and Android module attached to this ticket.""",Bug,Hyperloop
405919,"""I'm trying to use the Dropbox Official cocoa pod with Hyperloop and found an issue while trying to require an specific file with a special character """"+"""".  The official Dropbox documentation tells us to require only one file:    {code:java}  #import <ObjectiveDropboxOfficial/ObjectiveDropboxOfficial.h>  {code}    I know I can't require only this single file with Hyperloop (Which make the things harder for developers) and I have to require every single file I'm willing to use, so I've created the following requires:    {code:java}  var DropboxClient = require('ObjectiveDropboxOfficial/DropboxClient');  var DropboxClientsManager = require('ObjectiveDropboxOfficial/DropboxClientsManager');  var DropboxTeamClient = require('ObjectiveDropboxOfficial/DropboxTeamClient');  var ObjectiveDropboxOfficial = require('ObjectiveDropboxOfficial/ObjectiveDropboxOfficial');  var MobileAuth = require('ObjectiveDropboxOfficial/DropboxClientsManager+MobileAuth');  {code}    All of them work fine apart from the last one """"/DropboxClientsManager+MobileAuth"""".   All of them are specified as a """"public header"""".     I do think the issue might be the special character when Hyperloop created the wrappers.     This is my podfile:    {code:java}  install! 'cocoapods',           :integrate_targets => false    platform :ios, '8.0'  target 'tidropbox' do      pod 'ObjectiveDropboxOfficial'  end  {code}  """,Improvement,"iOS,Hyperloop"
406189,"""Android emulators that are created with Android Studio are not being recognized correctly. Because the Android Studio provides pretty fast emulators since a while, we might want to fix this issue.    Steps to reproduce:  1. Create a new emulator through Android Studio (e.g. Nexus 5, API level 23)  2. Check your environment with {{appc ti info}}  3. Note the android warning (also typo in """"Uknown""""):  {code}    The Android emulator """"Nexus_5_API_23"""" has a problem:       Uknown error  {code}  4. Run a new android-project with {{appc run -p android}}    Expected behavior: The CLI finds the Android emulator, starts the app  Actual behavior: An error log is thrown:  {code}  [ERROR] Unable to find any emulators    Please create an Android emulator, then try again.  {code}    I'm not sure if we ever supported the simulators from Android Studio, but I'm pretty sure we did in the past. And since Genymotion is not licensed for free non-private projects anymore, supporting the """"official one"""" should be a good reason to check this issue. Thanks!    Full log:  {code}  Hans-Macbook-Pro:hyperloop-examples <USER> appc run -p android  <USER>Command-Line Interface, version 5.4.0  Copyright (c) 2014-2016, <USER> Inc.  All Rights Reserved.    [INFO]  tiapp.xml <sdk-version> set to 5.4.0.GA, but current Titanium SDK set to 5.5.0.v20160904203801  [INFO]  Forking correct SDK command: """"/usr/local/bin/node"""" """"/Users/<USER>.<USER>install/5.4.0/package/node_modules/titanium/lib/titanium.js"""" """"build"""" """"--sdk"""" """"5.4.0.GA"""" """"--no-banner"""" """"--config-file"""" """"/var/folders/s_/lmtv34b926j0m22fx95ppz980000gq/T/build-1473442005789.json"""" """"--platform"""" """"android"""" """"--project-dir"""" """"/Users/<USER>Documents/Apps/hyperloop-examples"""" """"--log-level"""" """"info"""" """"--android-sdk"""" """"/opt/android-sdk"""" """"--target"""" """"emulator""""    [ERROR] Unable to find any emulators    Please create an Android emulator, then try again.  {code}""",Bug,Android
406250,"""iOS modules have a {{titanium.xcconfig}} which has a {{TITANIUM_SDK}} variable with a baked in absolute path. The problem is if the SDK moves or is updated, then this path is not updated.    A better solution is to default {{TITANIUM_SDK}} to the absolute path, but then overwrite it with an environment variable set by the iOS module build with the absolute path to the current SDK used to build the module.    I'm not 100% sure how to structure the xcconfig file so that the default absolute path can be overwritten by the env var, but it may be as easy as setting the {{TITANIUM_SDK = $(TI_SDK_PATH)}} in the {{titanium.xcconfig}}, then set the create a new {{defaults.xcconfig}} with a {{TITANIUM_SDK = /path/to/sdk}} and make sure the {{default.xcconfig}} is loaded before the {{titanium.xcconfig}}.    If this is not possible, then we can just resolve this ticket as invalid.""",Improvement,iOS
406340,"""I would like to raise this ticket to investigate what we would need in order to support extensions (share-extensions, watch-extensions, tvOS-extensions, messenger-extensions, ...) in Titanium using Hyperloop.     Obviously, we need a CLI for managing the linking of different targets to the main Titanium application as well as a directory structure to place extensions and their assets.    A new CSPEC might be the correct way to collect the different thoughts on this.""",Story,"iOS,Hyperloop"
406638,"""We have a long-standing issue with how Titanium CommonJS modules are require'd.    One would assume they'd follow typical NodeJS conventions with requires, so when you do:  {code:javascript}  require('module.id');  {code}    I would expect that it should attempt to load up {{modules/commonjs/module.id/<version>}} as a directory as per TIMOB-23382    Instead, our Android and iOS implementation looks for {{modules/commonjs/module.id/<version>/module.id.js}}, and ignore any package.json, index.js, or index.json in that directory.  """,Bug,"iOS,Android"
406804,"""Android Studio 2 is has a much better developer experience for keeping the SDK/NDK up to date and creating AVDs based on actual devices.    However, when I create an AVD in Android Studio I can successfully launch it from there, but they are not visible in <USER>Studio and when I run {{ti info -t android}} I get:    {code}    !  The Android emulator """"Nexus_5_API_23"""" has a problem:       Uknown error      !  The Android emulator """"Nexus_5X_API_22"""" has a problem:       Google Nexus 5X no longer exists as a device      !  The Android emulator """"Nexus_5X_API_23"""" has a problem:       Google Nexus 5X no longer exists as a device      !  The Android emulator """"Nexus_5X_API_23_2"""" has a problem:       Google Nexus 5X no longer exists as a device  {code}    The same errors (of course) show when I run {{android list}} directly.    When I open the stand-alone AVD manager via {{android avd}} the AVDs are listed but cannot be started. When I open _Edit_ them it displays an error _No device selected_ and _CPU/ABI_ has _No system images installed for this target._    Once I've started an AVD via Android Studio {{ti info -t android}} (and <USER>Studio) will show the emulator as a connected device:    {code}  Connected Android Devices  Android SDK built for x86    ID                          = emulator-5554    State                       = device    SDK Version                 = 6.0 (android-23)    ABIs                        = x86,  {code}    I can then successfully build to this emulator.""",Bug,Android
406862,"""h3. I'm using the latest SDK but this issue has been there since the release of San <USER>font. First of all, the semibold weight doesn't work, it renders a normal (regular) font (I think it would be a good idea for Titanium to support more weights, this is very limited for people who care a lot about UI)  Second thing, the system font on iOS (San francisco) is a dynamic font, it uses custom tracking for different font sizes, this is also not working, the tracking is always 0. Even when I try to embed the font and use it as a custom font, all font weights work but none of the tracking is applied which causes a not so good font display.   I don't know if this helps, but if you want to see the difference between properly rendered fonts and wrongly rendered ones, you can try using any constant of the textStyle, they render perfectly well. """,Bug,iOS
406961,"""{quote}As a developer,  I want to be able to be able to use a different version (or custom build) of Alloy when using the <USER>CLI,  So that I can be able to patch urgent issues and critical defects while waiting for a new version of Alloy{quote}.    With the old build system (Titanium or Ti), I could simply run `npm install -g alloy@versionx` to get a different or custom build of Alloy used with a build.  Now with the new build system (<USER>or Appc), A specific version of Alloy is bundled with the <USER>Platform and there is not a simple way to change it on all our developer and build machines.    If there is known way of doing this already, please let us know!    {color:#d04437}We have an Enterprise license and this is critical for us right now as we have a memory leak that we need to patch in Alloy and get it into production quickly.{color}  """,Improvement,CLI
407260,"""5.1 added support for TextActions. Works fine for local notifications, unfortunately for Remote notifications the typedText value is ignored and not passed back to Javascript side.     I've submitted a pull request taking the code that was added for returning 'typedText' in local notifications and added it to the remote notification callback.  """,Bug,iOS
407421,"""JS is being used within a webview to modify and/or return information about the contents. Above a certain yet-to-be determined size, it is now timing out (as of Android M). At first thought the issue might be Titanium 5.0; but reverting to the previous 3.5.1 (as I had skipped 4.x due to a host of other issues) didn't fix the problem.    For instance, I'm loading jQuery on the pages with the webview, and calling it externally (i.e., from the Titanium JS via evalJS()). One use is to set the body font. On most HTML files, it works. On a larger one (again, above some threshold), it times out with the warning from getJSValue() in TiWebViewBinding.java: """"Timeout waiting to evaluate JS"""". The same is the case for *any* evalJS() on the webviews holding larger HTML content.    Now, I don't know if this is a Titanium problem or an Android problem, so at this point I'm trying to gather enough information about how exactly Titanium does its evalJS() call — and how exactly it times out — to triage what's going on. So, for instance, in getJSValue(), by the time that try {} block is hit and we're waiting for returnSemaphore.tryAcquire(3500, TimeUnit.MILLISECONDS)) to time out, presumably the JS call has already been executed (and, in this case, failed for whatever reason)?    Where exactly does the JS call get dispatched to the webview? In AppBinding? I assume the (relative) complexity of this is due to the async nature of the webview vs. evalJS() waiting for a  return value (hence the semaphore and timeout)?    I'm faintly hopeful that increasing the timeout, or allowing the option to do so, might alleviate the problem — but not very. Because the effect of the failed JS call — such as the aforementioned jQuery call to change the body font — is never seen.  """,Bug,Android
408313,"""A long time ago, Titanium's iOS build was written in Python. The iOS build performed 3 major steps:    # Create all Xcode related files in build dir  # Run xcodebuild to compile project, which in turn fires the Xcode project's pre-compile phase script that calls the iOS build again to copy the Titanium-specific resources  # Xcode builds the final app and signs it    It was done this way so that you could open the generated Xcode project, modify JS files, and build from Xcode.    When we replaced they Python-based iOS build scripts with the Node.js version, there was a performance issue with Xcode calling the Titanium CLI to copy the Titanium resources. I changed things so that if you are building from the Titanium CLI, it would copy the Xcode project related files, then the Titanium related files, then finally call xcodebuild. This significantly sped things up.    The problem today is that when you run {{appc run}}, it generates a Xcode project who's pre-compile script references the Titanium CLI. Users do not have the Titanium CLI installed by default and thus it will fail. Even if they did, the encryption method would vary. Also the Xcode project's pre-compile should be calling the {{appc run}}, not {{ti build}}.    I'm proposing we remove the Xcode project's pre-compile script.    Pros:  * Remove cruft from the iOS build  ** There's a lot of code that accommodates the ability for Titanium resources to be processed when building from Xcode that we could rip out  ** As maintainers of the iOS build changes, not all of the knowledge is transferred and thus new contributors neglect the Xcode-specific build path  * Speeds up builds when building from Xcode  * Building from Xcode only really works when you have Titanium CLI installed  * Building via {{appc run}} encrypts differently than {{ti build}} and thus building from Xcode would render the new encrypted files unused and bloat the app  * Modifying Titanium JS files and building directly from Xcode is not officially supported    Cons:  * Deprecates a feature that has existed for years""",Improvement,iOS
408353,"""Something changed starting with Ti SDK 3.5.0 that affects how the translucent navigation bar is rendered on iOS.  With Ti SDK 3.4.1 and below, using the settings on the included sample app.js, the nav bar always rendered dark with light text.  The same code on 3.5.0+ always renders the nav bar light, with dark text.      One thing I've noticed is that changing the value of {{statusBarStyle}} with 3.4.1 (and below) does affect how the nav bar is rendered and toggles it from dark to light depending on the setting. This seems to be the correct behavior, however doing this has no effect with 3.5.0+ and the nav bar *always* renders light.    Hopefully there's a quick work around for this. Our app has a dark theme with heavy use of translucency and we cannot upgrade to 3.5.0+ until we get this solved.    Setting {{barColor: '#000000'}} is not ideal because you loose almost all of the translucency effect, so we keep this property {{undefined}} to get the best translucency effect.  """,Bug,iOS
408446,"""h2. Problem  The Ti.UI.ATTRIBUTE_FONT part of TIMOB-15998 doesn't appear to be working with custom fonts. TIMOB-15998 didn't demonstrate custom fonts working. The built-in font Roboto and its variants are working, as well as the built-in families monospace and serif. However, custom fonts placed in app/assets/android/fonts as well as app/assets/fonts are not working.    Below, I'm expecting to see the first line of text, """"Chantelli Antiqua,"""" to be displayed in the custom font, which is more of a serif font than the sans-serif shown. Instead, however, we see the text displayed in the default font.    h2. Test case  {code:lang=javascript|title=app.js}  var win = Titanium.UI.createWindow({   backgroundColor : """"#ffffff""""  });    var scrollView = Ti.UI.createScrollView({   height : Ti.UI.FILL,   width : Ti.UI.FILL,   layout : """"vertical""""  });    win.add(scrollView);    win.open();    var fonts = [""""Chantelli_Antiqua"""", """"serif"""", """"monospace"""", """"sans-serif"""", """"sans-serif-light"""", """"sans-serif-condensed""""];    _.each(fonts, function(font) {   var text = font;     var attr = Titanium.UI.createAttributedString({    text : text,    attributes : [    {     type : Ti.UI.ATTRIBUTE_FONT,     value : {      font : {       fontSize : 20,       fontFamily : font      }     },     range : [0, text.length]    },    {     type : Ti.UI.ATTRIBUTE_FOREGROUND_COLOR,     value : """"#000000"""",     range : [0, text.length]    }]   });     var label = Titanium.UI.createLabel({    top : 10,    left : 10,    right : 10,    height : Titanium.UI.SIZE,    attributedString : attr   });     scrollView.add(label);  });  {code}""",Bug,Android
408571,"""One of our Titanium apps uses ListViews extensively, including for building forms, where each ListItem contains a form control (TextField, Picker, etc...) If the form has more ListItems than the size of the screen, as you scroll down it causes issues where the values from certain TextFields are copied into other ListItems' TextFields.    This is clearly connected to the ListView's row caching, it's expected that it would cache the list rows so that you only have as many as are on the screen in memory, however it's clearly not setting the values correctly for each row as scrolling occurs.    I've boiled it down to a simple reproduction case below, simply input values into the first few rows (e.g. """"test 1"""", """"test 2"""", """"test 3"""") then scroll down and you'll see those 3 values populated into other rows.    {code:javascript}  var win = Ti.UI.createWindow({ backgroundColor: 'white' });    var defaultTemplate = {      childTemplates: [          {              type: 'Ti.UI.Label',              bindId: 'label',              properties: {                  top: '5dp', left: '5dp'              }          },          {              type: 'Ti.UI.TextField',              bindId: 'value',              properties: {                  top: '30dp', left: '5dp', right: '5dp', bottom: '5dp',                  backgroundColor: 'gray', color: 'black', height: '40dp'              }          }      ]  };    var listView = Ti.UI.createListView({      top: '20dp',      templates: { 'default': defaultTemplate },      defaultItemTemplate: 'default',      rowHeight: '70dp',      backgroundColor: '#eff3fa',      showVerticalScrollIndicator: true,      separatorColor: '#e3ece7', separatorInsets: { left: 0, right: 0 }  });    var items = [];  for (var k=0;k<40;k++) {      var label = 'Item ' + (k + 1);      items.push({          label: { text: label },          value: { text: '' }      });  }    var sections = [      Ti.UI.createListSection({          headerTitle: 'Only Section',          items: items      })  ];    listView.setSections(sections);    win.add(listView);  win.open();  {code}""",Bug,iOS
408582,"""Currently HAL has multiple ways to create JSExport object but basically following two API's are meant to do same thing.    {code}  auto object1 = js_context.CreateObject<Widget>();  auto object2 = js_context.CreateObject(JSExport<Widget>::Class());  {code}    I found there's no reason to keep the first one. It actually has a issue around TIMOB-18461.  """,Story,Windows
409240,"""If I run {{ti info}} with my Android device connected, it correctly tells me so:    {code}  $ ti info  ...  Connected Android Devices  SGH-M919    ID                          = 8fe13983    State                       = device    SDK Version                 = 4.4.2 (android-19)    ABIs                        = armeabi-v7a, armeabi  {code}    But when I build for Android, and let the CLI prompt me for the target device, the list doesn't include the connected device (""""SGH-M919"""" in this case), only emulators:    {code}  $ ti build -p android  ...  Which emulator do you want to launch your app in?  Android Emulators     1)  intel_nexus5 (4.4.2)     2)  Nexus4 (4.4.2) (Google APIs supported)     3)  Nexus4_1 (4.4.2)     4)  Nexus5 (4.4.2) (Google APIs supported)  Genymotion Emulators     5)  Custom Phone - 4.3 - API 18 - 768x1280 (4.3) (Google APIs support unknown)  Select an emulator by number or name:   {code}    I would expect the connected devices listed by {{ti info}} to be available as a build target. In Studio, the connected device does show up as an available target, as expected, just not in the CLI.""",Bug,"CLI,Android"
410408,"""On Mobile Web Buttons have two deficiencies.    1. Buttons are missing a font property, so you can't change the font size or face.  2. Buttons do not treat leading spaces in the """"title"""" the same as other platforms. HTML ignores the spaces, so button.title = """"   Some Text"""" would be treated as if it were button.title = """"Some Text"""". This is not true on Android or iOS    I'm combining these as I have a fix I'll submit as a pull request.""",Bug,MobileWeb
410942,"""Affected versions (Titanium SDK for iOS):    3.2.0.v20131204220843, 3.2.0.v20131102082008,  any version greater than 3.1.3.GA    iOS version : 7.x    Problem: Settings.bundle is not loaded into the settings app.    Attached is a sample app that demonstrates the issue.    To reproduce do the following:    - Reset iOS (iPad or iPhone) simulator    - Build the sample app using Titanium SDK 3.1.3.GA  exit the app, go to the settings app, notice that there is a setting for iosbug.    - Reset iOS (iPad or iPhone) simulator    - Clean the build    - Build the sample app using Titanium SDK 3.2.0.v20131204220843     - Observe there is no iosbug setting in the setting app    THIS IS THE LATEST NIGHTLY BUILD OF THE TITANIUM SDK.   I would consider this a critical bug. This feature has always worked before.      The zip file is 2.2MB because of default iOS graphics.      I have also included a simulator screenshot of the settings app screen when the titanium app is using 3.1.3.GA and 3.2.0.v20131204220843 the file names denotes which version you are viewing.  """,Bug,"CLI,iOS"
410988,"""If you do an Android device build, then do an emulator build without cleaning, the following error occurs:    {code}  [DEBUG] Removing old file: C:\Users\Thomas Anderson\testapp2\build\android\assets\alloy\backbone.js    C:\Users\Thomas Anderson\AppData\Roaming\npm\node_modules\titanium\node_modules\longjohn\dist\longjohn.js:184          throw e;                ^  Error: ENOENT, no such file or directory 'C:\Users\Thomas Anderson\testapp2\build\android\assets\alloy\backbone.js'      at fs.unlinkSync (fs.js:760:18)      at AndroidBuilder.<anonymous> (C:\ProgramData\Titanium\mobilesdk\win32\3.2.0.v20131127114913\android\cli\commands\_build.js:2810:7)      at Array.forEach (native)      at AndroidBuilder.removeOldFiles (C:\ProgramData\Titanium\mobilesdk\win32\3.2.0.v20131127114913\android\cli\commands\_build.js:2807:35)      at C:\ProgramData\Titanium\mobilesdk\win32\3.2.0.v20131127114913\node_modules\async\lib\async.js:548:21      at C:\ProgramData\Titanium\mobilesdk\win32\3.2.0.v20131127114913\node_modules\async\lib\async.js:224:13      at iterate (C:\ProgramData\Titanium\mobilesdk\win32\3.2.0.v20131127114913\node_modules\async\lib\async.js:131:13)      at C:\ProgramData\Titanium\mobilesdk\win32\3.2.0.v20131127114913\node_modules\async\lib\async.js:142:25      at C:\ProgramData\Titanium\mobilesdk\win32\3.2.0.v20131127114913\node_modules\async\lib\async.js:226:17      at AndroidBuilder.<anonymous> (C:\ProgramData\Titanium\mobilesdk\win32\3.2.0.v20131127114913\node_modules\async\lib\async.js:553:34)  {code}    This happens on Windows 8 while building an Alloy app, though I'm not sure if it's Windows or Alloy specific.""",Bug,Android
411041,"""On Ubuntu, we found that if node.js is installed via some sort of Debian package, that installs node.js as {{/usr/bin/nodejs}}. {{/usr/bin/node}} is a symlink to {{/etc/alternatives/node}} which is also a symlink to {{/usr/bin/nodejs}}.    The problem is {{<USER>args\[0\]}} is """"node"""", but {{<USER>execPath}} is """"/usr/bin/nodejs"""". The CLI compares these two to detect if the first arg is the node.js executable, then strip it off. Because """"node"""" != """"nodejs"""", the executable is not stripped off and the error happens.    {code}  [ERROR] """"/usr/bin/titanium"""" is an unrecognized command.  {code}    or    {code}  [ERROR] """"/usr/local/bin/titanium"""" is an unrecognized command.  {code}    To fix this, we need to properly resolve {{<USER>args\[0\]}}. That means that the CLI needs to call {{appc.subprocess.findExecutable(<USER>argv\[0\], ...)}} to convert """"node"""" to """"/usr/bin/node"""", then we need to run {{fs.realpathSync()}} on that result to resolve the symlinks to """"/usr/bin/nodejs"""" and finally compare that to {{<USER>execPath}}.    h4. Workaround    As a temporary workaround, you can copy /usr/bin/nodejs to /usr/bin/node.""",Bug,CLI
411255,"""h2. Problem    When the map is placed in a view hierarchy it can sometimes be placed below all views (even below the window background color). This happens for example when you animate the parent view position. But when placed in a quite large view hierarchy I've noted that it randomly can behave the same as described above.      Run the code on 4.2.x device with Google Play installed.  h2. Test case    {code:lang=javascript|title=app.js}  var win = Ti.UI.createWindow({     backgroundColor : 'transparent',     orientationModes : [Titanium.UI.PORTRAIT],     exitOnClose : true,     navBarHidden : true  });  win.open();     var view2 = Ti.UI.createView({      width : Ti.UI.FILL,      height : Ti.UI.FILL,      backgroundColor : 'transparent'  });     var view3 = Ti.UI.createView({      top : 0,      left : 0,      width : 500,      height : 200,      backgroundColor : 'yellow'  });  var label = Ti.UI.createLabel({      color:'black',      text: """"You should not see me if zOrderOnTop is set to true"""",      textAlign: Ti.UI.TEXT_ALIGNMENT_LEFT,      font: { fontSize: 30 },      top: 0,      left : 0  });  view3.add(label);     var MapModule = require('ti.map');  var mapview = MapModule.createView({      mapType:MapModule.NORMAL_TYPE,      //zOrderOnTop: true  });  view2.add(mapview);  view2.add(view3);  win.add(view2);     {code}    1. Run the above code, you should see yellow label on top  2. Uncomment zOrderOnTop property, run it again, you should NOT see yellow label. Depending on device, you may see the yellow label flashing for a little bit, as Android relayout its children. But once re-layout is done, you should NOT see the yellow label.    Fail case: yellow label on top of map  Expected case: should not see yellow label.    This only happens on the first launch of the app on certain devices (listed in environment).""",Bug,Android
411338,"""I'm testing a simple titanium app on an android avd obtained from a Xperia Z device definition (similar to samsung s4).    The app works but I got a wrong value for Titanium.Platform.displayCaps.dpi    The app report a wrong value for Titanium.Platform.displayCaps.dpi = 320 (avd has a dpi of 480).    If can help finding a solution the Ti.Platform.displayCaps.platformWidth correctly report 1080.    Attached AVD config""",Bug,Android
411463,"""h3. Steps to Reproduce  1. create 'location' event listener  2. Add Ti.API.debug output code to event listener  3. Run application several times to observe debug output results    h3. Actual Result  In a non-deterministic way, some runs result in debug output to the console, while others produce no output at all in the console, even from Ti.API.debug unrelated to the 'location' event listener, resulting in a totally blank console window. On a 'good' run I would expect to see output even before the first location event first triggers.    For a while I was convinced certain debug statements were causing the issue, but then I noticed conflicting results going back and forth between code revisions, until finally I started back to back runs on the same code and observed the reported behaviour. I have recently updated Titanium studio and the SDK, and while the issue arose around about that time, that was also about when I began writing the 'location' event code so I cannot say with certainty that the updates are part of the issue.    I have attached my index.js source file from my current project since it may be easier for you to reproduce by starting with it.    Obviously productivity hits rock bottom without reliable debug output, hence severity 'blocker'    h3. Expected Result  Debug output should display in the console.""",Bug,Android
411468,"""On iOS the proxy for each created ListView cell is generated on the Objective-C part when initiating the cell. It means that each reusable cell has its own proxies.    On Android it's different, the proxies are created on the JS side when creating the listview. This has 2 bad consequences:  # You can't modify the templates property after creating the view  # Each reusable cell share the same proxies.    The second part is the one bugging me the most. It's not a problem with the current UI widgets because each widget has a view attached to it and this is why the magic works. But in the future (in my case right now) you can imagine having proxies without views. My example is a shape module.    # I have a ShapeViewProxy + ShapeView  # In this ShapeViewProxy i can add ShapeProxies which are stored and drawn (Circle, rect …)    To have better performance I don't create a view per ShapeProxy, it's an abstract object.    The view works great but I can't make it work with the ListView. This is because the ShapeProxies are shared by all cells.    Let's say I want to change a shape color in ListView, I can have it record the change with bindId and DataItem but as a same proxy is the same for all cells, I can't update the reused cell correctly    If the proxies where created in the getView method like the iOS counterpart i could do that. I dont think it would be that hard to implement, my only problem to implement it is that right now, on the Java side, i can't map """"Ti.UI.View"""" to ti.modules.titanium.ui.proxies.ViewProxy and thus cant create the proxy on the Java side using the """"type"""" property of the template.    genBoostrap.py seems to have that map, but i dont really see how generate that map on the Java side.    With some help I could implement that.    """,New Feature,Android
411586,"""Adding a shadow to text is a very common request I get. For simple texts I can get away with stacking 2 labels, but as it gets more complex (alignment, wordWrap..) this gets more tricky.    I'm reading that Android {{TextView}} has a {{shadowLayer}} which seems to be able to do the job.    IMHO parity should be high priority.""",Bug,Android
411700,"""on iOS getEventsBetweenDates method is not working:  - it is misspelled: **getEventsBeteenDates** (missing """"w"""")  - if I call the method this way, no matter what I'm trying to put as arguments (dates or strings) is not working    if I put a strings (like """"2014-02-19T16:17:46+01:00"""") I get    {code}  """"*** -[EKEventStore predicateForEventsWithStartDate:endDate:calendars:]: startDate is nil"""";  {code}     if I put dates I get  {code}  message = """"Invalid type passed to function"""";  nativeLocation = """"-[TiCalendarCalendar getEventsBeteenDates:] (TiCalendarCalendar.m:157)"""";  nativeReason = """"expected: String, was: __NSDate"""";  {code}""",Bug,iOS
412044,"""addRow method of PickerColumn as per documentation must be passed with parameter as a row object.  Hence must give error if passed with unintended parameter.  iOS gives error but android is not giving any error.    Its not a Regression since it occurs in 3.1.1GA    Steps to reproduce:  1. Copy the sample code given below in app.js of titanium mobile classic project.  2. Click on button with title """"Add """"Manny"""".    Actual Result:    Both ios and android behave differently.  ios gives error.""""invalid type passed to function at app.js(line 93)""""  android does not gives error.    Expected Result:    Both ios and android must give error.""""invalid type passed to function""""    {code}  var w = Ti.UI.createWindow({      backgroundColor : 'white'  });  var status = Ti.UI.createLabel({      bottom : '5dp',      left : '5dp',      right : '5dp',      height : '40dp',      textAlign : 'center',      color:'black'  });  w.add(status);     function showStatus(s) {      status.text = s;  }     var names = ['Joanie', 'Mickey', 'Jean-Pierre', 'Gustav', 'Raul', 'Mimi', 'Emily', 'Sandra', 'Carrie', 'Chachi'];  var verbs = ['loves', 'likes', 'visits', 'loathes', 'waves to', 'babysits', 'accompanies', 'teaches', 'announces', 'supports', 'knows', 'high-fives'];       var rows1 = [];  for (var i = 0; i < names.length; i++) {      rows1.push(Ti.UI.createPickerRow({          title : names[i]      }));  }     var rows2 = [];  for ( i = 0; i < verbs.length; i++) {      rows2.push(Ti.UI.createPickerRow({          title : verbs[i]      }));  }     var rows3 = [];  for ( i = (names.length - 1); i >= 0; i--) {      rows3.push(Ti.UI.createPickerRow({          title : names[i]      }));  }     var column1 = Ti.UI.createPickerColumn({      rows : rows1,      font : {          fontSize : """"12""""      }  });  var column2 = Ti.UI.createPickerColumn({      rows : rows2,      font : {          fontSize : """"12""""      }  });  var column3 = Ti.UI.createPickerColumn({      rows : rows3,      font : {          fontSize : """"12""""      }  });     var picker = Ti.UI.createPicker({      useSpinner : true,      visibleItems : 7,      type : Ti.UI.PICKER_TYPE_PLAIN,      top : '150dp',      height : '250dp',      selectionIndicator: true,      columns : [column1, column2, column3]  });     picker.addEventListener('change', function(e) {      showStatus(e.selectedValue[0] + """" """" + e.selectedValue[1] + """" """" + e.selectedValue[2]);  });     w.add(picker);     var btnAdd = Ti.UI.createButton({      left : '5dp',      height : '40dp',      top : '50dp',      title : 'Add """"Manny""""'  });    btnAdd.addEventListener('click', function() {      picker.columns[0].addRow(Ti.UI.createPickerRow({          title : 'Test Manny'      }));      picker.columns[2].addRow(Ti.UI.createPickerRow({          title : 'Test Manny'      }));     // It must give error on both ios and android     picker.columns[1].addRow(""""something"""");         if (Ti.Platform.osname != 'android') {       picker.reloadColumn(picker.columns[0]);     picker.reloadColumn(picker.columns[1]);     picker.reloadColumn(picker.columns[2]);    }        });  w.add(btnAdd);  w.open();   {code}""",Bug,Android
412750,"""*Problem description*  I have a webview that on start up is empty on an iOS as it acts to show detail. So I used the html property to effectively set a default blank page. It seems that setting this property stops setUrl from loading a local resource.    *Steps to reproduce*  1. Run test case, with testpage.html as an additional resource  2. See that """"I'm the property"""" displays rather than """"I'm the local page""""  3. Comment out the html property to see that now """"I'm the local page"""" displays    *app.js*  {code:javascript}  var win = Titanium.UI.createWindow({   title : 'Window',   backgroundColor : 'white',   layout : 'vertical'  });    webView = Titanium.UI.createWebView({   width : Ti.UI.FILL,   height : Ti.UI.FILL,   html : '<html><body>I\'m the property</body></html>'   // ^^ Comment out html property to """"fix"""" problem  });  win.add(webView);    webView.setUrl(""""/testpage.html"""");    win.open();  {code}    *testpage.html*  {code:html}  <html>   <body>    I'm the local page   </body>  </html>  {code}""",Bug,iOS
412802,"""Android modules that declare properties using the 'propertyAccessors' annotation will lose the ability for javascript to access those properties after the main activity is closed under certain circumstances. Custom properties are not affected by this issue.    I have been able to reproduce this with a very simple module:    Attached is a reproduction case. It's a simple module that declares a proxy ('exampleProxy') and that proxy has a propertyAccessor ('name') as well as a custom property ('phone').    The app.js in the example folder demonstrates the issue:  1. Unzip the module  2. Build the module ('ant')  3. Create a new Titanium application in Studio  4. Copy the app.js from the module's example folder into your application  5. Run the application  6. Notice that the label displays '<USER>555-1212'. The '<USER> comes from the propertyAccessor and the '555-1212' comes from the custom property.  7. Hit the Android Back button (the activity closes but the <USER>is still active)  8. Tap on the application in the Android app launcher  9. Notice that the label displays 'undefined 555-1212'. You can also see the 'E/Proxy   ( 2221): Unable to lookup Proxy.prototype.getProperty' error message in the log.    For further weirdness, uncomment lines 29-31 in the app.js file and re-run steps 5-8 again. This time you will notice that everything works correctly. The only difference here is that a 2nd instance of the proxy is being created with a call from JS rather than the one that is being created internally.    A couple of observations:  1. The onStop and onDestroy methods are getting called when the activity closes (check the messages output to the log file)  2. The <USER>is still active after closing the activity  3. On the 2nd launch of the main activity you will notice a log message that it is registering an existing module again even though the module was supposedly destroyed. Probably expected behavior, but perhaps not.    I've tried a number of different things but have not been able to change this behavior. I also tried 'exitOnClose: true' and 'exitOnClose: false' with no change in behavior.    I've confirmed that the module is creating the proxy and calling setProperty on the 'name' property, but for some reason the 'name' property does not make its way back up to javascript land (ie. the application). Other properties that are defined with Kroll.getProperty are still accessible. It's just those defined in the propertyAccessors that are getting lost.    Thinking that perhaps something in the way propertyAccessors are bound is not occurring the 2nd time the module is loaded by the <USER>  """,Bug,Android
412950,"""As <USER>said:    {quote}  I think there are two competing schools of though here (at least):    1) We should expose a title on the tab group.  2) The action bar title should take the title of the current tab's window.    I've spent some time reviewing the Android docs, and it's not clear to me which of these is preferable.  {quote}    IMHO, if the tab already has a title, it's quite pointless to repeat this title in the ActionBar. In my application I'm using tabs to split the content of a shopping list between the items and the values... one tab is called """"products"""", the other is """"summary"""", and the actionBar title would be the list's name. I've attached a screenshot as an example.    *Bottom line*: Maybe the TabGroup could use the provided title property, and when it's not present, use the active tab title?""",New Feature,Android
413012,"""Once video / picture is shot and returned as a TiBlob, it has to be removed from the """"YourApp.app/tmp/capture"""" folder. Applications like PhoneView provide access to private application data. Since it isn't documented that Titanium leaves the trail behind, it may hurt privacy-related apps, like ours.    (I'll be working on this myself. Pull request to come soon)""",New Feature,iOS
413176,"""When I assign a function to titanium proxy object property it usually works ok. But in some cases accessing this property later on returns different object.  {noformat}  var view = Ti.UI.createView();  var properties = {}  view.prop = function (attr, value) {      if (arguments.length == 1) {          return properties[arguments[0]]      } else {          properties[arguments[0]] = arguments[1]      }  }    //later on    view.prop('someVal') //fails with error because view.prop return some non callable object or another, totally unrelated, JS function.  {noformat}    After some research I've found that view.prop is represented as a KrollWrapper instance with jsobject instance variable referencing my JS function. And there is nothing stopping JS runtime from garbadge collecting this jsobject which  happens in some cases.    BTW above code helps to attach complex objects to proxies without performance penalty for copying values from JS to iOS and vice versa. A proxy store reference to a JS function (which is cheap) and JS function deals with state  inside JS runtime without any copying.""",Bug,iOS
413365,"""I want ability create my own custom CSS theme rather than provided default, overriding common.css.   I don't want to apply to all project, only to specific project.   Customizing splash.css does not work because common.css override it.  For example, I want to have white as background but because common.css has 'black' for body, my application has annoying black background.""",New Feature,MobileWeb
413534,"""I've added an event to a button to change the layout property of a view. Each tap will turn the layout of this view to horizontal -> vertical -> composite then come back at horizontal, etc...    In the iOS simulator everything works like expected, the children views are relocated as the layout of the parent changes. But it does not work in the Android simulator or Android device (HTC Sensation, Android 4.1) They stay at the same place.    (The purpose of this app, was a simple test to understand the layout property.)    Could this be a bug?""",Bug,Android
414041,"""In the case of animations which change size or coordinates, this is basically a duplicate of TIMOB-1101.  But others like color or opacity animations are special.  If you run an animation on a view (such as an opacity animation) and then later run another one, you of course want the newer animation to start from what is the current visible state of the view on screen.  For example, if you animated to 50% animation and want to now animate to 100% opacity, you want this second animation to obviously start at 50% and move to 100%.  With old Android animation, the actual properties of the view do not change after the animation is complete.  And there are no properties available to tell you what the view currently looks like (post-first-animation).  So in the example, at the time I want to make the second animation (to 100%) there is no property I can check that will tell me that it's currently at 50% opacity, and for the second animation I must tell it the """"starting"""" opacity.  So Ti Android has ugly code in there to set variables to """"remember"""" the last-animated-to-opacity (and color, etc.) It's ugly.""",Bug,Android
414624,"""I'm using a custom C2DM module that calls Log.d() to send some stuff to the debug console. With 2.1.0GA everything works fine. With 2.2CI I see this stack dump:    {code}  W/System.err( 5628): java.lang.NoClassDefFoundError: org.<USER>titanium.util.Log  W/System.err( 5628):  at com.findlaw.c2dm.C2dmModule.registerC2dm(C2dmModule.java:57)  W/System.err( 5628):  at org.<USER>kroll.runtime.v8.V8Runtime.nativeRunModule(Native Method)  W/System.err( 5628):  at org.<USER>kroll.runtime.v8.V8Runtime.doRunModule(V8Runtime.java:140)  W/System.err( 5628):  at org.<USER>kroll.KrollRuntime.handleMessage(KrollRuntime.java:284)  W/System.err( 5628):  at org.<USER>kroll.runtime.v8.V8Runtime.handleMessage(V8Runtime.java:166)  W/System.err( 5628):  at android.os.Handler.dispatchMessage(Handler.java:95)  W/System.err( 5628):  at android.os.Looper.loop(Looper.java:137)  W/System.err( 5628):  at org.<USER>kroll.KrollRuntime$KrollRuntimeThread.run(KrollRuntime.java:108)  {code}    Here's the first 57 lines of C2dmModule.java:    {code}    /**   * This file was auto-generated by the Titanium Module SDK helper for Android   * <USER>Titanium Mobile   * Copyright (c) 2009-2010 by <USER> Inc. All Rights Reserved.   * Licensed under the terms of the Apache Public License   * Please see the LICENSE included with this distribution for details.   *   */     package com.findlaw.c2dm;    import org.<USER>kroll.KrollInvocation;  import org.<USER>kroll.KrollDict;  import org.<USER>kroll.KrollModule;  import org.<USER>kroll.annotations.Kroll;    import org.<USER>titanium.ITiAppInfo;  import org.<USER>titanium.TiContext;  import org.<USER>titanium.TiApplication;  import org.<USER>titanium.util.Log;  import java.util.HashMap;  import org.<USER>kroll.KrollFunction;    import android.app.Notification;  import android.app.NotificationManager;  import android.app.PendingIntent;  import android.content.Context;  import android.content.Intent;  import android.content.pm.PackageManager;  import android.net.Uri;    import com.google.android.c2dm.C2DMessaging;    import android.app.AlertDialog;  import android.content.DialogInterface;    @Kroll.module(name=""""C2dm"""", id=""""com.findlaw.c2dm"""")  public class C2dmModule extends KrollModule  {   // Standard Debugging variables   private static final String LCAT = """"C2dmModule"""";      private static C2dmModule _THIS;      private KrollFunction successCallback;   private KrollFunction errorCallback;   private KrollFunction messageCallback;      public C2dmModule() {    super();    _THIS = this;   }     // Methods   @Kroll.method   public void registerC2dm(String senderId, HashMap options) {    Log.d(LCAT, """"registerC2dm called"""");  {code}""",Bug,Android
414866,"""h1. Problem description  Calling two or more Ti.Analytics.featureEvent in a short period of time causes the app to crash.    h1. Steps to reproduce  Use the following code to test the problem:    {code}  var win = Ti.UI.createWindow({      title: """"Crash Events"""",      barColor: """"#ff00ff"""",      backgroundColor: """"#ffffff""""  }), subcatTable = Ti.UI.createTableView({      width: Ti.UI.FILL,      height: Ti.UI.FILL  });    var onItemClick = function (evt, featureName) {      Ti.Analytics.featureEvent(featureName, {          device: Ti.Platform.username,          time: (new Date()).getTime(),          env: """"env name"""",          sessionID: """"qwe123""""      });  };      subcatTable.addEventListener('click', function(e) {      // detail click can all be handled in super.      //if (_my && _my.onItemClick) {          // we would like to know it was a doc detail list click, otherwise the detail win handles meta detail track.          //_my.onItemClick.call(null, event, EventManager.VIEW_ALLDOCS_DETAIL_EVENT);          onItemClick(e, """"FEATURE_EVENT_CRASH1"""");      //}      Ti.Analytics.featureEvent(""""FEATURE_EVENT_CRASH2"""", {          device: Ti.Platform.username,          time: (new Date()).getTime(),          env: """"env name"""",          sessionID: """"qwe123""""      });      //Ti.API.info(""""Clicked, wait for crash"""");  });    subcatTable.data = [      Ti.UI.createTableViewRow({          title: """"a""""      })  ];    win.add(subcatTable);  win.open();  {code}    - Run the app  - Click on the table row: the app crashes    h1. Note  Sometimes, the app does not crash the first time, but clicking 2-3 times on the row will do that. I've tested with SDK 2.1.2.GA and the app crashes without any error in the console. Opening the app with XCode, however, shows this error: EXC_BAD_ACCESS    I also tried testing with SDK 3.0.0.v20121002161335 and I see the following error in the Titanium console:    {color:red}  [ERROR] A SQLite database error occurred on database '/Users/<USER>Library/Application Support/iPhone Simulator/6.0/Applications/C6577659-96DC-4234-9051-2FE866115B98/Library/Application Support/analytics/analytics.db': Error Domain=com.plausiblelabs.pldatabase Code=3 """"An error occured parsing the provided SQL statement."""" UserInfo=0xb216820 {com.plausiblelabs.pldatabase.error.vendor.code=1, NSLocalizedDescription=An error occured parsing the provided SQL statement., com.plausiblelabs.pldatabase.error.query.string=INSERT INTO pending_events VALUES (?), com.plausiblelabs.pldatabase.error.vendor.string=table pending_events has 1 columns but 2 values were supplied} (SQLite #1: table pending_events has 1 columns but 2 values were supplied) (query: 'INSERT INTO pending_events VALUES (?)')  {color}""",Bug,iOS
415240,"""h3. Issue    Branding the assets is difficult in Titanium's Splash Screen implementation (that assumes a full-size image file). So, as a workaround we need the 9-patch image to work for the splash screen. However, this does not work. When we add a default.9.png instead of default.png in a density specific folder such as res-port-hdpi, Ti would seem to try to pick a default.png in another density over the default.9.png. If we remove all density specific versions and just used default.9.png, Ti shows a default version of a splash screen that is not an asset in the project. If we rename the default.9.png to just default.png that would show up, just not as a 9-patch.      h3. Sample    Create a default Titanium project and swap the default.png with a 9-patch image (.9.png extension).    h4. Additional Info  There is a need to externally provide brandable assets that must be managed outside of development and inserted into build <USER> Need a way to limit the build <USER>complexity and want to keep down the number of splash screen assets that have to be created and uploaded.      """,New Feature,Android
415283,"""The task involves a few things before implementing the platform specific logic.    h4. {{cli/commands/build.js}}    In the {{project-dir}} callback function, determine if the project is an app or a module. If the project has a {{tiapp.xml}}, then it is an app. If the project has a {{timodule.xml}}, it is a module. When a module project is found, the {{timodule.xml}} should be parsed and saved into the {{cli.timodule}} variable. If the the project is neither a app or module, error and exit. You should set the {{cli.argv.type}} to the appropriate value. Remember that you need to scan up the directory tree since the {{project-dir}} may be a subdirectory.    In the {{validate()}} function, if it is an app, then do the {{ti.validateCorrectSDK()}} and {{ti.validatePlatform()}} stuff. If it is a module, then validate module stuff.    In the {{run()}} function, I think you only need to call {{ti.resolvePlatform()}} for apps. The {{buildModule}} should be {{path.join(__dirname, '..', '..', platform, 'cli', 'commands', '_build.js')}} for apps and {{path.join(__dirname, '..', '..', platform, 'cli', 'commands', '_build_module.js')}} for modules. If {{_build_module.js}} file does not exist, error and exit.    h4. {{node_modules/titanium-sdk/lib/builder.js}}    Add {{this.timodule = cli.timodule;}} to the {{run()}} function after the {{this.tiapp}} line.""",Epic,CLI
416284,"""Originally submitted as a doc bug, but it appears that we currently only support the ability to control whether or not files are backed up to iCloud. We don't support any of the synchronization and conflict resolution features of iCloud. For example, in answer the the last question in the original issue, I don't think we have any way to force a restore of a document that's synced to iCloud.      h4. PROBLEM DESCRIPTION  The community has several questions about how to interact with icloud and their Ti <USER>apps. Common questions:    - How to use icloud?  - How to hide stuff from icloud?  - Where to put a db?  - What if I want that db to be synced with icloud?  - How to force a restore after a reinstall of the app?    Would be nice if we have an entry in the wiki or a blog post where we post how we handle icloud stuff.  """,Story,iOS
416599,"""This is a suggested improvement for toolbars. iOS 5 now has ability to apply a proper BG image to toolbars. This means we can now make nice toolbars without the Glassy effect. Applied fix both to Ti.UI.iOS.Toolbar as well as to Window.setToolbar.     Why no pull request?    Because I'm not 100% confident in what I've done. It works for me, and is a good starting point - but someone with a more obj-c/titanium experience should probably review it. For example, you'll see some duplication. Feedback welcome!    I've attached a diff of the changes I made (against the 2.0.X branch) as well as example app.js showing usage.""",New Feature,iOS
416672,"""I recently updated from Titanium SDK 1.7x to 1.8.2. The Network change listener does not work anymore.    Ti.Network.addEventListener(""""change"""",function(e) {    Titanium.API.debug(""""Network changed"""");    if (Titanium.Network.online) {      Titanium.API.debug(""""Network is available"""");           }  });    Under 1.7.5 the log messages are printed, in 1.8.2 (and the most recent 1.8.3) it stopped working. I'm testing on an HTC Disre HD with Android 2.3.5. I was able to recreate it with above code added to the simplest test application""",Bug,Android
416836,"""The {{ScrollView}} class has a {{scroll}} event which is fired repeatedly during scrolling.  However, the {{ScrollableView}} class does not have anything like this.    We need an event for {{ScrollableView}} s that informs us when a {{ScrollableView}} is being moved.  Tragically, the {{scroll}} event for {{ScrollableViews}} already does something else -- it fires when the whole view has been scrolled to another page.    So, I suggest we call the event {{scrolling}}.  It will be emitted continuously as the view is scrolled, and will contain such properties as    * {{currentPage}} - currently active page index  * {{currentPageAsFloat}} - currently active page index as a float, so if halfway between pages, this will have a value of 0.5  * {{view}} - currently active view    I'm working on an implementation of this and will pull request the iOS implementation, docs, and tests within the next couple of days.""",New Feature,iOS
417076,"""*Expected Behavior:*    If a user is playing music (ipod,pandora,etc) and I play a sound in my code using Ti.Media.CreateSound I would expect the music to fade to a lower volume, play my sound, then music volume fade back in.    *Actual Behavior:*  Unfortunately what happens (assuming you are using AMBIENT as audioSessionMode) is they are played together which sounds pretty bad.      *Example*  To see an example of this first hand. Please do the following:  1. Download RunKeeper from the AppStore  2. Make sure music is playing from ipod or pandora or another 3rd party app.  3. Click 'Start Activity' button in RunKeeper.    You can see that RunKeeper will fade out the music, play there sound clip, then fade back in the music.""",New Feature,iOS
417225,"""Titanium uses the MGSplitView library, which fails to provide some functionality that the native iOS UISplitViewController component provides - namely tapping the menu button that displays the popover should also cause it to be hidden (i.e. it should toggle). It's also handy to be able to close the popover from code, for example when tapping a menu item that changes the detail view, and therefore the focus of the app. I've added this functionality, and am submitting it as a PR on Github - this issue is meant to complement the PR.    ---    Reproduction steps:  Apply the attached app.js file to a new Titanium Mobile project  In portrait orientation, tap the master button in the toolbar  With the popover displayed, tap the button again    Expected result:  The popover is dismissed    Actual result:  The popover remains visible    ---    Also, With the patch in place:  With the popover visible, tap the 'Close popover' option in the master view table. The popover should be dismissed.""",New Feature,iOS
417240,"""Action: Run->Android Emulator    Result: Last message in console """"[INFO] Compiling Javascript Resources ...""""    Task manager show python.exe using 100% of the CPU for 10+ minutes.    Caused by a block of JavaScript code. See attachment. I've tried putting the code in a c-style comment /* code */ and still seeing the problem. It seems that 'Ti...' has a special meaning to the build interpreter.  """,Bug,Android
417381,"""I came across an edge case which I think should really be fixed. By mistake I mistype the module which I wanted to include with the require function. The typo was a lowercase character instead of a uppercase character. Everything worked fine in the simulator but when I ran it on an iPad or iPhone the app crashed. After several hours of trail and error debugging by coincidence I saw the problem.    The expected result should in my opinion that the code should break also in the simulator.""",Bug,iOS
417387,"""h3. Problem      I've noticed TabGroup memory leak. When local JS object has a property that references TabGroup, it never gets released.      Same test case that uses heavyweight Window instead of TabGroup does not have that problem.      h3. Test case      Using TabGroup (produces memory leak)    {code}  (function() {            var create = function() {                    var tabGroupWrapper = {}, tabWrapper = {}, winWrapper = {}, tableViewWrapper = {}, buttonWrapper = {};                    tabGroupWrapper.Element = Ti.UI.createTabGroup();                    winWrapper.Element = Ti.UI.createWindow({ navBarHidden: true, backgroundColor: '#fc9', layout: 'vertical' });                    tabWrapper.Element = Ti.UI.createTab({ title: 'Test', window: winWrapper.Element });                    tabGroupWrapper.Element.addTab(tabWrapper.Element);                    tableViewWrapper.Element = Ti.UI.createTableView();                    var data = [];                    for (var i = 1; i <= 500; ++i) {              data.push({ title: 'Row ' + i});          }                    tableViewWrapper.Element.setData(data);                    buttonWrapper.Element = Ti.UI.createButton({              title: 'Create new tabgroup!'          });                    buttonWrapper.Element.addEventListener('click', function() {              create();          });                    winWrapper.Element.add(buttonWrapper.Element);          winWrapper.Element.add(tableViewWrapper.Element);                    tabGroupWrapper.Element.setActiveTab(0);                    tabGroupWrapper.Element.open();                };            create();              })();  {code}      Using Window (does not produce memory leak)    {code}  (function() {            var create = function() {                    var winWrapper = {}, tableViewWrapper = {}, buttonWrapper = {};                    winWrapper.Element = Ti.UI.createWindow({ navBarHidden: true, backgroundColor: '#fc9', layout: 'vertical' });                    tableViewWrapper.Element = Ti.UI.createTableView();                    var data = [];                    for (var i = 1; i <= 500; ++i) {              data.push({ title: 'Row ' + i});          }                    tableViewWrapper.Element.setData(data);                    buttonWrapper.Element = Ti.UI.createButton({              title: 'Create new window!'          });                    buttonWrapper.Element.addEventListener('click', function() {              create();          });                    winWrapper.Element.add(buttonWrapper.Element);          winWrapper.Element.add(tableViewWrapper.Element);                    winWrapper.Element.open();                };            create();              })();  {code}      h4. Steps  To see this issue, run first test case, click on the button and then close tabgroup by using hardware back-button. Repeat that and observe heap size. You should notice that heap size increases and that memory is not being released.      On the other hand, by running second example (the one that uses Window) and doing the same thing (open -> back -> open ...) you should see that memory is being released.""",Bug,Android
418261,"""h3.Issue  When I run kitchen sink html5 in 1.9, I'm not able to click any table row. It was able to click in 1.8.    h6.Tested on  1.8.0.1 > It does work  1.9.0 (Build release the 01.17) > It doesn't work    h6.Steps to reproduce  1) Create Mobile Web project with Studio  2) Embed Kitchen Sink Html 5 demos  3) Run as Mobile Web Preview in Browser    h6.Expected behavior  Navigate through the pages    h6.Unexpected behavior  Not able to click any row""",Bug,MobileWeb
418517,"""h2. Problem    I write this code to clean built-in remote images cache on sd-card, it works, but if I exit app with back button or with activity.finish(), at the next restart all images in my tableview are broken (empty), like the app don't recognize that cache file is missing.    I need to kill app with task killer to have all works fine again.    h2. Test case    {code:lang=javascript|title=app.js}  // delete cache files  var dir = Titanium.Filesystem.getFile(Titanium.Filesystem.externalStorageDirectory);   var cache_folder = dir.nativePath.toString()+'/remote-image-cache';  var deldir = Titanium.Filesystem.getFile(cache_folder);   deldir.deleteDirectory(true);    // exit app  var activity = Titanium.Android.currentActivity;  activity.finish();  {code}     Note that there is no issue with manually deleting (from adb shell) all files in {{/mnt/sdcard/(app folder)/remote-image-cache}}.    h2. Workaround    Titanium fails if dir remote-image-cache isn't found, so inserting a deldir.createDirectory(); after deldir.deleteDirectory(true); can be used as a workaround:    {code:lang=javascript|title=app.js}  // delete cache files  var dir = Titanium.Filesystem.getFile(Titanium.Filesystem.externalStorageDirectory);   var cache_folder = dir.nativePath.toString()+'/remote-image-cache';  var deldir = Titanium.Filesystem.getFile(cache_folder);   deldir.deleteDirectory(true);  deldir.createDirectory();    // exit app  var activity = Titanium.Android.currentActivity;  activity.finish();  {code}   """,Bug,Android
419828,"""Step 1: Add the attached image (checked.png) to your project   Step 2: Run the app below  Step 3: Notice the image in the webview does not display properly    Notes: Image is displayed properly with Google API's 2.2 and 2.3.3 as well as a Nexus One running 2.2.2.  It seems that this is a 2.1 issue.    Issue is related to TIMOB-4750    {code:title=app.js|borderStyle=solid}    var win = Titanium.UI.createWindow({});  var iv = Ti.UI.createImageView({    image: 'checked.png',    top: 100  });  var webview = Titanium.UI.createWebView({      html: '<html><body><img src=""""checked.png""""></body></html>',    });  win.add(webview);  win.add(iv);  win.open();  {code}""",Bug,Android
419853,"""Playing sound files repeatedly after each others by, for example pressing a button is very buggy in 1.7.2.  It works perfect in 1.6.2 so something must have happened. There are some forum threads about it also.    I've made a sample application that has a button which will play a sound if clicked (attaching sample soundfile: cat.wav). Note that the format doesn't matter at all.    When running the app, note that the sound won't play all of the times the button is pressed. You can also see that the complete event is not run on these occasions. This is in 1.7.2, switching to 1.6.2 the sound plays every time 100%. I've tried reusing the player, not releasing it and so on, no luck.    {code:title=app.js|borderStyle=solid}  var playCounter = 0;    var win = Titanium.UI.createWindow({        backgroundColor:'#fff',      orientationModes:[Ti.UI.LANDSCAPE_RIGHT]  });    var button = Ti.UI.createButton({   title:'click'  });  button.addEventListener('click', Playsound);  win.add(button);    win.open();    function Playsound(){   playCounter++;   Ti.API.info(""""Playing sound, nr: """" + playCounter);   var player =Ti.Media.createSound({    url: """"cat.mp3""""   });      player.addEventListener('complete', function(e) {    player.release();    player = null;    Ti.API.info(""""Sound played correctly"""");   });   player.play();  }  {code} """,Bug,iOS
420670,"""h3. Description  When I use '+2' in push notification (Apple), my app badge is correctly increased by 2 (for example, it it was at 0, it passes at 2).  Then, I use {code:javascript} Ti.UI.iPhone.appBadge = 0; {code}  or  {code:javascript} Ti.UI.iPhone.appBadge = null; {code}  and the badge correctly disappears.  BUT, if I redo a '+2' push notification, the badge is now at 4!    h3. How you can reproduce this issue:  There is a piece of code (which can be placed alone in app.js since it uses the function *Ti.Network.registerForPushNotifications*):  {code:javascript} // Ask for APN registration Ti.Network.registerForPushNotifications({  types: [   Ti.Network.NOTIFICATION_TYPE_ALERT,   Ti.Network.NOTIFICATION_TYPE_BADGE,   Ti.Network.NOTIFICATION_TYPE_SOUND  ],  success: function(e) {   // Send a request to urbanairship to register  },  error: function(e) {   Ti.API.warn('Push notifications disabled: ' + JSON.stringify(e));  },  callback: function () {   // Remove the badge on the main application icon   Ti.UI.iPhone.appBadge = 0;  } }); {code} Now, the different steps:  . be sure that the current value of the app application icon badge is 0 . with this code, subscribe to APN on an iPhone. . close your app (or put in in background)  . send a notification with '+2' as badge value and 'hi' has text. . on your iPhone, when you receive the badge, open the app by accepting the notification . check the badge icon of your app, it should be '0' (because the code passe its value to '0': Ti.UI.iPhone.appBadge = 0; )  . re-send the same notification . this time, don't accept the notification . check the badge icon of your app, it will get '4' as value in stead of '2'""",Bug,iOS
421181,"""{html}<div><p>Loading stereo sound using the following code:<br></p> <pre> <code class=""""javascript"""">var waitingSound = Ti.Media.createSound({   url: 'sounds/twing.caf' });</code> </pre> <p>Then sleeping OS X will max out the CPU until the simulator is closed or the app is backgrounded. Using the exact same code to load a mono audio file will not cause this problem. I've attached the original stereo file (twing.mp3), and a mono version (twing.caf).</p> <p>The same thing happens when saved as a stereo *.caf file, so I'm pretty sure it's not because of the file format.</p></div>{html}""",Bug,iOS
421279,"""{html}<div><p>I'm using Titanium Mobile 1.5/1.6's support for native Android intents and activities to let our StatusNet Mobile app accept text and image attachments from other social networking apps and the system Gallery and Camera apps.</p> <p>Unfortunately I'm having some trouble accepting images, which are attached as a Uri object in the EXTRA_STREAM extra field. Calling intent.getStringExtra(Ti.Android.EXTRA_STREAM) fails, because the returned object is a Uri, not a string:</p> <pre> <code>W/Bundle  ( 1569): Key android.intent.extra.STREAM expected String but value was a android.net.Uri$HierarchicalUri.  The default value &lt;null&gt; was returned. W/Bundle  ( 1569): Attempt to cast generated internal exception: W/Bundle  ( 1569): java.lang.ClassCastException: android.net.Uri$HierarchicalUri W/Bundle  ( 1569):  at android.os.Bundle.getString(Bundle.java:1040) W/Bundle  ( 1569):  at android.content.Intent.getStringExtra(Intent.java:3368) W/Bundle  ( 1569):  at org.<USER>titanium.proxy.IntentProxy.getStringExtra(IntentProxy.java:239) W/Bundle  ( 1569):  at org.<USER>titanium.proxy.IntentProxyBindingGen$3.invoke(IntentProxyBindingGen.java:187) W/Bundle  ( 1569):  at org.<USER>kroll.KrollMethod.call(KrollMethod.java:51) W/Bundle  ( 1569):  at org.mozilla.javascript.Interpreter.interpretLoop(Interpreter.java:1711) W/Bundle  ( 1569):  at org.mozilla.javascript.Interpreter.interpret(Interpreter.java:854) W/Bundle  ( 1569):  at org.mozilla.javascript.InterpretedFunction.call(InterpretedFunction.java:164) W/Bundle  ( 1569):  at org.mozilla.javascript.ContextFactory.doTopCall(ContextFactory.java:426) W/Bundle  ( 1569):  at org.mozilla.javascript.ScriptRuntime.doTopCall(ScriptRuntime.java:3161) W/Bundle  ( 1569):  at org.mozilla.javascript.InterpretedFunction.call(InterpretedFunction.java:162) W/Bundle  ( 1569):  at org.<USER>titanium.kroll.KrollCallback.callSync(KrollCallback.java:139) W/Bundle  ( 1569):  at org.<USER>titanium.kroll.KrollCallback.callSync(KrollCallback.java:113) W/Bundle  ( 1569):  at org.<USER>titanium.kroll.KrollCallback.callSync(KrollCallback.java:108) W/Bundle  ( 1569):  at org.<USER>titanium.kroll.KrollCallback.callSync(KrollCallback.java:104) W/Bundle  ( 1569):  at org.<USER>kroll.KrollProxy.fireSingleEvent(KrollProxy.java:629) W/Bundle  ( 1569):  at org.<USER>kroll.KrollEventManager$KrollListener.invoke(KrollEventManager.java:143) W/Bundle  ( 1569):  at org.<USER>kroll.KrollEventManager.dispatchEvent(KrollEventManager.java:273) W/Bundle  ( 1569):  at org.<USER>kroll.KrollProxy.fireSyncEvent(KrollProxy.java:600) W/Bundle  ( 1569):  at org.<USER>titanium.TiBaseActivity.onCreate(TiBaseActivity.java:294) W/Bundle  ( 1569):  at org.<USER>titanium.TiLaunchActivity.onCreate(TiLaunchActivity.java:97) W/Bundle  ( 1569):  at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1047) W/Bundle  ( 1569):  at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2627) W/Bundle  ( 1569):  at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2679) W/Bundle  ( 1569):  at android.app.ActivityThread.access$2300(ActivityThread.java:125) W/Bundle  ( 1569):  at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2033) W/Bundle  ( 1569):  at android.os.Handler.dispatchMessage(Handler.java:99) W/Bundle  ( 1569):  at android.os.Looper.loop(Looper.java:123) W/Bundle  ( 1569):  at android.app.ActivityThread.main(ActivityThread.java:4627) W/Bundle  ( 1569):  at java.lang.reflect.Method.invokeNative(Native Method) W/Bundle  ( 1569):  at java.lang.reflect.Method.invoke(Method.java:521) W/Bundle  ( 1569):  at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:868) W/Bundle  ( 1569):  at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:626) W/Bundle  ( 1569):  at dalvik.system.NativeStart.main(Native Method)</code> </pre> <p>I notice in ticket <a href=""""/projects/32238/tickets/2587"""" title= """"Ticket #2587"""">#2587</a>, someone encountered the same problem when <em>creating</em> such intents to send to other applications, and an Intent.putUriExtra() was added to accommodate it, converting from a string parameter to a Uri.</p> <p>It looks like it would be helpful to add an Intent.getUriExtra(), or else tweak things such that there's some other clear way to retrieve it.</p></div>{html}""",Bug,Android
421413,"""I need to be able to hide/show table view rows. The docs say hide() and show() exist, but they have no effect. Setting height=0 or visible=false (both in the docs) also do not work.    Alternatively, add support for deleteRow(tblRowObjectReference) and insertRowAfter(tblObjectReference) rather than by index number.    I need to be able to hide/show the same table row based on user's other selections. User selects a product, related options in a table need to hide/show as a result. But if they switch back, I need those original selections back in the same order. Using indexes gets very complex and all my attempts just result in exceptions/crashes.    Using Ti SDK 1.6.0 tested on both iOS 4.2 simulator and Android 2.1 SDK emulator.""",Bug,iOS
421462,"""{html}<div><p>Hi,</p>  <p>We just created a new app and are planning to create some more  with your platform. Works perfectly execpt for one major point. The  fact that all the scripting is publicly available. We made some  private services with api's for our apps, but as all the scripting  can be found in the apk file, the private services all of a sudden  can become public as everyone can use them. As you can imagine this  is a major porblem.<br>  What we did at the moment is obfuscate some parts, but not only  that is not really secure, it slows down the app a lot as well.</p>  <p>Compiling the javascript nativly would be the best of course,  but I don't see that happen soon. Therefor I was looking at the  encrypted database option and noticed that it was on the roadmap.  Could you please give us an estimated time of arrival of this  feature?</p>  <p>thanks for the project so far and thanks for you time.</p>  <p>Best regards,</p>  <p><USER>/p></div>{html}""",New Feature,iOS
421685,"""{html}<div><p>Some app developers want to clear the saved Facebook session information when a user backs out of an app, so that the session info doesn't automatically get remembered the next time the app is started. I want to recommend to them that they should use the <code>destroy</code> event of the root activity, such as in this example app.js:</p> <pre> <code class=""""javascript"""">Titanium.UI.setBackgroundColor('#000'); Ti.Facebook.appid = """"PUT A FACEBOOK APP ID HERE""""; var win = Titanium.UI.createWindow({       title:'Test',     backgroundColor:'#fff',     exitOnClose: true });  Ti.Android.currentActivity.addEventListener('destroy',function(){     Ti.API.info('Root activity destroying');     Ti.Facebook.logout(); }); win.add( Ti.Facebook.createLoginButton({style: 'wide'}) ); win.open();</code> </pre> <p>Run that example and login to Facebook. Then completely back out of the application. Then go back in to the application -- unfortunately you'll probably see the facebook button still says """"logout"""", so it thinks it is logged in even though we called <code>logout()</code> when the user backed out of the app, which is when <code>destroy</code> occurs (you can see that <code>logout()</code> really does get called if you watch logcat.</p> <p>So what has happened is that the user really is logged out of that Facebook session (the OAuth 2.0 token is no longer valid), but Titanium doesn't know about it, so the login button is screwed up and showing """"Logout"""" instead of """"Connect with Facebook"""".</p> <p>I think what's happening here is that the relevant Titanium code to clear the saved session info (saved as a private entry in the Android Shared Preferences store) does not run until an Asynchronous call to Facebook's logout API has returned. By the time that Async call is finished, there are no listeners available any more (the user has backed out of the app, the root activity has been destroyed, etc.)</p> <p>What we need to do is destroy the session info right when <code>Ti.Facebook.logout()</code> is called, rather than wait for a return from FB's logout API call. After all, if an app developer has called <code>.logout()</code>, he wants that session destroyed, so why wait? This way the destruction of the session occurs synchronously within the <code>destroy</code> thread.</p></div>{html}""",Bug,Android
421743,"""{html}<div><p>The click-event (as well as other touch events) does not  fire.</p>  <p>I would also like to get the index of the button clicked as an  event-property.</p></div>{html}""",Bug,iOS
422025,"""{html}<div><p>I have tried sdk 1.4.3, 1.5.2 and 1.6 on iPhone, they all showed poor performance when table view row has image background. My table has about 100 rows ( actually, it is kind of slow when it has only 20 rows too), each row has a nice image as its background. I set rowclass, prepopulated the rows into an array then set it to tableData, it is fast in simulator (of course it is fast with 12 CPUs), but it took 5 seconds even in iPhone 4. I ended up with a trick, I only set 8 rows to tableData first (user can only see first 8 anyway), then append the rest of rows to tableview with a setTimeout function. It kind of works in iPhone 4, but not good in older version of device.</p> <p>I would really appreciate you look into this performance issue, since table view is most widely used UI widget.</p></div>{html}""",Bug,iOS
422234,"""{html}<div><p>Hi,<br> I'm creating an app for iPad, tested in 3.2.2 and 4.2.1<br> I use a custom button to create an iPhone style button.</p> <pre> <code>var btn = Ti.UI.createButton({         backgroundImage:'img/btn.png',         backgroundSelectedImage:'img/btn_o.png',         height:57,         width:200     }); var lblBtn = Ti.UI.createLabel({         color:'#fff',         font:{fontSize:12,fontWeight:'bold'},         shadowColor:'#000',         shadowOffset:{x:1,y:1},         right:0,         left:0,         height:'auto',         text:'My button',         textAlign:'center'     }) btn.add(lblBtn);</code> </pre> <p>This code reproduce a button with a title with shaddow. This work perfectly in 1.4.3, but NOT in 1.5.0.<br> If my image is opaque i don't see this label, but if has an opacity, i see the label below the backgroundImage.</p></div>{html}""",Bug,iOS
422469,"""{html}<div><p>I have an iPhone app that uses a ScrollableView with usually 3  ImageView's as its views. In 1.4.1.1 it works great. But, in at  least the two 1.5.0 builds I've tried, the scrollability just  doesn't work. The view appears to be there, as I see the paging  control at the bottom, but I can't scroll amongst my views. I've  tried adding the views in a variety of ways (create views first,  add as array when creating ScrollableView, add after creation, with  scrollable.addView, with scrollable.setViews, etc.). The  KitchenSink's ScrollableView doesn't seem to suffer from this.</p>  <p>I figured since this works in 1.4.1.1, but not in 1.5 that's a  change, and thus potentially a bug. Below is how I'm creating the  ScrollableView. One additional note is that I now have to  explicitly set the top and height values on the ImageViews,  otherwise it seems to center them in the scrollview (and thus they  overlap the paging control a bit):</p>  <pre>  <code class=""""javascript"""">    win = Ti.UI.createWindow({        backgroundColor: '#000',        barColor: '#000',        translucent: true,        tabBarHidden: true,        backButtonTitle: 'Back',        title: hotel.name      });      scrollView = Ti.UI.createScrollableView({        views: photos.map(function(photo) {          return Ti.UI.createImageView({            image: photo,            backgroundColor: '#000',            top: 0,            height: 460          });        }),        backgroundColor: '#000',        showPagingControl: true,        pagingControlHeight: 20,        bottom: 0,        height: 480      });        win.add(this.scrollView);</code>  </pre></div>{html}""",Bug,iOS
422936,"""{html}<div><p>I have reason to believe that the close event on a window happens before all its assets are released from memory. I'm doing this in the iPhone only. I'll explain below.</p> <p>I create a modal window, and use the url property:</p> <pre> <code>var searchModal = Ti.UI.createWindow({       navBarHidden:true,       url:'includes/search.js'  });</code> </pre> <p><strong>search.js</strong> has about 400k in image assets and about 50k in JS includes (about a thousand lines of code total) inside it.</p> <p>When opening and closing the modal, I do the following:</p> <pre> <code>openButton.addEventListener('click',function(){       searchModal.open();  });</code> </pre> <p>Then inside <strong>search.js</strong>:<br></p> <pre> <code>closeButton.addEventListener('click',function(){     searchModal.close(); });</code> </pre> <p>If I open the modal, close it, then immediately click the open button again, the app crashes. I can make it crash over and over again. I put an event listener on the window close and notice that I'm clicking on the <em>openButton</em> after the 'close' event fires, and it still crashes.</p> <p>If I disable <strong>openButton</strong> once <strong>searchModal</strong> opens, then re-enable it on the window 'close' event, I can still get it to crash. <em>However</em>, if I re-enable <strong>openButton</strong> after about a 1000ms delay, I can't ever get it to crash:</p> <pre> <code>searchModal.addEventListener('close',function(){    setTimeout(function(){       openButton.enabled = true;            },1000); });</code> </pre> <p>Please advise. Thank you!</p></div>{html}""",Bug,iOS
423231,"""{html}<div><p>My app has a window with a tableview and about 20 ImageViews in  TableViewRows of 4. Upon upgrading from Ti 1.2 to 1.4, the images  were not loading. I just saw the default image in those spots.  Occasionally, 1 or 2 out of the 20 images would load successfully.  The behavior is the same whether I used local images or remote  images.</p>  <p>I tracked it down to a line of javascript where I am changing  the height of the tableview after adding the view hierarchy to the  window. If I comment that out, the images all load up just  fine.</p>  <p>I haven't been able to successfully reproduce this in a clean  app yet, but I figured I would add a ticket in case someone else is  experiencing this problem.</p></div>{html}""",Bug,iOS
424072,"""{html}<div><p>Blain -</p> <p>I've attached the Resources directory for a simple test case. To reproduce:</p> <p>1) open app<br> 2) click button on first window (opens win1)<br> 3) click button on second window (opens win2)<br> 4) close window<br> 5) close window (back on main window)</p> <p>crash</p> <p>The attributes that seem to cause the crash are:</p> <p>1) having 3 windows on the stack where each window has a focus event listener that makes a database function call<br> 2) the database function that is called calls db.close(). If you remove db.close() from the database function the crash does not occur<br> 3) each window JS file includes the same db.js file</p></div>{html}""",Bug,iOS
424126,"""{html}<div><p>Hi there,</p> <p>while working with Titanium Mobile 0.8.2 (OSX 10.5.8, iPhone-SDK 3.1.2) i could use these doc-coments (@var/@return etc) without problems, but suddenly the execution of my code stopped without any message whatsoever. It turns out the problem was a doc-comment in a prototype. A snippet to illustrate:</p> <p>function XXX()<br> {</p> <p>/<em>* * @var date</em> / this.thing = null;</p> <p>}</p> <p>does fine, while</p> <p>function XXX()<br> {</p> <p>/<em>* * @var Date</em> / this.thing = null;</p> <p>}</p> <p>does not.</p> <p>I'm not sure if i'm using the """"correct"""" way to anotate my code, but since it's inside a comment-block i tend to say it's an error. Correct me if i'm wrong, otherwise i hope its helpful to you and others.</p></div>{html}""",Bug,iOS
424134,"""{html}<div><p>Run this code - label gets cutoff</p> <p>var win = Ti.UI.createWindow();</p> <p>var scrollView = Ti.UI.createScrollView({<br></p> <pre> <code>contentWidth:'auto', contentHeight:'auto', showVerticalScrollIndicator:true, showHorizontalScrollIndicator:false, backgroundColor:'transparent'</code> </pre> <p>});</p> <p>var cleanstory = 'St Kilda has sacked recruit Andrew Lovett just a day after the former Essendon forward was charged with rape but insists it was not because of the criminal charge he faces.\n\nThe Saints confirmed on Tuesday afternoon that they intend to 'terminate Lovett's employment' with the club immediately, less than three months after giving up their first pick in the national draft in ';<br> cleanstory = cleanstory + 'order to secure the troubled star.\n\nLovett fell foul of club officials within weeks of joining the club when he was arrested for being drunk in a public place but after being fined following that incident he found himself in much more trouble in December.\n\nAfter drinking with a team-mate at a Richmond hotel late in December Lovett was accused of raping a sleeping woman at ';<br> cleanstory = cleanstory + 'a Port Melbourne apartment, with the club reacting to the allegations by immediately suspending him indefinitely.\n\nAnd while police charges were not laid until Monday the club has announced that it has made the decision to dismiss Lovett but claims the decision is 'not based on whether or not Andrew Lovett is innocent or guilty', that it was 'not part of the club's decision-making <USER>.\n\n""""During ';<br> cleanstory = cleanstory + 'Andrew's short tenure with the club, on a number of occasions he engaged in actions that were failures to comply with our standards of expected behavioral conduct,"""" said chief executive <USER>Nettlefold.\n\n""""These failures related to his training commitments and a failure to contact club officials in a situation where he should have done so. """"\n\n""""We simply could not ignore such breaches. Nor could we ignore ';<br> cleanstory = cleanstory + 'the damage being done to St Kilda's reputation and decided unanimously as a club to terminate Andrew Lovett's employment with the Saints.""""\n\nWhile St Kilda refused to discuss the case against Lovett as it does not want to prejudice him, the AFL Players' Association was 'extremely disappointed' with the Saints' decision.\n\n""""The AFL and AFLPA have agreed to a grievance resolution procedure which seeks ';<br> cleanstory = cleanstory + 'to resolve disputes between players and Clubs in good faith,"""" AFLPA CEO Matthew Finnis said.\n\n""""We expect that clubs and players will respect this procedure and use their best endeavours to resolve issues between them via this system.""""\n\n""""St Kilda's decision is, in our view, inconsistent with this principle and has the potential to undermine the commitment of players to the established codes and procedures for ';<br> cleanstory = cleanstory + 'dealing with disciplinary matters and disputes within the AFL industry.""""\n\n""""AFL players are employees, and AFL clubs are workplaces. Notwithstanding the nature of their work or their profiles, AFL players ought be able to rely upon basic principles of fairness and due <USER>before being subjected to disciplinary action.""""\n\n""""We have serious concerns as to whether Andrew's legal rights have been respected and will';<br> cleanstory = cleanstory + 'further consult with his legal representatives to address this concern in coming days.""""\n\nHaving reportedly signed a three-year, $1 million contract when he was drafted, Lovett may launch legal action against St Kilda, which refused to say whether he will have his contract paid out.\n\nLovett was seeking mediation to try and have the decision to ban him from the club overturned but with mediation no longer a possibility it appears almost certain that he will take the Saints to court.\n\nMeanwhile, AFL General Manager of Football Operations Adrian Anderson indicated in a statement on Tuesday that they would be leaving the matter in the hands of the club in question.\n\n""""This is a contractual matter between St Kilda and Andrew Lovett. The AFL respects the right of St Kilda to make this decision and we do not intend to interfere in the matter,"""" Anderson said.\n\nSt Kilda coach Ross Lyon admitted that the decision to give up pick 16 at last year's draft for Lovett had backfired and that he is now looking forward to Friday's NAB Cup clash with Collingwood at Etihad Stadium.\n\n""""We went through what we believed to be due diligence or research and referencing and at the end of that made a reasonable or unanimous decision,"""" Lyon said.\n\n""""It's easy to sit here in hindsight (and say we should not have traded for him) but I'm certainly not saying it's been a positive outcome.""""\n\n""""As a coach and a team what we need is to focus on our actions and prepare and the AFL season continues but we're one short.""""\n\n""""But it's undeniably a negative start (to the season). It's not something we could've foreseen.""""\n\n""""You try and prepare for challenges thrown up, this is one that's been put in front of us and that we'll deal with. (But) we're mentally strong.""""\n\nLovett, who booted 93 goals in 88 games with the Bombers after making his debut in 2005, was released by the club at the end of the 2009 season after a troubled five years at Windy Hill.\n\nA gifted player on the field, having won the Anzac Day medal as best afield in just his third game in the traditional battle against Collingwood in Round 5 of his debut season, Lovett found himself in trouble several times of it during his time with Essendon.\n\nThe most serious issue related an incident with a former girlfriend in which he was accused of beating her and refusing to let her out of his car as he kept her hostage driving around Melbourne for 45 minutes.\n\nHe was later fined for breaching an intervention order that legally prevented him from approaching her before last year also being suspended from driving for three months for driving without a licence.';</p> <p>var labelStory = Ti.UI.createLabel({<br></p> <pre> <code>text: cleanstory, color: '#576996', textAlign:'left', top:0, left:3, width:315, height:'auto', font:{fontFamily:'Helvetica',color:'#000000',fontWeight:'normal',fontSize:12}</code> </pre> <p>});</p> <p>scrollView.add(labelStory);<br> win.add(scrollView);<br></p> <p>win.open()</p></div>{html}""",Bug,iOS
424174,"""{html}<div><p>Loading a local HTML into a WebView works nicely. If that same HTML file has a DOCTYPE specified, however, the app will crash.</p> <p>See the attached app.js and simple.html for an example. Both are very simple, and will crash the App.</p> <p>Setup I'm using:</p> <p>Titanium 0.92<br> iPhone SDK 3.0 (or 3.1.3)</p></div>{html}""",Bug,iOS
424318,"""{html}<div><p>In TM 0.8.1 there is only the option to have a cancel button. I'd like to request we have more than this available.. such as a standard search button and that kind of thing.. whatever is standard on both the iPhone and Android platforms.</p> <p>Android seems to be something like this (see Marketplace Search):</p> <p>Optional Graphic (does nothing but let you indicate what you're searching for) Search box Search icon (magnifying glass)</p> <p>I'm sure there are lots of UI recommendations for the iPhone :)</p></div>{html}""",Bug,Android
424330,"""{html}<div><p>from community issue:</p> <p>I'm also pretty sure someone needs to look at MediaModule.m more too, for the fact that if I use the camera to take a picture for upload and have allowImageEditing to FALSE in my app, it seems to ignore the orientation a had the camera.<br></p> <p>I was looking around this part of the code to see if there was any way where the actual size of a cropped image could be set. Currently the 'image editor' square when it crops a photos seems to create a VERY small photo - 319x320 pixels - can the cropRect part be adjusted to make the inal cropped image larger?</p> <p>I also note that when NOT using image editing on a photo, uploading seems to take forever on a 'full' sized image taken.</p> <p>If we could have some more control over this - QUALITY_SMALL, MEDIUM, LARGE etc. possibly, that would be great.</p> <p>I also note that for video recording, the videoMaximumDuration and quality settings seem make no difference.</p></div>{html}""",Bug,iOS
424394,"""{html}<div><p>$(document).ready(function() {</p> <pre> <code>        $.ajax({             type: """"GET"""",             url: """"tutorial.xml"""",             dataType: """"xml"""",             success: parseXml         });      });</code> </pre> <p>I am using the above code for parsing the xml file using jquery, The code doesnt seem to work.I am using Titanium 0.7 version on Mac for iphone as well as andriod apps . Do i need to use 0.7.2 sdk for this to work? Guide me for the same.</p></div>{html}""",Bug,iOS
424874,"""As a heavy Jira user I would like to pull my emails through to a Jira board so I can track and move emails with actions via a Kanban board.    * Granting email access to Jira by sharing outlook credentials. This will be a dynamic connection to AD where Atlassian won't have visibility /access to the business AD or work emails.   * Emails should appear in the Backlog of a specialist personal email Kanban board.    * User should be able to read emails as a Jira ticket.   * Formatting with images/media and attachment support.    * Commenting on a email ticket in Jira will email out to the group / sender but without Jira formatting or styles. Or can be turned off.     * User to move emails with 'Actions' to 'ToDo' column and then progress them through the statuses defined in the board.   * Board is visible to the user and not the group.   * Status changes should not be emailed out. This board should not be shared however could be if its a shared mailbox.    * Assignees are not required as the email recipient is always the assignee.    * Other emails which don't have actions can be marked 'Closed' and will drop off the Backlog.   * Closed emails in Jira will be marked as Archived in AD.   * Emails which have gone through the Kanban process are Archived when marked Done.      As this is a new feature there are lots of possibilities so some whiteboarding would have to be done.    This could replace the standard email clients across businesses who use JIRA. """,Suggestion,Jira Core Board
424879,"""h3. Suggestion    As a user, I would like to set up DevOps automation while working with GitLab.""",Suggestion,Automation for Jira
424959,"""Automation for Jira cannot currently set Jira <USER>specific fields such as the request type. It also cannot access any of the Jira <USER>""""Portal-only users"""" which prevents it from being able to set those users as a reporter.    As such, I would like to propose that we provide a means by which to configure automation to allow for interacting with Jira <USER>  """,Suggestion,Automation for Jira
424960,"""{quote}As an automation rule writer  I want an action to edit a variable's value  So I do not clear it every time by using Create Variable  {quote}    Acceptance criteria:   * There are separate actions for Create Variable and Edit Variable   * The action Edit Variable supports accessing any built-in or custom fields, and variables. Thus, Edit Variable can refer to itself, such as with concatenation.   * The scope of the variable is clear and unambiguous, based upon the nesting level at which it Create Variable was called         Business Value:  It is unclear how variables work or their scope; specifically, there is no defined way to edit a variable's value. There is an article indicating that defining a variable earlier in a rule makes it available withing structures such as conditions or branches. However, the only available action is Create Variable, which appears to shadow and then replace the value for each usage.     """,Suggestion,Automation for Jira
424961,"""{quote}As an automation rule writer  I want all issue fields available to Lookup Issues  So I can access the valuable information in issues using automation  {quote}  Acceptance criteria:   * All built-in and custom fields are accessible in Lookup Issues   * The limit of 100 issues for Lookup Issues remains the same   * The smart values names used within Lookup Issues matches names used elsewhere. That is, no alternative names are used due to the context within a Lookup Issues list.      Business value:  Providing access to all issue fields within Lookup Issues can simplify rule definition that might otherwise require multiple rules or be impossible to implement with out-of-the-box JIRA.     """,Suggestion,Automation for Jira
424974,"""As a user, I'd like to be possible to create a transition validator that will display a dynamic message when there is more than 2 required fields for a transition to happen to display which fields are empty in a single message.    Currently, it displays that only one field is empty while there is more than one field required empty during the transition.""",Suggestion,Project Administration
424983,"""With the Archive Project feature, I would like to have an option to access all the issues from the archived project without restoring it as a view mode only""",Suggestion,Issue
425123,"""As a Jira admin, I need to be able to audit our email handlers. Please create a way for admins to extract email handler activity. In particular, we would need:   * Email handler name   * Count - the number of times the handler was processed during the log period   * Last issue processed time stamp    I understand that you only keep a few weeks to a month of data in the log. Even a period that short could be useful if we could just run a report ourselves periodically.    The goal here is to retire email handlers that are not being used. When there are lots of handlers, and we use the default 1 minute period for checking them, we discovered that some of them can take a long time to process. I had to increase the interval time for all of them to make sure that they all were processed in an acceptable amount of time. Cleaning out unused email handlers would mean I could cut down the interval again.     Thanks,     <USER>",Suggestion,Global Administration
425171,"""h3. Summary    As a user with multi-language teams, I would like that Jira could translate the content of fields such as comments, description, and summary.     The idea is that the field displays the content translated depending on the user country/timezone.""",Suggestion,Translations
425194,"""As an automation rule author    I want to edit the trigger for a rule    So I can rapidly implement rules which need a new trigger         Acceptance criteria:   * When creating a new rule, the trigger may be changed after selection   * When editing an existing rule, the trigger may be changed after selection   * After a trigger is changed, the editor quickly identifies problems that must be corrected (For example, accessing smart values which are no longer available due to the trigger change.)   * Bonus: when a trigger is changed, the editor graphically highlights problems that must be corrected, drawing the author's attention   * Big Bonus: adding a trigger of another rule completing execution could reduce the need to edit triggers for a chain of rules         Business case:    When creating complex rules, authors may discover during testing that the trigger is incorrect, or incomplete, to meet their needs.  Their only option is to recreate the rule from scratch.  Having the ability to edit the trigger would allow faster prototyping and reuse of rules, as existing rules could be copied and adjusted for new trigger conditions.     """,Suggestion,Automation for Jira
425385,"""As a Jira administrator, I would like to customize the image that is displayed when we select the Unassigned/Automatic option for user fields.         !abc.png!""",Suggestion,Issue
425490,"""As a user, I would like to be able to set Project Roles in Global Permissions. The reason for that is because I would like to allow only users in the Project Role administrators to perform some application-wide actions.""",Suggestion,Global Administration
425547,"""As a company that uses Jira <USER>(JSD) for all customer service, we need to be in full control over as many parts of the UI as possible. We need all UI to be translatable and use the wording that matches the communication we choose to have with our customers. This is absolutely vital for companies that wants to be in charge of the customer journey using JSD.    Problem: Currently there are hardcoded parts of the landing page, and not only the landing page as elements are used on all pages.    The section """"Learn more about"""". This refers to the categories used in the knowledge base. This should be possible to:   # Rename   # Translate   # Remove    The footer """"Need to raise a request? Contact us"""". This should be possible to:   # Rename   # Translate    I would love to visualize what I mean but have found no way to upload a screenshot unfortunatly.""",Suggestion,Translations
425612,"""As an admin , I would like to have the ability to remove the tab """"Discover"""" option from the Switch to tab. Please refer attachment. We do not want to see """"More atlassian products"""" under this Discover tab.""",Suggestion,Global Administration
425742,"""As a Software user, I'd like to be able to create new linked issues as it is possible for Business and <USER>projects issues.   !screenshot-1.png|thumbnail!     Currently, this feature is only available for Business and <USER>projects and it would be awesome if we can create a new linked issue directly from the issue view.  """,Suggestion,Issue
425821,"""*As a* JIRA user  *I want to* highlight flagged issues in epics  *so that* I can easily see which issues in an epic are flagged.    The """"Issues in Epic"""" section doesn't currently highlight flagged issues: this ticket proposes highlighting them in yellow, like on boards.""",Suggestion,Issue
425860,"""h3. Issue Summary    In the New Issue View, you can now change the issue type by clicking on the issue type icon, when choosing another issue type, the screen transitions to the Move action screen, in this screen the issue type that you chose defaults back to the original issue type of the issue before the change.     h3. Steps to Reproduce  # Open any issue in new issue view, ensure the issue type to be changed has a different workflow.  # Click on the issue type icon, let's say, the default issue type of the issue is """"A"""", the chosen new issue type is """"B"""", from the dropdown click on issue type """"B""""  !sampletest.png|thumbnail!   # The screen transitions to the Move Issue wizard assistance, the newly chosen issue type """"B"""" value from the previous stage is not carried over. You need to choose the issue type """"B"""" again in this step.   !project.PNG|thumbnail!       h3. Expected Results    When a new issue type is chosen in the new issue view screen, I expect the chosen option to carry over to the Move Issue Wizard screen, so I can click on """"Next"""" straight away.    h3. Actual Results    When a new issue type is chosen in the new issue view screen, I need to choose the same new issue type again in the Move Issue Wizard screen, before moving forward.    h3. Exception  - If you choose an issue type with the same workflow in the New Issue View, the transition is smooth, the change made works as expected.  - If you choose """"Sub-task"""" as the issue type option, the convert issue screen shows the chosen option correctly and works as expected.       h3. Workaround    You need to choose the option again from the dropdown which adds an extra step.""",Bug,Issue
425931,"""As a JIRA administrator,    I'd like to be able to disable the creation of new statuses by standard users through the Simplified Workflow's 'Add Status' button    so that I can:   * avoid the creation of effectively duplicated status names like """"To Do"""" & """"TODO"""", or Testing & Test & Tested & In Test & Integration Test   * make easier multi-project JQL queries to retrieve meaningful info based on statuses   * make more maintainable the order in which statuses are displayed in gadgets and JQL ORDER BY clause.   * avoid the creation of statuses for trying to store info when it would be better to use anything else instead (labels, the resolution field, a custom field...)""",Suggestion,Project Administration
426048,"""Recently Atlassian enabled 2FA for domain accounts which is a good first step...however you've only addressed half the problem.    What about all the accounts on a domain we don't manage?    As an <USER>for my organisation, I want to force ALL users to have 2FA enabled if they plan to access our sites. So I would like to see a setting added to Jira, Confluence, etc which is:    Force non-domain accounts to have 2FA enabled  Until they setup 2FA on their Atlassian account, they can't access our site.""",Suggestion,Global Administration
426097,"""h3. Summary    As a user, I would like to be able to display specific custom field values for each transition, similarly to the *<USER>field.resolution.include* and *<USER>field.resolution.exclude* properties. An example could be <USER>field.customfield_XXXXX.include or exclude.""",Suggestion,Project Administration
426146,"""As a user I want to a button to quickly Split tasks in backlog. Old gen Jira offers a very nice split option, which clones all the content and then I can quickly edit each task.""",Suggestion,Issue
426151,"""I often need to copy the title and id of a Jira issue and paste it somewhere else (Slack, Github, etc). I'd like to have a keyboard shortcut that does this for me, i.e copies the issue ID and the title as a nicely formatted string and puts it on my clipboard. Something like this:    *[PROJ-555] Fix all the things*    Thank you!""",Suggestion,Issue
426306,"""As a Jira software user, I would like to have the ability to browse Jira users while submitting smart commits in Bitbucket Server.    The goal of this improvement request is to have the option to submit smart commits with reference to Jira users, for example:    If we add comments via smart commit with the message """"<USER>"", Jira will read """"<USER>"" as <USER> and if you select the user, it will redirect you to the user profile page.    Right now, while submitting a smart commit with the text ~email address, for example, ~<EMAIL>, if the email address matches the username, Jira renders it as a mention, allowing the user to submit that smart commit with a reference for that user in Jira.    However, the username is no longer customizable and it will be deprecated in the near future, this workaround works for old usernames where the email address matches the username, however, it is not working for new users where we don't know the username and it won't work at all when we fully deprecate the usernames.""",Suggestion,Application Links
426312,"""JIra's priority Japanese translation is very strange and difficult to use.  Japanese translation is now as follows.    Blocker ⇒　ブロッカー  Critical ⇒　重要  Major　 ⇒ 重度  Minor　⇒　軽度  Trivial ⇒ 最低    ※ """"最低"""" have another meaning like """"worst"""",""""disgusting""""....       it means like:    Blocker ⇒ Blocker  Critical ⇒ Important  Major ⇒ Severe  Minor ⇒ Mild  Trivial ⇒ lowest    重要（Critical ） and  重度（Major）are quite similar Japanese,    I don't know which level is higher...          I know if someone have the global authority could change ,  but I would like to ask you to consider updating the default.    As a suggestion：    Blocker ⇒ ブロッカー  Critical ⇒ クリティカル （or 緊急）  Major ⇒ 重要　（or 高）  Minor ⇒ 中度　 (or 中)  Trivial ⇒  軽度　（or 低）    Please consider it.""",Suggestion,Translations
426338,"""h3. Issue Summary  For text-based JQL searches, if """"-a"""" is included at the end of the string, Jira looks at this as a wildcard {*} rather than a string which then gives more results than it should.    h3. Steps to Reproduce  # Create 2 Issues in Jira, one having the *Summary* as """"testing_46565-1-a"""" and the other as """"testing_46565-1-b"""".  # Do a JQL search on that *Summary* field for -a ({{summary ~ """"\""""testing_46565-1-a\""""""""}})    h3. Expected Results  I would expect the results to be all matches with the """"testing_46565-1-a"""" *Summary*     !Issue_navigator_-_b.png|thumbnail!     h3. Actual Results  The results include all Issues with a similar *Summary* and looks at the """"*-a*"""" as a wildcard, essentially making the JQL {{summary ~ """"\""""testing_46565-1*\""""""""}}.     !Issue_navigator_-_Jira.png|thumbnail!     h3. Workaround  As a workaround, you can change the *Indexing language* language in the General settings to """"Other"""" which may be related to the stemming of the word """"a"""".     !Edit_Jira_Configuration_-_Jira.png|thumbnail! """,Bug,Issue
426488,"""As a user, I would like to have the option to wrap the text in the Code Snippet when using the new issue view, instead of having the horizontal ruler.""",Suggestion,Issue
426534,"""h3. Issue Summary  Under /secure/admin/ViewPriorities.jspa# we have a field named as *Priority Color* when creating a new priority.   This field has a button that allows users to select the intended color to use. Yet, it also allows users to input any String to it.     The problem is if the String inserted is not a color hash, it breaks board views if their card colors are based on *Priorities*.    h3. Steps to Reproduce   # Access /secure/admin/ViewPriorities.jspa#    # Add a new Priority and instead of selecting a color for the field *Priority Color*, type the name of the color e.g.: Yellow.   # Add the priority   # Access any board   # Access the board settings   # Click on *Card colors* on the board settings menu    # Make the field """"Colors based on"""" field as *Priorities*    h3. Expected Results  When inserting the priority color, there should be a validation if the input is a valid color hash string and if not, it shouldn't allow the user to save it.    h3. Actual Results  System allows any String to be input in the priority color field leading to problems such as the one described on the """"Steps to Reproduce"""" section.    I've reproduced it with the String """"Purple"""" and the error that is triggered in the board settings page is the following:    !screenshot-1.png|thumbnail!     You might as well see an error on the board itself stating """"Failed to execute search"""" error."""" and not allowing you to see the board content at all.    h3. Workaround  The workaround is pretty much the fix of editing the priority color of the """"broken"""" priority by selecting the proper color using the UI instead of manually typing the String.""",Suggestion,Global Administration
426578,"""As a user creating USs I want a non-confusing interface when I use a pop-up blocker so I don't loos the information I just typed in the new US pop-up dialog.    *Reproduce*    Chrome + adblock+ Jira v7.12.3 (standard configuration at client site)    This situation prevents Jira from creating pop-ups. Upon clicking the Create button in the top of the window it seems that Jira detects this somehow and shows another kind of pop-up. But the situation with and without adblock has a subtle difference:   # without pop-up block the new-user-story window is modal. Also the Create button at the top of the screen is disabled.   # with pop-up block the new-user-story window is *almost* modal: the create button at the top is enabled.    In the situation with pop-up blocker the create button looks like the create button at the bottom and users can be confused and use the top button when they want to commit their changes. But the button at the top will always display a new and empty pop-up, where the user looses the just entered information.    Some more remarks:   # Upon clicking the upper Create button twice there are actually multiple popups created, overlaying each other. The user did actually not loose any information and just needed to close the accidental duplicate overlaying popup.   # the button Create is displayed twice. There might be UI design rules about this but the confusion might be less big if the buttons had different names.     """,Suggestion,Issue
426649,"""h3. Issue Summary    If you upload an attachment using the new view, it may not be kept, or you may not be able to preview it.    h3. Environment    Jira Cloud.    h3. Steps to Reproduce    * From a board, attach a file;  * Close the issue, and get back to it;  * You will see that the attachment is not there anymore, but if you click to attach a file again, that same file will be in the recent uploads, but it will work properly now if you select it.    There are some different use cases for that problem:  * If you upload a new file, and click on *Insert file* without waiting for the progress bar/upload to be completed. In this one, the file will be gone after you get back to it:  [^Docx quick insert file.mp4];  * If you upload a new file, and wait for the upload to be completed before clicking on *Insert file*:  [^Docx file full upload waiting.mp4]      h3. Expected Results    The file should be available in the issue after you come back to it. And you should be able to preview it even after the upload.    h3. Actual Results    You cannot preview the file after the upload, and the file will be gone from the issue after you come back to it.    h3. Notes    I've replicated that behaviour with the following extensions so far:  * Docx;  * Xlsx.    The biggest part of the occasions where I've replicated it, was on Next-gen projects. It looks to happen way less often in Classic projects.    Also, most of the occasions are on the detail view from the board. But I did get to replicate it in full view in some rare occasions.    If you upload a file after an unsuccessful one, the file that was affected will be fixed:  [^Next upload.mp4]     If you do not removed that affected file before selecting it back from the recent uploads, you may end up with a duplicate:  [^Cannot preview file.mp4]     Usually, the file affected by such problem will have the following highlighted icon on it:  !Screen Shot 2019-09-11 at 15.22.13.png|thumbnail!     h3. Workaround    Select the file from the recent uploads.     Uploading a new file will fix the affected file automatically (although the new one may be affected).""",Bug,Issue
426704,"""As a user, I'd like to have an option to sync, at the new issue view, the field configuration to the issue layout, respecting the selection and the field visibility.  The new issue view hides important fields and it is a time-consuming task to recreate the screens manually.""",Suggestion,Issue
426731,"""This probably needs to be broken into multiple tickets, but it's better told as a single journey of trying to add and delete a gadget.  h3. Experience    After clicking """"Add gadget"""" multiple times and deciding that """"Add gadget"""" was broken, I closed the window to report a bug (couldn't find anywhere in Jira to do that, but that's a different ticket).    When the """"Add a gadget"""" dialog closed, I could see that multiple gadgets had been added.    Not great, but I wanted to move forward.    Clicked """"cancel"""" and nothing happened, again.    Clicking """"save"""" doesn't do anything either.    Had to look in the meatball menu to find a 'delete' option to clear up all the excess gadgets that had been added.       h3. Issue summary   # When you click """"Add gadget"""" there is no indication that a gadget was added.   # """"cancel"""" and """"save"""" appear to do nothing.   # """"cancel"""" at this point in the flow would appear to offer the functionality of """"delete"""" in the meatball menu, but does nothing.   # You can't tell when you've deleted one of multiple gadgets   ** The gadget placeholder (thing with the """"chart title"""", """"chart theme"""", etc in it) is really tall and there is no delete effect.  This means that if you have added multiple and go to delete one, it appears that the delete didn't work.    ** The next one takes it's place with no delay or even a flicker.   ** There is no visible scrollbar so you can't even tell that there are multiple gadgets.               """,Bug,Dashboards & Gadgets
426780,"""h3. Summary    As an Admin, I would like to be able to choose which project roles will be displayed in each project type. This feature is especially necessary when separating Jira <USER>project roles from other project types.""",Suggestion,Project Administration
426847,"""Hi,    I would like to suggest that you modify the visibility of added fields shown on card layouts, so that these only shows if the field is included in the screen of a given issue type.    -------    A concrete example.    We use due date for our deploy issue type. But on none of the other issue types. It's very helpfull to have it visible in the sprint view as a talking point at stand-ups.    This conflicts with another focus of ours, which is to keep the sprint view lean and free of irrelevant fields and information.    In it's current state when adding a field to the card layout any issue will have the field added. Those without the field in their screen will all have """"None"""" added underneath them, expanding all ours cards and crowding the sprint view.""",Suggestion,Jira Core Board
426885,"""When viewing an issue in the new issue view, it is no longer possible to edit an issue using the edit screen and this creates some inconvenience in editing the issue by field as the notification from Jira trigger on every field modification.     I would like to suggest offering the edit screen as an option in the transition menu.""",Suggestion,Issue
426903,"""I have installed Jira software 8.2.2 as an evaluation.    I use English and Czech language.    I would like to ask or contact the person who created the czech language. I think there are a lot of translations for one word """"Issue"""". It means that sometimes it is translated as task or requirement and i think this is missleading.    See the attached file. Mainly it is in the administrator page.    Thank for your thought.    Best Regards""",Suggestion,Translations
427000,"""h3. Summary    As a user, I would like the + Create Issue in Epic button to open a pop-up that allows me to change the project of the issue I am creating in the Epic as it happened in the old issue view.  h3. Actual Scenario    The in-line creation only allows you to create issues in the Epic that belong to the same project.  h3. Expected Scenario    Open a pop-up window to allow me to select issues from other projects.  h3. Workaround    Set a field as required in the projects to enforce the pop-up screen to fill the required fields.""",Sub-task,Issue
427009,"""By default, the linked issues view will display the following fields:   * Issue key.   * Issue summary.   * Issue assignee.   * Issue priority.   * Issue status.    !Screen Shot 2019-07-15 at 16.41.05.png|width=357,height=184!    As a Jira administrator, I would like to have the option to customize the linked issues view, including more fields or changing the order of the fields. """,Suggestion,Issue
427035,"""The Batching Email Notifications functionality was a great first step, but when someone enters a comment on an issue that I'm assigned to or watching AND mentions me, I now get two emails, one for the mention and one for the issue update where before I got one.  And the goal is to reduce the number for folks.    It would be great to add an option to treat the the mentions the same as issue updates. """,Suggestion,Events & Notifications
427136,"""h3. Summary    As a Project Manager, I want to be able to forward an email into JIRA and   have it create a sub-task under a specific issue.  h3. Suggestion    Send an email to Jira, specifying the issue key of the target issue, as well as a suffix like *(sub-task)*.""",Suggestion,Issue
427158,"""As a user I would like to be able to see issues within a swim lane categorised in a particular order that is required for the board so that I don't have to use a range of custom filters instead.    E.g.,     _Parent Swim lane based on component type, Child swim lane based on user_    * Component a   ** <USER>  *** Issues....   ** User 2   ** User 3   * Component b    ** <USER>  *** Issues....   ** User 2   ** User 3   * Component c...""",Suggestion,Jira Core Board
427181,"""h3. Problem Definition  As a user, I want a feature to allow the user to accept the assignment    h3. Suggested Solution  A user should be able to accept the assignment    h3. Why this is important  Customers want a feature to allow the user to accept the assignment  """,Suggestion,Issue
427237,"""With the changes in the new issue view, the edit screen is no longer going to be a separate entity from the view screen. As a result the filter results gadget (which only displays in list mode) no longer has the """"edit"""" option in the drop down ellipsis menu.    I would like to propose that the gadget at least get the option to open the new issue view modal (like in the board view) when one wants to edit issues.  """,Suggestion,Dashboards & Gadgets
427244,"""As a user, I'd like to hide or collapse some columns on Business Boards to make cards better to read or just hide the columns that I'm not using at the moment, so I can have more space only for the columns that I need. """,Suggestion,Jira Core Board
427359,"""Currently, JIRA will send notifications to the e-mail registered on my profile settings. As a user, I would like to have an option available on my profile to select a secondary e-mail that will receive my notifications coming from JIRA. """,Suggestion,Events & Notifications
427363,"""h3. Summary    As <USER>when I try to paste an attachment it shows up very large within a comment so I need to scroll back and forth to see it.     !screenshot-3.png|thumbnail!     As you can see in the screenshot there is a bar (colored in yellow) and the user should scroll back and forth.    (Note: A video is pasted as an internal comment to describe better the behavior. )    h3. Environment  * Cloud  * New issue view    h3. Steps to Reproduce  # Access to a Jira issue in the new issue view  # Try to drag and drop a large image to the comment field    h3. Expected Results  * The image is resized automatically and it shows up in a friendly and clear way to ensure a good User Experience.       OR     * As it used to be in the old issue view, the image shows up as thumbnail.     h3. Actual Results  * The attachment shows up unreasonably large.     h3. Notes  * A valid option would be to provide the administrator the option to Configure thumbnail size via attachment settings.     Or,    * Give the user himself the option to resize the image easily.    Or,     * Show the image as thumbnail, by default, as it used to show in the old issue view.     h3.Workaround  * Either disable the new issue view from the personal settings or edit the comment in the old issue view (you can do this by appending *?oldIssueView=true* to the url) and modify the value shown in the screenshot below:   **  !screenshot-1.png|thumbnail!   * And change it to:   **  !screenshot-2.png|thumbnail!    """,Sub-task,Issue
427450,"""As a Jira Software user, I want the feature mark Epic as done available only after all the Epic's children are done.    An Epic should not be closed when its stories are still open.""",Suggestion,Project Administration
427469,"""As a customer, I would like to have the possibility of configuring the calendar on the Log Work Screen. As an example, hide week number information.""",Suggestion,Issue
427489,"""As a user of Jira, I'd like to be presented with a card containing profile information on rollover of assignee / reporter.""",Sub-task,Issue
427491,"""Why? As a consultant company we are working always together with our clients over Jira and Confluence. Sometimes even over multiple companies with Jira and Confluence. As employee or project member it is always hard to stay up-to-date with my personal tasks over multiple Jiras. I need to login everywhere and check where and what has the highest priority. Sharing projects between one or multiple companies would reduce complexity for single project members a lot and also would enable a huge potential for further plugins and helps.          Suggestion: Check also how Slack solve it over shared channels and see that each company can manage their own members and participate in a shared channel with the other company.""",Suggestion,Ecosystem
427684,"""All validators must pass in order for a transition to complete. Currently an issue will fail as soon as one validator fails, without running subsequent validators. The disadvantage of this is that it staggers the error messages.    For example in my transition I have several validators, in this order:  1. Fields Required (About a dozen fields must be filled out)  2. Date Compare (I want to validate some date fields)  3. Comment Required (Using a custom plugin I ensure a comment is entered)  4. User Picker Validator (Using a custom plugin I ensure a user picker has a minimum number of users selected and that they have been selected from a particular group)    When a user submits a ticket in violation of all four validators they only see the error from 'Fields Required'. When the required field is added and they resubmit they will then see the error from 'Date Compare', and so forth. The overhead of running all validators is low and, while not logically necessary, having the ability to display all error messages simultaneously would enhance usability.  """,Suggestion,Project Administration
427708,"""As a Jira cloud administrator, I want the permission helper to mention when a user has not permission on products' features.    For example, a user who is only a Jira Core member cannot access Jira Software features. I want the permission helper to warn that a user cannot access Jira Software features since this user is only a Jira Core member.""",Suggestion,Project Administration
427716,"""As a larger organization, we rely heavily on standardized views for issues.    Currently, Jira issue layout is based in part on the screens, however, due to the fact that the issue view screen is largely ignored by the new issue view, this means that the issue layout must be customized for each every project, and a standard view cannot be produced and used.    I would like to propose the option of making the issue layout something that can be copied or shared between projects.""",Suggestion,Issue
427773,"""h3. Problem Definition  As a Jira administrator I'm using the People feature and I created some teams.  Now, I would like to use REST API to access teams info and analyze teams-related data.     h3. Suggested Solution  Provide publicly available REST API endpoints to access this data.    h3. Why this is important  Data analysis """,Suggestion,Ecosystem
427919,"""h3. Problem Definition  As a Jira administrator, I have added a list of available sub-tasks on my cloud site (screenshot from my test cloud site with random names):   !screenshot-1.png|thumbnail!     As a Jira administrator, I want to have the option to select the default default Sub-Task issue type value based on the parent Issue Type. So, for example, for issue type *Bug* I want the default *Sub-Task issue type value* set as *Sub-Bug*.      !screenshot-2.png|thumbnail!     h3. Why this is important  Avoid user-related errors by forgetting to set the correct sub-task value.        h3. Workaround   Currently, there is no workaround to set the default value for sub-tasks. So, raising user awareness to not forget selecting the sub-task value is the only valid way. """,Suggestion,Issue
428117,"""{panel:title=Answer|bgColor=#F8F8F8}  The *m* (not M) stands for minute.    Month based search is not supported.    Considering that *m* stands for minutes, I've tested the searches again, and the results are correct.  {panel}    h3. *Summary*    JQL search towards relative date is inconsistent.    Here are some tests:  {panel}  h3. Tests in my test instance  Created three issues:  * RP-959:  ** Created: 21/Nov/2018  ** Duedate: 21/Dec/2018  * RP-960:  ** Created: 21/Nov/2018  ** Duedate: 20/Dec/2018  * RP-961  ** Created: 21/Nov/2018  ** Duedate: 21/Nov/2018    Searches and results:  * 1 -> {code}project = """"Right project"""" AND key >= RP-959 AND duedate < 1M{code} No results;  * 2 -> {code}project = """"Right project"""" AND key >= RP-959 AND duedate < 30d{code} *RP-961* and *RP-960* returned;  * 3 -> {code}project = """"Right project"""" AND key >= RP-959 AND duedate > 1M{code} *RP-960* and *RP-959*;  * 4 -> {code}project = """"Right project"""" AND key >= RP-959 AND duedate > 30d{code} no results.  {panel}      h3. Environment    Jira Cloud    h3. Expected Results    In these cases:  * Search number *1* should have returned RP-960 and RP-961  * Search number *3* should not have returned any result.    h3. Actual Results    * Search number *1* returned no result;  * Search number *3* returned two results.    h3. Workaround    As a suggestion, it did seem that using *duedate > startofday(1M)* (or with *<*) is working properly.""",Bug,Issue
428197,"""h3. *Summary*    I've identified inconsistencies in the results displayed by the Created vs. Resolved Gadget:  {panel}  *1.* The gadget is not handling correctly the values from the current week/previous week:  * Cumulative Collection Operation:   ** The previous week is displaying the same value from the current one (both Created and Resolved issues). In this case the value until the previous week should be lower than it is (you can check that by clicking to see the results in the issue navigator)   !Screen Shot 2018-11-06 at 14.40.12.png|thumbnail!  !Screen Shot 2018-11-06 at 15.13.53.png|thumbnail!   ** The value for the Created issues in the current week is displaying a lower value than it should. In this case when I've checked the issue navigator, there Were actually 409 issues created, not 406:   !Screen Shot 2018-11-06 at 14.48.14.png|thumbnail!  !Screen Shot 2018-11-06 at 15.17.33.png|thumbnail!   * Count Collection Operation:  ** The current week is not even counted:   !Screen Shot 2018-11-06 at 14.57.18.png|thumbnail!   ** The previous week is displaying a higher value than what it should be. It is actually including the value from the current week (even though the value it summed up with is lower than what it should be for the current week). I was able to identify that by creating issues. That would add to the value from the previous week. You can also check in the issue navigator:    !Screen Shot 2018-11-06 at 14.59.28.png|thumbnail!  !Screen Shot 2018-11-06 at 15.23.03.png|thumbnail!  !Screen Shot 2018-11-06 at 15.23.17.png|thumbnail!   {panel}    h3. Environment    Jira Cloud.      h3. Expected Results    The values for the previous week should not count towards the previous one. The value for the current week should match with the one in the issue navigator.    h3. Actual Results    The values for the previous week is higher than it should (it is probably counting values from the current week). The value for the current week does not match with the one displayed in the issue navigator.    h3. Workaround    No workaround at the moment""",Bug,Dashboards & Gadgets
428376,"""When managing dashboards from the """"All Dashboards"""", if you delete a dashboard, the process will redirect the user to a delete dashboard page. As a consequence of this, the filtering that the user had previously set will be lost as a result. This can negatively impact the user experience, particularly when the user is trying to manage many Dashboards.    I would like to suggest that the filtering be preserved when redirected back to the """"All Dashboards"""" page.""",Suggestion,Dashboards & Gadgets
428627,"""h3. Problem Definition  * As an administrator I want an easier way to identify when a component lead is an inactive user.   * Currently, I need to check the list of users through *site administration*, then verify if an inactive user is set as component lead through the *components* page.    h3. Suggested Solution  * Similarly to inactive assignee, we suggest to add the word *inactive* between parenthesis to the component lead who is set as inactive.     h3. Why this is important  * Component lead receives a notification when the component is set to an issue. When the component lead is inactive, losing track of the component is considered a risk.   * Identifying inactive component lead through the components page makes administering this feature easier.""",Suggestion,Directories
428646,"""h3. Summary  * Issue collector is requesting username as a mandatory field which is contradictory to our announcement that the username is deprecated.     h3. Steps to Reproduce  # choose a project  # Try to create an issue collector   # The *Reporter* field is mandatory  #* you cannot set the email address as a reporter.   #* you have to choose username     h3. Expected Results  * <USER>is supposedly deprecated.   * Full name or email address is supposed to be a valid input to the reporter field.   * Moreover, having the *Browse users and groups* global permission, I'm supposed to see the list of options when writing the reporter name (similarly to the behavior of the field assignee in the Issue view) but nothing shows up.     h3. Actual Results  * Only the username is accepted.     h3.Workaround  * You can get the username by hovering-over the full name of the user in the site administration section. """,Bug,Issue
428685,"""When viewing an issue in the new issue view, it is no longer possible to edit an issue using the edit screen.    Given that fields that do not have a value do not show up, this means that it will not be possible to set values that are otherwise not available to change on transitions, nor is it possible to set those fields without performing a transition.    I would like to suggest offering the edit screen as an option in the transition menu, also allowing the option to block the inline edit in the new issue view.""",Sub-task,Issue
428741,"""As a developer, when I request a review I would like that to initiate a trigger in Jira so that our workflow can ensure the related issue moves to Dev Review automatically.  Developers often forget to move the Jira issues and this would more fully automate it. It looks like Jira has that integration for Crucible.          Same request for Review Approved.  Review started and Review Rejected would be nice to have as well, but Request Review and Review approved would be most important. """,Suggestion,Ecosystem
428776,"""h3. Summary    * As an admin, I'm editing the workflow.   * I'm updating specifically the parameter of the post function *Update Issue Custom Field*.  * The custom field is of type *Text Field (multi-line)*.   * No Character count limit is stated for the use of the type *Text Field (multi-line)* nor for the post function *Update Issue Custom Field* parameter. However, a 404 error shows up when we addded more text.   * We estimated the text size for 16k character.       h3. Steps to Reproduce  # Create custom field of type *Text Field (multi-line)*.   #*  !screenshot-1.png|thumbnail!   # Add post function *Update Issue Custom Field* to the workflow.   #*  !screenshot-2.png|thumbnail!   # Try adding a big dummy text of 16k character.   # The post function will be updated.   # Try adding more one character to that big dummy text.     h3. Expected Results  * The parameter is updated.    h3. Actual Results  * A 404 error is thrown and the parameter is not updated.   *   !screenshot-3.png|thumbnail!     h3. Notes  # We can find on the developer panel that the *Header* include the *old value* (parameter is called *dummy field*) and the *new value* as shown in the short video below (which may be the reason for not completing the request and getting an answer or actually getting a 404 answer):  #*  [^Recording #221.mp4]   # I was not able to replicate the issue in one test instance and I was able to replicate it in another test instance (Please check internal comment for further details).     h3.Workaround  Currently no workaround to prevent this happening.""",Bug,Project Administration
428958,"""Through the UI, you are capable of granting a user the Administer Projects project permission but also denying them the Browse Projects project permission. As a result, this user can get to the admin console if they know the URL or also send admin requests via the REST API, but that user cannot see the project in the list of projects or view tickets, etc.    I'm guessing this is not an intentional state to allow users to get themselves into this situation.    Here's how I did this:   # Define a custom project role with no default actors (i.e., default members), _my-custom-project-role_.   # Define a custom permission scheme (i.e., not the default scheme) that uses custom groups. The scheme restricts access based on custom groups like _company-name-developers_, _company-name-admins_, etc., and does not use default groups to grant permissions.   # Have a user, _Leo_, that is not a member of any of the custom groups but who belongs to all default Jira groups and has application access to Jira.   # Grant _Leo_ admin permissions (Administer Project project permission) for the project via _my-custom-project role_ (depends upon this role being granted admin permission in the permission scheme).   # Use the Permission helper to confirm that <USER>cannot Browse Projects for the project but does have Administer Project.   # Observe that _Leo_ can send admin requests to below and get successful responses (works for other methods):  {code:java}  PUT /rest/api/2/project/{projectIdOrKey}/role/{id}{code}     # Observe that _Leo_ can access the admin console for the project and make changes to the project but cannot view tickets.    *Expectation*    It seems inconsistent that you'd be able to admin a project for which you cannot view issues or browse to the admin console. It should be either not possible to put the user in that state, or the consequences of such permissions set-up should be more clear when setting them.""",Suggestion,Global Administration
428968,"""JIRA server platform has a feature called '<USER>issue-detail'. It allows developers to display issue details page on their own plugin view.     I would like to have the same possibility on the JIRA cloud platform. I would like to be able to embed DetailView.""",Suggestion,Issue
429008,"""h3. Summary  When trying to use the option  *Export this filter in another format* > 'Export Excel CSV (All fields)' with issues that contain a combination of long comments and a great number of comments, breaks the .csv file exported. As can be seen below.    I was able to reproduce this BUG with an issue containing 3 comments, each comment containing 30000 characters, as this seems the easier way to reproduce this BUG, but this result can be achieved by reducing the number o characters in each comment and increasing the number of comments (for example 10 comments with 10000 characters) but the trigger to break the .csv file seems to be a comment with 30000 characters or more (this BUG will be triggered once at least 2 comments with 30000 characters were added, or more comments with a lower character number such as 10000 were added)      h3. Environment  * Any issue(s) that has a great number of comments and long comments (such as an issue with chain email responses as comments)  * Trying to export issues to .CSV, from filter/issue search, with at least one issue with the prerequisite described above    h3. Steps to Reproduce  # Create a test issue;  # Add at least 3 comments with 30000 characters each;  # Navigate to issue search, and search for this specific issue;  # Click """"Export this filter in another format""""   !Screen Shot 2018-06-19 at 13.18.12.png|thumbnail!  on the top right corner > Export Excel .CSV File (All fields)   !Screen Shot 2018-06-19 at 13.17.49.png|thumbnail!      h3. Expected Results  * Generated .CSV file is able to export each field to a single column, including all comments, for each issue    h3. Actual Results  * Generated .CSV file overflows the columns for comments, and end up filling the column """"Summary"""" with all the overflown comment content, and all the comments from that point on are added subsequently to the following rows, each comment line being added as a new row.   * For example, a 1000 issue export .csv file must contain 1001 rows, instead a massive file is generated, sometimes exceeding 65000 lines        h3. Notes  I've included some example .CSV files as examples  * Issue with 3 comments with 30000 characters each   [^Example issue with 3 comments.csv]   * Issue with 10 comments with 10000 characters and 1 comment with 30000 characters   [^Example issue with 11 comments.csv]     """,Bug,Issue
429011,"""As a JIRA administrator, I'd like to request the review of how the monospaced markdown is currently working in the new JIRA User Interface.     Currently, the markdown doesn't work very well depending on the brackets' position:    E.g.:     {code}  1 - {{testing}}    2 - {{ testing }}    3 - {{testing }}  4 - {{ testing}}  5 - foo{{bar}}   6 - see how multiple {{markdown}}s will not {{render}} correctly  {code}    The results of this markup are the following:   {panel:title=Results|borderStyle=solid|borderColor=#000000|titleBGColor=#99b3ff|bgColor=#e0e0eb}  1 - {{testing}}    2 - {{ testing }}    3 - {{testing }}  4 - {{ testing}}  5 - foo{{bar}}   6 - see how multiple {{markdown}}s will not {{render}} correctly  {panel}    """,Suggestion,Issue
429115,"""h3. Problem Definition  As a user, I want to be able to create Epics/Stories from within a Task just as we can create a task/story/subtask from within an Epic/Task and automatically link the issues.     h3. Suggested Solutions  Create a button or another feature to make that possible. """,Suggestion,Issue
429186,"""h3. Summary  Currently, the only possible way to move tickets from one JIRA instance to another is by using the CSV export/import method. This is a great method when there's a need to transfer tickets in bulk, but when moving a single issue, having a button in the issue view would be better. As a user, I'd like to have this button available which would make this process easier.    h3. Suggested Solution  A button could be available in the issue view, which would allow the user to move a ticket to another instance.    h3. Why this is important    Currently, we are seeing cases in the support channel where customers work in more than one instance (third-party vendors, partners, their own customers, etc). They have access to those instances, having a licensed user account registered on them. This feature is a good way to increase the collaboration between different teams, in different Cloud instances. """,Suggestion,Issue
429616,"""As an administrator, I would like to be able to enable the following new attach file dialog in issue collectors.     !image.png|thumbnail! """,Suggestion,Issue
429909,"""h3. Summary   * As a Jira administrator I want to change the value of *autowatch own issues* but find myself changing the *Default outgoing email format value* without noticing.   * The Default outgoing email format value changes automatically from html to text.    !screenshot-1.png|thumbnail!  h3. Steps to Reproduce    1. Go to System > under User Interface > User Default Settings   2. Check that Default outgoing email format is set to html   3. Click edit default values in order to switch *Autowatch own issues* value from yes to no   4. See that Default outgoing email format has been switched to """"text""""  h3. Expected Results:   * The value shown in the *email format* +select list+ is the same as the value *Default outgoing email format* screen.    h3. Actual Results   * The value shown in the *email format* select list is: text   * However the value shown in the *Default outgoing email format* in first screen is: html    h3. Notes   * This behavior occurs only when first editing the default values. so to test it, *you need to create a new test instance*.   * This is important because someone can follow the public documentation to change the value of *Autowatch own issues* but he finds himself changing the *Default outgoing email format* without noticing.""",Bug,Global Administration
430239,"""As a developer working on a Jira Integration I would like to ease the install process of the integration for our users by reducing the number of manual steps they need to perform. One of these steps is finding the customfield(s) created (and managed) by the integration and changing the renderers from the default of plain text to wiki markup.         It would be super cool if this could be done as part of the customfield creation API call for multiline text (text area) and other applicable types.""",Suggestion,Issue
430821,"""As a user, I'd like to have the ability to choose how checkboxes will be displayed on JIRA screens when using the *Checkboxes* type custom field.    Currently, when this custom field has too many options, all checkboxes will be displayed in sequence (in a list), which affects the design of the page where the field is included.    Ideally, I'd like to have the ability to display the available options in columns. """,Suggestion,Issue
430840,"""h3. Summary    I'm not 100% sure if this is the same bug as JRACLOUD-33054. That's why I raised it separately.  QuickCreate/Edit dialog *always* shows """"Some issue types are unavailable..."""" warning regardless of the field configuration and/or workflow association. I could reproduce this problem with fresh JIRA Cloud instance.    h3. Steps to Reproduce    Just opening QuickCreate/Edit dialog reproduced this bug.    h3. Expected Results    The waning message should be shown appropriately according to the field configuration and/or workflow association.    h3. Actual Results    The warning message was always shown regardless of the field configuration and/or workflow association as follows.    * QuickCreate/Edit dialog  !create_issue.png|thumbnail!  * The each issue type is using the same field configuration  !field_configuration.png|thumbnail!  * The each issue type is using the same workflow association  !workflow.png|thumbnail!     h3.Workaround    There is no workaround at this time.""",Bug,Issue
430857,"""As a user, I would like to have the ability to setup a workflow step on a JIRA project that would automatically create a new Confluence page when triggered. This could be designed as a post-function that could be associated with a screen. This screen would collect the information that will be used in the page creation.     Would be also interesting to have an option on this feature that would copy all the labels associated with the JIRA issue to the new Confluence page. """,Suggestion,Ecosystem
430873,"""As a user, I would like to have the possibility to render a Gadget by selecting the *statusCategory* on the *statistic type* field on JIRA Dashboards. This would allow me to display issues grouped by *statusCategory* instead of having to select a more generic display.    """,Suggestion,Dashboards & Gadgets
430970,"""As a user, I would like to discuss JIRA issues that are connected to a HipChat room from within the issue, without the need to open the HipChat app.     """,Suggestion,Issue
431225,"""AS a user, I would like to have the option to select which kind of security level will be used on my JIRA issues on a project basis. For example, I would like to restrict access to the issue's attachments if the user isn't the current assignee. """,Suggestion,"Issue,Global Administration"
431592,"""Even though the option to Enable Autofill is checked within the browser's configuration, when creating issues, for example, on any AOD JIRA instance, the """"cache"""" doesn't show if another issue has a similar name.    Reproduction steps:  Login on AOD JIRA.  Create a new issue, in my case with the summary: Test issue one.  Create another issue which summary starts with """"Test issue..."""". It should appear the """"suggestion"""" of the previous issue created """"Test issue one"""", bit it doesn't.    I'm using latest Chrome and Ubuntu 12.10. On latest Firefox on the same environment it's ok.  """,Bug,Issue
431846,"""<skipped>.atlassian.net was chain-OOM-ing earlier today. [~<USER> was able to narrow it down to an image attachment on a particular issue. It's only a 300KB PNG file (a screenshot from an Android device) but it causes JIRA to OOM almost immediately. I've been able to replicate that behaviour on my <USER>dev instance.  """,Bug,Issue
431970,"""... and has a few issues:    * No call to action (what am I supposed to do about this problem?). Not sure what it means, as the user does seem to have access to <USER>I'm guessing someone else fixed the problem?). Also there's no explanation of what """"the error"""" actually is.  * Footer has unsubstituted version variables.  * Atlassian logo is in email, but missing    !email.jpg!""",Bug,Ecosystem
432291,"""h3. Summary    When an issue is loaded, it appears that if any attachments are attached to the issue, the issue will load in the attachments section of the screen instead of at the top of the issue when the issue is selected. This happens in all browsers.    h3. Environment  (Optional - If Applicable)  * Most recent version of Cloud  * All browsers    h3. Steps to Reproduce  # Go into issue browser  # Select an issue that has an attachment (do not choose detailed view, just load the entire issue).    h3. Expected Results  The issue should load at the top of the page.    h3. Actual Results  The issue loads in the attachments section of the page.     h3. Notes  I've tested this in a test instance as well as a customer instance and was able to reproduce in both. Nothing unusual appears in the logs or the browser console.    h3.Workaround  Scroll up after page loads.""",Bug,Issue
432418,"""The ticket was actually created but the Create dialog was still open and there was no sign/notification about the new ticket. And I can click the Create button as many times as I want. This issue will easily lead to duplicated tickets. Below is an example screenshot  !Screen Shot 2015-06-15 at 3.09.14 PM.png|width=600!""",Bug,Issue
439851,"""Begin with a workflow in JIRA that is already associated with a project.  Copy that workflow and give it a new name. Create a new workflow scheme and associate the new workflow with that scheme. Create a new project and assign the new workflow scheme to it.  Make three changes to the new workflow:  1) Modify status, for instance change Open to ToDo 2) Modify transition, for instance change Start Progress to Begin Work 3) Associate the new workflow with a different set of issue types than the original workflow  Despite these changes being made to a copy that is associated with a different scheme and a different project, the original workflow will pick up at least some of the changes to statuses, but not to transitions or issue types.  The reverse is true as well. I had this happen accidentally this morning when I created a copy of an existing workflow and ran through the above. When it was brought to my attention, I went to fix the original workflow, only to discover that in the Administration -> Workflows view, the original workflow still looked fine and matched what was expected. Through that view, it had not been edited. The timestamp of the last edit had also not been updated. When viewing the original workflow through the project to which it was attached, however, the changes were visible. This despite no changes having been published to any workflow, including the copy.  You cannot directly edit a workflow through a Project -> Administration view, but I got to it through the workflow scheme and reverted the changes on the original workflow. I  then checked my copy and saw that the copied workflow had the statuses changed, but the additional status I'd created, as well as the transitions I'd modified, were still there.  It looks like a copied workflow continues to reference the same database rows/columns for statuses despite referencing new/different rows/columns for transitions. I would expect a copy to be completely autonomous of the original.""",Support Request,Project Administration
440419,"""1) Create custom fields using the Multi User Picker and User Picker. 2) Navigate to the specific workflow step. 3) Navigate to Validators. 4) Add the custom fields as required field as a validator.  Upon performing the workflow action specified, the user isn't prompted to enter a user and the action completes.  The more specific example: We'd like to notify the developer once we close an issue. So I've created a """"Notify"""" field using both Multi User Picker and User Picker and placed that as the last step of our workflow (Close Issue). On that step transition, I've set up a validator to make that field required (only during that step) but if the field is left empty, JIRA doesn't indicate the field is required. Is there a fix for this issue? """,Support Request,Issue
440602,"""We set up a collector to create bugs from an email account at our company.  I noticed that the bug number does not increment contiguously as would be expected (BUG-3, BUG-4, BUG-5, etc.)  Instead it seems to vary wildly.  Real issues: TEMP-1428, TEMP-1431, TEMP-2672, TEMP-3019, TEMP-4987, TEMP-5392, TEMP-7474, etc...  The collector was doing this in the main project so I pointed it to the temporary project to quit burning through insane amounts of issue numbers.  No one is manually creating issues in the project so there's no other way to account for this behavior.  I've tried to browse to some of the issues in between (TEMP-7300) but it does not exist.  The run up from TEMP-1428 to TEMP 7592 was only 5 days (March 29 to April 2).  I'm not filing this as a support request because I can't see any possible reason this would ever be desired behavior regardless of configuration.""",Support Request,Issue
446383,"""We are currently using JIRA to track and manage defects raised by multiple teams. Each team has a JIRA project - and users are generally permitted to raise issues within their own projects. Issues can be moved between projects to reflect whoever currently has responsibility for each particular issue.  When an issue is moved between projects the reporter changes (for some reason). It is important for us that the reporter remains the same irrespective of which project the issue resides within.  I would appreciate any help or explanation that anyone can provide of why this is happening - and how to stop it happening. As I cannot seem to determine the problem / solution by myself.""",Support Request,Issue
450783,"""In DNN 7.0.0, and 7.0.1 (Have not tested on later version.) if you have code that applies a custom css class to the body of the document, once the user switches to """"Edit Mode"""" on the page, your value is removed and overridden with """"dnnEditState"""".  This causes issues with anyone that might be trying to style things using a CSS class on the &lt;body&gt; tag.  Clear Steps to Reproduce:   In a skin add something similar to the following to add the top level tab as a CSS class to the body.        Protected Sub Page_Load(ByVal sender As Object, ByVal e As System.EventArgs) Handles Me.Load         Dim MyBody As Control = Me.Page.FindControl(""""Body"""")         Dim objMyBody = CType(MyBody, System.Web.UI.HtmlControls.HtmlGenericControl)         objMyBody.Attributes.(""""class"""") &amp;= PortalSettings.ActiveTab.BreadCrumbs(0).TabName.ToString.ToLower.Replace("""" """", """""""")     End Sub   Notice that on each page you should see the lowercase tab name as a class on the body tag.  (NOTE: You can just put in a hard coded class here too, just showing my full use case.)   Actual Result:   You get the desired result when not in edit mode, however, when switching to edit mode the class is dnnEditState ONLY and my custom value is gone :(  Expected Result:   When in edit mode I would assume the class would be something like this   class=""""mypagename dnnEditState""""   where mypagename is the value added by my code, and dnnEditState is the one needed for DNN edit mode.   (FYI this works properly pre DNN 7.0!)""",Bug,UI / Usability
451469,"""Not sure if this is a bug or not.  But its also a critical enhancement request.  See attached .gif.  The default for a group message is everyone and member is NOT selectable from the drop down.  I could not find a way to limit the post to group only. There is no option for group only permission for a post.  I would think that group only permission should be an option for a group post AND the default.  I set this as a show stopper bc I wouldn&apos;t post to a group, knowing that the post was not limited to just the members of the group.""",Bug,Journal
451586,"""My customers are discovering they can�t integrate DMX with the RadEditor Provider as before. As you may recall this was a feature back when <USER>first created the project on CodePlex. When DNN Corp integrated this into the core I was contacted about this and we discussed what to do. As no one wants to upset users/customers we decided to leave the integration as it was until a solution would be found. I.e. until DNN Corp would come up with a way to hook into this from your own module. I�ve been waiting for news on this ever since. But now it appears it has just been ripped out in 6.2.1. What�s more: no one bothered to inform me so I have not had a chance to inform my customers either and to prepare an alternative. All in all it makes all of us look bad, IMO. I�d like to know what the status is on this. I.e. how did this happen and what was the reasoning behind it? Is there a communication I missed about this maybe? How should we (my customers and I) move forward? It�s not the end of the world, but I need to answer my customers soon.   BTW: the FolderProvider is inadequate, IMO. It does not allow communication about permissions for instance, the cornerstone of the DMX module. It was really designed just for its current use which is S3/Azure/etc storage.   Peter""",Bug,Admin
451707,"""As a DotNetNuke module developer, I&apos;d like it if code used by the DotNetNuke framework did not walk the ASP.NET control tree, so that my modules containng non-visible databindable elements are not DataBound before I want them to be, which causes bugs and performance issues.  Clear Steps to Reproduce:      Create a new DotNetNuke module.  This module should contain:              A ListView         A DataSource               Set Visible=""""false"""" to the ListView     Add a breakpoint to the Select method being called via the DataSource     Run the module in a DotNetNuke site in debug mode  Expected Result:      The breakpoint is not hit because the select method was not called     Actual Result:      The breakpoint is hit, because the select method was called  Root Cause:   The Client Dependency Framework contains an extension method, FlattenChildren.  This extension method is being called during Page PreRender.  This extension method and its caller are walking the entire control tree.  As a result of the entire control tree being traversed, any control in the tree that&apos;s databindable is being databound.  At best, this causes performance issues, as Service / Presenter methods are called unintentionally, and at worst, this would cause bugs, because the datasources may not have enough contextual information to call the Service / Presenter methods correctly.""",Bug,Client Resources
451779,"""Brief Description of Issue:       dotnetnuke.com has an awards recognition program     the modules are not available for distribution/sale.     rather than make the modules available, I would like to suggest that you   design core support for a points awarding system with an api for module developers.       This way any activity performed within a module could be configured to award points to a user.     I think this would greatly enhance the adoption of social interactivity because most of the niche implementations I have seen failed because the incentives to participate were not adequate.  If a site rewards participation with incentives that cannot be obtained elsewhere then membership will be more cohesive and retentive.""",New Feature,Core
451915,"""Brief Description of Issue: I want to upload and use new favicons for my site. I can upload to my hearts content but when I try to actually set a new favicon in Admin  --&gt; Site Settings --&gt; Appearances the drop down never shows any file ending with the &apos;.ico&apos; extensions.    Clear Steps to Reproduce:      Step 1 - Install latest 6.2 build     Step 2 - Log in as Host     Step 3 - Navigate Admin  --&gt; Site Settings --&gt; Basic Settings --&gt; Appearance     Step 4 - Under """"Favicon.ico"""" select an existing folder     Step 5 -     Under """"Favicon.ico"""" click """"Upload File""""     Step 6 -  Click """"Browse""""     Step 7 - Select a .icon extended icon file and click """"Open"""" in the file selector pop up     Step 8 - Click """"Save File""""  Actual Result:       Result 1 - The whole SiteSettings.aspx page refreshes and the File drop down for Favicon.ico still showsn only """"&lt;None specified&gt;""""  Expected Result:       Result 1 - New file appears as a selection in the Favicon.ico drop down as well as     """"&lt;None specified&gt;""""""",Bug,Admin
452268,"""Brief Description of Issue - (fresh install vs. upgrade, browsers specific, user role, etc):  As a Registered User I want to use a Custom Link to a specific portal page I click """"Custom Links"""" in my Biography area of my Profile and select """"Our Service"""" Link is not added to the text area instead I get an error  Clear Steps to Reproduce:  1. Run Custom install latest 6.2.0 build 2. Register at least 2 users using a browser with Dev tools installed 3. As a Registered User click your username/avatar 4. Click """"Edit Profile"""" 5. Scroll down the Edit Profile page to Biography 6. In the """"Custom Links"""" drop down expand """"Portal Links""""  7. Click """"Our Services""""    Actual Result:  1. I get this error in the browser dev tools:          Uncaught TypeError: Cannot read property &apos;parentNode&apos; of null                       b.RadEditor.pasteHyperLinkTelerik.Web.UI.WebResource.axd:10304                           InsertCustomLinkTelerik.Web.UI.WebResource.axd:7055                           b.RadEditor.fireTelerik.Web.UI.WebResource.axd:9997                           b.RadEditor._onToolClickTelerik.Web.UI.WebResource.axd:9411                           (anonymous function)Telerik.Web.UI.WebResource.axd:6                           (anonymous function)Telerik.Web.UI.WebResource.axd:6                           Telerik.Web.UI.Editor.DefaultToolAdapter._raiseEditorEventTelerik.Web.UI.WebResource.axd:6168                           Telerik.Web.UI.Editor.DefaultToolAdapter._onDropDownValueSelectedTelerik.Web.UI.WebResource.axd:6133                           (anonymous function)Telerik.Web.UI.WebResource.axd:6                           (anonymous function)Telerik.Web.UI.WebResource.axd:6                           b.EditorButton.raiseEventTelerik.Web.UI.WebResource.axd:4436                           b.EditorDropDown._onPopupClickTelerik.Web.UI.WebResource.axd:4800                           (anonymous function)Telerik.Web.UI.WebResource.axd:6                           b                        2.  This error is logged into the log.resources file:    2012-05-04 12:05:45,401 [TEST-KG11][Thread:15][ERROR] DotNetNuke.Common.Utilities.CBO - IsOwner System.IndexOutOfRangeException: IsOwner    at System.Data.SqlClient.SqlDataReader.GetOrdinal(String name)    at System.Data.SqlClient.SqlDataReader.get_Item(String name)    at DotNetNuke.Entities.Users.UserRoleInfo.Fill(IDataReader dr)    at DotNetNuke.Common.Utilities.CBO.FillObjectFromReader(Object objObject, IDataReader dr)    Expected Result:  1. Our Services link is placed in the HTML editor OR 2. Since I did not click in the text editor first nothing in the Editor ribbon bar should be active before focus is placed in the text area   This is a NEW 6.2 issue I regressed in 6.1.5  and there&apos;s no error thrown""",Bug,Admin
452626,"""Brief Description of Issue: I want a Custom Registration form. I add """"PreferredTimeZone"""" as a registration field After I enable """"Custom Registration"""" and click """"Update"""" every person who registers is an """"Unverified User"""" and I am sent an email notifying of the registration and that I need to authorize the registration request  Clear Steps to Reproduce: 1. Install 6.2.0.1011 2. Go Admin --&gt; Site Settings  3. Click on the """"User Settings"""" tab 4. Click """"Custom"""" for Registration Type 5. """"Registration Fields"""" text box enter this string : FirstName,LastName,Email,City,Country,PreferredTimeZone 6. Click """"Update"""" 7. In a different browser hit the site (make sure browser dev tools are running) 8. Click """"Register"""" 9. Fill out at least the email value (only 1 required for registration"""" 10. Click """"Register"""" button  Actual Result: 1. UI just refreshes 2. In browser Dev tools you see this:  Uncaught Sys.WebForms.PageRequestManagerServerErrorException: Sys.WebForms.PageRequestManagerServerErrorException: An unknown error occurred while processing the request on the server. The status code returned from the server was: 500  3. In the log.resources file these 2 errors are logged:  2012-04-04 14:38:58,801 [Test-kg22][Thread:7][FATAL] DotNetNuke.Framework.PageBase - An error has occurred while loading page. System.InvalidCastException: Invalid cast from &apos;System.String&apos; to &apos;System.TimeZoneInfo&apos;. &nbsp;&nbsp; at System.Convert.DefaultToType(IConvertible value, Type targetType, IFormatProvider provider) &nbsp;&nbsp; at System.String.System.IConvertible.ToType(Type type, IFormatProvider provider) &nbsp;&nbsp; at System.Convert.ChangeType(Object value, Type conversionType, IFormatProvider provider) &nbsp;&nbsp; at System.Convert.ChangeType(Object value, Type conversionType) &nbsp;&nbsp; at DotNetNuke.Web.UI.WebControls.DnnFormItemBase.UpdateDataSourceInternal(Object oldValue, Object newValue, String dataField) &nbsp;&nbsp; at DotNetNuke.Web.UI.WebControls.DnnFormItemBase.UpdateDataSource(Object oldValue, Object newValue, String dataField) &nbsp;&nbsp; at DotNetNuke.Web.UI.WebControls.DnnFormEditControlItem.ValueChanged(Object sender, PropertyEditorEventArgs e) &nbsp;&nbsp; at DotNetNuke.UI.WebControls.EditControl.OnValueChanged(PropertyEditorEventArgs e) &nbsp;&nbsp; at DotNetNuke.UI.WebControls.TextEditControl.OnDataChanged(EventArgs e) &nbsp;&nbsp; at DotNetNuke.UI.WebControls.EditControl.RaisePostDataChangedEvent() &nbsp;&nbsp; at System.Web.UI.Page.RaiseChangedEvents() &nbsp;&nbsp; at System.Web.UI.Page.ProcessRequestMain(Boolean includeStagesBeforeAsyncPoint, Boolean includeStagesAfterAsyncPoint) 2012-04-04 14:38:58,804 [Test-kg22][Thread:7][ERROR] DotNetNuke.Framework.PageBase - ~/Default.aspx?tabid=55&amp;error=An unexpected error has occurred System.InvalidCastException: Invalid cast from &apos;System.String&apos; to &apos;System.TimeZoneInfo&apos;. &nbsp;&nbsp; at System.Convert.DefaultToType(IConvertible value, Type targetType, IFormatProvider provider) &nbsp;&nbsp; at System.String.System.IConvertible.ToType(Type type, IFormatProvider provider) &nbsp;&nbsp; at System.Convert.ChangeType(Object value, Type conversionType, IFormatProvider provider) &nbsp;&nbsp; at System.Convert.ChangeType(Object value, Type conversionType) &nbsp;&nbsp; at DotNetNuke.Web.UI.WebControls.DnnFormItemBase.UpdateDataSourceInternal(Object oldValue, Object newValue, String dataField) &nbsp;&nbsp; at DotNetNuke.Web.UI.WebControls.DnnFormItemBase.UpdateDataSource(Object oldValue, Object newValue, String dataField) &nbsp;&nbsp; at DotNetNuke.Web.UI.WebControls.DnnFormEditControlItem.ValueChanged(Object sender, PropertyEditorEventArgs e) &nbsp;&nbsp; at DotNetNuke.UI.WebControls.EditControl.OnValueChanged(PropertyEditorEventArgs e) &nbsp;&nbsp; at DotNetNuke.UI.WebControls.TextEditControl.OnDataChanged(EventArgs e) &nbsp;&nbsp; at DotNetNuke.UI.WebControls.EditControl.RaisePostDataChangedEvent() &nbsp;&nbsp; at System.Web.UI.Page.RaiseChangedEvents() &nbsp;&nbsp; at System.Web.UI.Page.ProcessRequestMain(Boolean includeStagesBeforeAsyncPoint, Boolean includeStagesAfterAsyncPoint)    Expected Result: 1. Registration completes without error 2. The user is taken to the default.aspx page or redirected as defined in the Registration settings""",Bug,Login/Registration
452635,"""Brief Description of Issue: I have specific fields I want ppl. to use during site registration. I enable Custom Registration and add the fields as a Comma Delimited text string. When users try to register the pop up/page just refreshes and Registration is NEVER completed  Clear Steps to Reproduce: 1. Install latest 6.2.0 build (6.2.0.980 or greater) 2. Log in as Host or Admin 3. Go Admin --&gt; Site Settings 4. Click """"User Account Settings"""" tab 5. Find """"Registration Form Type"""" 6. Click the radio button for """"Custom"""" 7. Enter a comma delimited text string like: Email,FirstName,LastName,City,Country,Biography  8. Scroll down and click """"Update"""" button 9. Open a different browser and hit the site 10. Click """"Register"""" link 11. Fill in/set all required fields 12 Click """"Register"""" button  Actual Result: 1. Registration page/pop up just refreshes 2.&nbsp; Every time you click """"Register"""" this error is logged in the log.resources file:  2012-04-03 10:50:23,566 [TEST-KG11][Thread:23][ERROR] System.Web.UI.Page - ~/Default.aspx?tabid=55&amp;error=An unexpected error has occurred System.NullReferenceException: Object reference not set to an instance of an object. &nbsp;&nbsp; at DotNetNuke.Modules.Admin.Users.Register.Validate() &nbsp;&nbsp; at DotNetNuke.Modules.Admin.Users.Register.registerButton_Click(Object sender, EventArgs e) &nbsp;&nbsp; at System.Web.UI.WebControls.LinkButton.RaisePostBackEvent(String eventArgument) &nbsp;&nbsp; at System.Web.UI.Page.ProcessRequestMain(Boolean includeStagesBeforeAsyncPoint, Boolean includeStagesAfterAsyncPoint)  Expected Result: 1. Registration is completed without error. 2. If there is an error it is trapped/captured and an appropriate message is rendered in the UI.""",Bug,Login/Registration
452650,"""Brief Description of Issue: As a Host or Admin I want to change the Security Role of users and without generating errors   Clear Steps to Reproduce: 1. Install 6.2.0.966 or later 2. In FF or Chrome w/dev tools running log in as Host 3. Go Admin --&gt; User Accounts 4. Create a few users 5. Click into Edit mode for a new user 6.&nbsp; Delete the """"Subscriber Role"""" 7. Add Administrator Security Role to the user account  Actual Result: 1. In Firebug I see this error:       Uncaught TypeError: undefined is not a function                       FCKTools.CreateXmlObjectfckeditorcode_gecko.js:37                           FCKXHtml.GetXHTMLfckeditorcode_gecko.js:44                           FCKDataProcessor.ConvertToDataFormatfckeditorcode_gecko.js:30                           FCK.GetDatafckeditorcode_gecko.js:31                           FCK.UpdateLinkedFieldfckeditorcode_gecko.js:31                       Expected Result: 1. There are no errors generated in Dev Tools/Event Log""",Bug,Admin
452721,"""Brief Description of Issue: I want to be able to set specific Registration requirments at Admin --&gt; User Accounts --&gt; Manage --&gt; User Settings  Clear Steps to Reproduce: 1. Install latest 6.2.0 build 2. Log in as Host 3. Go Admin --&gt; User Accounts 4. Hover over """"Manage"""" button 5. Click on """"User Settings 6. Click """"Registration Settings"""" tab 7. Change the page for """"Redirection after Registration"""" setting 8. Select a couple of the ther variables then click on """"Update""""  9. Check the log.resources file and/or the Event log for errors  Actual Result: 1. I get this mess of repeating errors in log.resources file: 2012-03-27 17:04:21,544 [TEST-KG11][Thread:23][ERROR] DotNetNuke.UI.WebControls.SettingInfo - String was not recognized as a valid Boolean. System.FormatException: String was not recognized as a valid Boolean.    at System.Boolean.Parse(String value)    at DotNetNuke.UI.WebControls.SettingInfo..ctor(Object name, Object value) 2012-03-27 17:04:21,548 [TEST-KG11][Thread:23][ERROR] DotNetNuke.UI.WebControls.SettingInfo - Input string was not in a correct format. System.FormatException: Input string was not in a correct format.    at System.Number.StringToNumber(String str, NumberStyles options, NumberBuffer&amp; number, NumberFormatInfo info, Boolean parseDecimal)    at System.Number.ParseInt32(String s, NumberStyles style, NumberFormatInfo info)    at DotNetNuke.UI.WebControls.SettingInfo..ctor(Object name, Object value) 2012-03-27 17:04:21,551 [TEST-KG11][Thread:23][ERROR] DotNetNuke.UI.WebControls.SettingInfo - String was not recognized as a valid Boolean. System.FormatException: String was not recognized as a valid Boolean.    at System.Boolean.Parse(String value)    at DotNetNuke.UI.WebControls.SettingInfo..ctor(Object name, Object value) 2012-03-27 17:04:21,554 [TEST-KG11][Thread:23][ERROR] DotNetNuke.UI.WebControls.SettingInfo - Input string was not in a correct format. System.FormatException: Input string was not in a correct format.    at System.Number.StringToNumber(String str, NumberStyles options, NumberBuffer&amp; number, NumberFormatInfo info, Boolean parseDecimal)    at System.Number.ParseInt32(String s, NumberStyles style, NumberFormatInfo info)    at DotNetNuke.UI.WebControls.SettingInfo..ctor(Object name, Object value) 2012-03-27 17:04:21,560 [TEST-KG11][Thread:23][ERROR] DotNetNuke.UI.WebControls.SettingInfo - String was not recognized as a valid Boolean. System.FormatException: String was not recognized as a valid Boolean.    at System.Boolean.Parse(String value)    at DotNetNuke.UI.WebControls.SettingInfo..ctor(Object name, Object value) 2012-03-27 17:04:21,564 [TEST-KG11][Thread:23][ERROR] DotNetNuke.UI.WebControls.SettingInfo - Input string was not in a correct format. System.FormatException: Input string was not in a correct format.    at System.Number.StringToNumber(String str, NumberStyles options, NumberBuffer&amp; number, NumberFormatInfo info, Boolean parseDecimal)    at System.Number.ParseInt32(String s, NumberStyles style, NumberFormatInfo info)    at DotNetNuke.UI.WebControls.SettingInfo..ctor(Object name, Object value) 2012-03-27 17:04:21,582 [TEST-KG11][Thread:23][ERROR] DotNetNuke.UI.WebControls.SettingInfo - String was not recognized as a valid Boolean. System.FormatException: String was not recognized as a valid Boolean.    at System.Boolean.Parse(String value)    at DotNetNuke.UI.WebControls.SettingInfo..ctor(Object name, Object value) 2012-03-27 17:04:21,594 [TEST-KG11][Thread:23][ERROR] DotNetNuke.UI.WebControls.SettingInfo - String was not recognized as a valid Boolean. System.FormatException: String was not recognized as a valid Boolean.    at System.Boolean.Parse(String value)    at DotNetNuke.UI.WebControls.SettingInfo..ctor(Object name, Object value) 2012-03-27 17:04:21,596 [TEST-KG11][Thread:23][ERROR] DotNetNuke.UI.WebControls.SettingInfo - Input string was not in a correct format. System.FormatException: Input string was not in a correct format.    at System.Number.StringToNumber(String str, NumberStyles options, NumberBuffer&amp; number, NumberFormatInfo info, Boolean parseDecimal)    at System.Number.ParseInt32(String s, NumberStyles style, NumberFormatInfo info)    at DotNetNuke.UI.WebControls.SettingInfo..ctor(Object name, Object value) 2012-03-27 17:04:21,600 [TEST-KG11][Thread:23][ERROR] DotNetNuke.UI.WebControls.SettingInfo - String was not recognized as a valid Boolean. System.FormatException: String was not recognized as a valid Boolean.    at System.Boolean.Parse(String value)    at DotNetNuke.UI.WebControls.SettingInfo..ctor(Object name, Object value) 2012-03-27 17:04:21,603 [TEST-KG11][Thread:23][ERROR] DotNetNuke.UI.WebControls.SettingInfo - Input string was not in a correct format. System.FormatException: Input string was not in a correct format.    at System.Number.StringToNumber(String str, NumberStyles options, NumberBuffer&amp; number, NumberFormatInfo info, Boolean parseDecimal)    at System.Number.ParseInt32(String s, NumberStyles style, NumberFormatInfo info)    at DotNetNuke.UI.WebControls.SettingInfo..ctor(Object name, Object value) 2012-03-27 17:04:21,641 [TEST-KG11][Thread:23][ERROR] DotNetNuke.UI.WebControls.SettingInfo - String was not recognized as a valid Boolean. System.FormatException: String was not recognized as a valid Boolean.    at System.Boolean.Parse(String value)    at DotNetNuke.UI.WebControls.SettingInfo..ctor(Object name, Object value) 2012-03-27 17:04:21,655 [TEST-KG11][Thread:23][ERROR] DotNetNuke.UI.WebControls.SettingInfo - String was not recognized as a valid Boolean. System.FormatException: String was not recognized as a valid Boolean.    at System.Boolean.Parse(String value)    at DotNetNuke.UI.WebControls.SettingInfo..ctor(Object name, Object value) 2012-03-27 17:04:21,677 [TEST-KG11][Thread:23][ERROR] DotNetNuke.UI.WebControls.SettingInfo - String was not recognized as a valid Boolean. System.FormatException: String was not recognized as a valid Boolean.    at System.Boolean.Parse(String value)    at DotNetNuke.UI.WebControls.SettingInfo..ctor(Object name, Object value)   Expected Result: 1. No errors are thrown when you save changed values in the Registration Settings tab""",Bug,Login/Registration
452977,"""Brief Description of Issue: As a Host I need to edit a users Profile  Clear Steps to Reproduce: 1. Clean Auto install 6.2.0 2. Log in as HOST 3. In separate browser register a new user 4. As host go Admin --&gt; Users 5. Click edit icon for newly registered user  Actual Result: 1. I get an error in the UI:   Error: is currently unavailable. DotNetNuke.Services.Exceptions.ModuleLoadException: Object reference not set to an instance of an object. ---&gt; System.NullReferenceException: Object reference not set to an instance of an object. at ASP.desktopmodules_admin_security_securityroles_ascx.__DataBinding__control15(Object sender, EventArgs e) at System.Web.UI.Control.OnDataBinding(EventArgs e) at System.Web.UI.Control.DataBind(Boolean raiseOnDataBinding) at System.Web.UI.Control.DataBindChildren() at System.Web.UI.Control.DataBind(Boolean raiseOnDataBinding) at System.Web.UI.Control.DataBindChildren() at System.Web.UI.Control.DataBind(Boolean raiseOnDataBinding) at System.Web.UI.WebControls.DataGrid.CreateItem(Int32 itemIndex, Int32 dataSourceIndex, ListItemType itemType, Boolean dataBind, Object dataItem, DataGridColumn[] columns, TableRowCollection rows, PagedDataSource pagedDataSource) at System.Web.UI.WebControls.DataGrid.CreateControlHierarchy(Boolean useDataSource) at System.Web.UI.WebControls.BaseDataList.OnDataBinding(EventArgs e) at DotNetNuke.Modules.Admin.Security.SecurityRoles.BindGrid() at DotNetNuke.Modules.Admin.Users.ManageUsers.ShowPanel() at DotNetNuke.Modules.Admin.Users.ManageUsers.BindData() at DotNetNuke.Modules.Admin.Users.ManageUsers.OnLoad(EventArgs e) --- End of inner exception stack trace ---   2. 2012-02-16 11:23:19,744 [TEST-KG11][Thread:5][ERROR] DotNetNuke.Modules.Admin.Users.ManageUsers - FriendlyMessage=""""Error:&nbsp; is currently unavailable."""" ctrl=""""ASP.desktopmodules_admin_security_manageusers_ascx"""" exc=""""System.NullReferenceException: Object reference not set to an instance of an object. &nbsp;&nbsp; at ASP.desktopmodules_admin_security_securityroles_ascx.__DataBinding__control15(Object sender, EventArgs e) &nbsp;&nbsp; at System.Web.UI.Control.OnDataBinding(EventArgs e) &nbsp;&nbsp; at System.Web.UI.Control.DataBind(Boolean raiseOnDataBinding) &nbsp;&nbsp; at System.Web.UI.Control.DataBindChildren() &nbsp;&nbsp; at System.Web.UI.Control.DataBind(Boolean raiseOnDataBinding) &nbsp;&nbsp; at System.Web.UI.Control.DataBindChildren() &nbsp;&nbsp; at System.Web.UI.Control.DataBind(Boolean raiseOnDataBinding) &nbsp;&nbsp; at System.Web.UI.WebControls.DataGrid.CreateItem(Int32 itemIndex, Int32 dataSourceIndex, ListItemType itemType, Boolean dataBind, Object dataItem, DataGridColumn[] columns, TableRowCollection rows, PagedDataSource pagedDataSource) &nbsp;&nbsp; at System.Web.UI.WebControls.DataGrid.CreateControlHierarchy(Boolean useDataSource) &nbsp;&nbsp; at System.Web.UI.WebControls.BaseDataList.OnDataBinding(EventArgs e) &nbsp;&nbsp; at DotNetNuke.Modules.Admin.Security.SecurityRoles.BindGrid() &nbsp;&nbsp; at DotNetNuke.Modules.Admin.Users.ManageUsers.ShowPanel() &nbsp;&nbsp; at DotNetNuke.Modules.Admin.Users.ManageUsers.BindData() &nbsp;&nbsp; at DotNetNuke.Modules.Admin.Users.ManageUsers.OnLoad(EventArgs e)""""  2. This error is logged in the log.resources file:  2012-02-16 11:23:19,744 [TEST-KG11][Thread:5][ERROR] DotNetNuke.Modules.Admin.Users.ManageUsers - FriendlyMessage=""""Error:&nbsp; is currently unavailable."""" ctrl=""""ASP.desktopmodules_admin_security_manageusers_ascx"""" exc=""""System.NullReferenceException: Object reference not set to an instance of an object. &nbsp;&nbsp; at ASP.desktopmodules_admin_security_securityroles_ascx.__DataBinding__control15(Object sender, EventArgs e) &nbsp;&nbsp; at System.Web.UI.Control.OnDataBinding(EventArgs e) &nbsp;&nbsp; at System.Web.UI.Control.DataBind(Boolean raiseOnDataBinding) &nbsp;&nbsp; at System.Web.UI.Control.DataBindChildren() &nbsp;&nbsp; at System.Web.UI.Control.DataBind(Boolean raiseOnDataBinding) &nbsp;&nbsp; at System.Web.UI.Control.DataBindChildren() &nbsp;&nbsp; at System.Web.UI.Control.DataBind(Boolean raiseOnDataBinding) &nbsp;&nbsp; at System.Web.UI.WebControls.DataGrid.CreateItem(Int32 itemIndex, Int32 dataSourceIndex, ListItemType itemType, Boolean dataBind, Object dataItem, DataGridColumn[] columns, TableRowCollection rows, PagedDataSource pagedDataSource) &nbsp;&nbsp; at System.Web.UI.WebControls.DataGrid.CreateControlHierarchy(Boolean useDataSource) &nbsp;&nbsp; at System.Web.UI.WebControls.BaseDataList.OnDataBinding(EventArgs e) &nbsp;&nbsp; at DotNetNuke.Modules.Admin.Security.SecurityRoles.BindGrid() &nbsp;&nbsp; at DotNetNuke.Modules.Admin.Users.ManageUsers.ShowPanel() &nbsp;&nbsp; at DotNetNuke.Modules.Admin.Users.ManageUsers.BindData() &nbsp;&nbsp; at DotNetNuke.Modules.Admin.Users.ManageUsers.OnLoad(EventArgs e)""""  Expected Result: 1. User Profile loads""",Bug,Admin
452998,"""Brief Description of Issue: I need to have a working STMP server setup so my Messaging/Collaboration/Notifications are all sent.  Clear Steps to Reproduce: 1. Install latest CE 6.2.0.477 build 2. Log in as Host 3. Go Host --&gt; Host Settings  4. Click on """"Advanced Settings"""" tab 5. Click on """"SMTP Server Settings"""" 6. Enter the following information: smtp.gmail.com Click """"Basic"""" radio button Tick """"SMTP Enable SSL"""" check box <EMAIL> dnn@dnnqa1  7. Click """"Update"""" at bottom of page 8. Click """"Test SMTP Settings"""" button  Actual Result: 1 I get an error message in the UI:  There has been an error trying to send the test email. The error is: The SMTP host was not specified.  2. in the log.resources file this error is logged:  2012-02-14 13:11:01,300 [Test-kg22][Thread:6][ERROR] DotNetNuke.Services.Mail.Mail - The SMTP host was not specified. System.InvalidOperationException: The SMTP host was not specified. &nbsp;&nbsp; at System.Net.Mail.SmtpClient.CheckHostAndPort() &nbsp;&nbsp; at System.Net.Mail.SmtpClient.Send(MailMessage message) &nbsp;&nbsp; at DotNetNuke.Services.Mail.Mail.SendMailInternal(MailMessage mailMessage, String subject, String body, MailPriority priority, MailFormat bodyFormat, Encoding bodyEncoding, IEnumerable`1 attachments, String smtpServer, String smtpAuthentication, String smtpUsername, String smtpPassword, Boolean smtpEnableSSL)  Expected Result: 1. test passes 2. No errors in the UI or logs  Able to reproduce this in CE/PE/EE I use this account all the time iit has never caused an error before today  Last tested in early, .300 series, builds and was able to set the SMTP server then""",Bug,Host
453071,"""Brief Description of Issue: I want to post a short, 12 paragraph 1100 plus word essay in my Journal  Clear Steps to Reproduce: 1. install latest 6.2.0 build of CE/PE or EE 2. Log in as Host 3. Create a new page and add Journal module or add Journal module to existing page 4. Copy and paste the attached Ipsum Lorum text into the Comment text box   Actual Result: 1. The text box doesn&apos;t expand to allow the text to fill the box.  2. The text overflows the Comment text box, blows the footer and extends the page  Expected Result: 1. The Comment text box expands as a single container to hold the text 2. The page will lengthen to accommodate the expanded text box and the page footer will still be at the foot of the page. 3. If there is a Max text size it&apos;s shown bottom left or right of the Comments text box  This is a browser agnostic bug""",Bug,Journal
453082,"""Tab Icon paths in portal templates created by DNN 6 contain portal IDs, causing them to be broken in new portals created from the template.  In portal templates created by DNN 3/4/5, tab iconfiles were correctly recorded with paths relative to the portal root, as follows:  &lt;tab&gt;  &lt;iconfile&gt;Icons/myicon.png&lt;/iconfile&gt; &lt;/tab&gt;  In templates created by DNN 6, the iconfile paths are now relative to the system root, as follows:  &lt;tab&gt;  &lt;iconfile&gt;/Portals/57/Icons/myicon.png&lt;/iconfile&gt; &lt;/tab&gt;  I would assume this also to be the case for the large tab icons.  This means that any new portal created from this template is referencing icons that may or may not even be there.  To reproduce this, simply apply a tab icon to a page, export the portal as a template and then examine the iconfile paths in the template file.   Obviously, a template should never refer to a path that is unique to the original portal. It should always be relative to the new portal.  The solution will be to restore this back to the way it was in DNN 3/4/5 and remove """"/Portals/xx/"""" from the recorded path.""",Bug,Host
453112,"""Brief Description of Issue: DNN authentication can be disabled when no other authentication system is enabled.    Clear Steps to Reproduce:  1. Login as Admin  2. Admin &gt; Extensions. 3. Expand Authentication Systems section. 4. Edit Default Authentication. 5. Expand Authentication Settings section. 6. Uncheck the Enabled? check box. 7. Update Authentication Settings section. 8. Logout. 9. Click the Login link.   Actual Result:  1. Dnn authentication still enabled.    Expected Result:  Expected that I would be prevented from unchecking the Enabled checkbox and receive a warning""",Bug,Admin
453640,"""When navigating via persona bar to Site Settings > Site Behavior > Default Pages, I'd receive an error message with a red background stating to check my internet connection. The issue is, I deleted the default search page from pages, and emptied my recycle bin. There is no FK restraint in the PortalLocalization table which references the TabId's for the default pages. I'd think there should be, and this should be set to default to NULL when a referenced tab is deleted.     The error was thrown in Dnn.PersonaBar.SiteSettings (SiteSettingsController.cs) GetDefaultPagesSettings(). The response attempts to retrieve the TabName but threw an error because the TabInfo object was null.    Temporary resolution is to simply go into the database and set it to NULL manually. If for some reason a FK should not be introduced (not sure why) can we at least disable the ability to delete a tab that is referenced as a default page? Similar to how this is done with the Home page?       !22549564_942275212577992_7107695571350270487_n.jpg|thumbnail! """,Bug,Admin
453773,"""The Site/Portal Level SMTP settings have been broken a long time in DNN.  Perhaps there is a workaround that I am not aware of, but for those of us that use multiple portals this is a real problem.  If someone has a workaround I would appreciate hearing about it.  Below I have listed the current behavior along with the desired behavior:.    +Current Behavior:+   * I had a Global SMTP provider configured.    * I changed the SMTP Server Mode to <MySiteName>  * I put in the correct settings for the site specific email.and clicked Save  * I then clicked the Test SMTP Settings button  * This resulted in a successful send from the Global email instead of the Site Level email.  * I Removed the Global SMTP Settings, saved the settings.  * I changed the SMTP Server Mode back to  <MySiteName, saved and retested..  * Even though I removed the Global SMTP Settings, it was still successfully sending email using these settings.    +Desired Behavior:+  * Sites without Site/Portal SMTP settings configured should use the Global SMTP settings.  * Site/Portal SMTP settings should override Global settings.  * These settings would be transparent so that when modules (without special SMTP provisions) send email, the email would be sent using the configured choice.   .  This would be a very useful feature to enable / complete.  Thanks You!   """,Bug,Host
454205,"""Install latest build of any 9.1.0 Product  Log in as any user with *Persona Bar* permissions  Open any feature within *Persona Bar*  Where you see *'i'* icon hover over it    Actual Result:  No tool tip     Expected Result:  Tool tip appears as you mouse-over the *'i'* icon""",Bug,UI / Usability
454644,"""Build latest DNN 9.0.1 site  In Safari 8/9.1/10 or IE 9/10/11 log in as any user with *Persona Bar* permissions    Actual Result:  Log in fails  In *Developer Console* this error appears:    Unable to get property 'init' of undefined or null reference      Expected Result:  User is able to log in    I'm not seeing this behavior in Evoq Products""",Bug,Login/Registration
454710,"""As a Host user, I need the ability to select which site I'm editing settings for. This needs to be a pervasive dropdown list that exists in the header of the following module pages:  * Site Settings    This enhancement affects Platform and Evoq.    Add the following to Site Settings page:  A. TWO DROPDOWNS IN THE HEADER:  1. Top left is Site Selection, which shows up if a Host User is logged in, and there are multiple sites.  The Site Selection dropdown would only ever be visible by a Superuser, and allows that user to at any time switch to edit anothr site (portal)'s settings.  2. Top right is Language Selection, which shows up if an Admin is logged in and there are multiple languages installed.  This dropdown fills a gap in functionality... the ability for an Admin to switch to the Site Settings for a different language (culture) in their site.     B. IF A USER NAVIGATES TO THE SITE SETTINGS PAGE FROM """"SITES"""", BY CLICKING ON THE GEAR ICON ON A PARTICULAR SITE:  1. Open the Site Settings Page to Site Info Tab.  Set the Dropdown to the Site which was  clicked to from.  2. A """"< Back to Sites"""" link appears below the header at top left, allowing the user to get back to the Sites page if they wish.""",Improvement,Admin
455169,"""When trying to create a new user (never mind if it's a regular user or a superuser) I get this critical error: """"Exception has been thrown by the target of an invocation.""""    I've been able to reproduce it, following this steps:    1. Install DNN from scratch, choosing empty web as template and spanish (I guess the language is irrelevant)  2. In Admin/Languages, add Catalan and Basque (they're two other languages spoken in certain regions of Spain). Leave Fallback Language as System Default  3. Nothing seems unusual ;-)  4. Go back to Admin/Languages and set Fallback Language, either for Catalan or for Basque, to Spanish.  5. Now, try to add a new user. Never mind if you use the Users>Add New User an Control Panel or the Add New User button in Users Admin or Superusers Admin: you won't be able to add the user and will get the error.  6. As far as I've seen, the rest of DNN functionalities are not affected.  6. As an aditional test, I've added French. Before installing the french language pack, I've set spanish as fallbacl language and got the same error. After installing the french language pack, everything is working correctly while I'm in french, but the other languages still throw the error where adding user.""",Bug,Admin
455708,"""This is a trivial issue that we have had to deal with for a long time now. Basically Journals Get More button in certain circumstances was not being displayed when we knew for a fact there were more records in the systems. At first we just though is something to do with time. As in id the system is abandoned for a while the posts go stale. But today we have traced this issue right down to its root which was hidden in stored procedures and it turned out that its not the records that were becoming stale but deleted records preventing correct number of good records being returned.     IF EXISTS(SELECT * from dbo.[Journal_TypeFilters] WHERE ModuleId = @ModuleId)   INSERT INTO @j     SELECT j.journalid, jt.datecreated from (     SELECT DISTINCT js.JournalId from dbo.[Journal] as j      INNER JOIN dbo.[Journal_Security] as js ON js.JournalId = j.JournalId      INNER JOIN dbo.[Journal_User_Permissions](@PortalId,@CurrentUserId ,1) as t        ON t.seckey = js.SecurityKey AND (js.SecurityKey = 'R' + CAST(@GroupId as nvarchar(100)) OR js.SecurityKey = 'E')       --WHERE j.PortalId = @PortalId       WHERE ((@IncludeAllItems = 0) AND (j.PortalId = @PortalId AND j.IsDeleted = @IsDeleted))          OR         ((@IncludeAllItems = 1) AND (j.PortalId = @PortalId))     ) as j INNER JOIN dbo.[Journal] jt ON jt.JournalId = j.JournalId AND jt.PortalId = @PortalId AND jt.GroupId = @GroupId     INNER JOIN dbo.[Journal_TypeFilters] as jf ON jf.JournalTypeId = jt.JournalTypeId AND jf.ModuleId = @ModuleId     ORDER BY jt.DateCreated DESC, jt.JournalId DESC;   ELSE   INSERT INTO @j     SELECT j.journalid, jt.datecreated from (     SELECT DISTINCT js.JournalId from dbo.[Journal] as j      INNER JOIN dbo.[Journal_Security] as js ON js.JournalId = j.JournalId      INNER JOIN dbo.[Journal_User_Permissions](@PortalId,@CurrentUserId ,1) as t        ON t.seckey = js.SecurityKey AND (js.SecurityKey = 'R' + CAST(@GroupId as nvarchar(100)) OR js.SecurityKey = 'E')       --WHERE j.PortalId = @PortalId       WHERE ((@IncludeAllItems = 0) AND (j.PortalId = @PortalId AND j.IsDeleted = @IsDeleted))          OR         ((@IncludeAllItems = 1) AND (j.PortalId = @PortalId))     ) as j INNER JOIN dbo.[Journal] jt ON jt.JournalId = j.JournalId AND jt.PortalId = @PortalId AND jt.GroupId = @GroupId     ORDER BY jt.DateCreated DESC, jt.JournalId DESC;      I've attached a screenshot of the same code just so I can point out where the issue was occurring.    Lines 8 and 22 show where I commented out original WHERE clause.  Lines 9-11 and 23-25 show what I replaced them with.    I had to do this in order to get around the issue created by deleted records in the final SELECT statement's WHERE clause. I am not sure why the procedures are setup the way they are as it would make more sense to check for IsDeleted at the very start and not have deleted records trickle down all the way to the final select only to run into a problem where deleted records get filtered resulting in fewer records being returned then can/should be returned.    Stored procedures that this affects:  [Journal_ListForGroup]  [Journal_ListForGroupSinceDate]  [Journal_ListForProfile]  [Journal_ListForSummary]    I am not entirely sure how long this has been around but looking as it hasn't been a fix in 7.4.2 no one has reported this as a problem.""",Bug,Journal
455860,"""h2. Card  As a DCC Viewer Editor, I want to be able to add content to items of type list and be able to see them.     h2. Conversation  This can be achieved by implementing the appropiate EditorFor methods in the class DotNetNuke.Web.Mvc.Helpers.HtmlEditorExtensions and the DisplayFor methods in the class HtmlDisplayExtensions for supporting fields of types list. This implementation will create html and javascript code to add/edit/delete elements to the list.  It mught be necessary to implement LabelFor methods in the DotNetNuke.Web.Mvc.Helpers.DnnLabelExtensions class""",Story,Dynamic Content Creator
455958,"""In DNN-6935 we state the Administrator parameters for *Data Type* tab  Nowhere in the *Card *does it say anything about what, if any* Data Types *an Administrator can *delete*.  I find that as *Administrator *I can *delete *any* Data Type* from the List    Log in as Administrator in a Platform 8 build  Go *Admin > Advanced Settings > Dynamic Content Creator*  Click the """"*Data Types*"""" tab  Hover over any *Data Type *in the list, even a *Global Data Type*  At far right click the down arrow  In the Edit screen click """"*Delete*""""    Actual Result:  I'm able to *delete *any *data type* in the list even a *Global data type*    Expected Result:  If logged in and viewing the *Data Type* as *Administrator* the """"*Delete*"""" button should either be *inactive *or *hidden*""",Bug,Dynamic Content Creator
456031,"""h2. Card  As a developer, I want to be able to define a JavaScript method that is called when a module action menu item is clicked.    h2. Conversation  This capability is defined in the core ModuleAction API but is not exposed in the new [ModuleAction: {}] token that is provided.    h2. Confirmation""",Improvement,Core
456065,"""h2. Card As a developer, I want to be able to create field which is a reference to another Content Type.   h2. Conversation There will not be any separate UI stories.  Cascading menu for snippet insertion context menu.  h2. Confirmation """,Story,Dynamic Content Creator
456280,"""I believe this issue probably existed for quite long time, and this is another issue I'd like to bring up this JIRA request again: DNN-7351      Steps:   - Try to log in as a RU with wrong password purposely for 5 times, it will cause the account locked.   - Log in as Host, got to that user account profile and change its password    Result:   Host got a error message: Password Answer must be provided.     Expected:   1) I didn't see any place I can input Password Answer on that window  2) Host should be able to change the password by override any password policies.   3) there should be some indication on that pop-up window, this account is still """"Locked"""".     !image-20150806121035.png|width=750!""",Bug,Admin
456316,"""h2. Card  As an viewer, I want to be able to view content.       h2. Conversation  Content should be displayed using the selected content-type, and view-template    If a content editor has not configured the module then the module should initially be displayed using an HTML content type and a """"Getting Started"""" template.  This should be a standard Global Template which is installed with the DCC.    h2. Confirmation""",Story,Dynamic Content Creator
456327,"""h2. Card  As a content manager, I want to be able to set the content type, view template and edit templates for a specific instance of the DCC viewer module.    h2. Conversation  The Content Type drop down list should display all of the portal and global types available in the current portal.    The view template drop down list should display all the view templates associated with a specific content type.    The edit template drop down list should display all of the view templates associated with a specific content type.  In addition, the edit template list will include an option to use an auto-generated view. The auto-generated option will be the default option.    If there is a file that matches the following pattern then it should be the default edit view: [view template name]_edit.cshtml.  For example, if you have a view template named customer.cshtml, then file named customer_edit.cshtml will automatically be selected if it exists, otherwise the auto-generated option will be the default value.    If the current module does not have any settings configured, then the view should show an informational message at the top of the view notifying the user that they need to configure the module.    h2. Conversation""",Story,Dynamic Content Creator
456328,"""h2. Card As an administrator, I want to have a link from the module action menu to the Dynamic Content Type Management page.     h2. Conversation It can't be assumed that the Dynamic Content Type Management module exists on an admin page.  The system should be able to find whatever page wherever it is located in the system.  This should open with the Templates tab open.  It should enforce the permissions. h2. Confirmation""",Story,Dynamic Content Creator
456329,"""h2. Card  As a content editor, I want to be able to display a specific edit screen which was assigned by the content manager.    h2. Conversation  Edit views can be created by an administrator in the DCC admin.  The Content manager can assign this view in the quick settings to be the edit view.    h2. Confirmation""",Story,Dynamic Content Creator
456330,"""h2. Card  As a content editor, I want to be able to access an edit screen which displays all the editable fields for the selected content type.    As a developer, I want the edit screen automatically generated based on the field order defined for a specific content type.    h2. Conversation  The edit screen should show all of the fields defined for the content type.  The fields should use the default editor defined for each content type.    h2. Confirmation""",Story,Dynamic Content Creator
456365,"""DNN 7.4.1 Community / Gravity Skin    I've set up a portal to *require* a valid profile for login. Once my users register and attempt to login the first time, they are automatically redirected to the page prompting them to complete their profile. The following issues are noted:    1) We have """"Biography"""" as a required field. The profile field is presented with the animated loading circle and the words """"CKEditor is loading please wait"""", but that never completes.    2) The """"Region"""" drop down is also required, but it is not populated and cannot be selected/set to any value.    3) An additional, unlabeled text field is displayed below the Region field, but I have no idea what that is.""",Bug,Admin
456383,"""h2. Card  As a content type editor, I want to be able to easily insert the code for a specific content type into a template that I am editing.      h2. Conversation  There should be a context menu which allows you to insert the code to display a content field at current cursor location.     h2. Confirmation""",Story,Dynamic Content Creator
456530,"""We have a large Evoq client who just recently lost the ability to create new sites (we've also seen this once on a non-Evoq site a few months ago).  After investigating, we found that there was a null reference exception happening within {{PortalAliasController.AddPortalAlias}}, where its {{_dataProvider}} field was blank.  Because {{PortalAliasController.Instance}} is a singleton, it can get created during application start up at a time when {{DataProvider.Instance()}} returns {{null}}.      I'm preparing a pull request which uses {{DataProvider.Instance()}} directly, removing that field (it was only used once in that {{AddPortalAlias}} method; everywhere else is already using {{DataProvider.Instance()}}).""",Bug,Host
456619,"""h2. Card  As an administrator, I want to be able to view a list of existing Content Fields associated with a specific Content Type. The Field list should only be displayed when editing an existing Content Type.    The list should show me the field names, the field label and the data type for the specific field.    As I move my cursor over a row in the list, I should see icons for the item which allows me to edit, delete, or re-order a given Content field.    Above the Field list, there should be a button for creating a new field.    The Field list should show a pager control if the list grows beyond 10 items.    Clicking the Add Field button should display the Add Field screen as described in that story.  Clicking the Edit Field button should display the Edit Field screen as described in that story.  Clicking the Delete Field button should delete the field record, and update the field list.  See the re-order field story for appropriate behavior associated with the re-order button.    h2. Conversation    h2. Confirmation  """,Story,Dynamic Content Creator
456651,"""h2. Card  As a host administrator, I want to be able to perform some security audits on my system to allow me to identify potential security problems and to quickly apply some standard security best practices.    1. Enforce security best practice of removing unneeded installation files. This should occur on module install/upgrade and on every module load.  2. Provide a search function for identifying database and file entries with a matching keyword.  3. Site configuration evaluation and recommendations:  * Convert to private registration  * Multi-line biography profile datatype    4. Display list of host accounts, creation dates and last login date.    The module should only execute for host users and should not rely on placement in the host menu for enforcement of security.    h2. Conversation    h2. Confirmation  This module should create a Host Menu item when installed.  The module should be installable on any version of DNN (v6.2 and above).""",Improvement,Security & Permissions
456673,"""h2. Card  As an administrator, I want to be able to view a list of existing Content Templates.  I should also be able to search for Templates based on the Template Name or Type Name.    The list should show me the Template names, the template type, which content type the template is associated with and whether the template is a system template.    As I move my cursor over a row in the list, I should see icons for the item which allows me to edit or delete a given Template.    Above the Template list, there should be a button for creating a new Template.    The Template List should show a pager control if the list grows beyond 10 items.    All values should be displayed in the current language selected by the user.    h2. Conversation    h2. Confirmation""",Story,Dynamic Content Creator
456674,"""h2. Card  As an administrator, I want to be able to edit a Data Type.  A Data Type will consist of a Type Name, a base type and boolean indicator whether the type is a system type.    The editor for the Data Types should be displayed just below the data type in the list.  Only Host users should be able to set the system type switch.  Once a type is marked as a system type (global) it cannot be reverted back to a non-system type (not-global).    When the delete button is clicked, it will delete the type from the system, hide the editor and then update the data type list.    When the save button is clicked, the record is saved, the editor is hidden and the list is updated to display the new Data Type.    When the cancel button is clicked, the data is discarded and the editor is hidden.    h2. Conversation    h2. Confirmation  """,Story,Dynamic Content Creator
456675,"""h2. Card  As an administrator, I want to be able to create a new Data Type.  A Data Type will consist of a Type Name, a base type and boolean indicator whether the type is a system type.    The editor for new Data Types should be displayed at the top of the data type list, just below the table header.  Only Host users should be able to set the system type switch.  All Data Types will default to being portal specific types for the portal in which they are created.    The delete button should not be visible on the editor.    When the save button is clicked, the record is saved, the editor is hidden and the list is updated to display the new Data Type.    When the cancel button is clicked, the data is discarded and the editor is hidden.    h2. Conversation    h2. Confirmation  """,Story,Dynamic Content Creator
456676,"""h2. Card  As an administrator, I want to be able to view a list of existing Data Types.  I should also be able to search for Data Types based on the Type Name.    The list should show me the data type names, the date the type was created and whether the type is a system type.    Above the Data Type list, there should be a button for creating a new Data Type.    Clicking on a row in the list should expand the row to show the data type editor screen.    The Data Type List should show a pager control if the list grows beyond 10 items.    h2. Conversation    h2. Confirmation  """,Story,Dynamic Content Creator
456679,"""h2. Card  As an administrator, I want to be able to edit a Content Field to a Content Type.  A Field will consist of a Name, a Label, a Data Type, and a Description.    The editor for new Content Types should be displayed as a new “page” which animates into view from the left toolbar.      The Content Field title bar will appear below the Content Type title bar.      When the save button is clicked, the field record is saved, the editor is closed and the Content Type Edit view for the parent Content Type is displayed.    When the cancel button is clicked, the data is discarded and the editor is closed and the Content Type Edit view for the parent Content Type is displayed.    h2. Conversation      h2. Confirmation  We are going to hold off implementing validator functionality until more of the system is complete.""",Story,Dynamic Content Creator
456680,"""h2. Card  As an administrator, I want to be able to add a Content Field to a Content Type.  A Field will consist of a Name, a Label, a Data Type, and a Description.    The editor for new Content Types should be displayed as a new “page” which animates into view from the left toolbar.      The Content Field title bar will appear below the Content Type title bar.      When the save button is clicked, the field record is saved and the screen is “converted” to a Content Field edit screen: Content Field name displayed in the title bar, the delete button displayed and -the validator list is displayed-.    When the cancel button is clicked, the data is discarded and the editor is closed and the Content Type Edit view for the parent Content Type is displayed.    h2. Conversation      h2. Confirmation  We are going to hold off implementing validator functionality until more of the system is complete.  When the save button is clicked, then you can return to the Content Type screen.""",Story,Dynamic Content Creator
456681,"""h2. Card  As an administrator, I want to be able to edit a Content Type.  A Content Type will consist of a Type Name, a Description and a boolean indicator whether the type is a system type.    The editor for new Content Types should be displayed as a new “page” which animates into view from the left toolbar.  Only Host users should be able to set the system type switch.  Once set to a system type and saved, you cannot revert a content type to a portal specific type.    The page title bar will include the general “Content Type” and the name of the Content Type.  The far right of the bar will include a close button which takes the user back to the default Content Type list view.     When the save button is clicked, the record is saved and the screen is closed and the user is taken back to the default Content Type list view.    When the cancel button is clicked, the data is discarded and the editor is closed and the Content Type list view is displayed.    When the delete button is clicked, the content type and any associated content fields are deleted, the screen is closed, and the user is taken back to the default Content Type list view.    The Content Type edit screen will include a Content Field list described in a separate story    h2. Conversation    h2. Confirmation    """,Story,Dynamic Content Creator
456682,"""h2. Card  As an administrator, I want to be able to create a new Content Type.  A Content Type will consist of a Type Name, a Description and a boolean indicator whether the type is a system type.    The editor for new Content Types should be displayed as a new “page” which animates into view from the left toolbar.  Only Host users should be able to set the system type switch.  All Content Types will default to being portal specific types for the portal in which they are created.    The page title bar will include the general “Content Type” and an indicator that this is a new record.  The far right of the bar will include a close button which takes the user back to the default Content Type list view.     When the save button is clicked, the record is saved and the screen is “converted” to a Content Type edit screen: Content Type title displayed in the title bar, the delete button displayed and the content field list is displayed.    When the cancel button is clicked, the data is discarded and the editor is hidden and the Content Type list view is displayed.    h2. Conversation    h2. Confirmation    """,Story,Dynamic Content Creator
456683,"""h2. Card  As an administrator, I want to be able to view a list of existing Content Types.  I should also be able to search for Content Types based on the Type Name, Type Label or Description.    The list should show me the content names, the date the type was created and whether the type is a system type.    As I move my cursor over a row in the list, I should see icons for the item which allows me to edit or delete a given Content Type.    Above the Content Type list, there should be a button for creating a new content type.    The Content Type List should show a pager control if the list grows beyond 10 items.    h2. Conversation    h2. Confirmation  """,Story,Dynamic Content Creator
456684,"""h2. Card  As an administrator, I want to have a single module that allows me to manage and edit Dynamic Content Types and the associated data types, validators and templates.    h2. Conversation  The module should be built using the new SPA module type.  This module will be a showcase for how to leverage the new SPA module type in DNN 8.    This module could be added to any page but should have a page in the admin menu by default.  Anyone with View permissions on the module should be able use the module.     h2. Confirmation  All buttons in the DCC should be rendered as links with appropriate styling to display them as buttons where desired.""",Story,Dynamic Content Creator
456730,"""On a site I am administering I am noticing that one recurrent tasks is to solve the problem of my users creating new accounts because they forgot their password or because they can't use Facebook auth at their place of work for instance. Even though I'm using """"email address as username"""" people seem to posses enough email addresses these days to keep on registering.    I believe that actually deprecating the new account is too drastic and probably just a temporary measure (they'll recreate the account again). Also it poses problems with data integraty across the application. The best way forward is to have the account """"redirect"""" to the main account silently. This operation can only be done by an admin. The idea is that all data that was created will still remain linked to the newly created account but from the moment of linking onwards the user will actually be logged on as the main account. The onlly drawback is that the user will not be able to change the contents of some items he/she created in the period the new account was active before it was linked. But IMO this is a small price to pay for the solution I'm proposing.""",Improvement,Admin
456811,"""h2. Card  As a developer, I want to be able to Authorize against DNN using the OAuth Authorization Code Grant flow.    h2. Conversation    h2. Confirmation  """,Story,Core
456814,"""h2. Card  As a developer, I want to be able to create, read, update and delete content templates.  A template should be associated with a given content type.  The template itself should be stored as a file in the portal or host content file stores.    h2. Conversation    h2. Confirmation  """,Story,"Core,Dynamic Content Creator"
456815,"""h2. Card  As a developer, I want the content I create using the DCC API to get automatically indexed for later searching.  The indexer should only index the actual field values and should not pick up any of the field names.    Given values are all stored in Json format, it is not easy to sort / filter / paginate through database calls. For example, we have a content type: Employee and we want to sort on First Name. The current APIs won't be able to do that as values are stored in Json. We are going to use Search APIs to perform sort / filter / pagination based on data in Lucene. This requires that we provide the correct names for the various fields in Search.    h2. Conversation    h2. Confirmation    """,Story,"Core,Dynamic Content Creator"
456816,"""h2. Card  As a developer, I want to call an API to create, read, update and delete content which is of a specific contenttype.  I should be able to create an individual content item or multiple content items (a list of content items) associated with a specific module  instance.    If multiple content items are associated with a specific module, then I should be able to specify an order for the items and to retrieve a subset of items based on a page number and page size.     h2. Conversation  Using a hidden """"order"""" field is sufficient to deal with the ordering requirement.  There is no requirement to be able to order records based on arbitrary fields.    Content should be stored as a serialized JSON object in the content item store.    h2. Confirmation  """,Story,"Core,Dynamic Content Creator"
456817,"""h2. Card  As a developer, I want to have access to some standard ContentTypes which are installed as part of a standard installation.    Standard ContentTypes should include:  * HTML  * Markdown  * Employee  * Product    The Employee ContentType should include the following ContentFields  * First Name (text - 50 chars, required)  * Last Name (text - 50 chars, required)  * Title (text - 50 chars)  * Picture (image)    The Product ContentType should include the following ContentFields  * Name (text - 100 chars, required)  * Description (text - required)  * Price (currency)  * Picture (image)    h2. Conversation    h2. Confirmation  """,Story,"Core,Dynamic Content Creator"
456818,"""h2. Card  As a developer, I would like to have access to a set of common datatypes with a new DNN installation.  Common DataTypes should include:  * Number  * Text  * Date  * Time  * DateTime  * Boolean  * Image  * File  * Page  * Url  * RichText  * Markdown    h2. Conversation  We may not need RichText or Markdown.  We also might not want to breakdown the datetime.    h2. Confirmation  These types cannot be deleted or updated.""",Story,"Core,Dynamic Content Creator"
456819,"""h2. Card  As a developer, I want to call an API to create, read, update or delete ContentFields associated with a specific ContentType.    Each ContentField should include the ability to store a label, a fieldname, a description, and a datatype.    Each ContentField should include the ability to optionally define one or more validation rules for the content stored in the field.  The validation rules should include the ability to specify: required fields, field size (string length), field range (date range, time range or numeric range), or a regex.  Validation rules should be extensible so that additional validation rules could be """"registered"""" with the system.    h2. Conversation  For this release, we do not have to implement any mechanism for managing the registration of validation rules.  Manual entries in the database is sufficient for """"registration"""".    h2. Confirmation  """,Story,"Core,Dynamic Content Creator"
456820,"""h2. Card  As a developer, I want to be able to create, read, update and delete DataTypes to be used with DCC ContentTypes.  These DataTypes should not define any presentation logic and instead should strictly identify the type of content to be stored in a single field.    h2. Conversation  A DataType cannot be deleted if it is used in any existing ContentType.  Attempting to call a method which deletes the DataType should result in an error.    -A DataType should raise a warning if you attempt to alter the datatype where data has already been stored using the datatype (e.g. A contentitem has been created that includes the datatype).-    h2. Confirmation  A DataType should raise an error if you attempt to alter the datatype where data has already been stored.  The API should include a flag which allows you to force the datatype change.   """,Story,"Core,Dynamic Content Creator"
456821,"""h2. Card As a developer, I want to call an API to allow me to Create, Read, Update or Delete ContentTypes.  ContentTypes which are created and managed by the DCC should be differentiated from ContentTypes which were created outside of the DCC.   A contenttype is just a collection of contentfields.  It does not define anything related to how the content might be presented or where content might be stored.  It is purely a container.   h2. Conversation It is important for the DCC to not get confused by existing ContentTypes which may not conform to the new DCC system.  Are contenttypes defined for a portal or a host?  h2. Confirmation ContentTypes are defined per portal.""",Story,"Core,Dynamic Content Creator"
456950,"""I'm the Chief Solution Architect and Consultant for several software companies. I have been working on several DotNetNuke enhancements for several customers in terms of making it easier and automating deployment of:    * Skins  * Modules  * Libraries  * Language packs  * Imporiting sites from remote templates  * Pages on current portal or any portal.  * Adding modules to pages.  * Changing module details on a page (e.g. Module Title).    This was a crucial step for customers managing multiple portals and multiple installations of DNN. Every time a some user controls on a few modules were updated, deployment used to take about an entire night doing most of these steps manually on all portals and all servers. While now, this can be done in minutes.    This can be done from:    * Command line. A new tool was created called *dnncmd.exe*  * TFS Build Agent. A set of custom TFS Build Agent activies were created in order to publish modules (or extensions) right after the DNN package is created on the TFS build agent.  * From code. A library was created to allow DevOps applications to publish modules to a DNN site from their own code.    On the DNN side, an extension (module) was create to expose a REST interface to accept commands and call DNN API within to process requests. This module was named *DNN Deployer*.    This is a work in progress and there's a lot to do yet. I have been discussing with my clients and I proposed to them and they accepted to make it open source and donate it to you guys so that it might be included as part of the core in a future release.    If you're interested in accepting the donation, I will be more than willing to send it to you. Although, I am not sure what is the correct way to go from that point on in terms of keep adding functionality to such module. Also, it might need to be adjusted to your code conventions too. I want to be able to be to continue updating the module.    Thoughts?""",Improvement,"Admin,Admin"
457099,""" !DNN-6493_741_Upgraded_742.png|thumbnail! When using the profanity filter, the value that is matched on is treated as a regular expression, not as text.  I'm assuming that this is unintentional.  h3. Steps to reproduce    # Go to Site Settings and turn on the Profanity Filter in the User Account Settings tab  # Go to the Lists page (under either Admin or Host) and edit the ProfanityFilter list  # Add an entry where the text is """"find.this"""" (any value is fine)  # Log out and click the Register link  # Attempt to register with """"find this"""" or """"findXthis"""" in the username or display name    h3. Actual Result  - message indicating that the name is invalid    h3. Expected Result  - the registration is successful""",Bug,Admin
457160,"""I have done this same test in multiple Platform 7.4.0 builds... never had an issue at all until build 321 created today  I just happened to be using IE10 I'm pretty sure, based on the error message, this is not an IE10 specific bug        Install Platform 7.4.0 build 321 as FQDN site w/remote SQL database  Using IE 10 log in as HOST  Hover over """"Tools"""" and select """"Upload File""""  In the File Upload UI click """"Decompress ZIP files""""  Drag n' Drop more than 5 images to be uploaded to the ROOT folder  Now go Host > File Manager  Add a Standard Folder to ROOT  Go Admin > File Manager    Actual Result:  Instantaneous error condition and File Manager will not load.    In the UI I see this:    Error: File Management is currently unavailable. DotNetNuke.Services.Exceptions.ModuleLoadException: Item has already been added. Key in dictionary: '8' Key being added: '8' ---> System.ArgumentException: Item has already been added. Key in dictionary: '8' Key being added: '8' at System.Collections.Hashtable.Insert(Object key, Object nvalue, Boolean add) at DotNetNuke.Modules.DigitalAssets.Components.Controllers.DigitalAssetsController.AreMappedPathsSupported(Int32 folderMappingId) in c:\TeamCity\buildAgent\work\DNN_Platform_740\Packaging\DNN.Platform\DNN Platform\Modules\DigitalAssets\Components\Controllers\DigitalAssetsController.cs:line 347 at DotNetNuke.Modules.DigitalAssets.Components.Controllers.DigitalAssetsController.GetUnlinkAllowedStatus(IFolderInfo folder) in c:\TeamCity\buildAgent\work\DNN_Platform_740\Packaging\DNN.Platform\DNN Platform\Modules\DigitalAssets\Components\Controllers\DigitalAssetsController.cs:line 352 at DotNetNuke.Modules.DigitalAssets.Components.Controllers.DigitalAssetsController.GetFolderViewModel(IFolderInfo folder) in c:\TeamCity\buildAgent\work\DNN_Platform_740\Packaging\DNN.Platform\DNN Platform\Modules\DigitalAssets\Components\Controllers\DigitalAssetsController.cs:line 1057 at DotNetNuke.Modules.DigitalAssets.Components.Controllers.DigitalAssetsController.GetRootFolder(Int32 moduleId) in c:\TeamCity\buildAgent\work\DNN_Platform_740\Packaging\DNN.Platform\DNN Platform\Modules\DigitalAssets\Components\Controllers\DigitalAssetsController.cs:line 557 at DotNetNuke.Modules.DigitalAssets.View.OnLoad(EventArgs e) in c:\TeamCity\buildAgent\work\DNN_Platform_740\Packaging\DNN.Platform\DNN Platform\Modules\DigitalAssets\View.ascx.cs:line 582 --- End of inner exception stack trace ---     In Dev Tools i see this:    SCRIPT5007: Unable to get property 'scopeWrapperId' of undefined or null reference   dnn.DigitalAssets.js, line 3431 character 9      In log.resources file:    2015-01-13 13:57:00,548 [TST-KG-W2012S1][Thread:33][ERROR] DotNetNuke.Services.Exceptions.Exceptions - FriendlyMessage=""""Error: File Management is currently unavailable."""" ctrl=""""ASP.desktopmodules_digitalassets_view_ascx"""" exc=""""System.ArgumentException: Item has already been added. Key in dictionary: '8'  Key being added: '8'     at System.Collections.Hashtable.Insert(Object key, Object nvalue, Boolean add)     at DotNetNuke.Modules.DigitalAssets.Components.Controllers.DigitalAssetsController.AreMappedPathsSupported(Int32 folderMappingId) in c:\TeamCity\buildAgent\work\DNN_Platform_740\Packaging\DNN.Platform\DNN Platform\Modules\DigitalAssets\Components\Controllers\DigitalAssetsController.cs:line 347     at DotNetNuke.Modules.DigitalAssets.Components.Controllers.DigitalAssetsController.GetUnlinkAllowedStatus(IFolderInfo folder) in c:\TeamCity\buildAgent\work\DNN_Platform_740\Packaging\DNN.Platform\DNN Platform\Modules\DigitalAssets\Components\Controllers\DigitalAssetsController.cs:line 352     at DotNetNuke.Modules.DigitalAssets.Components.Controllers.DigitalAssetsController.GetFolderViewModel(IFolderInfo folder) in c:\TeamCity\buildAgent\work\DNN_Platform_740\Packaging\DNN.Platform\DNN Platform\Modules\DigitalAssets\Components\Controllers\DigitalAssetsController.cs:line 1057     at DotNetNuke.Modules.DigitalAssets.Components.Controllers.DigitalAssetsController.GetRootFolder(Int32 moduleId) in c:\TeamCity\buildAgent\work\DNN_Platform_740\Packaging\DNN.Platform\DNN Platform\Modules\DigitalAssets\Components\Controllers\DigitalAssetsController.cs:line 557     at DotNetNuke.Modules.DigitalAssets.View.OnLoad(EventArgs e) in c:\TeamCity\buildAgent\work\DNN_Platform_740\Packaging\DNN.Platform\DNN Platform\Modules\DigitalAssets\View.ascx.cs:line 582""""    Now click """"File Manager"""" in the breadcrumb top right above the UI error message and Admin > File Manager will now load without error""",Bug,Admin
457524,"""In latest 7.3.4 build log in as HOST OR upgrade a 7.2.x or 7.3.x site to 7.3.4 latest build  Go Admin > File Manager    Upload a bunch of distinctly named images to the Root  Actual Result:    """"An item with the same key has already been added"""" error for these 2 images:    lead6-2013-dodge-dart-fd.jpg  Highway 61 road cars Dodge Dart Swinger 1970 orange-black 12.jpg    I only uploaded 1 each of the attached images so I'm not sure why the system is throwing this error on seemingly random uploaded images.    Even though I get an error on 2 images this error appears 1 time in log.resources file:    2014-10-27 12:16:16,696 [TST-KG-W2012S1][Thread:17][ERROR] DotNetNuke.Web.InternalServices.FileUploadController - Could not recognise image format.      Expected Result:  now that DNN-6013 has been rolled back there should be no further errors during uploads  All images/files are uploaded without error and appear in the file system for later use""",Bug,Admin
457539,"""For sites with many pages (e.g., > 1000), the Admin/Pages module is extremely slow to load and modify.  This is caused primarily by two issues, the first of which is the handling of the Tabs property (the 2nd issue is in another improvement report).    Here is how it's defined in Tabs.ascx.cs:    protected List<TabInfo> Tabs  {       get       {            return TabController.GetPortalTabs(rblMode.SelectedValue == """"H"""" ? Null.NullInteger : PortalId, Null.NullInteger, false, true, false, true);        }  }    As you can see, it gets called every time that the Tabs property is accessed.  Further in the code (I won't copy in each instance) you can notice that it's accessed many times each page load, including for every node in the TreeView.  In our instance, that means well over 1000 times the TabController.GetPortalTabs() is being called.  This meant a page load of over 60 seconds each time the page was accessed and upon saving any changes.      I fixed this simply by changing the getter to only call GetPortalTabs() once and then to store that in a session variable.  In the OnInit() I would clear it (I added a setter also) if the page was not in a postback.  Therefore, it calls GetPortalTabs() only one time.  This can be solved in several ways:  a private property, caching, my method, etc.  But, the improvements are significant when it's only called once.  This one change brought page load times from 60 seconds down to 15.    """,Improvement,Admin
457701,"""This script only works when logged in as Host. I know this worked as anonymous, regular user, and host in 7.1.1:    <script type=""""text/javascript"""">   $.dnnAlert({          text: """"hello..."""",          title: 'testing...'      });  </script>    I'm using the Gravity skin.""",Bug,"UI / Usability,jQuery"
457780,"""I wanted to upload SVG as my logo file, but it got rejected.    Thinking it would be good to add SVG as an allowable file type in """"Globals.glbImageFileTypes""""""",Improvement,Admin
458200,"""In my database, I had an entry in the ModuleControls table with a null ModuleDefID and null ControlKey. I'm not sure how this entry got in there or changed but thats not the issue.    Because both ModuleDefID and ControlKey were null, when I used mid=1 as a querystring, the module control showed (eg www.mysite.com/?mid=1). I removed this entry from the ModuleControls table and thought to experiment. I gave the SiteSettings module control both null for ModuleDefID and ControlKey, and although I wasn't logged in, I was shown the SiteSettings and was able to make changes as a public user.    If these values can be changed through a module installation (SQL script) or if I was a past employee, I could change these values or add my own that would give me access to any control.    The method GetModuleControlByControlKey(string controlKey, int moduleDefID) is responsible for returning the control. """,Improvement,Security & Permissions
458420,"""I cannot be more specific for now. But trying to reset a user's password while logged in as admin through the user's details popup screen. I got a red message at the top stating the user's password could not be changed. But no reason. So I'm left guessing.""",Bug,Admin
458741,"""The tabs and collapsible sections on the site settings page no longer function normally when Composite Files and Minify options are enabled.    At first I thought this might of been skin related but all of the sites I've updated are now experiencing this issue. I was also able to replicate the issue on a fresh install of 7.2.1 using the default skin.""",Bug,"Admin,Client Resources,jQuery"
458878,"""I'm somewhat baffled by the way the grid/popups work. I find that after I use the popup to edit a user's roles, for instance, the grid (a) refreshes and (b) it goes back to the first page. The refreshing takes a bit of time which is annoying and could be avoided if we haven't changed anything. The resetting to the beginning is quite annoying as I need to look up where I was again.""",Bug,Admin
459512,"""Directly from the customer:    I am missing a small but very useful feature in the DNN core notification system.  I have a custom NotificationTypeAction to perform a certain task.  Clicking on this action in the notifications module, triggers an API call to my Web-API Controller. This works just fine.    However, I would like to return a custom message to the user if the API call was a success.  The problem is that the script in DNN’s CoreMessaging.js file (located in DesktopModules/CoreMessaging/scripts) doesn’t check if the Message attribute in the return data exists and has a value.    It only does this in the else block: if result is not “success”. (See attachment).  It would be nice if the script also checks if “data.Message” has a value if the result is “success”.""",Improvement,Core
459562,"""h3. Card   As a user, I want to be able to prevent the Getting Started dialog from opening again until the next upgrade.    h3. Conversation     h3. Confirmation   * When a user first installs the app, the getting started should display.  * If closed by using the """"close"""" icon, then the getting started page should stay hidden until the user logs in again.  * If the user checks the """"hide"""" checkbox and closes the dialog, then the getting started page should stay hidden until the system is upgraded (e.g. the persistence mechanism should be version dependent).  * The getting started dialog should be displayed to any host user (e.g. all host users should have the option to signup for the newsletter and be able to """"hide"""" the dialog independently).  * There should be a host setting that suppresses the default display of the dialog box so that Cloud can display their own getting started experience.  * The host setting should be configurable in the dotnetnuke.install.config.""",Story,Templates
459563,"""h3. Card   As a user, I want to be able to signup for a newsletter from the getting started page.    h3. Conversation   * The update service URL and required parameters will be identified separately     h3. Confirmation   * The newsletter signup should collect the users email address and forward that content, along with website identification information to the update service.  * The email field should be pre-populated with the email address for the currently logged in user.  * Requesting to be added to the newsletter should result in the popup displaying a thank you message. """,Story,Templates
459564,"""h3. Card   As a user I want to be able to see key pieces of content when I first install or upgrade DNN.     h3. Conversation   Still need to finalize mockup    h3. Confirmation   * Getting started page should change according the attached mockup  * The DNN User manual download link, the email subscription block and the opt out block should all be static content on the getting started page.  * The getting started block should be iFramed content which exists on DNNSoftware.com  * Content in the getting started block should default to placeholder content.  Javascript should check for connection to DNNSoftware.com and update the iFrame src to point to html content on DNNSoftware.com """,Story,Templates
459605,"""A customer has asked for the following:    Large scale module management - There should be the ability to select from a list all the pages that you want a module to be displayed on. Right now there is only the option of all or one. But what if I want to add the module to 100 pages out of 175 pages. I would then have to choose all and then go and remove it from the other 75 pages. That is not a good method.""",Improvement,UI / Usability
459905,"""h3. Card   As a developer, I want extension conflicts to be automatically resolved.    h3. Conversation     h3. Confirmation   * If multiple versions of an extension are requested on a single page, then the page should load the latest version requested.""",Sub-task,Core
459906,"""h3. Card  As a developer, I want to be able to specify where the library is loaded on the page.  h3. Conversation   h3. Confirmation  * The registration API should allow an optional parameter which specifies where a library is loaded on the page. * If the location is not specified, then the library should be loaded at the bottom of the page. * If a library has dependencies, then the dependencies must be loaded on the page prior to the location in which the library is loaded (e.g. if I load a library in the head, then the libraries dependencies must also be loaded in the head) * The locations where a library can be loaded: ** PageHeader ** FormTop ** FormBottom """,Sub-task,Core
459907,"""h3. Card   As a developer, I want to be able to request a general or specific version of the framework.    h3. Conversation     h3. Confirmation   * The API should support requesting various levels of specificity  ** The latest available version (2.0.3)  ** The latest major version (1.x)  ** The latest minor version (1.1.x)  ** A specific version (1.1.5)  """,Sub-task,Core
459908,"""h3. Card   As a developer, I want to be able to request a JavaScript library be injected into the page    h3. Conversation   * The core platform already has methods and a framework for requesting jQuery and jQuery UI. A similar approach should be used for the generic framework.   * Each Library should have a unique name which can be used to request that library be loaded.  * A module should be able to request a CDN version if defined, and the system should automatically fall-back to a local version if CDN is not reachable.    h3. Confirmation   * If a library is requested, then any dependencies should also be loaded automatically (e.g. if lib B depends on lib A, if user request lib B then lib A is automatically injected as well). """,Story,Core
460049,"""I'd love for DNN to implement a """"debug"""" switch at host level on the host settings page. This would set a Debug boolean in the host settings that all components can leverage (e.g. more verbose output).    For years I've implemented this in my own module because I couldn't get it accepted as a general setting. But maybe now the time is ripe. It saves every module developer (a) having to do this themselves and (b) having to explain to users how to switch. A master switch makes life easier for both users and developers, IMO.    Peter""",New Feature,Core
460229,"""Per Social PM, the Toast API needs to be moved into platform. Currently, Social team has added a new column names SendToast to core table in Social 1.0. Ideally this should have been done at the beginning, but at that time we were past the deadline for core cut-off. This must be done in the next maint release.""",Story,Notifications
460399,"""(split from SERVER-41353)    Errors from different number spaces are being conflated and misreported. Fortunately C\+\+11 has a solution to this problem. We should be using {{<system_error>}} to propagate error *categories* when we capture {{errno}}-style results into C++ with type safety.    This should be a simple but repetitive change affecting places where we encounter system API errors and try to log them.    Then something like errnoWithDescription (which would be renamed) can be assured to give correct results. Windows has POSIX errno AND it also has its own number spaces like the GetLastError DWORD errors. Other APIs in Linux and Windows will introduce their own number spaces, asio, libresolv, openssl, even mongodb itself.    SERVER-41353 would introduce a wrapper to capture the code properly with number-space to associate the integer with a category context when it is captured.    I'm not sure if this ticket and SERVER-41353 can be done separately. SERVER-41353 is more about the immediacy of copying errno/GetLastError before logging statements begin. This one, maybe as follow-up work, or maybe simultaneously, would be more about making sure the captured integers are interpreted properly. It's more about the implementation of errnoWithDescription, and renaming it.""",Improvement,Internal Code
461122,"""We are seeing a few tasks that have long-running sub-suites. We could increase the number of sub-suites created by these suites to attempt to bring the runtimes down. The suites in question are:    * causally_consistent_hedged_reads_jscore_passthrough  * multi_shard_local_read_write_multi_stmt_txn_jscore_passthrough    We should make sure as we just these values the total runtime for a patch does is not impacted too much.    ----    As a Server Engineer,  I want to split certain tasks up more,  So that the makespan of my patch build will be reduced.    ----    AC:  * The tasks listed above are split into 5 subsuites instead of 3.  * The total processing time of a patch is not significantly impacted.""",Improvement,Test
461183,"""The following tests have been identified as bottlenecks and we would like to dynamically split them.     * concurrency_sharded_multi_stmt_txn_terminate_primary  * concurrency_sharded_multi_stmt_txn_kill_primary  * concurrency_replication  * concurrency_replication_wiredtiger_cursor_sweeps  * concurrency_sharded_terminate_primary_with_balancer  * concurrency_sharded_kill_primary_with_balancer  * aggregation_secondary_reads  * replica_sets_reconfig_jscore_passthrough  * -replica_sets_multiversion-    ----    As a server engineer,  I want the listed test dynamically split when running in evergreen,  So that I can have smaller patch runtimes..    ----    AC:  * The tasks listed above are split into subtasks when running in evergreen.""",Improvement,Test
461495,"""Good day,    I have 4 rows in the {{test}} collection:    {noformat}  {{{ """"_id"""" : ObjectId(""""5f4ce50e19b13337216dd477""""), """"test"""" : 1 }{ """"_id"""" : ObjectId(""""5f4ce50e19b13337216dd478""""), """"test"""" : 2 }{ """"_id"""" : ObjectId(""""5f4ce50e19b13337216dd479""""), """"test"""" : 3 }{ """"_id"""" : ObjectId(""""5f4ce50e19b13337216dd47a""""), """"test"""" : 4 }}}  {noformat}    After running {{db.test.createIndex(\{test:1},\{background:1});}} to create an index, it just hangs. It was hanging for at least a few hours. Here is what I found in the db.currentOp() about this operation:     {noformat}  {{{""""type"""" : """"op"""",""""host"""" : """"HOSTNAME:27017"""",""""desc"""" : """"IndexBuildsCoordinatorMongod-13"""",""""active"""" : true,""""currentOpTime"""" : """"2020-08-31T12:11:13.159+00:00"""",""""opid"""" : 8721867,""""secs_running"""" : NumberLong(20),""""microsecs_running"""" : NumberLong(20888590),""""op"""" : """"command"""",""""ns"""" : """"test.test"""",""""command"""" : {""""createIndexes"""" : """"test"""",""""indexes"""" : [{""""v"""" : 2,""""key"""" : {""""test"""" : 1},""""name"""" : """"test_1"""",""""background"""" : 1}],""""lsid"""" : {""""id"""" : UUID(""""07b43083-8ab9-4bcb-8768-919a3f27655f"""")},""""$clusterTime"""" : {""""clusterTime"""" : Timestamp(1598875647, 409),""""signature"""" : {""""hash"""" : BinData(0,""""+/YcdPyQriT8RL1LtFUhxe2BtCE=""""),""""keyId"""" : NumberLong(""""6861636045532823556"""")}},""""$db"""" : """"test""""},""""msg"""" : """"Index Build: draining writes received during build"""",""""numYields"""" : 0,""""locks"""" : {},""""waitingForLock"""" : false,""""lockStats"""" : {""""ReplicationStateTransition"""" : {""""acquireCount"""" : {""""w"""" : NumberLong(6)}},""""Global"""" : {""""acquireCount"""" : {""""r"""" : NumberLong(1),""""w"""" : NumberLong(4)}},""""Database"""" : {""""acquireCount"""" : {""""r"""" : NumberLong(1),""""w"""" : NumberLong(4)}},""""Collection"""" : {""""acquireCount"""" : {""""r"""" : NumberLong(1),""""w"""" : NumberLong(3),""""W"""" : NumberLong(1)}},""""Mutex"""" : {""""acquireCount"""" : {""""r"""" : NumberLong(4)}}},""""waitingForFlowControl"""" : false,""""flowControlStats"""" : {""""acquireCount"""" : NumberLong(3),""""timeAcquiringMicros"""" : NumberLong(1)}}}}  {noformat}    This Index Build: draining writes received during build makes no sense to me since there was no read/writes to the test collection during index creation.    Also index creation hangs only in non-empty collection. Index creates successfully in empty collection.    I'm running MongoDB server as a single-node replica set.    I haven't found any mentions of this message in the google, so I assume it's not a bug, but an issue on my side.    What might be an issue in this case? I'm out of ideas.    PS: apologies if I selected wrong Project for this issue/question.""",Question,Indexing
461519,"""There are occasionally issues where the tasks in the commit queue appear to hang. Since the default timeouts are around 2 hours, this can cause large back ups in the queue. We could add more aggressive timeouts to the tasks in the queue so that they timeout much earlier. The lint tasks already do this and timeout around 40 minutes.    We should look at the historic runtimes of the tasks to pick appropriate timeouts.    ----    As a server engineer,  I want hung commit queue tasks to time out earlier,  So that the commit queue does not get blocked for a long period of time waiting for a task that is just going to timeout anyway.    ----    AC:  * All tasks in the commit-queue have a timeout of under 1 hour.""",Improvement,Test
461729,"""As a server engineer,  I would like death tests to have a log message at the end of them, regardless of test failure or success, so that the logs can be parsed by build baron tools and show me the correct cause in my tests.    AC:  * Death tests end with a """"Death test finished"""" log message when they pass or fail""",Improvement,Test
461861,"""We have found that splitting tasks too aggressively we can create a lot of extra work in setup and teardown of the tests, which causes us to split tasks even more, which leads to more setup and teardown. By limiting the number of sub-tasks we create, we can contain the growth of the setup/teardown overhead.     In some exploration, it appears that a max split of 3 gives us similar makespans as we currently have, which minimizing the overhead of additional tasks.    ----    As a server engineer,   I want tasks in my patch builds split optimally,  So that I can minimize the amount of time I spend waiting for a patch build.    ----    AC:  * All required builders set the max_sub_suites value to 3.  * Non-required builders set the max_sub_suites value to 1.""",Improvement,Test
461870,"""Since getting results from mainline builds is usually doesn't have someone waiting for it like patch builds do, we may want to make different tradeoffs when splitting tasks in the maintain vs splitting tasks in patch builds. We are already treating dependencies differently for splitting tasks in patch builds vs mainline builds. So also applying that to the number of tasks split should be fairly straight-forward.    ----    As an evergreen server maintainer,  I want to be able to specify different task splitting configurations for mainline vs patch builds,  So that I can customize how tasks are split based on the situation.    ----    AC:  * mainline builds and patch builds can have different targets for number of tasks to split.  * mainline builds cap the numbers of subtasks to 1.""",Improvement,Test
462197,"""When running aggregation with $merge each document is processed as an insert or update (ignoring errors and discard options for the moment).   I would expect to see that as an effect in `serverStatus` metrics.  However, they show up inconsistently:  {noformat}  db.foo.aggregate({$sort:{_id:1}},{$limit:100}, {$merge:{into:""""test""""}})  db.foo.aggregate({$sort:{_id:1}},{$limit:200}, {$merge:{into:""""test""""}})  db.foo.aggregate({$sort:{_id:1}},{$limit:300}, {$merge:{into:""""test""""}})  {noformat}  For each time I ran this I got {{serverStatus}}  {noformat}  ss.forEach(function(s) { printjsononeline(s.metrics.document.updated); })  NumberLong(1792)  NumberLong(1792)   // diff 0  NumberLong(1892)   // diff 100  NumberLong(2092)  // diff 200  ss.forEach(function(s) { printjsononeline(s.opcounters.update); })  NumberLong(3157)  NumberLong(3257)   // diff 100  NumberLong(3457)   // diff 200  NumberLong(3757)   // diff 300  {noformat}    First run inserted 100 documents, second update 100 and inserted 100, third updated 200, and inserted 100.   Since second and third run updated 100 and 200 documents (though they should have been no-op updates) we see documents updated in document metrics, but we see opcounters show 100, 200 and 300 updates.      But no insert counters are incremented at all:  {noformat}  ss.forEach(function(s) { printjsononeline(s.metrics.document.inserted); })  NumberLong(330)  NumberLong(330)  NumberLong(330)  NumberLong(330)  ss.forEach(function(s) { printjsononeline(s.opcounters.insert); })  NumberLong(330)  NumberLong(330)  NumberLong(330)  NumberLong(330)  {noformat}    What's happening with this and how can we track documents changed by $merge?  As it is, when you do a massive $merge from a huge collection into an empty collection, it will show up as a single aggregate and a single use of $merge stage and num documents increments to opcounters.update...   how can documents updated or inserted not get incremented on insert here?  And why is updated being incremented on no-op updates?  """,Bug,Aggregation Framework
462320,"""After the fix for SERVER-49123 landed in master, I noticed a new failure starting happening in the """"sanity.js"""" test from the old SBE branch.    Upon investigating the new failure, I discovered that there was a latent bug in generateTraverse() in """"sbe_stage_builder_expression.cpp"""" that has been there for a while. If you look at generateTraverse() around line 304, you'll see a TraverseStage being constructed without a depth limit - let's call this TraverseStage """"T"""". Based on what I know about the code, I'm fairly confident that not having a depth limit on TraverseStage T is wrong.    Prior to the fix for SERVER-49123 (which included the fix for the """"TraverseStage depth limit"""" bug), TraverseStage T happened to produce correct results (at least for the cases that """"sanity.js"""" exercises) because another TraverseStage with a depth limit was filtering out all the array values first before they could reach TraverseStage T. When the """"TraverseStage depth limit"""" bug got fixed, this latent bug emerged (because now there isn't another TraverseStage earlier in the plan tree filtering out array values before they reach TraverseStage T).    The goal of this task is to fix the latent bug in ExpressionPostVisitor so that the """"sanity.js"""" test passes (ignoring the $abs related failures, which is a separate issue).""",Bug,Querying
462422,"""This issue is pretty similar to SERVER-49721.    When looking at possible ways to refactor some of the logic in """"sbe_stage_builder_filter.cpp"""", I tested out some cases that involved using find() to query the contents of an array field, and I noticed it was broken in some cases.    Given a document with a field """"a"""" that contains an array with two values of different types, it appears impossible for the operator to ever match against the second element in the array. See the example in the """"Steps to Reproduce"""" section for a specific example.    From my initial investigation, it seems this is happening because of the specific """"fold"""" expression that is being passed into TraverseStage in """"sbe_stage_builder_filter.cpp"""" and how this expressions behaves when it is translated to bytecode and executed in the VM.    The """"fold"""" expression being pased into TraverseStage is """"logicOr(traversePredicateVar, elemPredicateVar)"""". I looked at the bytecode that is generated for this expression and how it behaved. First, it is important to note that there is no implicit """"cast to bool"""" that is happening to the arguments being passed to logicOr(), but rather the values of the arguments are being directly fed to the logicOr(). The bytecode generated doesn't do quite what I would have expected. Here is a summary of what the bytecode essentially does:  1) If LHS is Nothing or Boolean True, then logicOr() returns LHS.  2) If LHS is not Nothing or Boolean True, then logicOr() returns RHS.    Now let's consider the example in """"Steps to Reproduce"""" and look at why db.c.find(\{""""a"""":""""foo""""}) doesn't match the first document in c even though it should. When TraverseStage applies the projection (== """"foo"""") to the first element of the array in field """"a"""", the == operator returns Nothing (see the implementation of the eq instruction in """"vm.cpp"""" to see why). Because this is the first element of the array, Traverse doesn't evaluate the fold expression - instead it just stores Nothing directly in the """"_outField"""" slot. Next, TraverseStage applies the projection (== """"foo"""") to the second element of the array, and == operator returns Boolean True. TraverseStage then evaluates the fold expression, which in this case is 'logicOr(Nothing,True)'. Because of the behavior of logicOr() that I described above, 'logicOr(Nothing,True)' returns Nothing. As a result, the first document in c doesn't pass the filter even though it should.    Given this, in order to fix this bug I think we need to do one or more of the following:  1) Change the fold expression we pass to TraverseStage in """"sbe_stage_builder_filter.cpp"""".  2) Change logicOr() to implicitly coerce both of its arguments to boolean first.  3) Change logicOr() to behave differently when one or both of its arguments is Nothing.""",Bug,Querying
462424,"""When looking at possible ways to refactor some of the logic in """"sbe_stage_builder_filter.cpp"""", I enabled SBE mode and tested out some cases that involved using dot notation in find() to query embedded documents inside an array field, and I noticed it was broken in some cases.    Given a document with a field """"a"""" that contains an array of with two embedded documents, when the dot notation """"a.b"""" is used in the find() command to match against the contents of field """"b"""", it appears impossible for the operator to ever match against the second embedded document in the array if the first embedded document does not contain field """"b"""". See the example in the """"Steps to Reproduce"""" section for a specific example.    From my initial investigation, it seems this is happening because of the specific """"fold"""" expression that is being passed into TraverseStage in """"sbe_stage_builder_filter.cpp"""" and how this expression behaves when it is translated to bytecode and executed in the VM.    The """"fold"""" expression being pased into TraverseStage is """"logicOr(_outField, _outFieldInner)"""". I looked at the bytecode that is generated for this expression and how it behaved. First, it is important to note that there is no implicit """"cast to bool"""" that is happening to the arguments being passed to logicOr(), but rather the values of the arguments are being directly fed to the logicOr(). The bytecode generated doesn't do quite what I would have expected. Here is a summary of what the bytecode essentially does:   1) If LHS is Nothing or Boolean True, then logicOr() returns LHS.   2) If LHS is not Nothing or Boolean True, then logicOr() returns RHS.    Now let's consider the example in """"Steps to Reproduce"""" and look at why db.c.find(\{""""a.b"""":""""foo""""}) doesn't match the first document in c even though it should. When TraverseStage applies the projection (getField(""""b"""")) to the first element of the array in field """"a"""", getField(""""b"""") returns Nothing (because the embedded document doesn't have a field named """"b""""). Because this is the first element of the array, Traverse doesn't evaluate the fold expression - instead it just stores Nothing directly in the """"_outField"""" slot. Next, TraverseStage applies the projection (getField()) to the second element of the array, and getField(""""b"""") returns """"foo"""". TraverseStage then evaluates the fold expression, which in this case is 'logicOr(Nothing,""""foo"""")'. Because of the behavior of logicOr() that I described above, 'logicOr(Nothing,""""foo"""")' returns Nothing. As a result, the first document in c doesn't pass the filter even though it should.    Given this, in order to fix this bug I think we need to do one or more of the following:   1) Change the fold expression we pass to TraverseStage in """"sbe_stage_builder_filter.cpp"""".   2) Change logicOr() to implicitly coerce both of its arguments to boolean first.   3) Change logicOr() to behave differently when one or both of its arguments is Nothing.""",Bug,Querying
462824,"""MongoDB Version: 3.4.24   MongoDB hosted on Linux server was abruptly shut down due to the over-utilization of memory.   Initiated the mongodb repair using: sudo mongod -f /etc/mongodrepair.conf --repair    *Repair Logs*  {noformat}      2020-07-04T17:17:07.441+0000 I INDEX [initandlisten] building index using bulk method; build may temporarily use up to 50 megabytes of RA$    2020-07-04T17:17:07.448+0000 I INDEX [initandlisten] build index on: ZionsBank.summary properties: { v: 1, key:    { totalVolume: -1 }    , name: """"totalV$   lume_-1"""", ns: """"ZionsBank.summary"""", background: true }   2020-07-04T17:17:07.448+0000 I INDEX [initandlisten] building index using bulk method; build may temporarily use up to 50 megabytes of RA$    2020-07-04T17:17:07.456+0000 I INDEX [initandlisten] build index on: ZionsBank.summary properties: { v: 1, key:    { ts: -1 }    , name: """"ts_-1"""", ns: """"Zi$   nsBank.summary"""", background: true }    2020-07-04T17:17:07.456+0000 I INDEX [initandlisten] building index using bulk method; build may temporarily use up to 50 megabytes of RA$    2020-07-04T17:17:08.673+0000 I - [initandlisten] Invariant failure rs.get() src/mongo/db/catalog/database.cpp 195    2020-07-04T17:17:08.673+0000 I - [initandlisten]    ***aborting after invariant() failure    2020-07-04T17:17:08.717+0000 F - [initandlisten] Got signal: 6 (Aborted).     {noformat}    *Restart Logs*  {noformat}    2020-07-04T17:39:14.476+0000 I CONTROL [main] ***** SERVER RESTARTED *****    2020-07-04T17:39:14.480+0000 I CONTROL [initandlisten] MongoDB starting : pid=20485 port=27017 dbpath=/home/db324 64-bit host=ip-*--**-*    2020-07-04T17:39:14.480+0000 I CONTROL [initandlisten] db version v3.4.24    2020-07-04T17:39:14.480+0000 I CONTROL [initandlisten] allocator: tcmalloc    2020-07-04T17:39:14.480+0000 I CONTROL [initandlisten] modules: none    2020-07-04T17:39:14.480+0000 I CONTROL [initandlisten] build environment:    2020-07-04T17:39:14.480+0000 I CONTROL [initandlisten] distarch: x86_64    2020-07-04T17:39:14.480+0000 I CONTROL [initandlisten] target_arch: x86_64    2020-07-04T17:39:14.480+0000 I CONTROL [initandlisten] options: { config: """"/etc/mongod.conf"""", net:    { bindIp: """"-.-.-.-"""", port: 27017 }    , replication: $    oplogSizeMB: 10240, replSetName: """"rs1"""" }, storage: { dbPath: """"/home/db324"""", directoryPerDB: true, engine: """"wiredTiger"""", journal:    { enabled$ true }    , wiredTiger: { engineConfig:    { cacheSizeGB: 108.0 }    } }, systemLog: { destination: """"file"""", logAppend: true, path: """"/var/log/mongodb/mongod.lo$   """" } }    2020-07-04T17:39:14.480+0000 W - [initandlisten] Detected unclean shutdown - /home/db324/mongod.lock is not empty.    2020-07-04T17:39:14.499+0000 W STORAGE [initandlisten] Recovering data from the last clean checkpoint.    2020-07-04T17:39:14.499+0000 I STORAGE [initandlisten]    2020-07-04T17:39:14.499+0000 I STORAGE [initandlisten] ** WARNING: The configured WiredTiger cache size is more than 80% of available RAM.    2020-07-04T17:39:14.499+0000 I STORAGE [initandlisten] wiredtiger_open config: create,cache_size=110592M,session_max=20000,eviction=(threads_min=4,t$   reads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000)$   checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),verbose=(recovery_progress),    2020-07-04T17:39:14.667+0000 I STORAGE [initandlisten] WiredTiger message [1593884354:667272][20485:0x7fdbcb287580], txn-recover: Main recovery loop$   starting at 73368/128    2020-07-04T17:39:14.667+0000 I STORAGE [initandlisten] WiredTiger message [1593884354:667951][20485:0x7fdbcb287580], txn-recover: Recovering log 733$   8 through 73369    2020-07-04T17:39:14.733+0000 I STORAGE [initandlisten] WiredTiger message [1593884354:733044][20485:0x7fdbcb287580], txn-recover: Recovering log 733$   9 through 73369    2020-07-04T17:39:15.164+0000 E STORAGE [initandlisten] WiredTiger error (-31802) [1593884355:164908][20485:0x7fdbcb287580], [file:ZionsBank/collectio$|file://zionsbank/collectio$]   -56-3854974571131417844.wt, WT_SESSION.open_cursor: /home/db324/ZionsBank/collection-56-3854974571131417844.wt: handle-read: pread: failed $   to read 4096 bytes at offset 28672: WT_ERROR: non-specific WiredTiger error    2020-07-04T17:39:15.164+0000 I - [initandlisten] Invariant failure: ret resulted in status UnknownError: -31802: WT_ERROR: non-specific WiredT$   ger error at src/mongo/db/storage/wiredtiger/wiredtiger_session_cache.cpp 113    2020-07-04T17:39:15.165+0000 I - [initandlisten]  ***aborting after invariant() failure  {noformat}     *{{}}*    Is there a way to repair/recover the last part of the DB? or   Is there a way to ignore the broken DB? or   Is it possible to cut out the whole 2.4TB data sans the last error db and create a new MongoDB instance with 2.4TB?    *{{}}*    I would greatly appreciate the help.   Thanks in Advance    *{{}}*""",Question,Indexing
463431,"""It looks like the causally_consistent_hedged_reads_jscore_passthrough suites was recently added. It looks like this suite has a runtime of over 1.5 hours. We should convert this suite to a dynamically generated suite so that we can split it into sub-suites that can run in parallel.    ----    As a Server engineer,  I want the causally_consistent_hedged_reads_jscore_passthrough to be split into subsuites,  So that it can be run in parallel and I can have lower makespans in patch builds.    ----    AC:  * causally_consistent_hedged_reads_jscore_passthrough is dynamically split into sub-suites.""",Improvement,Test
463530,"""This is a hypothetical that I'm documenting for investigation. The idea was inspired by [~<USER>'s discovery in SERVER-48518. Unlike SERVER-48518, this out of order timestamp condition is only problematic when WT is running with durable history.     Consider the following sequence where a primary accepts some writes, rolls them back, and then as a secondary replicates writes that use the same key-space.    * As primary  * Insert {_id: 1, doc: """"A""""} @ TS 20 RecordId(5)  * Delete {_id: 1, doc: """"A""""} @ TS 30  * Stepdown, perform Rollback via refetch to back to TS 10  * Rolls back delete -- refetch {_id: 1} from sync source. The document does not exist -- do nothing  * Rolls back insert -- the {_id: 1} document doesn't exist in the index -- do nothing  * Become secondary  * Replicate Insert {_id: 1, doc: """"B""""} @ TS 15 -RecordId(5)- RecordId(6)    In this state, there are two update chains with out of order timestamps:  -RecordStore: RecordId(5) V3(15) -> V2(30) -> V1(20)- See comments  Index: KeyString(1) V3(15) -> V2(30) -> V1(20)    Note the out of order updates in the RecordStore case are not alleviated by SERVER-48453 as this problem would still exist without lazy-initialization.""",Bug,"Storage,Replication"
463701,"""StringData::ComparatorInterface and most of its callers can be improved in several respects. I want to open this ticket to give the StringData::ComparatorInterface some attention. It's used in some very important places.    Move it out of the StringData class to be a first-class type. StringData::ComparatorInterface is not part of what StringData is. It's just one particular thing you can do with StringData. We already have a separate file for its definition, `mongo/base/string_data_comparator_interface.h`, so we can essentially rename `StringData::ComparatorInterface` to `StringDataComparator` in that file. We can remove """"Interface"""" from the name. I'm looking at the typedefs:      StringDataUnorderedSet      StringDataUnorderedMap    And they are hard to use correctly because their hasher and key_eq functors are stateful, so a factory function is needed, and the functors provided have lifetime caveats on an underlying StringData::ComparatorInterface everybody just uses the singleton kInstance, so the whole thing can just be a family of matched stateless functors for every use in the codebase. Many can now be migrated to use stdx::unordered_map and stdx::unordered_set and be more efficient.    The StringDataComparatorInterface is missing an {{equal}} virtual function. It has a compare and a hash_combine but no equal. This means that doing hash container lookups will have to use compare, which is potentially far more expensive than == on StringData, as equality can short-circuit in the case of unequal string lengths.    """,Improvement,Internal Code
463740,"""I'm not sure how $accumulator contributes to tracking group size but it appears to be way overcounting when passed documents (unless it's allocating a huge amount to JS in general?)  {noformat}db.scores.aggregate([{$match:{game:/^G[123]/}},{$count:""""c""""}])  { """"c"""" : 368061 }  db.scores.aggregate([{$match:{game:/^G[123]/}},{$group:{_id:0, size:{$sum:{$bsonSize:""""$$ROOT""""}}}}])  { """"_id"""" : 0, """"size"""" : 30215379 }  /* 28GBs */  db.scores.aggregate([{$match:{game:/^G[123]/}}, {$group:{_id:""""$game"""", top2: { $accumulator: { init: function() {  return <USER>   },  accumulateArgs: [ [1,2,3,4,5,""""$player"""",""""$score""""] ],                     accumulate: function(state, val) {    return state;     },                     merge: function(state1, state2) {                         return state1;  },      finalize: function(state) {   return state;   }   } }  } },{$count:""""c""""}])  { """"c"""" : 33 }  /* as soon as I pass a document as args */  db.scores.aggregate([{$match:{game:/^G[123]/}}, {$group:{_id:""""$game"""", top2: { $accumulator: { init: function() {  return <USER>   },  accumulateArgs: [ {score:""""$score""""} ],                     accumulate: function(state, val) {    return state;     },                     merge: function(state1, state2) {                         return state1;  },      finalize: function(state) {   return state;   }   } }  } },{$count:""""c""""}])  Error: command failed: {   """"ok"""" : 0,   """"errmsg"""" : """"Exceeded memory limit for $group, but didn't allow external sort. Pass allowDiskUse:true to opt in."""",   """"code"""" : 292,   """"codeName"""" : """"QueryExceededMemoryLimitNoDiskUseAllowed""""  } : aggregate failed :  _getErrorWithCode@src/mongo/shell/utils.js:25:13  doassert@src/mongo/shell/assert.js:18:14  _assertCommandWorked@src/mongo/shell/assert.js:618:17  assert.commandWorked@src/mongo/shell/assert.js:708:16  DB.prototype._runAggregate@src/mongo/shell/db.js:266:5  DBCollection.prototype.aggregate@src/mongo/shell/collection.js:1012:12  @(shell):1:1  /* with limit */  db.scores.aggregate([{$match:{game:/^G[123]/}},{$limit:335000}, {$group:{_id:""""$game"""", top2: { $accumulator: { init: function() {  return <USER>   },  accumulateArgs: [ {score:""""$score""""} ],                     accumulate: function(state, val) {    return state;     },                     merge: function(state1, state2) {                         return state1;  },      finalize: function(state) {   return state;   }   } }  } },{$count:""""c""""}])  { """"c"""" : 30 }  db.scores.aggregate([{$match:{game:/^G[123]/}},{$limit:340000}, {$group:{_id:""""$game"""", top2: { $accumulator: { init: function() {  return <USER>   },  accumulateArgs: [ {score:""""$score""""} ],                     accumulate: function(state, val) {    return state;     },                     merge: function(state1, state2) {                         return state1;  },      finalize: function(state) {   return state;   }   } }  } },{$count:""""c""""}])  Error: command failed: {   """"ok"""" : 0,   """"errmsg"""" : """"Exceeded memory limit for $group, but didn't allow external sort. Pass allowDiskUse:true to opt in."""", ...  {noformat}    Collection stats:  {noformat}  db.scores.aggregate({$collStats:{storageStats:{scale:1024*1024}}},{$project:{""""storageStats.wiredTiger"""":0,""""storageStats.indexDetails"""":0}}).pretty()  {   """"ns"""" : """"agg.scores"""",   """"host"""" : """"asyas-mbp-4.lan:27017"""",   """"localTime"""" : ISODate(""""2020-05-23T18:44:17.107Z""""),   """"storageStats"""" : {    """"size"""" : 95,    """"count"""" : 1130151,    """"avgObjSize"""" : 88,    """"storageSize"""" : 31,    """"freeStorageSize"""" : 0,    """"capped"""" : false,    """"nindexes"""" : 2,    """"indexBuilds"""" : [ ],    """"totalIndexSize"""" : 51,    """"totalSize"""" : 82,    """"indexSizes"""" : {     """"_id_"""" : 19,     """"game_1_score_-1"""" : 32    },    """"scaleFactor"""" : 1048576   }  }  {noformat}  """,Bug,Aggregation Framework
463995,"""The Selected Tests alias is using the regex """".\*\_fuzz.\*"""" to detect fuzzer tasks. However, most of the fuzzer tasks are named something like """"jstestfuzz_*"""", so none of those are being picked up by the fuzzer. We need to update that regex or come up with a better way of selected tasks.    ----    As a server engineer,  I want selected tests to pull in all fuzzer tasks,  So that I don't have to run them manually.    ----    AC:  * All js fuzzer tasks are run in a selected tests patch build.""",Bug,Test
464047,"""Update:    Per Max's astute observation in the comment below, the permission issue can be fixed simply by changing the 20 places that call {{remote_operations.py}} in evergreen.yml to use the {{Administrator}} user instead of {{$USER}}. We will likely refactor the code to set an evergreen expansion and merge remote_operations into resmoke as a new subcommand.    Powercycle has been failing on Windows due to an SSH permission issue. There is currently no easy way to investigate the problem due to an inability to access the ec2 instance running the powercycle test.    After chatting with [~<USER>, I'm postponing the work to investigate the permission issue.  """,Task,Test
464211,"""The {{src/mongo/util/concepts.h}} header defines a set of 4 macros:  {code:c++}  #define TEMPLATE(...)  #define REQUIRES(...)  #define REQUIRES_OUT_OF_LINE_DEF(...)  #define REQUIRES_FOR_NON_TEMPLATE(...)  {code}  Change these to  {code:c++}  #define MONGO_CONCEPTS_TEMPLATE(...)  #define MONGO_CONCEPTS_REQUIRES(...)  #define MONGO_CONCEPTS_REQUIRES_OUT_OF_LINE_DEF(...)  #define MONGO_CONCEPTS_REQUIRES_FOR_NON_TEMPLATE(...)  {code}  An examination of the sites that use these names shows they can afford the extra space.    {{TEMPLATE}} and {{REQUIRES}} are single common words that are overloaded to mean various unrelated things. Google uses {{REQUIRES}} annotations to enforce lock ordering, and even has its own {{#define REQUIRES}} in the vendored benchmark library, which I'm in the process of upgrading. It would be clearer and safer if Mongo's Concepts macros were renamed with a {{MONGO_CONCEPTS_*}} prefix.    I thought of this when I saw that the Catch framework uses a {{REQUIRE}} macro, which is almost but not quite a collision with our {{REQUIRES}} macro. It's a close call. In Catch (which has been tossed around as a possibility for us SERVER-37517), you can {{#define CATCH_CONFIG_PREFIX_ALL}} to avoid such conflicts, so their {{REQUIRE}} becomes {{CATCH_REQUIRE}} instead.""",Improvement,Internal Code
464502,"""If a node receives a heartbeat reconfig and can't find itself in the config due to a network issue, it sets TopologyCoordinator::_selfIndex to -1. It logs like:  {noformat}  Cannot find self in new replica set configuration; I must be removed{""""error"""":{""""code"""":74,""""codeName"""":""""NodeNotFound"""",""""errmsg"""":""""No host described in new configuration with {version: 3, term: 1} for replica set server7781-configRS maps to this node""""}}  {noformat}    If TopologyCoordinator::processReplSetRequestVotes then receives a request with the correct config term and version, it passes the check added in SERVER-46387, and goes on to check whether _selfConfig().isArbiter(). The node crashes with an invariant in _selfConfig() because _selfIndex is -1.    The root cause is a network problem that prevents the node from finding itself in the config. We've observed mysterious DNS issues in EC2 that sometimes prevent mongod from resolving its own address in repl::isSelf(), perhaps the build failure I'm debugging is an example of that. Regardless, we must prevent any scenario that uses -1 as a member index.""",Bug,Replication
464851,"""When installing MongoDB 4.2.5 on Windows 10 using the MSI package:  - If specifying a custom install directory, and  - Installing as a Windows service,    The follow error often appears:    {code:java}  Service 'MongoDB Server' (MongoDB ) failed to start. Verify that you have sufficient privileges to start system services""""  {code}    Status above progress bar just before error was """"Starting services""""    Tested with an administrator account.    The following custom directory locations all failed when I used the """"create a new folder"""" button in the installer:    - {{C:\TESTMDB\}}  - {{C:\TEST\TESTMDB\}}  - {{C:\Users\myuser\Desktop\TESTMDB\}}    (none of these folders were present on filesystem after exiting the installer post-error, whether using either """"cancel"""" or """"ignore"""").    The following custom install directory locations all failed without using the """"create a new folder"""" button in the installer (i.e. using an existing directory):    - {{C:\Users\myuser\Desktop\}}  - {{C:\TESTMDB2\}} ( I created this directory outside of the installer beforehand)    This occurs with both Community and Enterprise editions.    I only have a few Windows 10 boxes to test on at present unfortunately, but this occurs on all of them.    Probably due to Windows' UAC and permissioning restrictions, so there may not be a ton we can do about this, but I imagine it bears looking into. I'd like to create a DOCS ticket to update our Windows MSI install instructions with any findings or recommendations. The initial report of this issue came from a DOCs reader through our feedback channel.""",Bug,Packaging
465336,"""We introduced OperationKeys as part of SERVER-44167, which allows a user to provide a GUID in any command which it can later be killed by via the killOperations command.    We'd like to extend that functionality to also kill cursors created by that OperationKey, and would like to see if we can tag those UUIDs onto cursors and provide an accessor which gives a list of cursor ids for a given list operation keys (because we bulk kill operations).    Ideally I'd want something like:    {code:cpp}  std::vector<CursorId> getCursorIdsForOperationKeys(std::vector<OperationKey>)  {code}    as a method on the cursor manager.  That way I could write an external service to lookup those ids and kill them through the public api.    Optionally, I'd also be happy to consider a killCursor method which took a vector of OperationKeys""",New Feature,Querying
465370,"""We recently hit an issue where the amount of evergreen project config generated by generate.tasks exceeded the maximum document size and started causing issues. It looks like this was caused by the number of tasks being generated as each task adds to the project config. It also looks like this would only happen if all the build variants in the version were run.     We could reduce the chance of this happening by capping the number of sub-tasks we will generate for a task in non-required build variants.    ----    As a server engineer,  I want to limit the number of tasks dynamically generated in non-required builders,  So that versions do not hit the maximum document size.    ----    AC:  * non-required builders set a cap on number of tasks to generate.""",Improvement,Test
465384,"""I think this should all work if we complete SERVER-46717 and SERVER-46716, but I'm keeping it as a separate task just in case it doesn't. """,New Feature,"Aggregation Framework,Write Ops"
465459,"""We are frequently seeing eslint take over 30 minutes in the commit queue. Looking at recent occurrences of this, it appears to happen for enterprise only changes. My guess is that since there are no changes to the mongo repository, it is linting the entire repository.    ----    As a server engineer,  I want enterprise only changes to the commit queue to not take over 30 minutes to lint,  So that I don't have a long-running task blocking the commit queue.    ----    AC:  * Enterprise-only commit queue entries do not take over 30 minutes to process.""",Improvement,Test
465471,"""Currently the shell will not write some user entered commands to history. This makes using those commands in the shell painful, as any mistake in them requires the user to type the entire command out again (or use a separate editor to store & edit them).    Expected behavior:    As a user of MongoDB shell, I want all commands that I enter to be stored in history, so that I can quickly revise these commands if I made a mistake in them or they failed for any reason.    Actual behavior:    createUser commands are not recorded.""",Improvement,Shell
465499,"""When the commit quorum is turned on for the index build, the primary will always wait for secondaries """"ready to commit"""" votes before committing the index build. Since the index build was built on local collection, we don't replicate the startIndexBuild oplog entry to secondaries. As a result, secondaries won't be able to build the index build and participate in voting process. This can lead to index build on primary to get hang forever.    I would expect this ticket to make sure that index build on local db collections/non-replicated databases *should not use commit quorum*. And, should throw an error to user if the user tries to run createIndexes cmd on non-replicated databases with commit quorum options set.  """,Bug,Storage
465658,"""Add end to end tests for buildscripts/burn_in_tags.py.    As a server engineer,  I want end to end tests for burn_in_tags  so that I can makes changes to the scripts without worrying about breaking things.    AC    At least 1 test executes the main body of burn_in_tags.""",Improvement,Test
465660,"""It would be nice to have a way to understand the overhead associated with splitting up tasks into subtasks. This would allow us to watch any runtime issues that might be hidden by splitting up a task (e.g. large test runtime increases being masking by more aggressive splits of the tests). It would also help us understand if and where there are opportunities for improvements in task splitting.    One way to accomplish this would be to have a build variant that mimics a standard build variant, but without splitting the tasks. We wouldn't need to run the task frequently, once a week would likely be enough.     ----    As a <USER>Prod engineer,  I want a build variant without task split to run,  So that I can measure the overhead task splitting causes.    ----    AC:  * A way of measuring the overhead of task splitting exists.""",Improvement,Test
465755,"""In testing, DEB installs of latest MDB v4.2.3 run into the following error if MDB had been previously installed and then uninstalled on the machine:  {code:java}Failed to start mongod.service: Unit mongod.service not found.{code}       The fix is to manually run:   {code:java}sudo systemctl daemon-reload  {code}       We've gotten some light feedback on the docs concerning this, so I went and tested this:  h3. Findings:   * I was able to verify this on Ubuntu 18 and Debian 10, and have attached steps to reproduce (for Ubuntu 18).   * I believe the DEB install script just needs to add the {{systemctl daemon-reload}} command post-install. It would appear that our RPMs _do_ perform this step post-install. I've attached a comparison. Sure enough, in testing RPMs on CentOS 8 and 7, this problem does not occur (i.e. install, uninstall, reinstall does not require explicitly running daemon-reload on RPM distros)   * Interestingly, the DEB *uninstall* script ({{postrm}}) does run the {{systemctl daemon-reload}} command. Ironically, it might be its very inclusion in the uninstall step that causes this issue in the first place, since it forces systemd to realize that the {{mongod.service}} file has been removed.   * This probably affects any DEB-using distro that uses systemd until files, even those on which users might be running SysV Init commands (i.e. {{service}}) instead of {{systemctl}}, as those distros redirect those commands to systemd in the background. A tweak to the DEB install steps to include daemon-reload would fix these distros too, but should be done in a way that it fails silently on non-systemd machines. The RPM seems to handle this well, and might be a good source for comparison.    Hope this helps!""",Bug,Packaging
466135,"""User story:  As a server engineer,  I want to know that any changes made to code related to buildscripts/selected_tests_py will be tested via and end to end test of buildscripts/selected_tests_py, so that I can be sure that the script and its dependencies are functioning correctly.    AC:  * At least 1 end to end tests for buildscripts/selected_tests_py exists and is run as part of """"buildscripts_test"""".""",Improvement,Test
466321,"""As a EVG engineer,  I want the server evergreen.yml to remove uses of 'requires'  So that I no longer have to support 'requires' functionality.    ----    AC:  * All uses of requires in the server evergreen.yml have been removed.     ----    While discussing a request around creating dependencies dynamically, it became apparent that requires is poorly understood and rarely used. From grepping all static configs, it looks like only the server uses it.    From discussing this with [~<USER>, it sounds like as a result of moving towards more task generation and uses, the current uses of requires are no longer important. The original implementation was motivated (EVG-720 by a cleanup requirement that no longer exists and is better solved in other ways in modern Evergreen. We should therefore remove it from the server config so that Evergreen can remove it from its code base.""",Task,Test
466407,"""Currently, burn_in_tests picks up changed files in the mongodb/mongo repo. It should do the same for the 10gen/mongo-enterprise-modules repo.    User story:  As a MongoDB engineer, I should be able to run burn_in_tests patch builds that run the jstests I've changed in the 10gen/mongo-enterprise-modules repo, ensuring that my jstests changes do not cause them to fail.    AC:  * Burn_in_tests picks up changes to jstests in 10gen/mongo-enterprise-modules repo""",Improvement,Test
466616,"""It looks like there will be some C++ changes necessary to get this to work, along with some testing to confirm. I've attached a POC patch which is based of the one attached to SERVER-45452. It works, but has a strange use of {{expCtx->isParsingViewDefinition`}} which we should look into ways of avoiding.""",New Feature,Aggregation Framework
467185,"""For a particular BF I'm investigating, which has failed on all branches 3.6, 4.0, 4.2, and master, the hang analyzer finds interesting processes for python.exe, mongobridge.exe, and mongod.exe, but not mongo.exe.   For this particular BF, we believe that the hanging process *is* actually the shell, so for this case it would have been very helpful to have the shell process stack traces.  See linked BF for examples.  Note that the first BF does actually have a mongo.exe in the Hang Analysis output, but we believe that it's incorrectly linked as a dup; the other BFG's are the ones to look at here.""",Bug,Test
467704,"""As part of the migration to commit queue, pre-commit git hooks are no longer run. One of the hooks that was run validated that the commit message conformed to certain rules. With EVG-6445, we should be able to create a task that runs as part of the commit queue to validate the commit message.  ----  As a server engineer,  I want a commit queue check to validate the commit message  So that I don't know accidentally commit with a bad message.  ----  AC:  * A commit queue task is run that fails if the commit has an invalid message.""",Improvement,Test
467730,"""As a performance engineer  I want signal processing commands to have proper evergreen auth,  so that they can access data from the evergreen api.  ----  AC:  * detect_outliers can access evergreen API data.  * detect_changes can access evergreen API data.""",Improvement,Performance
467896,"""As a DAG engineer,  I would like signal processing to be run outside of DSI.    AC:  * performance and sys-perf projects in mongo repo use signal processing directly for detect-changes and detect-outliers""",Improvement,Test
468026,"""In the dsi supported, _run-dsi_ sets up a python virtualenv for python scripts to run in. As part of that, we do a `pip install` for the requirements. We should pip this output to a file and upload it, to  provide traceability about which python packages have been installed if anything needs to be investigated.  ----  As a Server engineer,  I want pip to list / persist the requirements,  So that it is easier to find what I'm looking for in the logs.  ----  AC:  * The packages and version installed by pip are still available if needed.""",New Feature,Test
468255,"""  An upgrade to PseudoRandom was reverted due to a test relying on the specific bits output by PseudoRandom(0). This is not a good situation (SERVER-43641).    Tests must not hardcode this sort of thing. If we do, we can never make improvements to the generators without updating all such tests.      Regarding db/repl/replication_coordinator_impl_elect_v1_test.cpp:  3 tests from the TakeoverTest/ suite are affected:      CatchupTakeoverCallbackCanceledIfElectionTimeoutRuns    DontCallForPriorityTakeoverWhenLaggedDifferentSecond    DontCallForPriorityTakeoverWhenLaggedSameSecond    These only seem to work reliably when fed a (now legacy) PseudoRandom  initialized with a seed of 0. Otherwise the election timeouts are randomized in such a way that the test doesn't reach the desired state, and it fails.  This is extremely fragile and should be fixed asap.    The ReplCoordinatorImpl takes a seed in its constructor. From this seed it makes a PseudoRandom which it uses to generate electionTimeout intervals. This is very hit-or-miss, and a test would have to hope to find a seed that puts the RS into a desired state, and such a seed, if found, would need to be updated with every little tweak of the random number generator or the interval upperBound, etc. Tests really need to directly control the election timeout durations in order to get the RS into their desired state. So really the ctor should take a Duration generator rather than a seed.    For the moment I'm going to bring the entire PseudoRandom """"XorShift"""" implementation into the test as a generator.    PS: Another way to go here would be to use a FailPoint to inject an electionTimeout result, overriding the randomly generated result.    See also SERVER-43767 (related issue in another test)    """,Bug,Internal Code
468424,"""There was an bug introduced to resmoke that caused burn_in_tests to start failing. We should add some end to end tests for burn_in_tests so that we can <USER>these type of errors in buildscripts_test.  ----  As a server engineer,  I want to <USER>errors in burn_in_tests before I commit,  So that I can trust burn_in_tests is running correctly.  ----  AC:  * At least 1 end to end tests for burn_in_tests exists and is run as part of """"buildscripts_test"""".""",Improvement,Test
468571,"""Since $convert doesn't work with BinData types, and there are no helper functions for binary data, nor for UUID in particular, it's very difficult to do something like converting UUID that's stored as a string to a proper BinData format.    Example: I have string """"867dee52-c331-484e-92d1-c56479b8e67e""""  and I want to have UUID(""""867dee52-c331-484e-92d1-c56479b8e67e"""") which is BinData(4, """"hn3uUsMxSE6S0cVkebjmfg=="""") and there doesn't seem to be a way to do that server-side.""",Improvement,Aggregation Framework
468576,"""In most evergreen tasks, we setup a python virtualenv for python scripts to run in. As part of that, we do a `pip install` for the requirements. This writes a lot of information to the logs that is rarely needed. We could pip this output to a file and upload it, that would clean up the logs, but still provide traceability if what python packages have been installed needs to be investigated.  ----  As a Server engineer,  I want pip install of requirements not to write to the evergreen logs,  So that it is easier to find what I'm looking for in the logs.  ----  AC:  * pip install does not write all the installed packages to the log.  * The packages and version installed by pip are still available if needed.""",Improvement,Test
468682,"""We are migrating a 3.2 standalone server to a 3.6.13 sharded/replicated cluster    We have 5 RHEL7 nodes, lot of RAM, SSD disks :   * node1 : mongos, config_server1   * node2: mongod_shard1_primary, mongodb_shard2_arbiter   * node3: mongod_shard1_secondary, config_server2   * node4: mongod_shard2_primary, config_server3   * node5: mongod_shard2_secondary, mongodb_shard1_arbiter    shard secondaries are hidden, priority 0    write_concerns=1 from clients    First 10 days after startup, the data ingestion was ok and our dataset reached 100G on each data_shard (we process a live flow + a migration flow from the 3.2 standalone)    Then for some reason, we had a first crash on both  shard1 primary/secondary    After this crash the secondary whas some hours behind the primary    We cannot now stabilize the shard1 replicaset. When we start the shard1 nodes, the r/w performances are very affected and both shard1 primary and secondary end with freezing and deadlock on clients.    We can see this in log of secondary:    _2019-09-12T11:25:37.926-0400 I REPL [replication-4] Error returned from oplog query (no more query restarts left): NetworkInterfaceExceededTimeLimit: error in fetcher batch callback: Operation timed out_  _2019-09-12T11:25:37.926-0400 I REPL [replication-4] Finished fetching oplog during initial sync: NetworkInterfaceExceededTimeLimit: error in fetcher batch callback: Operation timed out. Last fetched optime and hash: \{ ts: Timestamp(1568301901, 325), t: 43 }[8532403056184220739]_    If I disable the replication (standalone shard1) it lives as a charm    If I try to initial sync the secondary, it ends with a freeze after some Gb of data sync    Actually the shard1 rs seems Ok after a replica sync from direct data file transfer (the network throughput from node2 to node1 to transfer file was ~100M/s)    But I'm afraid of a crash in case of any secondary/primary lag      """,Bug,Replication
468750,"""A rare race, observed in catchup_takeover_two_nodes_ahead.js:   * The test reaches the end and calls ReplSetTest.stop   * ReplSetTest calls getPrimary() and determines that Node 0 is the primary   * For unexpected reasons an election and catchup takeover occur (e.g., Node 0 was blocked by LogKeeper and didn't do heartbeats for 10+ seconds), Node 0 begins to step down and reports ismaster: false   * ReplSetTest proceeds to the point where it fsyncLocks Node 0 before checking dbhashes, because it still thinks Node 0 is primary   * The new primary cannot complete catchup: queries on Node 0's oplog time out because Node 0 is fsyncLocked   * Deeper in ReplSetTest's dbhash code it calls getPrimary() again, but no nodes report ismaster: true so getPrimary() times out    Proposals:    1. Currently, ReplSetTest.checkReplicaSet() fsyncLocks the primary before comparing replicas' dbhashes, oplogs, and/or collection counts. There's a comment saying the fsyncLock's purpose is to prevent TTL indexes from reaping documents during these checks. We could call setParameter with ttlMonitorEnabled: false instead of fsyncLock, but I tried that and got test failures due to dbhash mismatches. As I had feared, it's not only TTL indexes that cause dbhash mismatches, so we need the blunt tool of fsyncLock to prevent *any* changes.    2. All ReplSetTest code that runs with the primary fsyncLocked could be updated so it works while all replicas report ismaster: false, by using the cached self._master value instead of calling getPrimary(). This is a big code change, and it's hard to maintain. Next year we might change the code, accidentally call getPrimary() while the primary is fsyncLocked, and introduce a new rare BF without knowing it right away.    3. Sames as #2, but in getPrimary(), if no replica reports ismaster: true, assert no replicas are fsyncLocked. This will help <USER>mistakes in the future and help diagnose BFs like the current one, however, it's still a big change to brittle ReplSetTest code.    4. Add a retry loop in ReplSetTest.checkReplicaSet(): fsyncLock the primary, try to complete a check. If it fails, unlock, wait for a primary, and relock.    I'll submit Proposal 4 for review.""",Bug,Replication
468789,"""The """"CleanEveryN"""" test hook gets run every """"N"""" tests. Due to the way tests are run, this could be run against a different test every execution. This means that when we use test runtime to calculate timeouts, we might not properly account for the """"CleanEveryN"""" runtime and set a timeout too short. This is most problematic on suites made up of lots of short running tests. If we had a maximum number of tests per suite we used when dividing the tests up, this would no longer be a problem.  ----  As a server engineer,  I want there to be a maximum number of tests per generated sub-suite,  So that the """"CleanEveryN"""" hook does not cause timeouts.  ----  AC:  * All suites that run the """"CleanEveryN"""" hook set a maximum number of tests per suite.""",Improvement,Test
468831,"""The lint task in evergreen normally takes around 15 - 20 minutes to complete. Over the last 6 months, the highest runtimes we have seen have been around 35 minutes. About a week ago, however, we saw the lint task get hung and didn't exit until the task timed out. The lint task just uses the default timeouts, so it took over 3 hours before the task actually ended.     Since the lint task is included as part of the commit queue, hangs like this are problematic. They would block the entire queue for a number of hours. To avoid this issue, we should add a more aggressive timeout to the lint task. Something around 40 minutes should be acceptable.    ----  As a server engineer,  I want to lint task to timeout if it runs for too long,  So that I am not waiting on a hung task.  ----  AC  * Lint tasks in evergreen time out if running for more than 40 minutes.""",Improvement,Test
468878,"""Hi all,    I have a 2-shards-3-replicas cluster in prod, config db is 3-replicas and 2 mongos instances, mongo version is 3.4.10.  I already enabled sharding at db level, then try to shard the collection    {noformat}  *mongos> db.user.ensureIndex(\{person_id: """"hashed""""}, \{background: true})*  {   """"raw"""" : {   """"prod1/172.31.0.16:27017,172.31.0.76:27017,172.31.0.85:27017"""" : {   """"createdCollectionAutomatically"""" : false,   """"numIndexesBefore"""" : 2,   """"numIndexesAfter"""" : 3,   """"ok"""" : 1,   """"$gleStats"""" : {   """"lastOpTime"""" : {   """"ts"""" : Timestamp(1567133797, 47),   """"t"""" : NumberLong(11)   },   """"electionId"""" : ObjectId(""""7fffffff000000000000000b"""")   }   }   },   """"ok"""" : 1  }  *mongos> sh.shardCollection(""""prod.user"""", \{ person_id : """"hashed"""" } )*  { """"collectionsharded"""" : """"prod.user"""", """"ok"""" : 1 }    *mongos> db.collections.find(\{_id:""""prod.user""""})*  { """"_id"""" : """"prod.user"""", """"lastmodEpoch"""" : ObjectId(""""5d68a55593c736ce7519fdb8""""), """"lastmod"""" : ISODate(""""1970-02-19T17:02:47.694Z""""), """"dropped"""" : false, """"key"""" : \{ """"person_id"""" : """"hashed"""" }, """"unique"""" : false }    *mongos> db.user.getShardDistribution()*    Shard prod1 at prod1/172.31.0.16:27017,172.31.0.76:27017,172.31.0.85:27017   data : 12.45GiB docs : 65857922 chunks : 399   estimated data per chunk : 31.96MiB   estimated docs per chunk : 165057    Totals   data : 12.45GiB docs : 65857922 chunks : 399   Shard prod1 contains 100% data, 100% docs in cluster, avg obj size on shard : 203B  {noformat}    Everything seems fine through mongos，but the output in shard instance is below, and the collection still holds on single shard server  {noformat}  *prod1:PRIMARY> db.user.getShardDistribution()*  Collection prod.user is not sharded.  {noformat}    I try to re-shard through mongos, which fails  {noformat}  *mongos> sh.shardCollection(""""prod.user"""", \{ person_id : """"hashed"""" } )*  {   """"code"""" : 20,   """"ok"""" : 0,   """"errmsg"""" : """"sharding already enabled for collection prod.user""""  }  {noformat}    There must be some confusions between mongos and shard, but I'm not sure how to debug, since it's prod, every action should be with caution.  Please help to give some tips, thanks!""",Question,Sharding
468903,"""Add end to end tests for buildscripts/evergreen_generate_resmoke_suites.py.    ----  As a server engineer,  I want end to end tests for evergreen_generate_resmoke_suites  so that I can makes changes to the scripts without worrying about breaking things.  ----  AC  * At least 1 test executes the main body of evergreen_genreate_resmoke_suites.""",Improvement,Test
469045,"""Before SERVER-35688, the ShardingTaskExecutor was able to observe IncompatibleWithUpgradedServer, thus preventing indefinite waits when a mongos connected to a newer mongod with a higher fcv.  That ticket changed the RSM so that rather than using dbclient to do scans, we instead used a task executor, which in turn changed things so that rather than failing, we repeatedly retry the connect.    We should probably do one of:  * move the check that's currently in the ShardingTaskExecutor into some other lib that we can link into the regular task executor, so that all task executors in sharding exhibit the correct behavior  * change the RSM to have a network connect hook which crashes on that status (in sharding)    I'm on the fence which is less invasive""",Bug,"Sharding,Networking"
469145,"""I'm the maintainer of the mongoDB package in the Arch Linux AUR repository.  We've always built wiredtiger as a separate package, and given the """"--use-system-wiredtiger"""" build option for mongoDB.  This works through 4.0.12.    We always check the git repo of the tag of the mongoDB release, for the latest commit in """"mongo/src/third_party/wiredtiger"""" which as of tag """"r4.2.0"""" is """"Import wiredtiger: 1a1197ef3c891458cd73290ad9b01c1e969f7e86...""""  We then package that wiredtiger commit.    Wiredtiger 1a1197 properly builds, and only contains """"libwiredtiger-3.2.0.so"""", """"libwiredtiger.so"""", and """"libwiredtiger_lz4.so"""".    But, when attempting to build mongoDB 4.2.0 with the """"--use-system-wiredtiger"""" option, it fails with this error:  {code:java}  build/opt/mongo/mongo failed: Implicit dependency `src/third_party/wiredtiger/libwiredtiger_checksum.a' not found, needed by target `build/opt/mongo/mongo'.{code}  I don't know if """"libwiredtiger-3.2.0.so"""" has what it needs.  """"nm -D | grep checksum"""" only shows """"__wt_checksum_sw"""".  Maybe that's all it needs, and this part of the mongoDB build doesn't know to use the system library.    Maybe a new configure option is needed for wiredtiger.  We currently use:       {code:java}  ./configure --prefix=/usr --enable-leveldb --enable-lz4 --enable-tcmalloc --enable-verbose --with-builtins=snappy,zlib{code}  {{""""./configure \-\-help""""}} doesn't show me any new option that might be needed like {{""""\-\-enable-checksum""""}}.  I see only: {{""""\-\-with-berkeleydb""""}}, {{""""\-\-with-helium""""}}, and {{""""\-\-enable-leveldb""""}} were removed (so I'll remove the leveldb); and, {{""""\-\-enable-page-version-ts""""}} was added.         Maybe wiredtiger's """"make install"""" doesn't contain a new .so?     """,Bug,Build
469188,"""when using arrayFilters, it is not possible to determine if the full query (including arrayFilters) matched anything.         For example, given the document:  {code:java}  use test;  db.test.drop();  db.test.insert({    name: 'A',    parents: [      {        name: 'AA',        children: [          {            name: 'AAA',            count: 2          }, {            name: 'AAB',            count: 2          }        ]      }    ]  });  {code}  Now run an update:  {code:java}  db.test.update({name: 'A'}, {    $set: {      'parents.$[parentMatch].children.$[childMatch].count': 1    },  }, {    arrayFilters: [      {'parentMatch.name': {$eq: 'AA'}},      {'childMatch.name': {$eq: 'AAA'}}    ]  });  {code}  So far so good, now let's run another update:  {code:java}  db.test.update({name: 'A'}, {    $set: {      'parents.$[parentMatch].children.$[childMatch].count': 1    },  }, {    arrayFilters: [      {'parentMatch.name': {$eq: 'AA'}},      {'childMatch.name': {$eq: 'AAC'}}    ],    upsert: true  });  {code}  Has no effect. The upsert: true has no effect because it only applies to the match predicate in the first parameter. My request is not to change the behavior here, I'm just noting for discussion.         Running this query in production, it would be impossible to know if the update just simply didn't have nModified > 0 because the arrayFilter didn't match anything, or because the count value was already 1 and so no data modification occurred.              I can see that the value for nMatched needs to be 1 because it is applied to the document level, not the subdocument level (as arrayFilters can be used across multiple sub documents within a single document, it would be difficult to assert the meaning of this value if it took that into consideration).         I'm not sure what the best solution is, but I believe it is a shortcoming.    Perhaps there is additional information returned in writeResult.    Perhaps the update positional operator needs to be expanded so that the arrays deeper than 1 level can be updated. (I think this is a bad idea for other reasons).         I like arrayFilters, and it is very useful and extendible, but it is hard to work with when """"upsert"""" functionality is desired.         Thank you for your consideration.                    """,Improvement,Usability
469254,"""Add a placeholder task that currently no-ops for the commit queue. This task should not be included in one of the required build variants.  ----  As a Server engineer  I want to be able to run the commit queue without any real tasks  So that it I can use the commit queue while minimizing the change of colliding with other merges.  ----  AC:  * A single no-op task is available to be part of the commit queue.  * The task is not part of the required builders.""",Improvement,Test
469382,"""As part of SERVER-33963, the unittest tests were split up into 2 tasks, one to compile the unittests and one to run the unittests. They were also put in a task group with max hosts of 1 since in order to run the unittests, you need the artifacts generated by compiling them. However, task groups do not have a hard guarantee that later tasks will run on the same host as earlier task. For tasks that can share a setup, task groups work well for saving some time by sharing setup execution, but they provide inconsistent results when sharing artifacts between tasks.     We should switch the compile and run tasks back to be a single task, so that the unittest  task can be more reliable.  ----  As a Server Engineer,  I want compile unittest and run unittest to be in a single task,  So that it will not fail due to the tasks being run on different hosts.  ----  AC:  * compile unittests and run unittests run as a single task. """,Improvement,Test
469515,"""When burn_in_tests runs the tests it discovered, it should run those tests on the distro they are normally run on. Otherwise, tests could fail due to resource constraints that are not normally there.  ----  As a Server Engineer,  I want burn_in_tests to run on their normal distro,  so that I don't spend time investigating failure due to resource constraints.  ----  AC:  * Tasks that normally run on non-default distros run on the same distros during burn_in_tests.""",Improvement,Test
469643,"""The test_generator tests create several files as they run, but do not clean them up. These files cause problems when running lint locally and should just be cleaned up after the test is run.  ----  As a server engineer,  I want test_generator to clean up the files it creates,  So they don't cause problems when I'm trying to do other things.  ----  AC:   * After running `buildscripts_test`, no extra files are left around.""",Bug,Test
469724,"""If burn_in_tests generates too many tests, it can push evergreen to the limit and cause bad slowdowns. We should put a cap on how many tasks burn_in_tests will generate and fail it we want to generate more tasks than that.  ----  As a server engineer,  I want burn_in_test to limit how many tasks are generated  So that it doesn't cause evergreen to slow down.  ----  AC:  * burn_in_tests should not generate more than 1000 tasks.""",Improvement,Test
469888,"""I got one mongo v2.4.10 server with 1.7TB data, I am trying to migrate and upgrade the mongo to mongo v.3.0.15 server         I've setup a new mongo v.3.0.15 and configured replication for v3.0.15 to be secondary to sync with v.2.4.10 primary mongo.         The secondary was in STARTUP2 and the sync was almost finish as I can check with the growth of my storage device for the new machine which running mongo v.3.0.15         However there were some socket exceptions which caused both of my machine to resyn again from the start, just to ask anything I can configure or setup to prevent the error to happen again because I don't want to waste another 7 days to fail to sync up 1.7TB again.         Below are some logs from my mongo:    Primary mongo (v2.4.10):    ```  Wed Jul 3 10:03:59.196 [conn21] SocketException handling request, closing client connection: 9001 socket exception [SEND_ERROR] server [101.0.0.182:32829]  ```    Secondary mongo (v.3.0.15)  ```    ...  2019-07-03T09:54:29.169+0800 I NETWORK [ReplExecNetThread-0] Socket recv() timeout 192.168.168.122:27017  2019-07-03T09:54:29.169+0800 I NETWORK [ReplExecNetThread-0] SocketException: remote: 192.168.168.122:27017 error: 9001 socket exception [RECV_TIMEOUT] server [192.168.168.122:27017]  2019-07-03T09:54:29.169+0800 I NETWORK [ReplExecNetThread-0] DBClientCursor::init call() failed  2019-07-03T09:54:29.169+0800 I REPL [ReplicationExecutor] Error in heartbeat request to 192.168.168.122:27017; Location10276 DBClientBase::findN: transport error: 192.168.168.122:27017 ns: admin.$cmd query: \{ replSetHeartbeat: """"ArchiverReplica"""", pv: 1, v: 1, from: """"x.x.x.x:27017"""", fromId: 1, checkEmpty: false }    2019-07-03T09:54:29.170+0800 W NETWORK [ReplExecNetThread-0] Failed to connect to 192.168.168.122:27017 after 1 milliseconds, giving up.  2019-07-03T09:54:29.170+0800 I REPL [ReplicationExecutor] Error in heartbeat request to 192.168.168.122:27017; Location18915 Failed attempt to connect to 192.168.168.122:27017; couldn't connect to server 192.168.168.122:27017 (192.168.168.122), connection attempt failed  ...  2019-07-03T10:07:41.452+0800 W NETWORK [ReplExecNetThread-0] Failed to connect to 192.168.168.122:27017 after 4995 milliseconds, giving up.  2019-07-03T10:07:41.452+0800 I REPL [ReplicationExecutor] Error in heartbeat request to 192.168.168.122:27017; Location18915 Failed attempt to connect to 192.168.168.122:27017; couldn't connect to server 192.168.168.122:27017 (192.168.168.122), connection attempt failed  2019-07-03T10:07:43.602+0800 I REPL [ReplicationExecutor] Member 192.168.168.122:27017 is now in state PRIMARY  ...  2019-07-03T10:08:03.845+0800 I NETWORK [rsSync] Socket recv() errno:104 Connection reset by peer 192.168.168.122:27017  2019-07-03T10:08:03.845+0800 I NETWORK [rsSync] SocketException: remote: 192.168.168.122:27017 error: 9001 socket exception [RECV_ERROR] server [192.168.168.122:27017]  2019-07-03T10:08:03.853+0800 I NETWORK [rsSync] trying reconnect to 192.168.168.122:27017 (192.168.168.122) failed  2019-07-03T10:08:03.928+0800 I NETWORK [rsSync] reconnect 192.168.168.122:27017 (192.168.168.122) ok  2019-07-03T10:08:03.939+0800 E REPL [rsSync] 16465 recv failed while exhausting cursor  2019-07-03T10:08:03.939+0800 E REPL [rsSync] initial sync attempt failed, 9 attempts remaining  2019-07-03T10:08:08.939+0800 I REPL [rsSync] initial sync pending  2019-07-03T10:08:08.958+0800 I REPL [ReplicationExecutor] syncing from: 192.168.168.122:27017  2019-07-03T10:08:09.204+0800 I REPL [rsSync] initial sync drop all databases  2019-07-03T10:08:09.205+0800 I STORAGE [rsSync] dropAllDatabasesExceptLocal 3  2019-07-03T10:08:09.221+0800 I JOURNAL [rsSync] journalCleanup...  2019-07-03T10:08:09.221+0800 I JOURNAL [rsSync] removeJournalFiles  2019-07-03T10:08:09.895+0800 I JOURNAL [rsSync] journalCleanup...  2019-07-03T10:08:09.895+0800 I JOURNAL [rsSync] removeJournalFiles  ...  resyn from the begining .......    ```""",Question,Replication
469949,"""As a Sever Engineer,  I want calls to evergreen to be retried,  So that my evergreen tasks don't fail during evergreen deploys.    AC:    Calls to evergreen in evergreen_* scripts under buildscripts test are retried before failing.""",Improvement,Test
470172,"""The burn_in_tags scripts generates build_variants to variants that would not normally run, but we only want to run burn_in_tests on. It currently also generates a compile task for each build_variant it creates. It should not need to do this. It should be able to depend and use the artifacts from an existing build_variant.  ----  As a server engineer,  I want burn_in_tags generated tasks to depend on existing compiles,  So that the compile work is not duplicated.  ----  AC:  * No build_variants generated by burn_in_tags have their own compile task.""",Improvement,Test
470251,"""The 'json.get_history' in evergreen will fail if a new task is being added since that task has no history. The """"etc/system_perf.yml"""" file added a workaround for this in SERVER-35207, but """"etc/perf.yml"""" was not updated at that time. We should also do this in """"etc/perf.yml"""" so that newly added tasks do not fail as well.  ----  As a Server Engineer,  I want etc/perf.yml to not use 'json.get_history'  So that I can add new tasks and have them succeed.  ----  AC:   * New tasks added to etc/perf.yml should be able to succeed despite not having any history.""",Improvement,Performance
470295,"""I have one table membership and another is promo codes.    *membership table:*   /* 1 */  \{      """"_id"""" : ObjectId(""""5b62e7050b13587e9febb8db""""),      """"country_id"""" : """"5a093507f1d45633844096ef"""",      """"type"""" : """"Bronze"""",      """"price"""" : """"30"""",  }  /* 2 */  \{      """"_id"""" : ObjectId(""""5b6961175140b477032291b7""""),      """"country_id"""" : """"5a093507f1d45633844096ef"""",      """"type"""" : """"Gold"""",      """"price"""" : """"1000"""",  }....  *promo code table :*   \{      """"_id"""" : ObjectId(""""5cfe23fd075be65883f6d921""""),      """"final_status"""" : 1,      """"membership"""" : [           ObjectId(""""5b62e7050b13587e9febb8db""""),           ObjectId(""""5b6961175140b477032291b7""""),           ObjectId(""""5b6961285140b477032291b8""""),           ObjectId(""""5b7567dd5b874856981b53d3""""),           ObjectId(""""5bba1c3794c6761db256edbc"""")      ],      """"month"""" : [           """"1"""",           """"3""""      ]  }  *here, the promomcode table has a membership id. I want promo code data with membership join query.get promo code data with membership detail Please help me*""",Bug,Aggregation Framework
470533,"""The validate_mongocryptd script checks that buildvariants that push the mongocryptd binary also add the variant to a list. However, the script currently does this for all buildvariants, including ones that are only for testing and do not contain a push task. This is causing problems with some dynamic variants we are trying to create. The script could also check for a push task.  ----  As a Server Engineer,  I want validate_mongocryptd to check for a push task,  So that variants that do not push mongocryptd do not need to be included in the variant list.  ----  AC:  * Variants the do not contain a push task will not fail to validate_mongocryptd if they are not included in the list of variants.""",Improvement,Test
470613,"""Create a task in evergreen.yml that will run the commit queue. This should use generate.tasks to generate the tasks the commit queue should run.  ----  As a server engineer  I was a commit queue task that runs as part of the commit queue  So that changes I want to merge are validated.    As a DAG engineer  I want to commit queue to be a generating task  So that I can update what tasks are included in the commit queue without disruption.  ----  AC:  * A new task, """"commit_queue"""", is added to evergreen which will generate other tasks that the commit queue should require.  * The commit_queue task should generate a """"lint"""" task on rhel-62 and a """"compile"""" task on all required (!) builders.  * The new tasks should no-op on mainline builds.""",New Feature,Test
470626,"""Since we do not explicitly set a timeout for burn_in generated tasks, a hung test would not fail until the default timeout is hit. This has lead to some timeouts causing large log files to be written and makes it difficult to access the log files. If a given test has test history, we should be able to get an approximation of how long the test should run for and set timeouts dynamically. The """"generate_resmoke_tasks"""" already does this.   ----  As a server engineer,  I want burn_in generated tasks to timeout if they run for too long,  so that the log file for the test stays a manageable size.  ----  AC:  * Every task generated by burn_in that has a test history, sets a timeout based on that test history.""",New Feature,Test
470988,"""The upload_debug_symbols task should be required by the compile task so that it always runs if compile is run.    ----  As a mongo engineer,  I want upload_debug_symbols to always run when compile is run  So that I always have debug symbols for debugging.  ----  AC  * Compile task requires upload_debug_symbols to be run.""",Improvement,Test
470996,"""If I'm reading BF-13062 correctly, it failed because it wasn't able to download the debug symbols .tgz file and ended up marked as a timed out test.    I think that either the """"fetch debugsymbols"""" command should have failed with """"System Failure"""" when all 10 download attempts failed or the """"extract debugsymbols"""" command should have failed with """"System Failure"""" when it didn't see any file to extract.""",Improvement,Test
471022,"""There are certain build variants that are only around to ensure newly added or modified tests are tagged correctly. These variants copy the flags used from other variants and just run 'compile' and 'burn_in_tests'. We could use generate.tasks to dynamically build these variants, which would make them easier to manage and remove some configuration from evergreen.  ----  As a mongo engineer,  I want a script to generate burn_in_tests for testing tagged variants  So there doesn't have to be an explicit evergreen configuration for each one.  ----  AC  * The following build variants are built dynamically and removed from etc/evergreen.yml.  ** ! Enterprise RHEL 6.2 (majority read concern off)  ** ! Linux (No Journal)""",Improvement,Test
471426,"""We are no longer using the Test Lifecycle code. We should go through and remove the code executing it. This would include the references in etc/evergreen.yml, buildscripts/fetch_test_lifecycle.py, buildscripts/update_test_lifecycle.py, and any other related files.    ----    As a server engineer,  I want the test lifecycle code removed.  so that I don't have to spend time maintaining it.    ----    AC  * etc/evergreen should not run any test_lifecycle related tasks.  * test lifecycle scripts (and tests for them) should be removed from mongo repository.""",Improvement,Test
471474,"""I'm using Mongo change streams to send database changes in realtime from my node backend to an angular webapplication.    When I mutate a lot of data rows using a transaction, I want to send those changes to the client as one accumulated change and not one by one.    To achive that, I queue new changes if they are part of a transaction.    And here starts my problem:    How to identify the last change event of a transaction? At the moment I set a timeout and wait if another change with the same lsid arrives within the timeout. If not I consider the transaction complete.    Aside from being a very ugly hack, this has two severe downside:    * If the timeout is too short, I can't rely on the transaction changes to arrive fast enough  * The timeouts accumulate and lead (in case of transactions affecting a lot of documents) to very high response times on the client side    I think the best solution would be to either set a flag on the change event (something like a """"last"""" or """"finished"""" boolean in addition to the lsid) or emit a specific event in the stream after a transaction's last change.    If anybody has already solved that issue in another fashion, please let me know.    Thanks very much!""",Improvement,Querying
471989,"""The test executes a dropDatabase command, which is written to the oplog as a dropCollection entry followed by a dropDatabase entry. The test wants the dropCollection entry to be majority committed, so it can test what happens when only the dropDatabase entry is rolled back.    Expected test sequence:   # Start a 3-node set, nodes are named """"rollback"""", """"tieBreaker"""", """"syncSource""""   # Create a database with one collection, then drop the database   # The dropDatabase command first drops the collection, writes the dropCollection oplog entry, and waits (by default) for the dropCollection entry to be majority-committed.   # *Both* secondaries are lucky enough to receive the entry, and at least one of them acknowledges it.   # Now the dropCollection entry is majority-committed, so dropDatabase proceeds on the primary to drop the database.   # The primary hits the dropDatabaseHangBeforeLog failpoint - it doesn't write the dropDatabase oplog entry. It hangs holding the global write lock.   # The RollbackTest fixture in transitionToRollbackOperations sees that *both* secondaries are caught up, so it proceeds to the next state.    Failure sequence:   # Start a 3-node set, nodes are named """"rollback"""", """"tieBreaker"""", """"syncSource""""   # Create a database with one collection, then drop the database   # The dropDatabase command first drops the collection, writes the dropCollection oplog entry, and waits (by default) for the dropCollection entry to be majority-committed.   # *Only one of the secondaries* is lucky enough to receive the entry.   # Now the dropCollection entry is majority-committed, so dropDatabase proceeds on the primary to drop the database.   # The primary hits the dropDatabaseHangBeforeLog failpoint - it doesn't write the dropDatabase oplog entry. It hangs holding the global write lock, *which blocks the other secondary from receiving the dropCollection entry.*   # The RollbackTest fixture in transitionToRollbackOperations sees that *one* of the secondaries is not caught up, so waits until the test times out.    This race was introduced in SERVER-38865, when the tieBreaker node was changed from an arbiter to a secondary. Before that change, when the dropCollection entry was majority committed that meant all secondaries (which was just """"syncSource"""") had replicated it.    Tess hypothesizes there are other rollback tests that also rely on the old meaning of """"majority committed"""" to be the same as """"all secondaries have replicated"""". I'll restore that guarantee by stopping replication on the new secondary, """"tieBreaker"""", at the beginning of the RollbackTest, and in transitionToRollbackOperations() I'll only await replication on """"syncSource"""".""",Bug,Replication
472038,"""Diagnosing a test failure and this is something that I wanted to know that was absent.""",Improvement,Storage
472405,"""HI,   Is there something I am missing regarding where in the GUI to go to test and build code for Project 1?    After signing up, the GUI for SimpliLearn, the MongoDB course is great for watching the demos.  The Practice Lab worked the first time in.  On initial start, it appears that 27017 is started as an attempt to start it results in """"address already in use failed to setup sockets during startup"""".  Given it has already started, an attempt to start the second one to try out the project,   {code:java}  mongod --port 27018 --dbpath /data/data2 -replSet rs0 --smallfiles --oplogSize 128{code}  causes the code to hang.   I'd like to take a stab at Project 1 using Sharding to create the telephone java script based on the new regulations for telecom to maintain all details of their customers.   To do so, it appear to require mongod running on 27017, 27018, and 27019, and then, to set up the config server on 27019 to host (I think this is right), the three configdbs on 27021, 27022, and 27023 - connect them together, then connect back to the host on 27017 ---- and so on.  But the 27018 won't start thus no way to get to the sharding portion.     Is there something I am missing regarding where in the GUI to go to test and build code for Project 1?""",Question,Sharding
472477,"""Switch evergreen.yml to use task tags for specifying which tests to run on which variant.    This will involve some investigation to determine a good way of selecting which tags to use and then switching etc/evergreen.yml to use those tags.    ----    As a MongoDB engineer,  I want to be able to specify the build variants a task run on via tasks,  so that I don't have to manually add a task to all buildvariants it should run on.    ----    AC: The tasks run of each build variant should not change.""",Improvement,Test
472969,"""As a few people have mentioned on SERVER-12266, the change to throw errors on empty updates is highly disruptive and has kept me from updating for a long time. I'm finally in a position in which I need to move to a more recent version of MongoDB and have followed the lead of others in writing an application layer that detects empty patches and aborts the call to MongoDB in those cases. But, in doing so, I'm struck by the absurdity of the application needing to include code for that. Surely it would be much more efficient to implement such functionality within MongoDB, perhaps enabled by a run-time flag? Or perhaps a default, and disabled only via a run-time flag, since I'm sure that the people who want empty patches to work vastly outnumber those who want them to fail. I guess that this could also be a client option.    An action triggered via an option on a patch request to a database-driven service is a very common thing. The success of the operation is seldom desired to be contingent on the presence of patch data, and there are probably millions of lines of code in the world that work around Mongo's inconsistent treatment of empty and non-empty patches. It would be great to be able to remove them.""",Improvement,Write Ops
473040,"""I have a collection of areas:  {code:java}  { """"_id"""" : 1, name: """"Area 1"""", """"geometry"""" : { type: """"Polygon"""", coordinates: <coordinates>} }   { """"_id"""" : 2, name: """"Area 2"""", """"geometry"""" : { type: """"Polygon"""", coordinates: <coordinates>} }   { """"_id"""" : 3, name: """"Area 3"""", """"geometry"""" : { type: """"Polygon"""", coordinates: <coordinates>} }{code}  I want to aggregate this so that every area (document) returned has a property containing all other areas within it's boundaries.   The purpose is to be able to make holes in a polygon if other polygons are fully contained within it's boundaries.    Result example where Area 2 and Area 3 are contained within the boundaries of Area 1.  {code:java}  { """"_id"""" : 1, name: """"Area 1"""", """"geometry"""" : { type: """"Polygon"""", coordinates: <coordinates>}, holes: [{ """"_id"""" : 2, name: """"Area 2"""" }, { """"_id"""" : 3, name: """"Area 3"""" }] }   { """"_id"""" : 2, name: """"Area 2"""", """"geometry"""" : { type: """"Polygon"""", coordinates: <coordinates>} }   { """"_id"""" : 3, name: """"Area 3"""", """"geometry"""" : { type: """"Polygon"""", coordinates: <coordinates>} }{code}  I have tried using the `$lookup` aggregate but it seems like $geoWithin isn't supported inside the pipeline with my `let` variable as input.  As i understand it I need to wrap `$geoWithin` part in a `$expr` to be able to use the `$$area_geometry` variable from the `let` statement but these operators doesn't seem compatible.    So far my query ended up like this, but obviously this doesn't work.  {code:java}  db.Areas.aggregate([   {     $lookup:      {       from: 'Areas',       let: { area_geometry: """"$geometry"""" },       pipeline: [         {            $match:            {              Geometry: {                $geoWithin: {                  $geometry: $$area_geometry               }              }           }         },         { $project: { geometry: 0 } }       ],       as: 'holes'     }   }  ]){code}""",New Feature,Aggregation Framework
473540,"""The IndexBuildsCoordinator is for managing index builds across a replica set. It links in networking and has a ThreadPool on which to run index builds.    The embedded server cannot link to networking code, nor can it use ThreadPools. Therefore, we must make a simple class that builds indexes via the IndexBuildsManager. There is no need for asynchronous threads on an embedded server, which is effectively a standalone without networking or asynchronously running threads.    I haven't looked into whether it is possible to just make a separate class to link into embedded, or inheritance is necessary, necessitating splitting the existing IndexBuildsCoordinator into and interface and implementation. I'd guess inheritance is necessary, since commands in the standalone library are probably all included in the greater repl inclusive libraries? Shims are also a potential tool, I haven't explored that idea, either.""",Task,Storage
474064,"""First of all, sorry for my english.  I'm trying to implement a simple example of ACID transaction in MongoDB, using NodeJS driver. I also use Mongoose. I'm just experimenting with transaction to understand how it works and to foresee and prevent all possible errors.   I have a collection 'testcollection', and i have a single document in it, like:       {code:java}  var testcollection = new mongoose.Schema({   id: Number,   name: String,   number: Number  });  {code}    This is code:       {code:java}  var mongoDB = require('./libs/db');  var _ = require('underscore');    mongoDB.connect(""""mongodb://testuser:name32@localhost:27018/testbase"""");    var testcoll = require('./libs/db/models/testcollection');    var _session = mongoose.startSession({readPreference: {mode: """"primary""""}});  (async () => _session = await _session)(); // wait for ClientSession    async function transactionOperations(id, session){         async function transactionOperations(id, session){ await      testcoll.updateOne({name: """"ilya"""", id: 1}, {$inc: {number: 2}}).session(session);      // change document as transaction !!!         await testcoll.updateOne({name: """"ilya"""", id: 1}, {$inc: {number: 3}});   // this is not transaction!!!  }    var performTransaction = function(id){          async function commit() {          try {              await _session.commitTransaction();          } <USER>(error) {              if (error.errorLabels && error.errorLabels.indexOf('UnknownTransactionCommitResult') >= 0) {                  console.warn('UnknownTransactionCommitResult, retrying commit operation.', error);                  await commit();              } else {                  console.error('Transaction aborted. Caught exception during transaction.', error);              }          }      }            async function runTransactionWithRetry(txnFunc, _transactionOperations) {          try {              await txnFunc(_transactionOperations);          } <USER>(error) {              if (error.errorLabels && error.errorLabels.indexOf('TransientTransactionError') >= 0) {                  console.warn('TransientTransactionError, retrying transaction.', error);                  await _session.abortTransaction();                  await runTransactionWithRetry(txnFunc, _transactionOperations);              } else {                  if(_session.inTransaction()){                      await runTransactionWithRetry(txnFunc, _transactionOperations);                  } else {                      await _session.abortTransaction();                      console.error('Transaction aborted. Caught exception during transaction.', error);                  }              }          }      }        async function createTransaction(_transactionOperations) {                  // wait for ClientSession          _session = await _session;          console.log(""""START """" + id + """" TRANSACTION"""");          _session.startTransaction({              readConcern: { level: 'snapshot' },              writeConcern: { w: 'majority',}          });          // ----- all transaction operations here -----                  await _transactionOperations(id, _session);                   // ----- all transaction operations here -----                    try {              await commit();          } <USER>(error) {              await _session.abortTransaction();              console.error('Transaction aborted. Caught exception during transaction.', error);          }      }    runTransactionWithRetry(createTransaction, transactionOperations);  }  {code}  In my transaction i'm trying to simulate the situation, when at first i change document as a transaction (increment field 'number' +2), and than change it not as a transaction (increment same field +3 in same doc, but i don't pass session parameter in query function). So it must cuase some write conflict, and throw some error.    When i running this transaction, it waits for a 60 seconds (as i understood, max txn time), and than throw an error:  {code:java}    errorLabels: [ 'TransientTransactionError' ],    operationTime: Timestamp { _bsontype: 'Timestamp', low_: 1, high_: 1540719109 },    ok: 0,    errmsg: 'Transaction 1 has been aborted.',    code: 251,    codeName: 'NoSuchTransaction'    {code}  It seems, that transaction runs, can't perform update operation because of write conflict and than just waiting for txn timeout. If i'm not mistaking, this is a specific write-conflict error, so why it wait 60 seconds and throws 'NoSuchTransaction' MongoError?    BUT!!! Somitimes (for example, when i add some console.log-s in transaction body) it throws another error, such as:  {code:java}    errorLabels: [ 'TransientTransactionError' ],    operationTime: Timestamp { _bsontype: 'Timestamp', low_: 1, high_: 1540719759 },    ok: 0,    errmsg: 'Unable to acquire lock \'{8613801417341912551: Database, 1696272389700830695}\' within a max lock request timeout of \'5ms\' milliseconds.',    code: 24,    codeName: 'LockTimeout'{code}  Here transaction don't have enough time (5ms) to lock the document. As i understood, transactions in mongo locks document not in the time, when calls session.startTransaction(), but when the operations, that are assosiated with this session and transaction, are executing.         So, what i should do with this problem? How can i handle correctly this write conflicts? Why in one situation i <USER>one MongoError, and in other situation - another, but both errors is caused by single reason?  Thanks a lot.""",Question,"Replication,JavaScript"
474104,"""I'm finding issues in one of our databases that houses over 100GB of data where queries are using wrong indexes... where they could use a better index designed specifically for it's query.      For example: I have a table of leads with field names i.e., Name, Email, Source (And about 50 other <USER>with lead details).  There are several reports (let's call it a dozen) that utilize a 10+ or so of the <USER>  Originally we designed multiple indexes to serve each report individually, but several <USER>were needed across the indexes... wasting a good chunk of ram. Eventually aggregation was falling behind too because it was automatically picking indexes not best meant for it (i.e. it would pick one index for the date range, but it ended up doing a COLLSCAN for a second part of the query that was in another index.  It could have picked up the correct index that had the combination, but it didn't).  Thus, we removed all multi indexes and created one single index with all the key <USER>used across the reports.   Memory was reduced quickly and initially the index worked perfectly, but as our data grows we see this index starting to move VERY slowly.           Is it possible to create an index that is used for hint only, where aggregation and other query/cursors do not pull from?""",Question,Indexing
474195,"""We hit a segmentation vault on version 3.6.6 today on our primary. I'll give some additional context which may or may not be relevant to the error.    Recently we have been running up against the connection limit due to a OS limit we haven't lifted yet (~32000), which was causing new connections to fail. This was happening and we made a change to reduce the number of connections to the primary (~18000), which was working and then the primary seg faulted.    We have three replicas in this cluster, and one replica was already down for another issue (certificate expiration). I assume this prevented re-election causing our cluster to be unavailable until we manually cycled this replica by restarting the process.    This issue is around the segfault error itself, but I am also curious why the process did not crash itself (it was left hanging with a single core pegged–not sure what it was doing) which would have auto-restarted in our system.    2018-10-17T18:55:43.278+0000 F - [listener] Got signal: 11 (Segmentation fault).    0x56036294d8b1 0x56036294cac9 0x56036294d136 0x7f9764421390 0x7f9764417e8f 0x5603628016db 0x56036223c9ff 0x56036132709f 0x56036132789a 0x5603613256f1 0x5603624853b2 0x5603624915c9 0x560362491811 0x56036249ba5e 0x56036248377e 0x560362a5ce80 0x7f97644176ba 0x7f976414d41d  ----- BEGIN BACKTRACE -----  {""""backtrace"""":[\{""""b"""":""""5603606FA000"""",""""o"""":""""22538B1"""",""""s"""":""""_ZN5mongo15printStackTraceERSo""""},\{""""b"""":""""5603606FA000"""",""""o"""":""""2252AC9""""},\{""""b"""":""""5603606FA000"""",""""o"""":""""2253136""""},\{""""b"""":""""7F9764410000"""",""""o"""":""""11390""""},\{""""b"""":""""7F9764410000"""",""""o"""":""""7E8F"""",""""s"""":""""pthread_create""""},\{""""b"""":""""5603606FA000"""",""""o"""":""""21076DB"""",""""s"""":""""_ZN5mongo25launchServiceWorkerThreadESt8functionIFvvEE""""},\{""""b"""":""""5603606FA000"""",""""o"""":""""1B429FF"""",""""s"""":""""_ZN5mongo9transport26ServiceExecutorSynchronous8scheduleESt8functionIFvvEENS0_15ServiceExecutor13ScheduleFlagsENS0_23ServiceExecutorTaskNameE""""},\{""""b"""":""""5603606FA000"""",""""o"""":""""C2D09F"""",""""s"""":""""_ZN5mongo19ServiceStateMachine22_scheduleNextWithGuardENS0_11ThreadGuardENS_9transport15ServiceExecutor13ScheduleFlagsENS2_23ServiceExecutorTaskNameENS0_9OwnershipE""""},\{""""b"""":""""5603606FA000"""",""""o"""":""""C2D89A"""",""""s"""":""""_ZN5mongo19ServiceStateMachine5startENS0_9OwnershipE""""},\{""""b"""":""""5603606FA000"""",""""o"""":""""C2B6F1"""",""""s"""":""""_ZN5mongo21ServiceEntryPointImpl12startSessionESt10shared_ptrINS_9transport7SessionEE""""},\{""""b"""":""""5603606FA000"""",""""o"""":""""1D8B3B2""""},\{""""b"""":""""5603606FA000"""",""""o"""":""""1D975C9"""",""""s"""":""""_ZN4asio6detail9scheduler10do_run_oneERNS0_27conditionally_enabled_mutex11scoped_lockERNS0_21scheduler_thread_infoERKSt10error_code""""},\{""""b"""":""""5603606FA000"""",""""o"""":""""1D97811"""",""""s"""":""""_ZN4asio6detail9scheduler3runERSt10error_code""""},\{""""b"""":""""5603606FA000"""",""""o"""":""""1DA1A5E"""",""""s"""":""""_ZN4asio10io_context3runEv""""},\{""""b"""":""""5603606FA000"""",""""o"""":""""1D8977E""""},\{""""b"""":""""5603606FA000"""",""""o"""":""""2362E80""""},\{""""b"""":""""7F9764410000"""",""""o"""":""""76BA""""},\{""""b"""":""""7F9764046000"""",""""o"""":""""10741D"""",""""s"""":""""clone""""}],""""processInfo"""":\{ """"mongodbVersion"""" : """"3.6.6"""", """"gitVersion"""" : """"6405d65b1d6432e138b44c13085d0c2fe235d6bd"""", """"compiledModules"""" : [], """"uname"""" : { """"sysname"""" : """"Linux"""", """"release"""" : """"4.4.0-1049-aws"""", """"version"""" : """"#58-Ubuntu SMP Fri Jan 12 23:17:09 UTC 2018"""", """"machine"""" : """"x86_64"""" }, """"somap"""" : [ \{ """"b"""" : """"5603606FA000"""", """"elfType"""" : 3, """"buildId"""" : """"F63278FD698B5843222FE7A6C8FF17D6AEFBBE38"""" }, \{ """"b"""" : """"7FFF7935A000"""", """"elfType"""" : 3, """"buildId"""" : """"3A8AFEDA6CA80FBF2589D7E5803A58BA8F13FE62"""" }, \{ """"b"""" : """"7F9765605000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libresolv.so.2"""", """"elfType"""" : 3, """"buildId"""" : """"6EF73266978476EF9F2FD2CF31E57F4597CB74F8"""" }, \{ """"b"""" : """"7F97651C1000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libcrypto.so.1.0.0"""", """"elfType"""" : 3, """"buildId"""" : """"250E875F74377DFC74DE48BF80CCB237BB4EFF1D"""" }, \{ """"b"""" : """"7F9764F58000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libssl.so.1.0.0"""", """"elfType"""" : 3, """"buildId"""" : """"513282AC7EB386E2C0133FD9E1B6B8A0F38B047D"""" }, \{ """"b"""" : """"7F9764D54000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libdl.so.2"""", """"elfType"""" : 3, """"buildId"""" : """"8CC8D0D119B142D839800BFF71FB71E73AEA7BD4"""" }, \{ """"b"""" : """"7F9764B4C000"""", """"path"""" : """"/lib/x86_64-linux-gnu/librt.so.1"""", """"elfType"""" : 3, """"buildId"""" : """"89C34D7A182387D76D5CDA1F7718F5D58824DFB3"""" }, \{ """"b"""" : """"7F9764843000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libm.so.6"""", """"elfType"""" : 3, """"buildId"""" : """"DFB85DE42DAFFD09640C8FE377D572DE3E168920"""" }, \{ """"b"""" : """"7F976462D000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libgcc_s.so.1"""", """"elfType"""" : 3, """"buildId"""" : """"68220AE2C65D65C1B6AAA12FA6765A6EC2F5F434"""" }, \{ """"b"""" : """"7F9764410000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libpthread.so.0"""", """"elfType"""" : 3, """"buildId"""" : """"CE17E023542265FC11D9BC8F534BB4F070493D30"""" }, \{ """"b"""" : """"7F9764046000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libc.so.6"""", """"elfType"""" : 3, """"buildId"""" : """"B5381A457906D279073822A5CEB24C4BFEF94DDB"""" }, \{ """"b"""" : """"7F9765820000"""", """"path"""" : """"/lib64/ld-linux-x86-64.so.2"""", """"elfType"""" : 3, """"buildId"""" : """"5D7B6259552275A3C17BD4C3FD05F5A6BF40CAA5"""" } ] }}   mongod(_ZN5mongo15printStackTraceERSo+0x41) [0x56036294d8b1]   mongod(+0x2252AC9) [0x56036294cac9]   mongod(+0x2253136) [0x56036294d136]   libpthread.so.0(+0x11390) [0x7f9764421390]   libpthread.so.0(pthread_create+0x4FF) [0x7f9764417e8f]   mongod(_ZN5mongo25launchServiceWorkerThreadESt8functionIFvvEE+0xDB) [0x5603628016db]   mongod(_ZN5mongo9transport26ServiceExecutorSynchronous8scheduleESt8functionIFvvEENS0_15ServiceExecutor13ScheduleFlagsENS0_23ServiceExecutorTaskNameE+0x2FF) [0x56036223c9ff]   mongod(_ZN5mongo19ServiceStateMachine22_scheduleNextWithGuardENS0_11ThreadGuardENS_9transport15ServiceExecutor13ScheduleFlagsENS2_23ServiceExecutorTaskNameENS0_9OwnershipE+0x15F) [0x56036132709f]   mongod(_ZN5mongo19ServiceStateMachine5startENS0_9OwnershipE+0x13A) [0x56036132789a]   mongod(_ZN5mongo21ServiceEntryPointImpl12startSessionESt10shared_ptrINS_9transport7SessionEE+0x881) [0x5603613256f1]   mongod(+0x1D8B3B2) [0x5603624853b2]   mongod(_ZN4asio6detail9scheduler10do_run_oneERNS0_27conditionally_enabled_mutex11scoped_lockERNS0_21scheduler_thread_infoERKSt10error_code+0x389) [0x5603624915c9]   mongod(_ZN4asio6detail9scheduler3runERSt10error_code+0xD1) [0x560362491811]   mongod(_ZN4asio10io_context3runEv+0x3E) [0x56036249ba5e]   mongod(+0x1D8977E) [0x56036248377e]   mongod(+0x2362E80) [0x560362a5ce80]   libpthread.so.0(+0x76BA) [0x7f97644176ba]   libc.so.6(clone+0x6D) [0x7f976414d41d]  ----- END BACKTRACE -----""",Bug,Stability
474295,"""I've attached the conf files I used for my sharded cluster.     I was running this on 4.0.3-ent.     I tried running the csrs with and without {{inMemory}} (wasn't sure if it's allowed for csrs) and saw the same behavior both ways.     When I try to add the rs as a shard, I get the following:  {noformat}MongoDB Enterprise mongos> db.runCommand({""""addShard"""": """"rs1/louisamac:5000,louisamac:5001,louisamac:5002"""", """"name"""": """"shard0""""})MongoDB Enterprise mongos> db.runCommand({""""addShard"""": """"rs1/louisamac:5000,louisamac:5001,louisamac:5002"""", """"name"""": """"shard0""""}){ """"ok"""" : 0, """"errmsg"""" : """"can't add shard with a local copy of config.system.sessions, please drop this collection from the shard manually and try again. :: caused by :: failed to run command { drop: \""""system.sessions\"""", writeConcern: { w: \""""majority\"""" } } when attempting to add shard rs1/louisamac:5000,louisamac:5001,louisamac:5002 :: caused by :: NetworkInterfaceExceededTimeLimit: timed out"""", """"code"""" : 96, """"codeName"""" : """"OperationFailed"""", """"operationTime"""" : Timestamp(1539203544, 1), """"$clusterTime"""" : { """"clusterTime"""" : Timestamp(1539203544, 1), """"signature"""" : { """"hash"""" : BinData(0,""""AAAAAAAAAAAAAAAAAAAAAAAAAAA=""""), """"keyId"""" : NumberLong(0) } }} {noformat}       When I tried the same repro with wiredTiger, I did not see this issue.     *Note*: I found this while testing, not in a production or customer issue.     cc [~<USER> [~<USER>""",Bug,Sharding
475618,"""I'm using mongo shell 4.0.0 against MongoDB 2.4.9 to run an aggregation pipeline but it fails with this error:  {code:java}  E QUERY    [js] TypeError: invalid assignment to const `res' :  DB.prototype._runAggregate@src/mongo/shell/db.js:250:19 {code}  I had a look at the source code and it looks like {{res}} is being declared as a constant on line 242  {code:java}  const res = doAgg(cmdObj);{code}  but is being reassigned on line 250.  The problem may be that the version of MongoDB I'm using is way older than the client but, in my limited knowledge of Javascript, a {{const}} cannot be reassigned    I can reproduce the error with this example query:  {code:java}  db.users.aggregate([{      $match: {          userId: 'a598fd6f-b947-44e7-9f60-8ecbbde136a7'      }  }]);{code}""",Bug,Shell
475628,"""Hi Security dev team!    I was configuring auditing at my new workplace. The basic idea is:    {code:yaml}  auditLog:     destination: file     format: JSON     path: /tmp/audit.json     filter: '{atype: {$in: [                 """"authenticate"""", """"authCheck"""",                  """"renameCollection"""", """"dropCollection"""", """"dropDatabase"""",                  """"createUser"""", """"dropUser"""", """"dropAllUsersFromDatabase"""", """"updateuser"""",                  """"grantRolesToUser"""", """"revokeRolesFromUser"""", """"createRole"""", """"updateRole"""",                  """"dropRole"""", """"dropAllRolesFromDatabase"""", """"grantRolesToRole"""", """"revokeRolesFromRole"""",                  """"grantPrivilegesToRole"""", """"revokePrivilegesFromRole"""",                  """"enableSharding"""", """"shardCollection"""", """"addShard"""", """"removeShard"""",                  """"shutdown"""",                  """"applicationMessage""""             ]}}'  {code}    Whilst I was doing this I realized for the first time there is no auditing for replSetConfigure actions. So a Naughty DBA could for example execute start a node on their desktop or some useful computer, then _rs.add('my\_desktop\_fqdn:27017')_, sync, then _'rs.remove('my\_desktop\_fqdn:27017')_, and they'd have a copy of the data directory without anything appearing in the audit log. It would be in the normal logs, but that's not as hard to cover up.    I couldn't find any existing JIRA tickets that mention this, now that I'm logged in as a public user.    Is there any reason that auditing replSetConfigure actions has been excluded? If not I'd like to request this as an enhancement. (Ideally backported to 3.6 too.)    Cheers from Tokyo,    Akira""",Improvement,Security
475895,"""The problem is that $limit doesn't affect the amount of documents to be loaded into memory for examination in the aggregation pipeline. For example, check this one:       {noformat}  > db.system.profile.find().limit(1).sort( { ts : -1 } ).pretty()  {   """"op"""" : """"command"""",   """"ns"""" : """"datasetTreeDB.datasetMappingAnnotation"""",   """"command"""" : {    """"aggregate"""" : """"datasetMappingAnnotation"""",    """"pipeline"""" : [     {      """"$match"""" : {       """"rootNode"""" : """"markingtool"""",       """"imageCreatedDate"""" : {        """"$gt"""" : ISODate(""""2016-03-27T00:00:00Z"""")       },       """"imageMeta__league"""" : """"Major League Baseball""""      }     },     {      """"$limit"""" : 100000     },     {      """"$sort"""" : {       """"randomIndex"""" : 1      }     },     {      """"$limit"""" : 1     },     {      """"$project"""" : {       """"_id"""" : 0,       """"fileName"""" : 1,       """"rootNode"""" : 1      }     },     {      """"$count"""" : """"count""""     }    ],    """"allowDiskUse"""" : true,    """"cursor"""" : {         }   },   """"keysExamined"""" : 100000,   """"docsExamined"""" : 100000,   """"hasSortStage"""" : true,   """"cursorExhausted"""" : true,   """"numYield"""" : 816,   """"locks"""" : {    """"Global"""" : {     """"acquireCount"""" : {      """"r"""" : NumberLong(1654)     }    },    """"Database"""" : {     """"acquireCount"""" : {      """"r"""" : NumberLong(827)     }    },    """"Collection"""" : {     """"acquireCount"""" : {      """"r"""" : NumberLong(826)     }    }   },   """"nreturned"""" : 1,   """"responseLength"""" : 130,   """"protocol"""" : """"op_command"""",   """"millis"""" : 1346,   """"planSummary"""" : """"IXSCAN { rootNode: 1, imageMeta__league: 1, imageCreatedDate: 1, randomIndex: 1, imageId: 1, secondLevelNode: 1 }"""",   """"ts"""" : ISODate(""""2018-06-28T17:05:57.702Z""""),   """"client"""" : """"127.0.0.1"""",   """"appName"""" : """"MongoDB Shell"""",   """"allUsers"""" : [ ],   """"user"""" : """"""""  }{noformat}       I want to get only 1 result from the aggregation and include into this result a field that doesn't exist in the index. The expected behaviour that it will scan 100000 keys for sorting in memory using values from indexes (it makes sense as after """"imageCreatedDate"""" search it can't use further indexes for sorting anymore) and then load and test only 1 first document. However it tried to load all 100000 documents (""""docsExamined"""" : 100000) - it completely kills the performance of this operation.         If do not include the field that doesn't exist in the index, I can see that it doesn't try to fetch any document and can return the result using indexes only:            {noformat}  db.system.profile.find().limit(1).sort( { ts : -1 } ).pretty()  {   """"op"""" : """"command"""",   """"ns"""" : """"datasetTreeDB.datasetMappingAnnotation"""",   """"command"""" : {    """"aggregate"""" : """"datasetMappingAnnotation"""",    """"pipeline"""" : [     {      """"$match"""" : {       """"rootNode"""" : """"markingtool"""",       """"imageCreatedDate"""" : {        """"$gt"""" : ISODate(""""2016-03-27T00:00:00Z"""")       },       """"imageMeta__league"""" : """"Major League Baseball""""      }     },     {      """"$limit"""" : 100000     },     {      """"$sort"""" : {       """"randomIndex"""" : 1      }     },     {      """"$limit"""" : 1     },     {      """"$project"""" : {       """"_id"""" : 0,       """"rootNode"""" : 1      }     },     {      """"$count"""" : """"count""""     }    ],    """"allowDiskUse"""" : true,    """"cursor"""" : {         }   },   """"keysExamined"""" : 100000,   """"docsExamined"""" : 0,   """"hasSortStage"""" : true,   """"cursorExhausted"""" : true,   """"numYield"""" : 785,   """"locks"""" : {    """"Global"""" : {     """"acquireCount"""" : {      """"r"""" : NumberLong(1586)     }    },    """"Database"""" : {     """"acquireCount"""" : {      """"r"""" : NumberLong(793)     }    },    """"Collection"""" : {     """"acquireCount"""" : {      """"r"""" : NumberLong(792)     }    }   },   """"nreturned"""" : 1,   """"responseLength"""" : 130,   """"protocol"""" : """"op_command"""",   """"millis"""" : 182,   """"planSummary"""" : """"IXSCAN { rootNode: 1, imageMeta__league: 1, imageCreatedDate: 1, randomIndex: 1, imageId: 1, secondLevelNode: 1 }"""",   """"ts"""" : ISODate(""""2018-06-28T17:16:10.787Z""""),   """"client"""" : """"127.0.0.1"""",   """"appName"""" : """"MongoDB Shell"""",   """"allUsers"""" : [ ],   """"user"""" : """"""""  }{noformat}            These ones are synthetic example and in real environment I have ~100.000.000 items in collection and my goal is to use indexes at the """"$match"""" step, then """"$sort"""" (like in this example) and then return only first 100.000 items ($limit with 100.000). However if """"$match"""" step returns 5.000.000 items it tries to load and examine all 5.000.000 documents and only then return top 100.000.    Is there a way how I can fix this behaviour?          Thank you!     """,Bug,Aggregation Framework
476043,"""# I want to update the equivalent of a foreign key field that has an ObjectId string value with the ObjectId itself. I have tried using the aggregation $addFields and have discovered that ObjectId(""""$hexStringField"""") errors with:     Failed to execute script.     Error: invalid object id: length :  @(shell):2:26       # I suspected that there might be a problem using Object constructor methods so I tried using $addFields with UUID(""""$uuidStringField"""") and Date(""""$dateField"""").  Using these methods did not produce an error, but did not give the expected results.     """,Bug,Aggregation Framework
476132,"""We have some dev/staging environments which are locally hosted in our office building. They are entirely for internal usage, so there uptime isn't critical. We have recently been experiencing power outages that cause all 3 members of the replica set to go down and then when power restores come back up at the same time. This has happened about 5 times now and each time when the replica set comes back up both the primary/secondary end up in the REMOVED status and never recover unless we manually restart one of the mongo processes.    mongo-dev1 rs.status()  {code:java}  {          """"state"""" : 10,          """"stateStr"""" : """"REMOVED"""",          """"uptime"""" : 199841,          """"optime"""" : {                  """"ts"""" : Timestamp(1529137449, 1),                  """"t"""" : NumberLong(590)          },          """"optimeDate"""" : ISODate(""""2018-06-16T08:24:09Z""""),          """"ok"""" : 0,          """"errmsg"""" : """"Our replica set config is invalid or we are not a member of it"""",          """"code"""" : 93,          """"codeName"""" : """"InvalidReplicaSetConfig""""  }  {code}  mongo-dev2 rs.status()  {code:java}  {          """"state"""" : 10,          """"stateStr"""" : """"REMOVED"""",          """"uptime"""" : 199879,          """"optime"""" : {                  """"ts"""" : Timestamp(1529137449, 1),                  """"t"""" : NumberLong(590)          },          """"optimeDate"""" : ISODate(""""2018-06-16T08:24:09Z""""),          """"ok"""" : 0,          """"errmsg"""" : """"Our replica set config is invalid or we are not a member of it"""",          """"code"""" : 93,          """"codeName"""" : """"InvalidReplicaSetConfig""""  }    {code}  mongo-dev1 show log rs  {code:java}  2018-06-16T09:10:25.236+0000 I REPL     [replExecDBWorker-0] New replica set config in use: { _id: """"dev_cluster1"""", version: 140719, protocolVersion: 1, members: [ { _id: 0, host: """"mongo-dev1.220office.local:27017"""", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 2.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: """"utility-dev1.220office.local:27017"""", arbiterOnly: true, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 4, host: """"mongo-dev2.2  2018-06-16T09:10:25.236+0000 I REPL     [replExecDBWorker-0] transition to REMOVED  2018-06-17T21:28:09.139+0000 I REPL     [ReplicationExecutor] Member utility-dev1.220office.local:27017 is now in state ARBITER  {code}  mongo-dev1 rs.conf()  {code:java}  {          """"_id"""" : """"dev_cluster1"""",          """"version"""" : 140719,          """"protocolVersion"""" : NumberLong(1),          """"members"""" : [                  {                          """"_id"""" : 0,                          """"host"""" : """"mongo-dev1.220office.local:27017"""",                          """"arbiterOnly"""" : false,                          """"buildIndexes"""" : true,                          """"hidden"""" : false,                          """"priority"""" : 2,                          """"tags"""" : {                        },                          """"slaveDelay"""" : NumberLong(0),                          """"votes"""" : 1                  },                  {                          """"_id"""" : 3,                          """"host"""" : """"utility-dev1.220office.local:27017"""",                          """"arbiterOnly"""" : true,                          """"buildIndexes"""" : true,                          """"hidden"""" : false,                          """"priority"""" : 1,                          """"tags"""" : {                        },                          """"slaveDelay"""" : NumberLong(0),                          """"votes"""" : 1                  },                  {                          """"_id"""" : 4,                          """"host"""" : """"mongo-dev2.220office.local:27017"""",                          """"arbiterOnly"""" : false,                          """"buildIndexes"""" : true,                          """"hidden"""" : false,                          """"priority"""" : 1,                          """"tags"""" : {                        },                          """"slaveDelay"""" : NumberLong(0),                          """"votes"""" : 1                  }          ],          """"settings"""" : {                  """"chainingAllowed"""" : true,                  """"heartbeatIntervalMillis"""" : 2000,                  """"heartbeatTimeoutSecs"""" : 10,                  """"electionTimeoutMillis"""" : 10000,                  """"catchUpTimeoutMillis"""" : 60000,                  """"getLastErrorModes"""" : {                },                  """"getLastErrorDefaults"""" : {                          """"w"""" : 1,                          """"wtimeout"""" : 0                  }          }  }    {code}  As I read the documentation I can't find much information about the REMOVED status. In our setup, since mongo-dev1 has a priority of 2 and mongo-dev2 has a priority of 1 I would expect mongo-dev1 to be elected as the primary after the reboot.    Is this a bug or are we doing something wrong. If it's a bug, what is the proper procedure for this when all members of the replica set come online at the same time?""",New Feature,"Replication,Networking"
476657,"""We have 5 instances in our replica set:    1. ARBITER, Mongo *3.6.4*, Ubuntu 16.04  2. PRIMARY, Mongo *3.6.4*, Ubuntu 16.04  3. SECONDARY, Mongo *3.6.4*, Ubuntu 16.04  4. SECONDARY, Mongo *3.6.{color:#FF0000}3{color}*, Debian 8.1  5. NEW MEMBER, Mongo *3.6.4*, Ubuntu 16.04    The 4th member was a primary in the past and its data were cloned using a snapshot (we are in a cloud) to create members 2 and 3. What's important *it had Mongo 3.4*, but we had upgraded it before proceeding with replication. Now we are trying to attach another member, but we want it to be synchronized via an initial sync because we want it to have a different data structure (separate folders for databases). Unfortunately the initial sync was running for more than 40 hours and restarted because of the following error (colored by me):  {code:java}  2018-05-20T18:11:27.888+0000 I STORAGE [repl writer worker 10] createCollection: <anonymized>.tmp.agg_out.253 with no UUID.  2018-05-20T18:11:27.903+0000 E REPL [repl writer worker 4] Error applying operation: OplogOperationUnsupported: Applying renameCollection not supported in initial sync: { ts: Timestamp(1526688414, 116), t: 10, h: 8574474825826084483, v: 2, op: """"c"""", ns: """"<anonymized>.$cmd"""", wall: new Date(1526688414589), o: { renameCollection: """"<anonymized>.tmp.agg_out.253"""", to: """"<anonymized>.mime_types"""", stayTemp: false, dropTarget: true } } ({ ts: Timestamp(1526688414, 116), t: 10, h: 8574474825826084483, v: 2, op: """"c"""", ns: """"<anonymized>.$cmd"""", wall: new Date(1526688414589), o: { renameCollection: """"<anonymized>.tmp.agg_out.253"""", to: """"<anonymized>.mime_types"""", stayTemp: false, dropTarget: true } })  2018-05-20T18:11:27.904+0000 E REPL [replication-515] Failed to apply batch due to 'OplogOperationUnsupported: error applying batch :: caused by :: Applying renameCollection not supported in initial sync: { ts: Timestamp(1526688414, 116), t: 10, h: 8574474825826084483, v: 2, op: """"c"""", ns: """"<anonymized>.$cmd"""", wall: new Date(1526688414589), o: { renameCollection: """"<anonymized>.tmp.agg_out.253"""", to: """"<anonymized>.mime_types"""", stayTemp: false, dropTarget: true } }'{code}  As far as I understand this kind of error was a bug in earlier versions of Mongo and should not happen in our case. However I can see _createCollection [...] with no UUID_ which looks strange according to what I've read so far about this issue.    Unfortunately we cannot stop aggregations, so we will surely have more these {{$out}} commands which makes me not very optimistic in terms of next attempts...""",Bug,Replication
476815,"""The oldest timestamp is only advanced at the end of every batch during oplog replay in initial sync. This means that all dirty content generated by the application of the operations in a single batch will be pinned in cache. If the batch is large enough and the operations are heavy enough this dirty content can exceed eviction_dirty_trigger (default 20% of cache) and the rate of applying operations will become dramatically slower because it has to wait for the dirty data to be reduced below the threshold.    In extreme cases the node can become completely stuck due to full cache preventing a batch from completing and unpinning the data that is keeping the cache full (although I'm not sure if that's a necessary consequence of this or a failure of the lookaside mechanism to keep the node from getting completely stuck.)    This is similar to SERVER-34938, but I believe oplog application during initial sync is a different codepath from normal replication. If not feel free to close as a dup.""",Bug,Replication
477629,"""Having a getShardIdAt method on the chunk (which is provided by the  ChunkManager) exposes the internal implementation of the ChunkManager. If you  get a ChunkManager with timestamp 100, you shouldn't be allowed to ask for where  that chunk was at timestamp 50, because this is a potential for programmer  error.    That's why I want the Chunk that you get from the CM to come with the same  Timestamp as the CM.    If doing Misha's suggestion unblocks integration with GPiTR I am fine with  checking it in, but we need to have a ticket to disallow that.""",Task,Sharding
477968,"""The use case is:  # Specifically about security and NOT resource utilization  # Specifically about interactive client connections (not sessions or transactions): a human being using mongo shell    This feature would also require a new field with a Role: {{interactiveIdleConnectionTimeout: <seconds>}}    User Story:  As a DBA, I need to prevent users from (accidentally) leaving authenticated shells connected to my critical databases open on their desktop/laptop. This is related to global security requirements at my company, which apply to all software we use. """,New Feature,"Shell,Networking,Security,Admin"
478901,"""I am having issues with progressively worse performance for inserts and Updates on my MongoDB cluster. Initially, I was getting speeds upto 10 cr records in 19 minutes. Now, it is down to 31.30L / hr.     The data configuration is as follows -  1. 8 collections having a cumulative of 30cr records.  2. 1 collection having upwards of 190cr records.    The delay only gets significant on the part 2 (I am getting the speeds of 30 L/hr, on inserts after 160 cr records are already on that collection).     There are 4 indices on that collection. One default on (_id), two on 1 string field each and finally a compound index on 4 <USER> The final compound index is being used for sharding.    I am running a MongoDB Sharded cluster, with two shards, each having one primary and two secondary data nodes. I also have a set of three configuration servers (1 primary and 2 secondaries). The nodes are divided as follows     1. Machine 1 -> 1 node from replica set A, one node from replica set B.  2. Machine 2 and Machine 3 -> Same as above.  3. Machine 4,5,6 -> Each having a Configuration Node  4. Machine 7 -> Runs a Mongos (Query Router).    Configuration of the Systems (128GB RAM and 12 cores). Do I need to change anything in terms of the distribution of my nodes across servers and/or any changes in the collection's indexing and sharding?    Any other advice to improve the data insertion and updation is also appreciated.""",Question,"Indexing,Performance"
478989,"""I have a 3-node replica set running version 3.4.10 on Ubuntu 16.04.    I ran a schema update that touched all 7 million rows of a collection with a $set and a $rename. Because one of the secondaries is about 30ms away in Azure, I used majority write concern to slow down the update and make sure at least one of the secondaries would stay in sync.    The query started at 14:19:29. At that point the Azure slave was probably 3-5 minutes behind because of earlier schema migrations. But by 14:27:00, the main secondary was unable to get results for oplog queries:    {code}  Jan 13 14:27:00 secondary mongod.27017[28273]: [replication-163] Restarting oplog query due to error: ExceededTimeLimit: Operation timed out, request was RemoteCommand 18482564 -- target:primary:27017 db:local expDate:2018-01-13T14:27:00.216+0000 cmd:{ getMore: 16483339842, collection: """"oplog.rs"""", maxTimeMS: 5000, term: 25, lastKnownCommittedOpTime: { ts: Timestamp 1515853539000|8343, t: 25 } }. Last fetched optime (with hash): { ts: Timestamp 1515853555000|1852, t: 25 }[1175973526525408650]. Restarts remaining: 3  {code}    That's also the time the replica set stopped accepting connections from clients.    To get things running again I had to kill all three mongod processes (and then kill -9 because the shutdown tends to hang while in this state).    After letting the nodes sync up, I was able to reproduce this again with the same query.    I can provide logs and the query privately if that would be useful.    Just guessing based on what I learned in SERVER-32398, maybe the primary froze up because it ran out of cache while waiting for the secondary to apply changes. But the update was running with majority read concern so I would have thought the secondary couldn't have gotten far enough behind for that to occur.""",Bug,Replication
479122,"""The sharding continuous stepdown suite was introduced in MongoDB 3.2 as a way to test CSRS and over time has surfaced numerous sharding and replication bugs. At that time, the sharding metadata commands were all implemented on mongos and were doing reads and writes against the config server.    As of MongoDB 3.6 however, most of the metadata commands have been moved to run on the config server primary. As a result of this, the continuous stepdown suite has now become a test of the repeatability of these operations in the context of running sharding tests. However, due to the different command return values which can happen after an operation is repeated, this suite is now generating build failures, which are not necessarily bugs, but behaviour, which is not expected by tests and making tests account for this behaviour is not a trivial task, nor it is something which has a great customer value.    Because of this I would like to propose that we get rid of the sharding continuous stepdown suite in lieu of the already running concurrency stepdown suite.""",Task,"Test,Sharding"
479364,"""As a part of SERVER-30761, we refactored {{parseInExpression()}} and changed a loop that looked like  {code:cpp}  BSONObjIterator iter(theArray);  while (iter.more()) {      auto e = iter.next();      // ...  }  {code}    into  {code:cpp}  for (auto e : theArray) {      // ...  }  {code}    This has caused a performance regression of roughly 5% when parsing a {{$in}}, and is apparent with very large in expressions. The real fix seems like achieving performance parity between the two ways to iterate through a {{BSONObj}}, but I'm not sure how much we want to dive down the rabbit hole in terms of seeing the machine code that's generated by the two constructs.""",Improvement,Internal Code
479783,"""Trying to match a field with another one in an array does not seem to be working.    Also, using $expr to compare a projected field work, but not a new field created with $addFields    Step to reproduce demonstrate the issue.    I'm filling this as a major bug because this looks like important issues in the $expr mechanism to me.""",Bug,Querying
480231,"""Hi :)    I was trying to add document validation to avoid an issue with our software where a document, containing an array of DBRefs was saved back with those DBRefs resolved to their foreign documents. In other words, to validate that the items in this array member are indeed DBRefs.    As DBRef is not available as a $type to check against, I had to get creative and instead opted for this not-exactly-foolprof idea: Either, the member must not exist, or it must be an array of objects which does not have an _id property.    In my testing, it becomes clear that it stops rejecting invalid updates once the array contains one valid member. In essence, the validator seems to say, """"yep, everything okay"""" so long as the validator rule, if used as a query, would yield the document being validated.    I don't see this behavior mentioned anywhere in the docs, so I'm thinking it's a bug. Please see my steps to reproduce for clarification.""",Bug,"Querying,Internal Code,Admin,Internal Client"
480968,"""MongoDB 3.3.9 onwards added a very significant performance regression running large aggregation queries.  Query times now seem to grow quadratically with query size.  My bug was found using the aggregation framework but it may be a more general bug.  I've only tested up to version 3.5.11.    We recently explored upgrading our current mongo at 3.0.11 to 3.4.x.  However we found the execution time of one of queries which averaged a respectable ~660ms had jumped 200x to a staggering ~40 000ms.  I benchmarked against a few versions to narrow it down.  All queries were matched to a single document.    3.0.15  633ms  3.2.16  730ms  3.3.6  687ms  3.3.8  661ms  3.3.9  132 089ms  3.3.10  134 875ms  3.3.11  38 787ms  3.4.0  38 117ms  3.4.4  38 348ms  3.4.7  38 097ms    3.5.11  41 480ms    It appears something changed in 3.3.9 which was then partially improved in 3.3.11. None of the JIRA issues over those versions stood out for me so I assume it’s a side affect or something else.    Our aggregation query does happen to be very long at ~300K lines and is auto generated.  The query is essentially performing a long list of $max operations in the $group stage to compute the union of many hyper-log-log counters.  A similarly long $project stage just renames the <USER> The schema has ~75 counters with ~1K buckets per counter, each bucket having its own field.  I’ve attached a sample generated query and sample collection with a single corresponding document as well as a screenshot of the schema to help visualise the schema.    I ran some benchmarks on 3.4.4 varying both the contents of the document as well as the number of counters queried in a single aggregation.  The former had no affect.  For the latter I  took some sample times.  Note I used a different environment compared to the query used for comparing the versions above so the times won’t match:    counters  ms  ms/counter  1   100   100  2   182   91  3   272   91  4   367   92  5   495   99  6   637   106  7   840   120  8   965   121  9   1231   137  10   1400  140  11   1700   155  17   3933  231  25   8581  343  40   18605  465  50   34863  697  60   47746  796  75   67665  902  90   89121  990  100   145390  1454    Initially query time looks linear but seems to grow quadratically as n becomes larger. I can only assume there’s a linear scan in an inner loop somewhere :)    We currently rely on being able to query all counters within a few seconds at most and this is blocking us from upgrading beyond 3.2.  One temporary workaround would be issuing multiple smaller queries for those counters. However even when doing that the query performance looks too poor for users.  Perhaps there’s a better schema / approach we could take for this type of problem of storing many HLL over time and computing their unions at query-time you could suggest?  I know I’d love a feature to perform parallel operations on arrays in the $group stage? Eg given two arrays [1,4,2] and [2,3,1] compute [2,4,2]?    The ideal outcome would be for performance to be improved to pre 3.3.9 levels.    If you need more information please let me know. Also thanks for regularly publishing docker images, it made comparing versions a breeze.  """,Bug,Aggregation Framework
481652,"""When a change stream is initiated against a sharded cluster, merging the results can be tricky. A change stream will establish a cursor against each shard, and merge the results from those shards in order of 'clusterTime'. If each cursor returns at least one result, the {{_id}} will contain the cluster time, and can be used to merge the results and return the smallest one. However, if at least one shard returns an empty batch, we cannot yet return anything to the client, for fear that it will later return something at a smaller clusterTime. To avoid this issue, we should do the following:  * To both the aggregation response, and the response of each getMore, add a new internal field called something like {{_internalLatestClusterTimeObserved}} to the {{cursor}} section of the response. This will report the highest cluster time that the cursor has observed, even if it didn't match the criteria and thus was not returned.  * This value can be treated as a guarantee from the shard that it will never return data before that cluster time on that cursor.  * The logic merging cursors on mongos can then return any results that have a cluster time smaller than the minimum reported by all cursors.  * The merging logic can then safely run a getMore using a 'read after cluster time' with it's desired time to advance a cursors guaranteed-minimum-optime. However, it still needs to inspect the results of this response, since it's possible the cursor will _still_ return a smaller optime if it exceeded its maxTimeMS.    In order to report this information back on each response, we will modify the capped insert notifier to keep track of the most recent optime observed. One proposal is to pass a callback to the notifier for it to use to decide when to wake the waiting thread. If we take this approach, the callback can save a reference to the OperationContext, and continually advance some sort of {{boost::optional<Timestamp>}} decoration on the OperationContext. The aggregate and getMore commands can then consult whether this is set to decide whether/what to include in this new response field.    Please forgive me if I've used poor terminology, I'm pretty new to this whole 'cluster time' thing.""",Task,"Querying,Sharding"
481675,"""Many of our suites are really just some other suite with a single small change. As an example, I'd like to introduce sharding_jscore_passthrough_opquery which is the same as sharding_jscore_passthrough, except it sets executor.config.shell_options.rpcProtocols to opQueryOnly. Unfortunately this requires duplicating the whole file which almost guarantees that they will fall out of sync in the future.""",New Feature,Test
482155,"""IMHO writing aggregation for simple join operations are big efforts and some syntax improvements will be good. I know JSON has limitations and lots of stuff that I don't know about server core. But let me express my ideas:    For instance basic calculations and string concats:    {code:javascript}  db.collection.find({}, {_id: 1, name: 1, """"nonExistField"""": {$statement: """"1 + 4""""}});  {code}    for just concatanation:    {code:javascript}  db.collection.find({}, {_id: 1, name: 1, """"newField"""": {$concat: """"$firstName + ' ' + $lastName""""}});  {code}    or sum operations:    {code:javascript}  db.collection.find({}, {_id: 1, """"total"""": {$sum: """"$age""""}});  {code}    and for join operations:    this is a sample User collection:    {code:javascript}  {    """"_id"""" : ObjectId(""""592e75ebe076c31cee205b72""""),    """"name"""": """"Test User"""",    """"tags"""": [      DBRef(""""tag"""", ObjectId(""""592e75ebe076c31cee205b77"""")),      DBRef(""""tag"""", ObjectId(""""592e75ebe076c31cee205b78""""))    ]  }  {code}    {code:javascript}  {    'tags.$join': {'$elemMatch': {'name': {'$regex': '^test', '$options': 'i'}}}  }  {code}    Server can convert my query to an aggregation query but as a simple user It's a big effort.""",New Feature,"Querying,Aggregation Framework"
482310,"""{{WiredTigerRecordStore::updateRecord}} has an unnecessary {{WT_CURSOR.set_key}}.    The key doesn't need to be re-set after the search, and, in fact, setting it here may slow down the subsequent {{WT_CURSOR.insert}} operation.    The (untested) change I'm suggesting:  {noformat}  *** wiredtiger_record_store.cpp.orig 2017-05-16 17:25:39.768013595 -0400  --- wiredtiger_record_store.cpp.new   2017-05-16 17:25:45.920785403 -0400  ***************  *** 1434,1440 ****            return {ErrorCodes::IllegalOperation, """"Cannot change the size of a doc  ument in the oplog""""};        }          -     c->set_key(c, _makeKey(id));        WiredTigerItem value(data, len);        c->set_value(c, value.Get());        ret = WT_OP_CHECK(c->insert(c));  --- 1434,1439 ----  {noformat}""",Improvement,Storage
482475,"""Currently mongo shell does not gossip clusterTime. I needs to add it to LogicalTimeMetadata as a field in a command that is up/down converted.""",Task,Sharding
482486,"""We appear to be hitting an issue where MongoDB is choosing to use a less efficient index when performing a regex query, that it actually could.    Our data has the following structure  {noformat}  {   title : """"String title"""",   cms_title_sort : """"string title"""",   tags : [""""foo"""", """"bar"""", """"baz""""],   rank : 1  }  {noformat}  With two indexes:  {noformat}  ensureIndex({ cms_title_sort : 1 })  ensureIndex({ tags : 1, rank : 1, cms_title_sort : 1 })  {noformat}  If we perform   {noformat}find({ cms_title_sort : /keyword/, tags : { $in : [""""foo""""] } }){noformat}  sometimes it uses the {{cms_title_sort}} index, other times it uses the tags index. The table contains between 15k and 100k records, so not the largest of tables. When it uses the {{cms_title_sort}} index, it's very fast and finishes in <100ms. When it uses the tags index, it can take up to a second. The reason for this is that it first uses the tags index (which have marginal cardinality), then it performs the regex component on the documents themselves in a FETCH step. When it uses the {{cms_title_sort}} index, it performs the regex in memory, and then does the tag portion on the documents. Because the regex has a very high cardinality the result set is significantly smaller, and it goes quickly. In general I would think that most regex queries have high cardinality, so then, should that not instruct mongo to possible lean towards those indices?    We have tried a few different index formats. According to the docs, it would seem that an index which looks like {{ensureIndex(\{ tags : 1, cms_title_sort : 1 \})}} should allow for a query to be entirely performed in memory. Even in that case, the system still performs the cms_title_sort component in a FETCH step, rather than in the index step where it checks the array. In the docs for indexed arrays, it mentions some other caveats, if they don't work with regex it may be best to call that as well.    I have created a test script which shows the issue. Consistently with this test data the {{cms_title_sort}} index performs the most optimally for all 3 queries, yet mongo sometimes chooses the tags index. In this example it's not as big of a problem, both queries are fairly fast, but as the data gets larger and more documents, then things can slow into the seconds. From my tests the index chosen changes depending on how many records you put in the db and the specific keys you attempt to search on.    Lastly, we are using 3.2.6, this behavior may have changed in newer versions. If it is better in newer versions that would be great to know!""",Bug,Querying
483082,"""If you're considering zlib for compression (with existing snappy), you may be better off considering lz4. It's substantially faster at compression and gets very close to snappy in terms of overall ratios. In some basic tests we've done for in-memory compression, lz4 is the only one of the 3 that appears to be capable of feeding data at line speed without recognizable latency.    We get 9:1 compression out of snappy and about 7.5:1 out of lz4. However, our systems compress with lz4 at least twice as fast, in memory.    I'd be happy to run some additional tests... and would love to see lz4 as a compression option everywhere. (lz4 file compression request coming soon)    Relates to: SERVER-3018 and SERVER-25620 """,New Feature,Networking
483555,"""Hi,    Today my app crashed suddenly when viewing the log file i found that it was unable to authenticate the user.     I've tried to log into <USER>shell with the admin user but i get the following message :  Error: 18 { code: 18, ok: 0.0, errmsg: """"auth fails"""" } at src/mongo/shell/db.js:228    Then i've updated the mongo config file and commented out """"auth=true"""" then restart mongodb service. I was able to login to mongo shell as admin and managed to change the password. Back to the old config (with auth=true) and i still get a """"auth fails"""".    Can anyone help me ?    MongoDB shell version: 2.4.10  db version v2.4.10""",Bug,Security
483767,"""Materialized views would allow developers that use MongoDB to move data denormalization from the application layer to the database layer. While the process of materializing the view will require additional writes and slow the effective throughput of the database per write request, if used properly it should be a net wash in terms of real world performance as applications will not be sending additional requests to write the same denormalized data.    h3. Proposed Command    A simple expansion of the command for creating a view. A new materialized: <bool> option which defaults to false would allow the view to be created as a materialized view:    {code}  db.runCommand( { create: <view>, viewOn: <source>, pipeline: <pipeline>, materialized: <bool> } )  {code}    h3. Proposed Behavior    * Materialized views exist on disk but cannot be written to by the user. Internally a materialized view is a normal collection that is maintained by MongoDB using a defined pipeline rather than maintained by writes from an application.  * Updates to the materialized view are not atomic. In other words, queries against the collection(s) that back(s) a materialized view may return results that are inconsistent with the results of the materialized view.  * Materialized views do not use the indexes or sharding of the underlying collection(s)  * Materialized views can be indexed and/or sharded like a normal collection  * The aggregation pipline for a materialized view must emit an _id  * Materialized views write the result of the pipeline as an upsert against _id  * When updating a document in the underlying collection the pipeline will only be run against the document being updated. To ensure correct results materialized views should only use operators that can properly work against a single document. Materialized views typically should not use pipeline stages that operate against a set of documents such as:     * $limit     * $skip     * $group     * $sample     * $sort     * $out     * $bucket     * $bucketAuto     * $sortByCount     * $count   * The database should not enforce any limitation on the above commands because they may still be useful in connection with other stages such as $unwind.   * Materialized views may use $lookup, however, updates against the joined collection will not update associated documents in the materialized view. Applications may force an update of associated materialized documents by forcing an update of the appropriate backing documents in the collection that the materialized view is based on.    h3. Other Considerations   * It seems that the ability to update the definition of a materialized view would be extremely desirable, however, this could also be a monumentally expensive operation during which data on the collection would be in an inconsistent state. Ensuring consistency doubles the space requirements. If updates to the definition are allowed, it might make sense to allow the user to choose between consistency or an in-place strategy (inPlace: <bool>, default false).   * I'm not sure how to handle the $match stage, especially in the case where a record matched, and so was materialized, and later does not match, and should be removed. The simplistic implementation could simply dictate that match is bad, and you should emit an empty record instead, but that seems messy. It may be that materialized views require an extra internal collection of some sort to store the relationships between the input document(s) and the output record(s). Such a collection might conceivably also be useful for allowing a strong materialization of records pulled in from $lookup.""",New Feature,Usability
483812,"""I'm told by [~<USER> that it is sometimes useful to be able to correlate a process via its command line arguments to the test that started the process. Currently we achieve this by passing a """"name"""" to ReplicaSetTest and ShardingTest. This has a few downsides relative to having resmoke inject the test name:  * It is optional so not every test passes it in  * The name may not exactly match the test name  * There is nothing to prevent two tests from passing the same name  ** This is actually the default state since most tests start as a copy of another  * Updating the name manually when writing or renaming tests is busywork    Injecting the test name can be done in a few ways:  * Adding a {{\-\-comment}} argument to mongo binaries that is ignored and can be repeated  * Setting a default name that ReplSetTest and ShardingTest use by default if none is passed in  * Adding the test name to the dbpath prefix passed to tests""",Improvement,Test
484160,"""Hello,    First of all I'm not looking for support on this, I'm not even sure if it's considered a bug but it seems like it's a bit dangerous.    I'm working on adding a new feature to mongodb and have been modifying the code for mongo S (+not mongod/config server+, which is the one I'm able to crash remotely).    Essentially I modified the mongo/s/shard_key_pattern.cpp to also accept """"2dsphere"""" as a valid sharding key and then from the mongo shell called shardCollection with a 2dsphere key (index already configured). The config server promptly crashed with a core dump and the following backtrace:    {code}  2016-12-01T19:16:15.150+0000 I SHARDING [Balancer] ChunkManager loading chunks for wells_US.points sequenceNumber: 2 based on: (empty)  2016-12-01T19:16:15.151+0000 I SHARDING [Balancer] ChunkManager load took 0 ms and found version 1|0||584074beec62237f36f52c7e  2016-12-01T19:16:15.151+0000 I -        [Balancer] Invariant failure !ci.key().isEmpty() src/mongo/s/config.cpp 296  2016-12-01T19:16:15.151+0000 I -        [Balancer]     ***aborting after invariant() failure      2016-12-01T19:16:15.165+0000 F -        [Balancer] Got signal: 6 (Aborted).     0x56062610df71 0x56062610d069 0x56062610d54d 0x7fc771ae93e0 0x7fc771744428 0x7fc77174602a 0x5606253db006 0x560625fbde17 0x560625ff44de 0x560625c55903 0x560625c57310 0x560625c47045 0x560625c4dbe9 0x7fc7722ccc80 0x7fc771adf70a 0x7fc77181582d  ----- BEGIN BACKTRACE -----  {""""backtrace"""":[{""""b"""":""""560624C42000"""",""""o"""":""""14CBF71"""",""""s"""":""""_ZN5mongo15printStackTraceERSo""""},{""""b"""":""""560624C42000"""",""""o"""":""""14CB069""""},{""""b"""":""""560624C42000"""",""""o"""":""""14CB54D""""},{""""b"""":""""7FC771AD8000"""",""""o"""":""""113E0""""},{""""b"""":""""7FC77170F000"""",""""o"""":""""35428"""",""""s"""":""""gsignal""""},{""""b"""":""""7FC77170F000"""",""""o"""":""""3702A"""",""""s"""":""""abort""""},{""""b"""":""""560624C42000"""",""""o"""":""""799006"""",""""s"""":""""_ZN5mongo17invariantOKFailedEPKcRKNS_6StatusES1_j""""},{""""b"""":""""560624C42000"""",""""o"""":""""137BE17"""",""""s"""":""""_ZN5mongo8DBConfig15getChunkManagerEPNS_16OperationContextERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb""""},{""""b"""":""""560624C42000"""",""""o"""":""""13B24DE"""",""""s"""":""""_ZN5mongo18ScopedChunkManager11getExistingEPNS_16OperationContextERKNS_15NamespaceStringE""""},{""""b"""":""""560624C42000"""",""""o"""":""""1013903"""",""""s"""":""""_ZN5mongo32BalancerChunkSelectionPolicyImpl32_getSplitCandidatesForCollectionEPNS_16OperationContextERKNS_15NamespaceStringERKSt6vectorINS_17ClusterStatistics15ShardStatisticsESaIS8_EE""""},{""""b"""":""""560624C42000"""",""""o"""":""""1015310"""",""""s"""":""""_ZN5mongo32BalancerChunkSelectionPolicyImpl19selectChunksToSplitEPNS_16OperationContextE""""},{""""b"""":""""560624C42000"""",""""o"""":""""1005045"""",""""s"""":""""_ZN5mongo8Balancer17_enforceTagRangesEPNS_16OperationContextE""""},{""""b"""":""""560624C42000"""",""""o"""":""""100BBE9"""",""""s"""":""""_ZN5mongo8Balancer11_mainThreadEv""""},{""""b"""":""""7FC772214000"""",""""o"""":""""B8C80""""},{""""b"""":""""7FC771AD8000"""",""""o"""":""""770A""""},{""""b"""":""""7FC77170F000"""",""""o"""":""""10682D"""",""""s"""":""""clone""""}],""""processInfo"""":{ """"mongodbVersion"""" : """"3.4.0-rc5"""", """"gitVersion"""" : """"7df8fe1099135d137516f1670d2a0091ace63ca0"""", """"compiledModules"""" : [], """"uname"""" : { """"sysname"""" : """"Linux"""", """"release"""" : """"4.4.0-47-generic"""", """"version"""" : """"#68-Ubuntu SMP Wed Oct 26 19:39:52 UTC 2016"""", """"machine"""" : """"x86_64"""" }, """"somap"""" : [ { """"b"""" : """"560624C42000"""", """"elfType"""" : 3, """"buildId"""" : """"150938CE464DB3BAEAD640507516D58AEAE68656"""" }, { """"b"""" : """"7FFE89DFD000"""", """"elfType"""" : 3, """"buildId"""" : """"263FD15D8149B82F2DB14A1E6C3351C2D9F4852C"""" }, { """"b"""" : """"7FC77279A000"""", """"path"""" : """"/lib/x86_64-linux-gnu/librt.so.1"""", """"elfType"""" : 3, """"buildId"""" : """"4ABAAF20AC90D3E282D770F6A0C58A80BF16145A"""" }, { """"b"""" : """"7FC772596000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libdl.so.2"""", """"elfType"""" : 3, """"buildId"""" : """"09758D5225753BBF1FE3F66AADCCB6EC6F6E244B"""" }, { """"b"""" : """"7FC772214000"""", """"path"""" : """"/usr/lib/x86_64-linux-gnu/libstdc++.so.6"""", """"elfType"""" : 3, """"buildId"""" : """"144E588F94CAFAFDBD0BD1499C74190F678DAD88"""" }, { """"b"""" : """"7FC771F0B000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libm.so.6"""", """"elfType"""" : 3, """"buildId"""" : """"9247F19167971267B6FADF4BA633290188A5483B"""" }, { """"b"""" : """"7FC771CF5000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libgcc_s.so.1"""", """"elfType"""" : 3, """"buildId"""" : """"68220AE2C65D65C1B6AAA12FA6765A6EC2F5F434"""" }, { """"b"""" : """"7FC771AD8000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libpthread.so.0"""", """"elfType"""" : 3, """"buildId"""" : """"3B58B373BD25A13045DCD3FCE540203DE330AC8E"""" }, { """"b"""" : """"7FC77170F000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libc.so.6"""", """"elfType"""" : 3, """"buildId"""" : """"A594A9C73A6067AB00A0F8DB78D665BE147ACDC1"""" }, { """"b"""" : """"7FC7729A2000"""", """"path"""" : """"/lib64/ld-linux-x86-64.so.2"""", """"elfType"""" : 3, """"buildId"""" : """"F6DCBEE8DCAAE97C8BF7E73B56514E67118E6118"""" } ] }}   mongod(_ZN5mongo15printStackTraceERSo+0x41) [0x56062610df71]   mongod(+0x14CB069) [0x56062610d069]   mongod(+0x14CB54D) [0x56062610d54d]   libpthread.so.0(+0x113E0) [0x7fc771ae93e0]   libc.so.6(gsignal+0x38) [0x7fc771744428]   libc.so.6(abort+0x16A) [0x7fc77174602a]   mongod(_ZN5mongo17invariantOKFailedEPKcRKNS_6StatusES1_j+0x0) [0x5606253db006]   mongod(_ZN5mongo8DBConfig15getChunkManagerEPNS_16OperationContextERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbb+0x1267) [0x560625fbde17]   mongod(_ZN5mongo18ScopedChunkManager11getExistingEPNS_16OperationContextERKNS_15NamespaceStringE+0xAE) [0x560625ff44de]   mongod(_ZN5mongo32BalancerChunkSelectionPolicyImpl32_getSplitCandidatesForCollectionEPNS_16OperationContextERKNS_15NamespaceStringERKSt6vectorINS_17ClusterStatistics15ShardStatisticsESaIS8_EE+0x53) [0x560625c55903]   mongod(_ZN5mongo32BalancerChunkSelectionPolicyImpl19selectChunksToSplitEPNS_16OperationContextE+0x270) [0x560625c57310]   mongod(_ZN5mongo8Balancer17_enforceTagRangesEPNS_16OperationContextE+0x55) [0x560625c47045]   mongod(_ZN5mongo8Balancer11_mainThreadEv+0x11C9) [0x560625c4dbe9]   libstdc++.so.6(+0xB8C80) [0x7fc7722ccc80]   libpthread.so.0(+0x770A) [0x7fc771adf70a]   libc.so.6(clone+0x6D) [0x7fc77181582d]  -----  END BACKTRACE  -----    {code}    Like I said, I'm not sure if this is considered a problem or not.  I can give more details if this looks relevant to the stability/security of the config server.""",Improvement,Sharding
484384,"""We're doing aggregations on tens of millions of documents, every like 30 minutes and what we're facing is most of the time, aggregations fail since one of the returned """"documents"""" exceed the strange 16Mb limit.    I know that MongoDB's internal structure, the way it copies and moves documents into RAM and out of it, will be heavily influenced if this limit was to be increased for """"*Stored Documents*"""", but aggregation results vary from this in a profound way, Most of the times we don't want to store them, we just want to use them.    My usecase here is simple, I want to $group documents by an Id, and then $push ALL unprocessed documents that point to that ID into a sub-field of that document, get them an do some logic with them, and finally, bulk updating them to set them to processed. (We don't care about the RAM since we've thrown ~200GBs of RAM for this)    It's really a headache that the 16MB limit is even in place for aggregation results ! Why is that required ? Is it just a code problem or is it a design decision ?     (As a solution, we're currently evaluating ElasticSearch for what we do, but we were really happy with MongoDB other than the aggregation limits)""",Improvement,Aggregation Framework
484581,"""Hey everyone, we have a production replica set consisting of three nodes that has been running well for two years. It looks like this from a configuration standpoint:    - 3 m4.2xlarge instances (8 cores)  - xfs filesystem for data drives  - ebs volumes (with 8000 provisioned iops -- max for instance type)  - wired tiger storage engine  - ssl, auth, etc    The performance is great but we want to scale up our nodes to handle a potential spike in usage over the next two weeks. As we are not particularly i/o bound given our usage of MongoDB and appear to be largely cpu bound on these boxes (from what I can tell) we have transitioned these nodes from m4.2xlarge (8 cores) to m4.4xlarge (16 cores).    To my surprise it _appears_ as though mongod is only using the first 8 (0-7) of the 16 cores available on this machine. Now, I realize that:     - In going from 8 to 16 cores we may now have two physical cpus backing our instance   - {{taskset}} and {{cpuset}} can be used to set core/processor affinity and I do not believe they are in use (we are using the init script from the Amazon linux package)   - numactl should specify that memory usage be interleaved instead of preferring a node or physical cpu (again, confirmed via package init script)   - Using `htop` as a view onto cpu usage on virtualized hardware is a potentially flawed metric for various reasons    So I have the question: *Why is mongod appearing to use only 8 of 16 cores available on these boxen?*    It's possible that the linux scheduler doesn't bother scheduling tasks onto the second physical cpu until there are a greater number of running threads so as to take advantage of CPU caches? Right now there is not much load to speak of so that is my current running theory.    Having never run mognod on a machine having multiple physical cpus in production before I'm only guessing as to what the issue may be. Any clues as to what I might be seeing 10geneers?""",Question,Concurrency
484930,"""We have upgraded a production replica set from 3.0.12 to 3.2.9, secondary members first, leaving the two last members (primary and its datacenter active fallback) to the end of the process.    The other day we've changed priorities between the afore mentioned two remaining 3.0.12 members, upgraded one to 3.2.9 and then set it back as a primary node, leaving us with one secondary member left as 3.0.12    Once that newly upgraded 3.2.9 host (172.16.144.3) became primary again (after DB restart, to have the new binaries apply),   Application using this replica set had their performance and response times were deteriorating rapidly.    Node log file was showing """"cursorExhausted:1"""" for almost every query logged,  which wasn't occurring in any of the other members.    When examined server status for cursor metrics, number of timed out cursors was rising gradually while number of pinned cursors was around 100 and number of no timeout cursors was around 70.    One thing that was changed apart from upgrading this primary node to 3.2.9 was to explicitly set its maximum configured WT cache size to 36Gb (based on the new 3.2 wiredTiger rule of thumb saying it's 60% of free physical RAM minus one Gb)     We suspected that the fact that the recently upgraded 3.2.9 node was just restarted, thus having its cache empty - being completely """"cold"""" when massive application traffic started performing reads and writes, was the root cause for these cursor exhaustion and performance drop - so we shortly after fallen back to the left 3.0.12 secondary (172.16.23.3) to become primary - which has resulted an immediate significant performance improvement and a stop to cursor exhaustion.     Please note that on the 3.0.12 we had no explicit configuration of cacheSizeGB but left it to the default behaviour of wiredTiger 3.2.9.    After primary was set on the last remaining 3.0.12, we decided to have leave the 3.2.9 node which failed to take the load as primary (172.16.144.3) to """"pre heat"""" its cache for on query traffic (about 600 read statements per second) and gave it another go as primary the day after (without restarting it, as it was already set on version 3.2.9 - yet the same behaviour of cursor exhaustion and massive app performance drop occurred again forcing yet another fallback to the 3.0.12 node to become primary again - which again mitigated things back to what they were before the change.    Enclosed please find are log files from both the 3.2.9 and 3.0.12 nodes, notice how same queries generate different outcomes in terms of cursor exhaustion even though they share the same execution plans, one extremely popular query is the one issuing find on lists.items-Posts in its different permutations:    329_member.log.tar.gz: for host 172.16.144.3, I've omitted the biggest log (1.5G - which covered Sep 29 09:30 to 12:43) as other logs also contain the symptoms reported here.    3012_member.log.tar.gz: for host 172.16.23.3    329_member_server_status.out  3012_member_server_status.out    We have other applications based on other replica sets which are fully upgraded to 3.2.9 (including primary of course)  which doesn't display this behaviour - so this could very much have to do with the way the application driver is setup or simply on how its written (or a mix of both...)    Kindly try and assist in analysing how come this behaviour occurs and recommend of methods to try and overcome it.  This is quite urgent for us as we wish to have it completed by the end of next week.    Many thanks in advance,   Avi K, DBA  WiX.COM""",Bug,Concurrency
485577,"""I've been testing the speed of chunk migrations in an all-on-one-server test cluster. Even when the chunks being migrated are empty (i.e. the chunk move takes only ~0.1 secs) the entire cycle run by the balancer takes a lot longer.     || version || balance round time ||   | 3.0.8 | ~4.5 secs |   | v3.3.11-30-gc96009e | ~ 9 secs |     From someone's else case with v3.2 and different servers / network to my test I heard of a ~6 second cycle. Not sure if that was a replica set config db or the older SCCC-style one.    Can the balancer be changed so that the balance round will do multiple chunks of each collection so long as they finish quickly? E.g. balance round identifies candidate chunks for migrations, and keeps on doing chunk moves for them serially until a, say, 10 sec window completes.    At any rate if data has been completely deleted for a big fraction of chunk ranges before adding a new shard, it would be good if those chunks moves happened a lot more quickly.""",Bug,Sharding
485596,"""As a DBA I would like to really troubleshoot performance. A great source of information would normally be found in Index usage. However, from what I can work out (and the documentation states), a restart of the Mongod process resets this information.   I would like to know if this can be written out somewhere instead. This is <USER>information that is useless if it gets reset on restart.   For example:  I deploy new code and my performance falls off a cliff.   It could be that an index is causing the problem because of too many inserts being added and maybe disabling this index during load is the solution. However, the production server crashed and I can no longer see the stats of usage.    It would be very helpful to add this to rs.status type view so that I can ses the index across all replica's""",Improvement,"Diagnostics,Indexing,Logging,Aggregation Framework"
485784,"""Our team is working on a migration from a clustered replica set mongo on AWS EC2 to a replica set cluster on Google's Compute Engine.     We were able to add one of the new Google mongo instances as a member to the AWS cluster with a priority 0, however it appears that there is an issue with authentication which won't allow the new mongo to sync.    Google Compute Instance:   {code}  INFRA-GENERAL-00:SECONDARY> rs.status()  {   """"set"""" : """"INFRA-GENERAL-00"""",   """"date"""" : ISODate(""""2016-08-03T00:22:37.275Z""""),   """"myState"""" : 2,   """"term"""" : NumberLong(2),   """"heartbeatIntervalMillis"""" : NumberLong(2000),   """"members"""" : [    {     """"_id"""" : 0,     """"name"""" : """"****:27017"""",     """"health"""" : 0,     """"state"""" : 8,     """"stateStr"""" : """"(not reachable/healthy)"""",     """"uptime"""" : 0,     """"optime"""" : {      """"ts"""" : Timestamp(0, 0),      """"t"""" : NumberLong(-1)     },     """"optimeDate"""" : ISODate(""""1970-01-01T00:00:00Z""""),     """"lastHeartbeat"""" : ISODate(""""2016-08-03T00:22:35.205Z""""),     """"lastHeartbeatRecv"""" : ISODate(""""1970-01-01T00:00:00Z""""),     """"pingMs"""" : NumberLong(0),     """"lastHeartbeatMessage"""" : """"exception: field not found, expected type 16"""",     """"configVersion"""" : -1    },    {     """"_id"""" : 1,     """"name"""" : """"****:27017"""",     """"health"""" : 0,     """"state"""" : 8,     """"stateStr"""" : """"(not reachable/healthy)"""",     """"uptime"""" : 0,     """"optime"""" : {      """"ts"""" : Timestamp(0, 0),      """"t"""" : NumberLong(-1)     },     """"optimeDate"""" : ISODate(""""1970-01-01T00:00:00Z""""),     """"lastHeartbeat"""" : ISODate(""""2016-08-03T00:22:34.728Z""""),     """"lastHeartbeatRecv"""" : ISODate(""""1970-01-01T00:00:00Z""""),     """"pingMs"""" : NumberLong(0),     """"lastHeartbeatMessage"""" : """"exception: field not found, expected type 16"""",     """"configVersion"""" : -1    },    {     """"_id"""" : 2,     """"name"""" : """"****:27017"""",     """"health"""" : 0,     """"state"""" : 8,     """"stateStr"""" : """"(not reachable/healthy)"""",     """"uptime"""" : 0,     """"optime"""" : {      """"ts"""" : Timestamp(0, 0),      """"t"""" : NumberLong(-1)     },     """"optimeDate"""" : ISODate(""""1970-01-01T00:00:00Z""""),     """"lastHeartbeat"""" : ISODate(""""2016-08-03T00:22:35.248Z""""),     """"lastHeartbeatRecv"""" : ISODate(""""1970-01-01T00:00:00Z""""),     """"pingMs"""" : NumberLong(0),     """"lastHeartbeatMessage"""" : """"exception: field not found, expected type 16"""",     """"configVersion"""" : -1    },    {     """"_id"""" : 3,     """"name"""" : """"****:27017"""",     """"health"""" : 1,     """"state"""" : 2,     """"stateStr"""" : """"SECONDARY"""",     """"uptime"""" : 338,     """"optime"""" : {      """"ts"""" : Timestamp(1470169610, 1),      """"t"""" : NumberLong(2)     },     """"optimeDate"""" : ISODate(""""2016-08-02T20:26:50Z""""),     """"configVersion"""" : 175485,     """"self"""" : true    }   ],   """"ok"""" : 1  }  {code}    AWS Primary Instance:  {code}  INFRA-GENERAL-00:PRIMARY> rs.status()  {   """"set"""" : """"INFRA-GENERAL-00"""",   """"date"""" : ISODate(""""2016-08-03T16:00:59Z""""),   """"myState"""" : 1,   """"members"""" : [    {     """"_id"""" : 0,     """"name"""" : """"****:27017"""",     """"health"""" : 1,     """"state"""" : 1,     """"stateStr"""" : """"PRIMARY"""",     """"uptime"""" : 690001,     """"optime"""" : Timestamp(1470240059, 4),     """"optimeDate"""" : ISODate(""""2016-08-03T16:00:59Z""""),     """"electionTime"""" : Timestamp(1470167971, 1),     """"electionDate"""" : ISODate(""""2016-08-02T19:59:31Z""""),     """"self"""" : true    },    {     """"_id"""" : 1,     """"name"""" : """"****:27017"""",     """"health"""" : 1,     """"state"""" : 2,     """"stateStr"""" : """"SECONDARY"""",     """"uptime"""" : 72094,     """"optime"""" : Timestamp(1470240058, 29),     """"optimeDate"""" : ISODate(""""2016-08-03T16:00:58Z""""),     """"lastHeartbeat"""" : ISODate(""""2016-08-03T16:00:58Z""""),     """"lastHeartbeatRecv"""" : ISODate(""""2016-08-03T16:00:59Z""""),     """"pingMs"""" : 1,     """"syncingTo"""" : """"mongorep01.restdev.com:27017""""    },    {     """"_id"""" : 2,     """"name"""" : """"****:27017"""",     """"health"""" : 1,     """"state"""" : 2,     """"stateStr"""" : """"SECONDARY"""",     """"uptime"""" : 72094,     """"optime"""" : Timestamp(1470240057, 215),     """"optimeDate"""" : ISODate(""""2016-08-03T16:00:57Z""""),     """"lastHeartbeat"""" : ISODate(""""2016-08-03T16:00:57Z""""),     """"lastHeartbeatRecv"""" : ISODate(""""2016-08-03T16:00:58Z""""),     """"pingMs"""" : 0,     """"syncingTo"""" : """"mongorep01.restdev.com:27017""""    },    {     """"_id"""" : 3,     """"name"""" : """"****:27017"""",     """"health"""" : 1,     """"state"""" : 2,     """"stateStr"""" : """"SECONDARY"""",     """"uptime"""" : 54470,     """"optime"""" : Timestamp(1470169610, 1),     """"optimeDate"""" : ISODate(""""2016-08-02T20:26:50Z""""),     """"lastHeartbeat"""" : ISODate(""""2016-08-03T16:00:58Z""""),     """"lastHeartbeatRecv"""" : ISODate(""""1970-01-01T00:00:00Z""""),     """"pingMs"""" : 37    }   ],   """"ok"""" : 1  }  {code}    While checking the Google Compute Instance's mongo log, I see the following:  {code}  2016-08-03T00:45:31.801+0000 I REPL     [ReplicationExecutor] waiting for 6 pings from other members before syncing  2016-08-03T00:45:32.251+0000 I NETWORK  [conn272] end connection ***:60176 (4 connections now open)  2016-08-03T00:45:32.288+0000 I NETWORK  [initandlisten] connection accepted from ***:60179 #275 (5 connections now open)  2016-08-03T00:45:32.325+0000 I ACCESS   [conn275]  authenticate db: local { authenticate: 1, nonce: """"xxx"""", user: """"__system"""", key: """"xxx"""" }  2016-08-03T00:45:32.809+0000 I REPL     [ReplicationExecutor] Error in heartbeat request to ***:27017; Location13111: exception: field not found, expected type 16  2016-08-03T00:45:32.850+0000 I REPL     [ReplicationExecutor] Error in heartbeat request to ***:27017; Location13111: exception: field not found, expected type 16  2016-08-03T00:45:32.890+0000 I REPL     [ReplicationExecutor] Error in heartbeat request to ***:27017; Location13111: exception: field not found, expected type 16  {code}    The Primary and other AWS instances seem to require authentication to be done through the 'admin' db, not 'local'. I'm assuming this is where the problem lies.    Is there a way to have the new replica set member authenticate against admin, not local? """,Question,"Replication,Admin"
486078,"""Sometimes, with no particular explicit reason, MongoDB gives me this error:    {noformat}  [initandlisten] Detected data files in /data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.  [initandlisten] wiredtiger_open config: create,cache_size=1G,session_max=20000,eviction=(threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),  [initandlisten] Assertion: 28595:-31803: WT_NOTFOUND: item not found  [initandlisten] exception in initAndListen: 28595 -31803: WT_NOTFOUND: item not found, terminating  [initandlisten] dbexit:  rc: 100  {noformat}    After restarting the associated Docker container with `Ctrl+C` or with `docker stop`.    The container has a volume to persist `/data/db` directory on the host.    Once again, *most of the times* (that is 99%), stopping and restarting the container works perfectly and MongoDB has no problem with that. But *sometimes*, I'm not able to restart the container because of the error shown above.    When this happens, the only solution that has """"worked"""" so far has been to empty the `data/db` directory on the host, and here we start with an empty (but working) db.    I have to say that informations about `WT_NOTFOUND` error on the internet is quite sparse, so no amount of digging MongoDB issues has solved this problem so far.""",Bug,Storage
486275,"""We had a replicaset with this configuration (pseudocode):    {code:json}  {  mongo1: { priority: 3, votes: 1, role: PRIMARY, mongo: 3.0.2, storage: MMAPv1 },  mongo2: { priority: 0.5, votes: 1, role: SECONDARY, mongo: 3.0.2, storage: MMAPv1 },  mongo3: { priority: 0.5, votes: 1, role: SECONDARY, mongo: 3.0.2, storage: MMAPV1 },  mongo4: { priority: 0, votes: 1, role: SECONDARY, mongo: 3.2.7, storage: WiredTiger },  mongo5: { priority: 0, votes: 1, role: SECONDARY, mongo: 3.2.7, storage: WiredTiger },  mongo6: { priority: 0, votes: 0, role: SECONDARY, hidden: true, mongo: 3.2.7, storage: WiredTiger }  }  {code}    The plan was to switch primary to mongo4 (3.2.7 + WT), so we could upgrade mongo{1,2,3}, but I managed to crash mongo{4,5,6} with this, on mongo1:    {code: javascript}  conf = {     members: [        { host: mongo1..., priority: 3, votes 1},        { host: mongo2..., priority: 0.5, votes 1},        { host: mongo3..., priority: 0.5, votes 0},  // <-- culprit?        { host: mongo4..., priority: 5, votes 1},        { host: mongo5..., priority: 0.8, votes 1},        { host: mongo6..., priority: 0.8, votes 1} // was hidden before  };  rs.reconfig(conf);  {code}    In effect, on mongo{4,5,6} mongod crashed, leaving mongo{1,2,3} in SECONDARY state without being able to elect a PRIMARY.    In logs we found this:  {code}  2016-07-04T07:05:31.842+0000 W REPL     [replExecDBWorker-1] Not persisting new configuration in heartbeat response to disk because it is invalid: BadValue: priority must be 0 when non-voting (votes:0)  2016-07-04T07:05:31.842+0000 E REPL     [ReplicationExecutor] Could not validate configuration received from remote node; Removing self until an acceptable configuration arrives; BadValue: priority must be 0 when non-voting (votes:0)  2016-07-04T07:05:31.842+0000 I REPL     [ReplicationExecutor] New replica set config in use: { _id: """"repl1"""", version: 106772, members: [ { _id: 23, host: """"mongo1:27017"""", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 0.8, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 24, host: """"mongo2:27017"""", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 0.4, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 25, host: """"mongo3:27017"""", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 0.3, tags: {}, slaveDelay: 0, votes: 0 }, { _id: 26, host: """"mongo4:27017"""", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 3.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 27, host: """"mongo5:27017"""", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 0.6, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 28, host: """"mongo6:27017"""", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 0.5, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 } } }  {code}    And then:    {code}  2016-07-04T07:05:31.865+0000 I -        [ReplicationExecutor] Invariant failure i < _members.size() src/mongo/db/repl/replica_set_config.cpp 560  {code}    I'm sure it was a mistake on my end to set priority > 0 and votes = 0 on mongo3, but the way 3.2 + WT nodes reacted was certainly not nice.    Also, please suggest how to perform this switch and upgrade in least dangerous fashion.""",Bug,Replication
486334,"""Hello All,    *The setup*  I am running a replica set in production without sharding. All nodes are running latest stable mongo 2.6 except one hidden node which has mongo 3.0 with MMAPv1.    *The data*  I have around 4TB worth of data on each node (MMAPv1), with close to 7000 databases.    *The plan*  I decided to upgrade to 3.2 and as an intermediate step, I have to upgrade to 3.0 first. Initially I had used wiredTiger with this node, but encountered a problem when I sent prod traffic to that node. Here's the full description on the JIRA issue - SERVER-24514. For not being blocked on the issue, I decided to go ahead with 3.0.12 with MMAPv1 instead of wiredTiger. So to start with that, I added the aforementioned hidden member to the existing replica set. I started sending prod like read query traffic to this node to check if it will be able to withstand that much load. I did this for over a week.    The plan was to roll out 3.0 on all secondaries if latencies and <USER>are close to prod like pattern.    *The observation*  It was observed that the node couldn't entertain the read traffic at a consistent rate. It can be seen that the node entertains ~1k queries per seconds, shoots to ~3.5k qps for a very brief moment of time and drops back to ~1k qps. (Please check Operations Per Second graph). This particular pattern is not observed when same traffic is sent to 2.6.x nodes. Those nodes can entertain same traffic at ~4k qps consistently.  In the process of understanding what exactly is happening, I ran db.currentOp()  on that node. I didn't find anything in particular. But some queries took ~200ms to return. For those queries """"timeAcquiringMicros"""" is ~198ms. According to the docs it is the """"cumulative time in microseconds that the operation had to wait to acquire the locks"""". I would appreciate any help here.""",Bug,Performance
486407,"""If I'm grouping by something, unless I explicitly specify an accumulator for every field, that field will be lost. The only work around that exists right now is to re-query for the document by its _id and fill in the <USER>    There should be some way to specify a $default accumulator that's performed on any unspecified field. For example:    { $group: { _id: '$_id', number: { $sum: 'number'},  $default: {""""defaulted$property"""": {$first:$property}}} }    The above would group by _id, accumulate the number property as a sum, and for every other property it would simply add in the first value in that group for that property as the name """"defaulted""""+propertyName""",New Feature,Aggregation Framework
486438,"""This is a new issue with a deployment created with Cloud Manager.  The server was provisioned on AWS through CM's provisioning tool.  It was added to a replica set as secondary so that the old primary could subsequently be shut down.  I've noticed that once every ~7 seconds, mongod will consume all CPU for about ~10 seconds.  I will attach logs of mongostat, iostat & {{db.serverStatus()}} during and after a spike.""",Bug,Admin
486915,"""The planner now uses ErrorCodes::BadValue when it fails due to the absence of any available query solutions. This is too generic. Instead we should introduce ErrorCodes::NoQuerySolutions in order to allow clients to check for this particular error case.    h5. Original description    I'm doing some work trying to get MongoDB 3.2 to work with parse-server. parse-server has a feature where if you do a geo query and don't have the appropriate index, it catches the error, creates the index, then retries the query. Previously this worked by catching the error code 17007 but in MongoDB 3.2 it appears that this error code is now 2.    I wanted to check if this was expected behavior before coding around the issue.""",Improvement,Querying
487373,"""I have a problem with duplicated keys on a replica.  My replica is made by 1 primary (A), 1 secondary (B) and 1 arbiter.    The parameter """"WriteConcern.REPLICA_ACKNOWLEDGED"""" is used in all insertion and update operations in those machines to keep both nodes synchronized and most operations are made in bulk.    There are around 500 collection in the database. 250 of the have an unique index on """"pid, channel"""" and the other 250 have an unique index on """"uid, channel"""".    Today I needed to chance the machines' priority to set the """"most idle"""" one as the primary. I've simply changed the priority of machine B to, in the next election, become primary.    A short while after the election, the machine B - which was now primary - detected the following problem:    {quote}  2016-04-13T09:40:21.966-0500 F STORAGE  [conn372] Unique index cursor seeing multiple records for key { : """"478713872142575"""", : """"Facebook"""" }  2016-04-13T09:40:21.999-0500 I -        [conn372] Fatal Assertion 28608  2016-04-13T09:40:21.999-0500 I -        [conn372]    ***aborting after fassert() failure    2016-04-13T09:40:22.702-0500 F -        [conn372] Got signal: 6 (Aborted).  {quote}    As soon as this happened, the machine A became the primary one again. A short while later, some of the following initializations of the machine B had the same problem with a different """"key"""". 40 minutes later the duplicate error happened in the machine A too.    This BUG has already happened in a similar way and it was solved by reindexing all unique indexes in the collections, but now it happened again and I'm concerned about using mongo on production. What can cause this type of error? Why MongoDB itself can't stay up and try to solve it instead of simply abort? Can it be the usage of WriteConcern method with REPLICA_ACKNOWLEDGED the cause of this problem? The priority change plus the election process can be the cause?""",Bug,"Replication,Indexing"
487423,"""Take the following update:  {code:javascript}  db.test.update(    {""""someField"""" : {""""$all"""": [""""a"""", """"b""""]}},     {""""$addToSet"""": {""""someOtherField"""": 123},       """"$setOnInsert"""": {""""someField"""": [""""a"""", """"b""""]}},    {""""upsert"""": true})  {code}    This results in the following error, because of the {{$all}} query:  {code:javascript}  """"writeError"""" : {    """"code"""" : 54,    """"errmsg"""" : """"cannot infer query <USER>to set, path 'someField' is matched twice""""}  {code}    Given that I explicitly stated what I want the value for {{someField}} to be using {{$setOnInsert}}, it doesn't have to be a problem that the value can't be inferred from the query. It could just use value I specified as a fallback for inference, or even instead of inference.""",Improvement,Querying
487995,"""I want to replace the collection field based on a map table, like   || old_src || new_src ||  | 1 | 448 |  | 5 | 449 |  |...|...|  So I write a very simple JavaScript.    {code:javascript}  db_name = 'mydb'  collection_name = 'mycoll'    src_map = [      [1,448],      [5,449],      [6,450],      [8,451],      [9,452],      [10,453],      [11,454],      [12,455],      [14,456],      [15,457],      [16,458],      [17,459],      [18,460],      [20,461],      [22,462],      [23,463],      [24,464],      [25,465],      [26,466],      [27,467],      [29,468]  ]    db = db.getSiblingDB(db_name)  for (x in src_map) {      old2new_map = src_map[x];      print(""""Changing src: """" + String(old2new_map[0]) + """" --> """" + String(old2new_map[1]) + """" ... """");      db[collection_name].update({""""src"""":old2new_map[0]},{$set:{""""src"""":old2new_map[1]}},{multi:true});      print(""""Done."""")  }  {code}    I use update function within a for loop to do this. But it's so slow. This collection contains more than 1000000 objects, and each update will execute more than 10 mins(even if src field has a index).    How to optimize this script? Is there any way to update the whole may in a single function? Or use multi thread way to let the update function parallelly?""",Question,JavaScript
488221,"""I'd like to request the addition of a SQL View like capability for security.     To be as brief as possible-       .  You have a collection with 8 keys. Remove all permission from this        collection for a given user. Create a view that references 6 of the 8        keys. Give permission to this view to the same user. Now this user         can read/write/whatever a vertical slice of this collection. This serves        a use case of data privacy.          OR, create the view with all columns, but a filter criteria is in place.        Eg., where { """"state"""" = """"NY"""" }. Now this user can read/write/whatever        a horizontal slice of this collection. This serves a use case of data        privacy, and multi-tenancy.       --       .  How others (SQL) does this-            The view either enforces the underlying security of the source/        referenced collections, or not.            By default, security is inherited from the source collection. If you         can't read/write the source table, the view will not give you this         ability. In this use case, the view acts more like a macro, and its        value add is to centralize definitions, or code.          If the view is created { """"with_admin_authority"""" : true }, then the view        overrides base permissions. This is how you suppress access to        base collections, yet still poke through any read/write any subsets.        In this case, the user can do whatever read/write they were given         the view object, and not the underlying/base collection object.           You might also want a { """"with_check_option"""" }. In effect, if you        should only see """"NY"""", can you insert """"NJ"""" ?       .  Phase 1, I'd be happy with projection and filter, and we'll call these        """"simple views"""". They are writable like a normal collection.          Phase 2, """"compound views"""", containing aggregated keys or simply        keys that are calculated. MongoDB doesn't know how to simply write        back to a calculated key. You can read compound views, but not         update.       .  Query optimizer enhancement-         If the view has a filter as part of its definition, and the user adds       filters, yadda, in their request, you should someday flatten all of that,       remove duplicates and such. This is a performance feature only.       . Views on views-         Added someday. Not needed on day one perhaps.       """,New Feature,"Querying,Concurrency,Security,Usability"
488505,"""Currently a string literal as a value in a $project stage will cause an error:  {noformat}  db.foo.aggregate([{$project: {a: """"hi""""}}])  assert: command failed: {   """"errmsg"""" : """"exception: FieldPath 'hi' doesn't start with $"""",   """"code"""" : 16873,   """"ok"""" : 0  } : aggregate failed  Error: command failed: {   """"errmsg"""" : """"exception: FieldPath 'hi' doesn't start with $"""",   """"code"""" : 16873,   """"ok"""" : 0  } : aggregate failed      at Error (<anonymous>)      at doassert (src/mongo/shell/assert.js:11:14)      at Function.assert.commandWorked (src/mongo/shell/assert.js:254:5)      at DBCollection.aggregate (src/mongo/shell/collection.js:1278:12)      at (shell):1:8  2016-02-02T12:12:03.131-0500 E QUERY    Error: command failed: {   """"errmsg"""" : """"exception: FieldPath 'hi' doesn't start with $"""",   """"code"""" : 16873,   """"ok"""" : 0  } : aggregate failed      at Error (<anonymous>)      at doassert (src/mongo/shell/assert.js:11:14)      at Function.assert.commandWorked (src/mongo/shell/assert.js:254:5)      at DBCollection.aggregate (src/mongo/shell/collection.js:1278:12)      at (shell):1:8 at src/mongo/shell/assert.js:13  {noformat}    We should allow this, with the semantics that it creates or replaces a field, and has that string literal as its value.""",New Feature,Aggregation Framework
489564,"""Hi Team,    I want to install MongoDB 64 bit on my windows 10 machine.  Problem is, it is being installed as a 32-bit application and I'm unable to setuup it as a windows service.    I have downloaded  """"Windows 64-bit 2008 R2+"""" version.    Please help!    Thanks in advance.""",Question,Admin
489750,"""As a way to improve the user experience when debugging locally, it would be useful to have an option where, if you were running many tests in parallel, it would repeat the failing ones in a serial execution once it was finished.    The common use case I can think of, is something like the following. A user runs  {noformat}  python buildscripts/resmoke.py --suites=<suite> -j8  {noformat}  Halfway through that, it fails on some test. The problem is, it is very difficult to figure out why that test failed, since all the logs are intermingled. The user then runs the following to start debugging the test:  {noformat}  python buildscripts/resmoke.py --executor=<suite> path/to/failing/test.js  {noformat}    It would be nice if resmoke.py had some sort of option to automatically do that. I would be tempted to have it on by default, since we could easily turn it off everywhere in our evergreen.yml where we wouldn't want it on. The only thing that would be weird is logging to a file. If we did nothing intelligent, the second run would override the first. If the test were flaky, it might not fail the second time, and then the logs of the first run (and all other successful tests) would be gone. It would be better to either append to the file, or to write to a different file, say repeated_tests.log or something like that.""",Improvement,Test
489819,"""I am evaluating NoSQL options for using as an lightweight-embedded storage for desktop applications. At the same time, I would like to be the same storage technology used for our server-side backend.  MongoDB is great for reducing the complexity of local storage, very similar to HTML5 database. However, its installer is really heavy (80+ MB).  I think there should be a lightweight-embedded distribution, just with the basic server, no diagnostic tools or even drivers, in a format that can enable unattended installation (MSI, DMG, etc.), so it can be part of the application installer.  This shouldn't have impact in the MongoDB product itself, but only in the distribution package generation.""",Improvement,Packaging
490293,"""In our slow query logs we can see that this query below is using the incorrect index which causes it to run slowly.    {code}  [conn3522] command haa_msgs.$cmd command: count { count: """"messages"""", query: { $or: [ { targetId: ObjectId('55fc54eecb05d8d2547334a1') }, { targetId: <USER>} ], type: { $in: [ 2 ] }, createdAt: { $gt: new Date(0) } }, <USER> <USER>} planSummary: IXSCAN { createdAt: -1.0 } keyUpdates:0 writeConflicts:0 numYields:271 reslen:44 locks:{ Global: { acquireCount: { r: 544 } }, Database: { acquireCount: { r: 272 } }, Collection: { acquireCount: { r: 272 } } } 103ms  {code}    We have an index on this collection that I would expect to be used.  But the log implies it uses a different index to run the query.  {code}  {targetId:1, type:1, createdAt:-1}  {code}    However, if I manually run a *find* query using the same query conditions, the explain plan uses the expected index.    This type of query has always behaved as expected in the past.  This problem occurs on version 3.0.4 and 3.0.6.    Not sure what is going on here, but this is causing serious performance problems in our database.      """,Bug,Querying
490299,"""I have 40050 document within a collection. The document has about 48 attributes and with size being 935 (used Object.bsonsize(..) to identify the size).   When I try to run,   db.collection.find({someSearhCriteria}).sort({attribute1:1}).limit(50).skip(40000)  it results in   error: {   """"$err"""" : """"Runner error: Overflow sort stage buffered data usage of 33555101 bytes exceeds internal limit of 33554432 bytes"""",   """"code"""" : 17144  } at src/mongo/shell/query.js:131    I understand that creating an index on """"attribute1"""" would fix this problem. In my application however, we enable sorting on all the 48 attributes the document has. Also, the number of documents satisfying the criteria could run into few hundred thousands. I want to understand if you still suggest creating index on all the 48 attributes? Or is there a better approach?     Would appreciate your help. """,Question,Querying
490615,"""is it possible there was a regression re-surfaced any of these issues?     SERVER-9365 SERVER-9498 SERVER-9690 SERVER-9792 SERVER-10271    i'm seeing very very similar behavior:    mongodb 3.0.4 sharded collections with 64MB chunks using wiredtiger    one colleciton with documents that average 2kB in size  one collection with documents that average 40B bytes in size    the collection with 2kB size docs is even distributed  the collection with 40B size docs is nearly entirely jumbo chunks    running the balancer does not seem to automatically split chunks - just marks them as jumbo.    i can run pass after pass of sh.splitFind on each chunk until there are no jumbo chunks left and then more things get balanced.     except then when i run the balancer again i get more chunks marked as jumbo and then i need to do splits again.    basically to get the cluster evenly distributed after an initial load i have to alternate splitting and balancing for days.""",Bug,Sharding
490631,"""Real world example of issue: searching the phrase {{what the}} renders no results. Searching {{""""what the""""}} also renders no results. However, searching {{""""what the hell""""}} finds results.    It seems that quite simply, dropped words are not indexed at all, so even when you do an explicit search (ie, quoted phrase) you will always receive 0 results.    I am reporting this as a bug as I'm unsure if this was intentional. If it was, then this is an enhancement request to either:    # Let us disable or otherwise configure the drop words for a specific language  # Enhance indexing to handle this.""",Bug,"Indexing,Text Search"
490651,"""Our Mongodb server (3.0.4) is running on GCE, with 2 replicas and one arbiter. I set it up 3 days ago.  Now it randomly stopped with these errors in /var/log/messages, and I need to start it everytime:    [  721.309875] mongod[3178] trap int3 ip:1205c31 sp:7f1cb1fe7668 error:0  [ 1875.272667] mongod[5433] trap int3 ip:1205c31 sp:7f973b8f51b8 error:0  [ 2725.869045] mongod[5988] trap int3 ip:1205c31 sp:7ee22fd49148 error:0  [12406.247920] mongod[11255] trap int3 ip:1205c31 sp:7f05acc459c8 error:0  [13922.031178] mongod[13517] trap int3 ip:1205c31 sp:7f1eebef8958 error:0""",Bug,Admin
491016,"""shellPrintHelper() usually calls print(), but in the case of an object with a shellPrint() function it instead returns the result of calling shellPrint().      Sometimes (often?) shellPrint() calls print() (eg. DBQuery), but sometimes it returns a string (eg. WriteResult).  In this latter case, the printing happens via printResult = true being passed to ImplScope::exec() when shellPrintHelper() is run.    This is all a bit confusing and inconsistent.  We should settle on print()ing conventions for shellPrint() and shellPrintHelper(), and then audit all uses to make them compliant.  The possible options are:    # shellPrint() and shellPrintHelper() always call print() in their dynamic scope, and never return any value.  # shellPrint() and shellPrintHelper() never call print() in their dynamic scope, and always return the string to be printed.  This is then actually printed via scope->exec()/printResult (as above).    I'm in favour of the latter, since it allows the exact shell output to be determined programmatically (and potentially tested in jstests) by the expression """"{{shellPrintHelper(foo)}}"""", rather than having to mess around capturing print() calls ala SERVER-19368.""",Bug,Shell
491248,"""Hi,    I would like to know if MongoDB has a watchdog when we run it as a windows service? What happens if the server crashes ... will it restartes or we have to do it manually ?""",Question,Packaging
491419,"""This issue is very similar to SERVER-5053, but focusing on the index field, not the duplicate keys.    In other words, I'm not really so much interested in    {code}  """"key"""": { """"_id"""" : ObjectId('4e683574bc4135eee514713f') }  {code}    as I am in    {code}  """"index"""" : """"test.people.$_id_""""  {code}    or better yet, <USER>like these:    {code}  """"ns"""": """"test.people""""  """"index"""" : """"_id_""""  {code}    SERVER-5053 got closed as a duplicate of SERVER-4637 which doesn't even mention the index field, but suggests adding the keys as well as the _ids, which in turn resulted in some ambiguity on how to solve some problems that came up and the issue has been stalled since 2012.    So, without getting bogged down by issues related to keys and _ids, would it be possible to just add the index name to the error object first and then figure out the rest later?    I find nearly all of MongoDB's design and interfaces very elegant and pleasant to work with, but this is a major exception among the most common use cases. The duplicate key error message parsing is the messiest part of the Mongo interface in many applications.    So this (hopefully simple) change would be a lot of help to many developers struggling with this problem.""",Improvement,Usability
491520,"""It would be desirable to allow logical operators over non-top-level expressions in $pull operations. Currently, the following is an error, and there is no equivalent way to express it in one update:  {code:js}  db.foo.update({}, {$pull: {a: {$or: [{$lt: 1}, {$gt: 10}]}}})  {code}      Original description:  At present it is only possible to present exact conditions to the $pull operaor. Take the following document:    {code:javascript}      {          foo: {              bar: [1, 2, 3, 3, 4, 5, 5]         }      }  {code}    Now let's say I want to $unset the first occurence of 5 in the array, so the  you could do:    {code:javascript}      db.collection.update({ """"foo.bar"""": 5 },{ """"$unset"""": { """"foo.bar.$"""": 1 } })  {code}    Which is all fine as now the first of the values for 5 is <USER>    {code:javascript}      {          foo: {              bar: [1, 2, 3, 3, 4, <USER> 5]         }      }  {code}      But now if you want to remove both """"<USER>"" and values """"less than"""" 4 you need to perform several updates:    {code:javascript}      db.collection.update({ },{ """"$pull"""": { """"foo.bar"""": { """"$lt"""": 4 } } })      db.collection.update({ },{ """"$pull"""": { """"foo.bar"""": <USER>} })  {code}    So the current syntax to $pull seems to consider that the immediate """"key"""" identifier inside the $pull operation needs to be the identifier for the array from which the items would be pulled from.    It would be nicer if full query conditions could be set such as:    {code:javascript}      db.collection.update(          { },          { """"$pull"""": {               """"$or"""": [                 { """"foo.bar"""": { """"$lt"""": 4 },                 { """"foo.bar"""": <USER>}             ]         }}  {code}    But there is of course an ambiguity on """"which field"""" is set for the array within an $or condition. That does not error presently, but of course does not work either.    A possible alternate syntax could be:    {code:javascript}      db.collection.update(          { },          { """"$pull"""": { """"foo.bar"""": { """"$in"""": [{ """"$lt"""": 4 },<USER> }} }      )  {code}    Being a shorter version of $or via $in, but of course that does error due to $in not allowing such a contruct for evaluation.    On the other hand using $pullAll does not error here:    {code:javascript}      db.collection.update(          { },          { """"$pullAll"""": { """"foo.bar"""": [{ """"$lt"""": 4 },<USER>] }}      )  {code}    But of course only removes """"<USER>"" values and ignores the conditional statement as a literal.    There must be some way to work the syntax in for multiple conditions without performing multiple update operations. Even the first $or example would be fine if there were some check to make sure you were not operating a condition on a different document element within the $pull operation, so that both elements had to be """"foo.bar"""" in this case.    Allowing this but producing an error where the field conditions did not match would seem a valid approach.  """,Improvement,Write Ops
491838,"""The hidden secondary server I use for backups cannot sync its data from the replica set. It's a replica set of 3 servers including the one which get killed by the kernel. All are running mongod 3.0.3.    I'm currently trying to migrate from mmapv1 to WiredTiger. The two main servers use mmapv1. The initial sync goes fine, copying our 292 million documents, mongod takes around 80% of the RAM. Then it starts building the index on _id. After a few %, the kernel kills mongod. I attached the logs from /var/log/messages.    If I relaunch mongod, it retries building the index, then exits because of (_the famous ?_):   {noformat}  Index Build """"Index rebuilding did not complete: 11000 E11000 duplicate key error collection: rum.reports index: _id_ dup key: { : """"11jjgtfncmtn9w47mco7vtbbj"""" }""""  {noformat}    If I relaunch mongod with indexBuildRetry=false, it restarts cloning the collection from the beginning after showing:    {noformat}  _2015-06-05T16:17:37.683+0000 W REPL     [ReplicationExecutor] Failed to load timestamp of most recently applied operation; NoMatchingDocument Did not find any entries in local.oplog.rs_  {noformat}    This server has only 16GB RAM, and can do a sync with mmapv1 without any problem. The database size is 946GB on mmapv1, and 243GB on WiredTiger (snappy).    I started running the following command when the cloning was at 236M docs. I attached the result.    {noformat}  mongo --eval """"while(true) {print(JSON.stringify(db.serverStatus({tcmalloc:1}))); sleep(10*1000)}"""" > ss.log  {noformat}    Cloning finishes at 2015-06-05T16:09:38.003+0000.  Index creation starts at 2015-06-05T16:09:45.606+0000.    The last log before the kill:    {noformat}  2015-06-05T16:12:00.565+0000 I -        [rsSync]   Index Build: 13494400/292298635 4%  {noformat}      On another replica set, this migration worked fine with a database size of 402GB mmapv1 / 133GB WT. For the record, the sync also took all the RAM, but index creation went fine. What's interesting is that after a restart of mongod on this server, RAM usage went from 100% to 17%. After a few days of continuous writes from the oplog, it's now at 43% RAM. I did not launch any query or make any backup from this secondary.    The two replica sets contain the same kind of documents, and the same indexes.    My _storage_ configuration:    {noformat}  storage:    dbPath: /a/b/c    indexBuildRetry: true    journal:      enabled: true    directoryPerDB: false # default    syncPeriodSecs: 60 # default      engine: wiredTiger      wiredTiger:      engineConfig:  #      cacheSizeGB: 12 # Default: the maximum of half of physical RAM or 1 gigabyte.        statisticsLogDelaySecs: 0 # If set to 0, WiredTiger does not log statistics.        journalCompressor: snappy # Default: snappy        directoryForIndexes: false # Default: false      collectionConfig:        blockCompressor: snappy # Default: snappy      indexConfig:        prefixCompression: true # Default: true  {noformat}    There's this _storage.wiredTiger.engineConfig.cacheSizeGB_ setting. I don't really get what it means. I tried setting the default, or setting it at 14. RAM usage still goes beyond the server capacity.    This issue could be related to SERVER-17456 and SERVER-17424. I created a new issue, because this one happens during an initial sync, not because of benchmarks.    In case someone at MongoDB is interested, the servers are monitored by MMS:  * The failing group is 556dc2ffe4b0ba305ae49354. The hidden Secondary is the only one running on WiredTiger. It's the one that I try to sync again and again.  * The ok group is 556dce19e4b0eea5366e6ae5. The hidden Secondary is the only one running on WiredTiger.""",Bug,Indexing
492181,"""*Background*    The current election protocol gives preference to a node who has the most recent opTime in his oplog. Essentially out of a pool of """"electable"""" nodes, the one with the most advanced opTime wins the election. The rationale for this is to minimize the amount of data rolled back due to a failover.    The Raft protocol does not have such a component in the election protocol. We do not want to re-introduce it because then we could no longer rely on the robustness/correctness of the Raft protocol in our implementation. In fact it seems this part of the current election protocol has been a source of bugs (e.g. no master elected, etc...). I agree with this design.    The result is that in MongoDB 3.2, users will be much more likely to see data rollbacked that was written with write concern < majority. From a strict point of view this is ok, as we never have guaranteed that such writes would be safe during failover. From a practical point of view users can today run with w:2 or even w:1 and expect to loose a minimal amount of data on failovers, and in MongoDB 3.2 they could suddenly lose hundreds of milliseconds of transactions and this is arguably a regression in our capability.      *Example test case*    (Note, this is based on my understanding of our current election protocol, I didn't actually verify this so far. If we want to address this problem at all, first step would be to run this test. If this test doesn't trigger the behavior I'm describing for 3.2, it will nevertheless be possible to construct a more contrived test case that will.)    write concern = 2    * Primary in North America  * 2 secondaries in North America  * 2 secondaries in Europe  * All nodes have equal/default priority  * Network RTT Europe-NA is roughly 100 ms    In MongoDB 2.4-3.0, a primary failure is likely to cause one of the other NA secondaries to become primary, because they will of course be 100 ms ahead their EU peers in replication.    In proposed MongoDB 3.2 design, all secondaries have equal probability to become primary. Therefore there is 50% chance that a EU node becomes primary and therefore the US secondaries would roll back 100 ms of their oplog.      *Proposal*    I believe it is possible to give more attention to not rolling back oplog as follows:     - Execute the Raft-based election as currently planned   - When a new primary is elected, he will first check all other reachable nodes for their oplog state.   - If another node has a more recent opTime, connect to that and copy the missing part of the oplog and apply those on the primary.   - Now start operating as the new primary.      Benefits of my proposal:     - Doesn't mess with election protocol. Primary is elected as per Raft, then this fix is applied as additional step.   - Ensures that operations that existing on any one available node will not be rolled back, rather will be applied on the primary    Drawbacks of my proposal:     - Will make the failover time longer. Potentially this increase in failover time is unbounded too. But it would be possible to create some upper bound for this, for example by continuing the current rule that nodes that are more than 10 seconds behind are considered un-electable. (Such nodes must then be considered failed from a Raft point of view: they cannot participate in elections and therefore not in majority acknowledgements either.)   - This is also true in cases where an application has been using w:majority and wouldn't care about losing transactions that exist on one node but weren't majority acknowledged. Hence users who want to minimize failover time must be able to turn this functionality off. (Possibly this could be turned on/off automatically by the primary detecting which write concerns are used by clients?)    *Proposed priority*    3.1 Desired or less. The justification for this is that per documentation we don't promise that rollback wouldn't happen to non-majority committed data.  """,Improvement,Replication
492314,"""{panel:title=Issue Status as of Sep 29, 2015|borderColor=#ccc|titleBGColor=#6CB33F|bgColor=#EEEEEE}    *ISSUE SUMMARY*  On MongoDB instances running with document-level concurrency storage engines, concurrent modifications of the same document by multiple clients may cause {{WriteConflictExceptions}} to be thrown. These write conflicts are handled internally by retrying the operation.    A bug that affects both {{findAndModify}} remove operations and {{findAndModify}} update operations with {{new=false}} may cause the """"value"""" field to be included in the server's response multiple times. Each occurrence of the field is likely to be associated with different documents or different versions of the same document.    The server's response was mutated to include the old version of the document prior to the delete or update operation taking place. If faced with a write conflict, the operation would be retried on a potentially different document. Each attempt to perform the operation would cause that document to be appended to the server's response under a new """"value"""" field. This means that the last """"value"""" field present in the server's response is the actual document that was updated or removed by the client.    *USER IMPACT*  How a driver handles duplicate <USER>in the server's response isn't standardized behavior. The most common scenarios are that:    # The driver may choose the first """"value"""" field in the response. This corresponds to the document that was first attempted to be updated or deleted, but wasn't actually the one modified by this client.  # The driver may choose the last """"value"""" field in the response. This corresponds to the document that was updated or deleted by this client.  # The driver may return an error because the """"value"""" field was present in the response multiple times.    For sufficiently large documents with many concurrent {{findAndModify}} operations, it is possible to trigger enough write conflicts such that the server's response exceeeds 16MB. This causes an error to be returned to the client saying that the BSONObj size is invalid.    *WORKAROUNDS*  None.    *AFFECTED VERSIONS*  The 3.0 release series up until (and including) 3.0.2 are affected by this issue. Only MongoDB instances running with document-level concurrency storage engines (e.g. WiredTiger and RocksDB) are affected. MMAPv1 doesn't have a notion of write conflicts and is *not* affected.    *FIX VERSION*  The fix is included in the 3.0.3 production release.  {panel}    h6. Original description  I'm using a mongo collection as a queue, on mongo v3.0.2 with wiredtiger; producers use simple inserts and consumers use findAndModify with a sort descending on a timestamp, and remove option    I've noticed that when I use one producer and multiple consumers in separated threads, in quite a many cases more than one thread return or obtain the same element out of the findAndModify. I've reread the docs on the semantics, and this seems a bug to me""",Bug,"Querying,Concurrency"
492381,"""Hello,    I have a replica set with three nodes (Primary, secondary and arbiter). MongoDB version is 3.0.2 and I started my replica set by this command:    {code}  sudo mongod --storageEngine wiredTiger --dbpath /data/ --replSet rs0 --fork --logpath /var/log/mongodb/fork.log  {code}    This is a db.stats():  {code}  rs0:PRIMARY> db.stats()  {          """"db"""" : """"test"""",          """"collections"""" : 52,          """"objects"""" : 1697582895,          """"avgObjSize"""" : 745.3563943956916,          """"dataSize"""" : 1265304265805,          """"storageSize"""" : 647557865472,          """"numExtents"""" : 0,          """"indexes"""" : 176,          """"indexSize"""" : 22991790080,          """"ok"""" : 1  }  {code}    And this is how it looks like while I am syncing the Secondary on mongostat:    {noformat}  root@mongodb-replica1:/data# mongostat --discover                           insert query update delete getmore command % dirty % used flushes vsize   res qr|qw ar|aw netIn netOut conn set repl     time         localhost:27017     56     6     13     *0       6     8|0     0.0   80.0       0 31.9G 31.3G   0|2   2|2   49k   214k   58 rs0  PRI 09:49:24  mongodb-replica1:27017     56     6     13     *0       6     4|0     0.0   80.0       0 31.9G 31.3G   0|0   2|0   52k   213k   58 rs0  PRI 09:49:24  mongodb-replica2:27017     *0    *0     *0     *0       0     1|0     1.2    1.3       0 31.9G 29.5G   0|1   1|0   79b    15k    5 rs0  UNK 09:49:24           localhost:27017     26     6      8     *0       5     5|0     0.0   80.0       0 31.9G 31.3G   0|0   1|0   37k   634k   58 rs0  PRI 09:49:25  mongodb-replica1:27017     27     6      8     *0       5     5|0     0.0   80.0       0 31.9G 31.3G   0|0   1|0   34k   633k   58 rs0  PRI 09:49:25  mongodb-replica2:27017     *0    *0     *0     *0       0     7|0     1.2    1.4       0 31.9G 29.5G   0|1   1|0  596b   147k    5 rs0  UNK 09:49:25           localhost:27017     35     3      8     *0       5    20|0     0.0   80.0       0 31.9G 31.3G   0|0   2|0   39k   146k   58 rs0  PRI 09:49:26  mongodb-replica1:27017     34     3      8     *0       5    20|0     0.0   80.0       0 31.9G 31.3G   0|0   2|0   39k   146k   58 rs0  PRI 09:49:26  mongodb-replica2:27017     *0    *0     *0     *0       0     2|0     1.3    1.4       0 31.9G 29.5G   0|1   1|0  137b    16k    5 rs0  UNK 09:49:26           localhost:27017     22     6     24     *0       4     5|0     0.0   80.0       0 31.9G 31.3G   0|0   2|0   54k   212k   58 rs0  PRI 09:49:27  mongodb-replica1:27017     22     6     24     *0       4     4|0     0.0   80.0       0 31.9G 31.3G   0|0   2|0   53k   197k   58 rs0  PRI 09:49:27  mongodb-replica2:27017     *0    *0     *0     *0       0     4|0     1.3    1.5       0 31.9G 29.5G   0|1   1|0  422b    16k    5 rs0  UNK 09:49:27  {noformat}    This is a second time I wanted to full sync my secondary and at the end when the storage is almost equal (650GB) and secondary is building indexes the Primary suddenly has a high cpu usage and eventually freezes. The SSH connection will drop and the machine is not accessible. By the look at alerts on both MMS and application level I can see that all the operations also blocked on Primary and there is no insert/update/and query.    I didn't wait to see what would've happened when the secondary finishes its building index as it was at 22% and I had to wait for a long time without primary but when I restarted the primary the secondary suddenly removed everything and started from the beginning.    The hardware spec is 10-core CPU with 80GB of memory and 3TB of storage on both Primary and Secondary. I don't have CPU profiling on MMS enabled as I remember I couldn't do it way back so let me know if you need more info or to log something for the next time.""",Bug,Admin
492568,"""Currently, low disk space conditions only cause write operations to fail when the actual attempt to get/use more disk space from the OS fails.  It would help (operationally) if writes could additionally fail when the amount of free disk space falls below some configurable threshold (eg. 10GB).  This would leave some disk space as headroom that can be used to restore service, as opposed to the alternative of a filesystem that is either completely full or has a uselessly small amount of free space (eg. under 2GB).    A simple workaround for this (and the behaviour that I'm effectively asking for) is to allocate on disk a large dummy file of the desired size.  This will cause the filesystem to fill up """"early"""", and if this happens the dummy file can be deleted to gain access to the """"spare"""" disk space.""",New Feature,Storage
492604,"""It would be helpful to be able to keep tabs on when/where orphans are occurring in a cluster. Most of the time, {{cleanupOrphaned}} runs without cleaning anything, but we've had instances where the number of orphans went up significantly. In these cases, we didn't realize we had an orphan problem until we started getting reports from users.    Ideally, we'd be able to get this information from MongoDB so we can investigate proactively. Right now, we use how long {{cleanupOrphaned}} takes as a proxy for figuring out if it did any work (e.g. if it returns in under 15s, it probably didn't find any). This produces too many false positives to be useful, though. We've looked at options for parsing this out of logs, but it's not easy to grep for.    I think a reasonable API would be to add a field, {{ndeleted}} to the output of {{cleanupOrphaned}}. That would make it easy to setup thresholds by collection for what's worth investigating. Alternatively, a bool field that just indicates if any documents were deleted would probably be sufficient for my use case.""",New Feature,Sharding
492610,"""First off, installing mongodb-org (3.0.1) worked without errors on  {quote}  Distributor ID: Debian  Description:    Debian GNU/Linux 7.8 (wheezy)  Release:        7.8  Codename:       wheezy  {quote}  and  {quote}  Distributor ID: Ubuntu  Description:    Ubuntu 12.04.5 LTS  Release:        12.04  Codename:       precise  {quote}  Since I wanted to test the WiredTiger engine I had to change the configuration file format to yaml:  {quote}  root@debian:/etc# cat mongod.conf   systemLog:     destination: file     path: """"/var/log/mongodb/mongodb.log""""     logAppend: true  storage:     dbPath: """"/var/lib/mongodb""""     engine: wiredTiger     journal:        enabled: true  processManagement:     fork: true  net:     bindIp: 127.0.0.1     port: 27017  setParameter:     enableLocalhostAuthBypass: true  {quote}  Now, the start script /etc/init.d/mongod fails because the PID it saves to   {quote}  cat /var/run/mongod.pid   10142  {quote}  is wrong - that PID does not exist.  And since the dpkg post-install and prerm scripts try to start / stop mongod they will fail and therefor any removal or upgrade to MongoDB 3.0.2 must also fail:  {quote}  Unpacking replacement mongodb-org ...  Processing triggers for man-db ...  Setting up libdpkg-perl (1.16.16) ...  Setting up dpkg-dev (1.16.16) ...  Setting up mongodb-org-shell (3.0.2) ...  Setting up mongodb-org-server (3.0.2) ...  [FAIL] Starting database: mongod failed!  invoke-rc.d: initscript mongod, action """"start"""" failed.  dpkg: error processing mongodb-org-server (--configure):   subprocess installed post-installation script returned error exit status 1  Setting up mongodb-org-mongos (3.0.2) ...  Setting up mongodb-org-tools (3.0.2) ...  dpkg: dependency problems prevent configuration of mongodb-org:   mongodb-org depends on mongodb-org-server; however:    Package mongodb-org-server is not configured yet.    dpkg: error processing mongodb-org (--configure):   dependency problems - leaving unconfigured  Errors were encountered while processing:   mongodb-org-server   mongodb-org  E: Sub-process /usr/bin/dpkg returned an error code (1)  {quote}  Luckily, I was able to get a screenshot of what goes wrong:  {quote}  \# ps -ef | grep mongodb  root      9951     2  0 08:49 ?        00:00:00 [kworker/1:1]  mongodb  10142     1  0 08:51 ?        00:00:00 /usr/bin/mongod --config /etc/mongod.conf  mongodb  10144 10142  0 08:51 ?        00:00:00 /usr/bin/mongod --config /etc/mongod.conf  mongodb  10145 10144 12 08:51 ?        00:00:04 /usr/bin/mongod --config /etc/mongod.conf  root     10206  4584  0 08:52 pts/1    00:00:00 ps -ef  root@debian:/var/lib# kill 10142  bash: kill: (10142) - No such process  {quote}  When using a yaml style config file and the WiredTiger storage engine the MongoDB init script starts a process whose PID is written to /var/run/mongod.pid. This process then starts the real mongod and terminates itself so that the PID inside /var/run/mongod.pid is now defunct.    An easy workaround is to edit the init script and paste these two lines into /etc/init.d/mongod:  {quote}   start-stop-daemon --background --start --quiet --pidfile $PIDFILE \                          --make-pidfile --chuid $DAEMONUSER \                          --exec $NUMACTL $DAEMON $DAEMON_OPTS              errcode=$?  \#\#\#\#\#\#\#\# next two lines added by <USER>                 sleep 5                  ps -ef | grep mongodb | grep -v grep | awk '\{print $2}' > $PIDFILE  {quote}  We now save the correct PID to file and starting / stopping works - but it is not a pretty solution, just a workaround hack which needs to be improved.""",Bug,Packaging
492757,"""In a sharded cluster that has auditing enabled, the following error occurs every 30s when a distributed lock is pinged:    {noformat}  2015-03-26T08:00:32.332+1100 W SHARDING [LockPinger] distributed lock pinger 'genique:22025,genique:22026,genique:22027/genique:22023:1427317232:2093216429' detected an exception while pinging. :: caused by :: SyncClusterConnection::update prepare failed:  genique:22025 (127.0.1.1):getLastError command failed: Audit metadata does not include both user and role information. genique:22026 (127.0.1.1):getLastError command failed: Audit metadata does not include both user and role information. genique:22027 (127.0.1.1):getLastError command failed: Audit metadata does not include both user and role information.  2015-03-26T08:01:02.385+1100 W SHARDING [LockPinger] distributed lock pinger 'genique:22025,genique:22026,genique:22027/genique:22023:1427317232:2093216429' detected an exception while pinging. :: caused by :: SyncClusterConnection::update prepare failed:  genique:22025 (127.0.1.1):getLastError command failed: Audit metadata does not include both user and role information. genique:22026 (127.0.1.1):getLastError command failed: Audit metadata does not include both user and role information. genique:22027 (127.0.1.1):getLastError command failed: Audit metadata does not include both user and role information.  2015-03-26T08:01:32.386+1100 W SHARDING [LockPinger] distributed lock pinger 'genique:22025,genique:22026,genique:22027/genique:22023:1427317232:2093216429' detected an exception while pinging. :: caused by :: SyncClusterConnection::update prepare failed:  genique:22025 (127.0.1.1):getLastError command failed: Audit metadata does not include both user and role information. genique:22026 (127.0.1.1):getLastError command failed: Audit metadata does not include both user and role information. genique:22027 (127.0.1.1):getLastError command failed: Audit metadata does not include both user and role information.  {noformat}    I've only seen this on shard primaries, but it might happen on mongoses too (haven't tried/checked).""",Bug,"Sharding,Security"
492863,"""If a sharded query incurs a failure (such as a dead cursor) on any one shard, when that error is returned to the mongos, mongos should terminate the query on the remaining shards automatically.    Otherwise, we will continue utilize processing power and time on a query we already know is going to fail. This is particularly an issue if you have many long running cpu intensive queries, such as for analytics. I would like a failed query to be cleaned up across the cluster as fast as possible, or within some reasonable timeout, in order to free up resources more quickly for other queries.""",Improvement,Sharding
493004,"""This might be a non-trivial request, in which I understand it might not be implemented soon.    Based on customer feedback and googling some forums, a common """"search engine experience"""" that users often want from full text search is the following query:    {code}  db.ftscollection.find( { $text : { $search : """"<USER>Smith New York"""" } },                                    { score : { $meta : """"textScore"""" } } )                          .sort( { score : { $meta : """"textScore"""" } } )                          .limit(10)  {code}    Ie the user just wants to search for a bunch of words and is hoping for the weighted scores to efficiently come up with a top ten result set. In reality MongoDB will proceed to scan all matching entries from the FTS index, then sort them.    I'm not familiar with the index structure used by our FTS. It seems to me if the score for each indexed word is included in the index (and index is sorted by that) then this would be possible.    For example, say that an FTS index contains mens and womens names with different weighted scores:    {code}  { '' : 'Alice', score : 3 }  { '' : 'Alice', score : 2 }  { '' : 'Alice', score : 1 }  { '' : '<USER>, score : 5 }  { '' : '<USER>, score : 6 }  { '' : '<USER>, score : 4 }  { '' : '<USER>, score : 3 }  { '' : '<USER>, score : 3 }  { '' : '<USER>, score : 2 }  { '' : '<USER>, score : 1 }  {code}    Assume now that a user is executing this query on a collection with the above index:    {code}  db.ftscollection.find( { $text : { $search : """"<USER>Alice"""" } },                                    { score : { $meta : """"textScore"""" } } )                          .sort( { score : { $meta : """"textScore"""" } } )                          .limit(2)  {code}    I'm assuming that the total score is just a sum of the scores for each word (which is already weighted in the index).    A greedy index scan would now start by scanning for """"<USER>"", which has a higher top score compared to Alice. Say that it finds 2 documents where the total score is (for <USER>Alice): 6+0=6 and 4+3=7. By looking at the next index entries we see that the remaining highest score for <USER>is 3 and for Alice is 2. This means that the highest possible score we could still expect to find is 3+2=5, which is < 6. Therefore the index scan can be terminated and sorted results returned to client.    The statistics for this query would have been:    {code}  n = 2  nscanned = 5  nscannedObjects = 2  {code}""",Improvement,Text Search
493075,"""Hi all,  Starting a server (from scracth) in 3.0 and  authenticationMechanisms=MONGODB-CR and auth enabled makes impossible to login with that mechanism.  I've traced it a little bit: when users are created (even with this auth mechanism) the credentials created are only for """"SCRAM-SHA-1"""". see Sample: {code} {         """"_id"""" : """"admin.admin"""",         """"user"""" : """"admin"""",         """"db"""" : """"admin"""",         """"credentials"""" : {                 """"SCRAM-SHA-1"""" : {                         """"iterationCount"""" : 10000,                         """"salt"""" : """"FPnmqmCI04KHJVZunfaI2Q=="""",                         """"storedKey"""" : """"i+jvORcFsnx6CXt0Bd924e2f804="""",                         """"serverKey"""" : """"PQHG8nYYcJTjFEClqjFRZ8PTLTA=""""                 }         },         """"roles"""" : [                 {                         """"role"""" : """"userAdminAnyDatabase"""",                         """"db"""" : """"admin""""                 },                 {                         """"role"""" : """"root"""",                         """"db"""" : """"admin""""                 }         ] } {code}  But if I go to a 2.6 server create the same credentials, then update the 3.0 server user document credentials with the MONGODB-CR it can successfully log in. {code} {         """"_id"""" : """"admin.admin"""",         """"user"""" : """"admin"""",         """"db"""" : """"admin"""",         """"credentials"""" : {                 """"SCRAM-SHA-1"""" : {                         """"iterationCount"""" : 10000,                         """"salt"""" : """"FPnmqmCI04KHJVZunfaI2Q=="""",                         """"storedKey"""" : """"i+jvORcFsnx6CXt0Bd924e2f804="""",                         """"serverKey"""" : """"PQHG8nYYcJTjFEClqjFRZ8PTLTA=""""                 },                 """"MONGODB-CR"""" : """"8aab8902fd862afad8064b73bd149d00""""         },         """"roles"""" : [                 {                         """"role"""" : """"userAdminAnyDatabase"""",                         """"db"""" : """"admin""""                 },                 {                         """"role"""" : """"root"""",                         """"db"""" : """"admin""""                 }         ] } {code}  There's still a lot of code/programs (as admin UIs robomongo, nosqlmanager and so on) that still is not ready to the new auth schema and makes impossible to login, even if I specify the MONGODB-CR auth.  I think that the credentials created for a new user need to include also the auth mechanism enabled, this will possible a softer transition, for us right now means a lot of user collection manual hack or don't upgrade.  Thanks!""",Bug,Security
493242,"""I'm running version 2.6.7 the 64 bit MongoDB as a standalone DB (no replication) on my  Dell Precision M4600 running 64bit Windows 7 Pro with 16GB RAM, Quad Core i7, and a 1TB SSD with 600GB free. I was doing InsertBatch (batches of 300 items) of around 80,000 records/documents. About 7/8 of the way through the insertions, MongoD service crashes with the following error (not enough system resources):    {noformat}  2015-02-12T16:33:22.453-0800 [journal] error exception in dur::journal error appending to file c:\mongodb\data\journal\j._4 8503296 8388608 errno:1450 Insufficient system resources exist to complete the requested service.  2015-02-12T16:33:22.465-0800 [journal] dbexception in groupCommit causing immediate shutdown: 13517 error appending to file c:\mongodb\data\journal\j._4 8503296 8388608 errno:1450 Insufficient system resources exist to complete the requested service.  {noformat}    I've attached the log file of the run.  """,Bug,"Internal Code,Stability"
493627,"""If I put a high continuous write load onto a a replica set I fall behind on the oplog until I fall off the end.   All three nodes are the same (High) spec - using write concern defaults and a multi threaded insert only workload ( OPLOG, Journal and Collection on same Drive). They are all interconnected by the same network, as is the client.  I know I could use a higher write concern but this is a simple and likely test scenario and I would expect the replication mechanism to be as fast as any external loader.   """,Bug,Replication
493847,"""The mongodb regex parser cannot create accurate bounds to allow indexed lookups when the regex includes the """"|"""" character.  This was allowed previously, but could lead to incorrect results.    *Original Description*  I've got a database with 14 mio rows representing an imaginary """"file system"""". There are (amongst others) the <USER>""""type"""" (only """"folder"""" or """"file""""), """"folder"""" and """"filename"""".    I now try this:  {noformat}  {  type:'folder',  folder:/^example1/  }    Very fast. No results - as expected. In fact, no rows even start with """"example"""".    I now try this:    {  type:'folder',  folder:/^example2/  }  {noformat}      Very fast, too. No results again - as expected.    Now, I try this:  {noformat}    {  type:'folder',  folder:/^example(1|2)/  }  {noformat}    This query never returns. The whole database becomes horribly slow to the point of being unusable. I cannot even run any other queries. This used to work on 2.4.x. Why? :-(  """,Improvement,Querying
494105,"""What I'm about to suggest could be (imho) a game changer about how we (developers) use databases.  Please note that this feature maybe won't apply/benefit 100% of the cases, but it will surely benefit the vast majority.  What I'm proposing is a way to save different data in a single field and make the database engine return one value or another depending on the scope of the query. Let me explain further...  Translations are really common in web-development. A single product's description (in an e-commerce website) has different translations in multiple languages (english, spanish, italian, french, etc...). It doesn't matter if we're using SQL or NoSQL, we're used to create as many <USER>as languages we need to support (desc_en, desc_es, desc_it, desc_fr, etc...) or even worse, different tables/documents.  But what if we could store all those different translations in the same field and make MongoDB return a specific translation or if that one is missing, return a """"default"""" one?  Something like: {code} db.collections.products.find({     _id: 123456789,     $scope: {         try: """"spanish"""",         <USER> """"english""""     } }, { product_name: 1, product_desc: 1 }); {code}  That will query the database for the name and the description of the product with _id 12346789, first trying to get the spanish translation of those (scoped) <USER> and if any of those <USER>doesn't have a spanish translation, it will fetch the english one. That way we could end up with a result like this:  {code} {     name: """"T-Shirt"""",     desc: """"Camiseta de verano, 100% algodon"""" } {code}  The same way we could insert different translations in a single field using that same syntax (setting the """"scope"""" of the field).  This could be used in translations (as my example shows), different metric units (car top speed -> km/h vs mph), different data representation (value of color -> human vs hex vs rgba), etc...   The possibilities are endless, and it will improve a lot how we currently deal with those kind of problems.  As an extra, it would be nice to """"query the scopes"""" of a document, so we can know that the document containing a product does actually contain the english, spanish, italian and french """"scopes"""" (translations).""",New Feature,Querying
494248,"""Second time I've seen the timeout in normal activities.  Captured the stack trace this time.  Customers and users will no doubt be confused by the appearance of the stack trace.  Assuming the diagnostics are wanted by the developers, one is provided in this ticket.  The first time the 30 sec timeout was seen was during a heavy YCSB load stage.  Hit ctrl-c to shut down the foreground mongod process and encountered the 30 second lock manager timeout while in-memory data was being flushed to disk.  The second time was on mongod startup, as a newly-ugraded (to 2.8.0-rc0) member of a replica set.  Upon startup you can see an index build on a 5 million document collection (sharded with a single shard).  Index build came in on the oplog.  Stack trace occurs around the 50% <USER>  Partial mongod log follows:  {noformat} 2014-11-16T18:06:56.984+0000 I JOURNAL  [initandlisten] recover /data/shard1/journal/j._0 2014-11-16T18:06:57.517+0000 I JOURNAL  [initandlisten] recover cleaning up 2014-11-16T18:06:57.517+0000 I JOURNAL  [initandlisten] removeJournalFiles 2014-11-16T18:06:57.616+0000 I JOURNAL  [initandlisten] recover done 2014-11-16T18:06:57.617+0000 I JOURNAL  [initandlisten] preallocating a journal file /data/shard1/journal/prealloc.0 2014-11-16T18:07:00.006+0000 I -        [initandlisten]   File Preallocator Progress: 807403520/1073741824 75% 2014-11-16T18:07:07.835+0000 I INDEXING [initandlisten] found 1 index(es) that wasn't finished before shutdown 2014-11-16T18:07:08.418+0000 I INDEXING [initandlisten] found 1 interrupted index build(s) on ycsb.usertable 2014-11-16T18:07:08.418+0000 I INDEXING [initandlisten] note: restart the server with --noIndexBuildRetry to skip index rebuilds 2014-11-16T18:07:08.419+0000 I INDEXING [initandlisten] build index on: ycsb.usertable properties: { v: 1, key: { n: 1.0 }, name: """"n_1"""", ns: """"ycsb.usertable"""" } 2014-11-16T18:07:08.419+0000 I INDEXING [initandlisten]          building index using bulk method 2014-11-16T18:07:11.004+0000 I -        [initandlisten]   Index Build: 172700/5001000 3% 2014-11-16T18:07:14.004+0000 I -        [initandlisten]   Index Build: 461500/5001000 9% 2014-11-16T18:07:17.004+0000 I -        [initandlisten]   Index Build: 774100/5001000 15% 2014-11-16T18:07:20.004+0000 I -        [initandlisten]   Index Build: 987400/5001000 19% 2014-11-16T18:07:23.004+0000 I -        [initandlisten]   Index Build: 1243600/5001000 24% 2014-11-16T18:07:26.005+0000 I -        [initandlisten]   Index Build: 1526900/5001000 30% 2014-11-16T18:07:29.004+0000 I -        [initandlisten]   Index Build: 1762000/5001000 35% 2014-11-16T18:07:32.025+0000 I -        [initandlisten]   Index Build: 2016400/5001000 40% 2014-11-16T18:07:35.014+0000 I -        [initandlisten]   Index Build: 2343900/5001000 46% 2014-11-16T18:07:38.014+0000 I -        [initandlisten]   Index Build: 2625600/5001000 52% 2014-11-16T18:07:42.411+0000 I -        [clientcursormon] LockerId 56 has been waiting to acquire lock for more than 30 seconds. MongoDB will print the lock manager state and the stack of the thread that has been waiting, for diagnostic purposes. This message does not necessary imply that the server is experiencing an outage, but might be an indication of an overloaded server. 2014-11-16T18:07:42.412+0000 I -        [clientcursormon] Dumping LockManager @ 0x1b562e0 2014-11-16T18:07:42.412+0000 I -        [clientcursormon] Lock @ 0x2AE27E0: {9: Global, 1} GRANTED:         LockRequest 20 @ 0x23C6600: Mode = IX; ConvertMode = NONE;          LockRequest 56 @ 0x23C6C00: Mode = IS; ConvertMode = NONE;   PENDING: 2014-11-16T18:07:42.412+0000 I -        [clientcursormon] Lock @ 0x2AE28A0: {18: MMAPV1Flush, 2} GRANTED:         LockRequest 20 @ 0x23C6600: Mode = IX; ConvertMode = NONE;   PENDING:         LockRequest 310 @ 0x23C7200: Mode = S; ConvertMode = NONE;  2014-11-16T18:07:42.412+0000 I -        [clientcursormon] Lock @ 0x16E9AD00: {8527796682173025563: Database, 1065974585271628195} GRANTED:         LockRequest 20 @ 0x23C6600: Mode = X; ConvertMode = NONE;   PENDING:         LockRequest 56 @ 0x23C6C00: Mode = IS; ConvertMode = NONE;  2014-11-16T18:07:42.413+0000 I -        [clientcursormon]  2014-11-16T18:07:42.502+0000 I -        [clientcursormon]   0x1022c36 0x9f998c 0xa00dbb 0x9f1687 0x9f319d 0x94f55e 0x94f952 0x934894 0x955468 0xfaf38d 0x107126b 0x7f1f027e6e9a 0x7f1f018f53fd ----- BEGIN BACKTRACE ----- {""""backtrace"""":[{""""b"""":""""400000"""",""""o"""":""""C22C36""""},{""""b"""":""""400000"""",""""o"""":""""5F998C""""},{""""b"""":""""400000"""",""""o"""":""""600DBB""""},{""""b"""":""""400000"""",""""o"""":""""5F1687""""},{""""b"""":""""400000"""",""""o"""":""""5F319D""""},{""""b"""":""""400000"""",""""o"""":""""54F55E""""},{""""b"""":""""400000"""",""""o"""":""""54F952""""},{""""b"""":""""400000"""",""""o"""":""""534894""""},{""""b"""":""""400000"""",""""o"""":""""555468""""},{""""b"""":""""400000"""",""""o"""":""""BAF38D""""},{""""b"""":""""400000"""",""""o"""":""""C7126B""""},{""""b"""":""""7F1F027DF000"""",""""o"""":""""7E9A""""},{""""b"""":""""7F1F01801000"""",""""o"""":""""F43FD""""}],""""processInfo"""":{ """"mongodbVersion"""" : """"2.8.0-rc0"""", """"gitVersion"""" : """"b6c4e2491c1442b05a160acda0d78001f56a2ade"""", """"uname"""" : { """"sysname"""" : """"Linux"""", """"release"""" : """"3.6.11-gentoo"""", """"version"""" : """"#3 SMP Wed Dec 18 20:52:24 CST 2013"""", """"machine"""" : """"x86_64"""" }, """"somap"""" : [ { """"elfType"""" : 2, """"b"""" : """"400000"""", """"buildId"""" : """"C6D8BFF1AC987C07A66AD78250664B9EDA577DA2"""" }, { """"b"""" : """"7FFF9ACEA000"""", """"elfType"""" : 3 }, { """"b"""" : """"7F1F027DF000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libpthread.so.0"""", """"elfType"""" : 3, """"buildId"""" : """"C340AF9DEE97C17C730F7D03693286C5194A46B8"""" }, { """"b"""" : """"7F1F025D7000"""", """"path"""" : """"/lib/x86_64-linux-gnu/librt.so.1"""", """"elfType"""" : 3, """"buildId"""" : """"352C5B373A50E6C4AB881A5DB6F5766FDF81EEE0"""" }, { """"b"""" : """"7F1F023D3000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libdl.so.2"""", """"elfType"""" : 3, """"buildId"""" : """"D181AF551DBBC43E9D55913D532635FDE18E7C4E"""" }, { """"b"""" : """"7F1F020D3000"""", """"path"""" : """"/usr/lib/x86_64-linux-gnu/libstdc++.so.6"""", """"elfType"""" : 3, """"buildId"""" : """"B534DA725D06A04267EB2FEB92B9CC14C838B57B"""" }, { """"b"""" : """"7F1F01DD7000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libm.so.6"""", """"elfType"""" : 3, """"buildId"""" : """"817AA99B3DD02501F8BC04A3E9A9358A08F20D7D"""" }, { """"b"""" : """"7F1F01BC1000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libgcc_s.so.1"""", """"elfType"""" : 3, """"buildId"""" : """"ECF322A96E26633C5D10F18215170DD4395AF82C"""" }, { """"b"""" : """"7F1F01801000"""", """"path"""" : """"/lib/x86_64-linux-gnu/libc.so.6"""", """"elfType"""" : 3, """"buildId"""" : """"53514F7C9AD6614703AC01FD7CBCC9D3C6F621CD"""" }, { """"b"""" : """"7F1F029FC000"""", """"path"""" : """"/lib64/ld-linux-x86-64.so.2"""", """"elfType"""" : 3, """"buildId"""" : """"83B40B93BD2FD266F12E008E61309FC03EB406A5"""" } ] }}  mongod(_ZN5mongo15printStackTraceERSo+0x26) [0x1022c36]  mongod(+0x5F998C) [0x9f998c]  mongod(_ZN5mongo10LockerImplILb1EE4lockERKNS_10ResourceIdENS_8LockModeEjb+0x1BB) [0xa00dbb]  mongod(_ZN5mongo4Lock6DBLock6lockDBEv+0x57) [0x9f1687]  mongod(_ZN5mongo4Lock6DBLockC1EPNS_6LockerERKNS_10StringDataENS_8LockModeE+0x10D) [0x9f319d]  mongod(_ZN5mongo9AutoGetDbC1EPNS_16OperationContextERKNS_10StringDataENS_8LockModeE+0x3E) [0x94f55e]  mongod(_ZN5mongo24AutoGetCollectionForReadC2EPNS_16OperationContextERKSs+0xB2) [0x94f952]  mongod(_ZN5mongo19GlobalCursorIdCache14timeoutCursorsEPNS_16OperationContextEi+0x1B4) [0x934894]  mongod(_ZN5mongo19ClientCursorMonitor3runEv+0xA8) [0x955468]  mongod(_ZN5mongo13BackgroundJob7jobBodyEv+0x11D) [0xfaf38d]  mongod(+0xC7126B) [0x107126b]  libpthread.so.0(+0x7E9A) [0x7f1f027e6e9a]  libc.so.6(clone+0x6D) [0x7f1f018f53fd] -----  END BACKTRACE  ----- 2014-11-16T18:07:48.769+0000 I -        [initandlisten]   Index Build: 2688700/5001000 53% 2014-11-16T18:07:51.016+0000 I -        [initandlisten]   Index Build: 2759100/5001000 55% 2014-11-16T18:07:54.014+0000 I -        [initandlisten]   Index Build: 2924600/5001000 58% 2014-11-16T18:07:57.015+0000 I -        [initandlisten]   Index Build: 3088800/5001000 61% {noformat} """,Bug,Storage
494277,"""This may be """"works as designed"""" (i.e. that WT is going to be slower for traversing large data structures in memory) but I would like to make sure and quantify the expected behavior here if that is the case.  While attempting to profile the benefits of compression in terms of bandwidth savings, the expected performance of the default snappy compression (which delivered decent on-disk compression) looked slower than expected, significantly slower than mmapv1.  That led to a round of testing to better understand what was going on here.  So, I used 4 basic storage engine configurations:   * mmapv1 * WT with no compression ({{""""block_compressor=""""}}) * WT with snappy (default, so no block_compressor specified) * WT with zlib ({{""""block_compressor=zlib""""}})  The only WT config that came close to the mmapv1 performance was zlib, and that was on the read from disk test.  So, I decided to test on SSD rather than spinning media, the result was that everything got a bit faster, but the relative differences remained - WT was still significantly slower.  For my initial testing methodology, since I was trying to demonstrate the benefits of compression for IO bandwidth savings, I had been clearing the caches on the system after each run.    Now that IO appeared to have no effect I decided to do consecutive runs of the collection scan, which would make the second run all in-memory (the collection is <16GiB and the test machine has 32GiB RAM, even with indexes it would fit in memory, but indexes are not in play)  *However* the collection scan was still slow with WiredTiger even when the data was already loaded into RAM.  The mmapv1 test dropped from the ~300 second range down to 13 seconds, but the WT testing showed no similar reduction - it did improve, but was still in the hundreds of seconds range rather than double digits.  I have tried tweaking cache_size, lsm, directio, readahead to no effect (the last two before I had ruled out IO issues completely), but no significant improvement either.  Basic initial graph attached, I will add detailed timing information, graphs, perf output below to avoid bloating the description too much.  """,Bug,"Storage,Performance"
494379,"""db version v2.6.4  I have ~100 entries (dump in attach) in my collection and try to do mapReduce on it with the following command  {code:javascript} db.features.mapReduce(   function () {     emit(this._quadKey.substr(0, 1), this );   },   function (key, values) {     var result = { q: [], ids: [] };     values.forEach(              function (it) {         result.q.push(it._quadKey);         result.ids.push(it._id);       }     );     return result;    },   {     query: { _quadKey: { $regex: '^(0|1|2|3)' } },     out: { inline: 1 }    } ) {code}  """"_quadKey"""" field which is used for grouping in Map function is a string of base-4 numbers that looks like """"33100321300223132221023""""  As a result of this command i got 5 reduce calls instead of 4 (see stat) and a """"<USER>"" value in last group with """"3"""" key and i got only 2 (except <USER> entries there (should be 20+).   {code:json} {         """"results"""" : [                 {                         """"_id"""" : """"0"""",                         """"value"""" : {                                 """"q"""" : [                                         """"00211222022320022310003"""", ...                                         """"03332223130001223231310""""                                 ],                                 """"ids"""" : [                                         ObjectId(""""545e7065eb6be425a700003a""""), ...                                         ObjectId(""""545e7052eb6be425a700002c"""")                                 ]                         }                 },                 {                         """"_id"""" : """"1"""",                         """"value"""" : {                                 """"q"""" : [                                         """"10033101320022232203331"""", ...                                         """"13331332032210022212221""""                                 ],                                 """"ids"""" : [                                         ObjectId(""""545e709beb6be425a700005f""""), ...                                         ObjectId(""""545e7043eb6be425a700001e"""")                                 ]                         }                 },                         """"_id"""" : """"2"""",                         """"value"""" : {                                 """"q"""" : [                                         """"20003331310022201013110"""", ...                                         """"23302313120113202111313""""                                 ],                                 """"ids"""" : [                                         ObjectId(""""545e7050eb6be425a700002a""""), ...                                         ObjectId(""""545e7063eb6be425a7000038"""")                                 ]                         }                 },                 {                         """"_id"""" : """"3"""",                         """"value"""" : {                                 """"q"""" : [                                         <USER>                                         """"33100321300223132221023"""",                                         """"33101232232201203113211""""                                 ],                                 """"ids"""" : [                                         <USER>  // Here it is!!!                                              ObjectId(""""545e7059eb6be425a7000031""""),                                         ObjectId(""""545e7081eb6be425a7000052"""")                                 ]                         }                 }         ],         """"timeMillis"""" : 12,         """"counts"""" : {                 """"input"""" : 102,                 """"emit"""" : 102,                 """"reduce"""" : 5,                 """"output"""" : 4         },         """"ok"""" : 1 {code}  Also I have noticed that group where I've got """"<USER>"" depends on index if I crete index for """"_quadKey"""" field I'he got <USER>in last key """"3"""" group {code:javascript} db.features.ensureIndex({ _quadKey: 1 }) {code}  If i drop this index I've got this issue in first """"0"""" group {code:javascript} db.features.dropIndex({ _quadKey: 1 }) {code}  {code:json} {         """"results"""" : [                 {                         """"_id"""" : """"0"""",                         """"value"""" : {                                 """"q"""" : [                                         <USER>                                         """"00222330102200231213001"""",                                         """"00211222022320022310003""""                                 ],                                 """"ids"""" : [                                         <USER>                                         ObjectId(""""545e701eeb6be425a7000004""""),                                         ObjectId(""""545e7065eb6be425a700003a"""")                                 ]                         }                 }, ...         ],         """"timeMillis"""" : 6,         """"counts"""" : {                 """"input"""" : 102,                 """"emit"""" : 102,                 """"reduce"""" : 5,                 """"output"""" : 4         },         """"ok"""" : 1 } {code}""",Bug,MapReduce
494831,"""So long as there are features that are prohibited, limited, or that underperform on a sharded cluster, it might be nice to let people spin up standalones and replica sets that prohibit or warn about the use of those features, so that the developer can ensure that they're not relying on features they'll have to give up when they choose to shard.    I'd envision this as a command-line flag that either disallows or logs when an application tries something that doesn't play well with sharding.    Broadly speaking, there are three categories of things I could imagine flagging   in such a mode:    # features that mongos unconditionally refuses to route (e.g., db.eval, $snapshot queries, $isolated updates, etc.),  # features that are disallowed on sharded collections because they play poorly with sharding key (unique indexes, single-document updates, etc.)   # queries that don't incorporate shard keys. [This might be overly broad. Perhaps it ought to be queries that don't incorporate shard keys and that supply a limit and/or that use an index? Basically, things that are attempting to be selective (and so are presumably things for which users want or expect low latency), however that might be operationally characterized.]    For the last two to work, it'd be necessary to configure a mongod that was started in the appropriate mode with fake shard key definitions for some collections. I have no opinions about how that ought to work, other than that it'd be nice for such fake shard key definitions to be both persistent and easy to delete in a non-sharded mongod.    Observation: even if the general """"fake sharding mode"""" idea were implemented piecemeal (i.e., not everything that ought to be caught were for initial iterations), it might still be incrementally useful.    Documentation changes needed: if implemented, must be documented.    CAP Verification: if implemented, then it ought to be tested.""",Improvement,Sharding
494901,"""Two things we have noticed, installing mongodb-win32-x86_64-2008plus-2.6.4.zip to Windows 8.1 Professional.  - if the target directory has a deep path (I had something under my Cygwin directory, creating a rather long Windows path) launching the services fails, but none of the error messages give any indication that the long path name would be the reason. Doing the same in another folder (s.a. C:\MongoDB) works without problems.  I would like you to document this, and if possible, make sure services would not even attempt to start, or they would give appropriate error messages. It may be a Windows issue, though, outside of your reach.  - Having the """"win32-x86_64-2008plus"""" in the installer name seems confusing to me:  I would like a 64-bit version, why is there """"win32"""" in the name?  Is it somehow optimized for """"2008plus""""?  I know that it works for pretty much anything. Then why the name? You could simply call these """"win64-2.6.4"""", couldn't you?  Our customers need to install MongoDB by themselves, so it's not just me, it's what we place to instructions for them.  My apologies for pushing two into one (bad bug management habits, I know). Just wanted you to be aware of these.""",Bug,Packaging
495238,"""Currently the DocumentSourceUnwind::getDependencies() method assumes that we need the whole array referenced by the $unwind, including all subfields. This is actually necessary to handle specific edge cases, such as when an array looks like \[\{subfield: 1, unneeded: """"something large and expensive""""}, """"Not an object""""]. In the current encoding scheme used by our dependency analyzer, if we were to depend on just """"array.subfield"""", the $unwind would emit only 1 document from that array rather than the currently specified 2. While that wouldn't effect the outcome of all $groups, it would for groups with a  {count: {$sum: 1}} clause in them. We can handle this correctly with a better encoding of dependencies in our analysis pass that would note that we depend on the exact length of the array, but only a subset of its contents.    If we add some way to encode that we only need the length of the array, {{$size}} should also use this.    h3. Original description:    A thread for this issue has already been started in SERVER-13703. Since it was not related anymore to the original issue of SERVER-13703, I open this new issue.     I have a collection that contains 10000 documents. Each document looks like the following:    {noformat}  {   """"_id"""" : NumberLong(1),    """"NonIndexedField"""" : NumberLong(1),    """"DateTime"""" : ISODate(""""2014-06-02T19:59:09.836Z""""),    """"Field1"""" : """"72eda069-1737-4db6-9367-8399ebdc0f8e"""",    """"Field2"""" : """"2e72ccdc-0e40-49b8-9d13-2b74855fd938"""",    """"Field3"""" : """"29e1caad-355e-4351-9d4b-f160225f41b3"""",    """"Field4"""" : """"ef6083c5-b919-411b-b942-d984b050d91f"""",    """"Field5"""" : """"39437c80-7423-4c50-b06f-59018b0437f5"""",    """"Field6"""" : """"35d2301f-cb96-4392-9084-ff057142f35b"""",    """"ChildrenLevel1"""" : [a fairly big array]  }  {noformat}    Each document is on average 512 kB big. So, it means that ChildrenLevel1 is quite a big array. In fact it contains only 10 elements but each of these elements contain a lot of other elements, sub arrays, etc.     If I try to find the max value of a top level field using the aggregation framework, it takes virtually 0ms. Here is the query that I use:    {noformat}  db.Demo.aggregate([    {$group: {              _id : 0,               max : {$max : """"$DateTime""""}             }    }   ])  {noformat}    If I try to do the same thing on a date that is within the ChildrenLevel1 array, it now takes 36 seconds at the minimal. The first time that I ran the query it took even longer since data was not in RAM. On the subsequent call, it took 36s. This makes for an average of 3.6 milliseconds per document. IMHO, this is quite long. Here is the query:    {noformat}  db.Demo.aggregate([    {$unwind: """"$ChildrenLevel1""""},    {$group: {              _id : 0,               max : {$max : """"$ChildrenLevel1.DateTime""""}             }    }   ])  {noformat}    I created a backup of my test database. This backup can be provided if you want, but I have to say that it is somewhat big. The compressed backup is about 1GB big. Instead of transferring this big backup, I can provide a little Windows command line executable that can be used to create the database.    What was interesting while creating the backup was to see how fast the operation was:    {noformat}  d:\MongoDb\bin>mongodump.exe --db MongoProjectPerformanceProblem --out d:\MongoDb\PerfProblem.direct.backup --dbpath d:\MongoDb\Data  2014-08-27T06:23:07.916-0400 DATABASE: MongoProjectPerformanceProblem    to     d:\MongoDb\PerfProblem.direct.backup\MongoProjectPerformanceProblem  2014-08-27T06:23:08.181-0400 [tools] query MongoProjectPerformanceProblem.system.indexes planSummary: COLLSCAN ntoreturn:0 ntoskip:0 nscanned:1 nscannedObjects:1 keyUpdates:0 numYields:0 locks(micros)   W:50769 r:199372 nreturned:1 reslen:110 108ms  2014-08-27T06:23:08.181-0400    MongoProjectPerformanceProblem.system.indexes to d:\MongoDb\PerfProblem.direct.backup\MongoProjectPerformanceProblem\system.indexes.bson  2014-08-27T06:23:08.181-0400             1 documents  2014-08-27T06:23:08.181-0400    MongoProjectPerformanceProblem.Demo to d:\MongoDb\PerfProblem.direct.backup\MongoProjectPerformanceProblem\Demo.bson  2014-08-27T06:23:09.834-0400 [tools] warning Listener::getElapsedTimeMillis returning 0ms  2014-08-27T06:23:11.004-0400 [tools]            Collection File Writing Progress: 3900/10000    39% (documents)  2014-08-27T06:23:11.114-0400 [tools] warning Listener::getElapsedTimeMillis returning 0ms  2014-08-27T06:23:12.486-0400 [tools] warning Listener::getElapsedTimeMillis returning 0ms  2014-08-27T06:23:13.875-0400 [tools] warning Listener::getElapsedTimeMillis returning 0ms  2014-08-27T06:23:14.000-0400 [tools]            Collection File Writing Progress: 8300/10000    83% (documents)  2014-08-27T06:23:15.029-0400             10000 documents  2014-08-27T06:23:15.513-0400    Metadata for MongoProjectPerformanceProblem.Demo to d:\MongoDb\PerfProblem.direct.backup\MongoProjectPerformanceProblem\Demo.metadata.json  2014-08-27T06:23:15.528-0400 [tools] dbexit:  2014-08-27T06:23:15.528-0400 [tools] shutdown: going to close listening sockets...  2014-08-27T06:23:15.528-0400 [tools] shutdown: going to flush diaglog...  2014-08-27T06:23:15.528-0400 [tools] shutdown: going to close sockets...  2014-08-27T06:23:15.528-0400 [tools] shutdown: waiting for fs preallocator...  2014-08-27T06:23:15.528-0400 [tools] shutdown: closing all files...  2014-08-27T06:23:15.981-0400 [tools] closeAllFiles() finished  2014-08-27T06:23:15.981-0400 [tools] shutdown: removing fs lock...  2014-08-27T06:23:15.981-0400 [tools] dbexit: really exiting now  {noformat}    As you can see, it took only about 8 seconds. So, reading the whole database from disk and then writing it to disk was 4.5 times faster than processing with the aggregation framework the same amount of data... and this data was already loaded in RAM. Can you please confirm that you think like me that the aggregation framework query should be much faster than the backup operation?    Note that all these tests were made on a machine that has lots of available memory. After my tests, there was still about 30 GB of unused RAM.    I have the feeling (I may be TOTALLY wrong here) that the aggregation framework copies the BSON documents contained in Mongo's memory map file into a set of new objects and that this is the cause of the performance degradation. Copying the object may not be as efficient as it can be. Can you confirm if I am wrong or right? I did a quick profiling of the source code and found out that a lot of the time was spent around the following code:    {noformat}  case Array: {     intrusive_ptr<RCVector> vec (new RCVector);     BSONForEach(sub, elem.embeddedObject()) {         vec->vec.push_back(Value(sub));     }     _storage.putVector(vec.get());     break;  }  {noformat}    Note that I would really like to contribute to resolving this issue. I am opened for discussion and/or to contribute on Mongo's source code. I really need this issue to be fixed because it is blocking us to go forward in dropping the costly SQL Server Enterprise + a Microsoft OLAP Cube. I really believe that Mongo's aggregation framework coupled with the memory mapped file can really compete (and probably win) against our current DataWarehouse and Cube.""",Improvement,Aggregation Framework
495268,"""It should be possible to configure processes like mongod and mongos, which sometimes need to read sensitive files like PEM and key files, to change the user they're running as after reading those files. If it is intended that mongod/mongos be able to run on privileged ports then binding to those ports should also happen before changing user.  The use case would be to allow sensitive files to be owned by root, start these processes as root, but then have them quickly change to running as an unprivileged user (e.g. mongodb) after reading the sensitive files or performing other privileged operations. Of particular interest would be the following files configurable through the command-line: * keyFile * sslPEMKeyFile * sslClusterFile  We can't protect against in-memory access, but we can still up the bar and make grabbing the secrets require the more sophisticated skill of accessing and decoding RAM rather than just using cat the key for cracking offline. Increasing security by requiring different skillsets to pull off an attack can be an effective technique. For example, the guy who knows how to exploit V8 and JavaScript would likely find it easy to then issue shell commands and use an offline cracking tool, but is less likely to know how to access and then decode RAM. _NOTE: I know of no V8/JavaScript exploit; I'm just using it as a plausible example of an attack that you might imagine someone pulling off who doesn't know how to or wouldn't think to try to read the data from RAM._ """,New Feature,Security
495522,"""I use the Java driver to query the database like this: {noformat} docdCursor = docDb.find(query, <USER>.batchSize(nFromServerLimit).setReadPreference(ReadPreference.secondaryPreferred()); {noformat}  In one particular cluster, I have a sharded DB with 1 shard of 2 replicas and an arbiter (see rs.status at the bottom) - the secondary is in state RECOVERING  I would therefore expect to see my reads all go to the primary. In fact they were being routed to the secondary resulting in errors returning from the above reads (referencing the fact that the queried node was recovering)  When I changed my config so that all reads went to the primary, the problems disappeared {noformat}         """"members"""" : [                 {                         """"_id"""" : 0,                         """"name"""" : """"192.168.8.106:27018"""",                         """"health"""" : 1,                         """"state"""" : 1,                         """"stateStr"""" : """"PRIMARY"""",                         """"uptime"""" : 133328,                         """"optime"""" : Timestamp(1407176295, 181),                         """"optimeDate"""" : ISODate(""""2014-08-04T18:18:15Z""""),                         """"lastHeartbeat"""" : ISODate(""""2014-08-04T18:21:43Z""""),                         """"lastHeartbeatRecv"""" : ISODate(""""2014-08-04T18:21:43Z""""),                         """"pingMs"""" : 0                 },                 {                         """"_id"""" : 1,                         """"name"""" : """"192.168.8.108:27018"""",                         """"health"""" : 1,                         """"state"""" : 3,                         """"stateStr"""" : """"RECOVERING"""",                         """"uptime"""" : 1729948,                         """"optime"""" : Timestamp(1406690192, 40),                         """"optimeDate"""" : ISODate(""""2014-07-30T03:16:32Z""""),                         """"maintenanceMode"""" : -16,                         """"errmsg"""" : """"still syncing, not yet to minValid optime 53db9bc5:34"""",                         """"self"""" : true                 },                 {                         """"_id"""" : 2,                         """"name"""" : """"192.168.8.106:27218"""",                         """"health"""" : 1,                         """"state"""" : 7,                         """"stateStr"""" : """"ARBITER"""",                         """"uptime"""" : 133328,                         """"lastHeartbeat"""" : ISODate(""""2014-08-04T18:21:42Z""""),                         """"lastHeartbeatRecv"""" : ISODate(""""2014-08-04T18:21:43Z""""),                         """"pingMs"""" : 0                 }         ], {noformat}    """,Bug,Querying
495746,"""Configured a mongod instance where I had dedicated a separate drive for storage and assigned it the drive letter M.  While trying to setup mongod as a Windows service, it would not take """"M:\"""" as a valid dbpath, But it would take """"M:"""".  For simplicity's sake (nothing else would be using the drive) I wanted to store all data at the root directory.  Things appeared to be working correctly because files were being allocated in M:\ as the databases grew.  However, upon inspection, all data was actually being written to C:\Windows\System32\m_<db name>.x and the files on M:\ were zeroed out and empty.  I created a directory on M for data and moved the datafiles from C to M with no apparent loss of data.""",Bug,Storage
495762,"""Hi,  I have a problem with one of our shards. For some reason I'm getting the next usage  15:38:42 up 4 days, 10:28,  1 user,  load average: 3.67, 3.50, 3.56 In the mean time all other shards are ok. Their average load approx:  16:49:58 up 11 days, 56 min,  1 user,  load average: 0.03, 0.03, 0.05 I have 3config, 2 query and 7 shards (all of them are primary replicaset. don't ask why). I have 2 databases: 20Gb and 30gb. All servers are the same in their configuration. Also, in each database only 1 collection. The problem with inapropritae shard key. Please let me know what will be better:   a) make backups of current 2 databases, destroy them via mongos shell, create new databases with correct shard keys and upload our dumps b) follow with suggestion on StackExchange from on of your staff and Unshard collection?  Our cluster is working as a test environment. Thank you.""",Question,Sharding
496055,"""Hi,   I have downloaded the source code of Mongo 2.6.1 and I tried to compile it under VS2010.  Here is how I downloaded the code:    git clone git://github.com/mongodb/mongo.git   cd mongo   git checkout r2.6.1  I then opened a VS 2010 command prompt and executed the following commands:    cd src   msbuild vs2010_mongodb.sln /p:Configuration=Win2008PlusRelease /p:Platform=x64 > out.txt  The compilation failed. See the attached log file. Note that I was able to compile it with scons. But, since what I wanted to do was to use the VS profiler against Mongo to understand a performance degradation of the aggregation framework, this does not help me. It needs to be compilable in VS for the profiler to be usable.  Regards, Maxime""",Bug,Build
496147,"""Currently the shell processes --eval prior to any .js files on the command line.  Is there any particular reason this order is used?  I find it confusing and backwards to what I expect, probably because I think of --eval as a non-interactive replacement for the interactive REPL (unless --shell is used), and when .js files are specified without --eval they are loaded prior to presenting the prompt.    My use case is a library of code contained in a .js file which defines some functions that I can then call from --eval.  The code isn't general enough that I want to have it always loaded in my ~/.mongorc.js file.  I can work around it by using load() from inside --eval, but that's ugly.    {code}  $ cat foo.js  print(""""foo"""");  foo = function () { print(""""bar""""); }  $ mongo foo.js --eval 'foo(); print(""""baz"""")'  MongoDB shell version: 2.4.9  connecting to: test  Wed May 28 12:16:43.912 ReferenceError: foo is not defined  $ mongo --eval 'load(""""foo.js""""); foo(); print(""""baz"""")'  MongoDB shell version: 2.4.9  connecting to: test  foo  bar  baz  $   {code}""",Improvement,Shell
496410,"""MongoDB 2.6 introduced the .maxTimeMS() cursor method, which allows you to specify a max running time for each query. This is awesome for ad-hoc queries, but I wondered if there was a way to set this value on a per-instance or per-database (or even per-collection) level, to try and prevent locking in general.    And if so, could that value then be OVERWRITTEN on a per-query basis? I would love to set an instance level timeout of 3000ms or thereabouts (since that would be a pretty extreme running time for queries issued by my application), but then be able to ignore it if I had a report to run.""",Improvement,"Stability,Admin"
496600,"""{panel:title=Issue Status as of April 17, 2014|borderColor=#ccc|titleBGColor=#6CB33F|bgColor=#EEEEEE}  *ISSUE SUMMARY* The {{OplogReplay}} query flag is an internal optimization for queries on the oplog (and other capped collections) using the {{ts}} (timestamp) field. Version 2.6.0 introduced a bug whereby a query using the {{OplogReplay}} flag with additional predicates not on the {{ts}} field could return incorrect results.   *USER IMPACT* Internally, replication issues queries with predicates only on the {{ts}} field, and thus is unaffected by the bug. Third-party tools or other user implementations to query or tail the oplog can break, if they use the query flag and have additional query predicates.  *WORKAROUNDS* A query without the {{OplogReplay}} query flag would return correct results but may be less efficient as it would not employ the OplogReplay optimization.  *RESOLUTION* The fix restores the behavior to ignore predicates on other <USER>when walking the collection backwards to find the earliest oplog entry that matches the query. This brings the behavior back in line with that of version 2.4.x. Additionally, validation was added to ensure that the predicate on {{ts}} is either {{$gt}} or {{$gte}}, as the OplogReplay optimization does not work with other operators.  *AFFECTED VERSIONS* Version 2.6.0 was affected by this bug.  *PATCHES* The patch is included in the 2.6.1 production release.  {panel}  h6. Original description  Querying the oplog on a 2.6.0 secondary with the oplog replay flag and another predicate in addition to """"ts"""" gives incorrect results vs. a 2.4.8 primary.  The query returns correct results if """"ts"""" is the only predicate in the find.  query: db.oplog.rs.find({ts : {$gte : Timestamp(1397347250,1)}, """"o2._id"""" : ObjectId(""""52a41544e4b04cb2bcc59814"""")}).addOption(8).limit(1)  I do not have a 2.4 secondary to test this on now, but I've issued similar queries on 2.4 secondaries in the past and they returned correct results.""",Bug,Querying
496603,"""{panel:title=Issue Status as of April 15, 2014|borderColor=#ccc|titleBGColor=#6CB33F|bgColor=#EEEEEE}  *ISSUE SUMMARY* Queries that use tailable cursors do not stream results if {{.skip()}} or negative values for {{.limit()}} are applied but instead close the cursor after returning the first batch.  *USER IMPACT* Internal operations (replication etc.) are not affected but user-issued queries using tailable cursors may return incomplete results.  *WORKAROUNDS* Try avoiding {{.skip()}} with tailable cursors and use other query predicates (e.g. {{$gt}}) if possible.  *RESOLUTION* Tailable cursors now correctly continue to return batches via the {{getmore}} command when combined with {{.skip()}} or negative values of {{.limit()}}.  *AFFECTED VERSIONS* Version 2.6.0 is affected by this bug.  *PATCHES* The patch is included in the 2.6.1 production release.  {panel}  h6. Original description  Queries that use tailable cursors do not stream results if skip() is applied.   e.g. {code} db.foo.find().addOption(34).skip(1) {code}  Original description: {quote} A capped collection created by an older version of the server can no longer be 'tracked' by a tailable cursor in 2.6.0; there was no issue with these collections using tailable cursors in v2.4.9.  The capped collection being used was probably created pre-v2.2, but I'm not sure and can't find a way to determine.  This collection did not have an index on '_id', which was added prior to the v2.6.0. upgrade.  This is not an issue if the capped collection is created with v2.6.0.  Steps to Reproduce:  Using a capped collection created in an older version of the server, e.g. 'test.cap1', attempt to track the inserts to this collection using two mongodb shell sessions.  In the first shell, perform inserts, e.g.,  use test db.cap1.insert( \{ 'msg' : 'test' \} )  In the second shell, attempt to track new insertions, e.g.,  use test db.cap1.find().addOption(34)  // 2 = tailable, 32 = await_data  When running with 2.6.0, no updates are displayed.  If a new capped collection is created using v2.6.0, and the shell commands above are run, the updates are displayed. {quote}""",Bug,Querying
496613,"""I need to find the biggest value for the field """"tourney"""" in the collection """"games"""". I have added an appropriate index which was automatically used by all MongoDB versions prior to 2.6. In 2.6. MongoDB doesn't use the index but does a full table scan instead. Given the right hint the query uses the index and works as expected. Here are more details.   I have a sparse index in the """"games"""" collection: {code}  {   """"v"""" : 1,   """"key"""" : {    """"tourney"""" : 1,    """"tourneyRd"""" : 1   },   """"ns"""" : """"server.games"""",   """"name"""" : """"tourney_1_tourneyRd_1"""",   """"sparse"""" : true  } {code}  This query does a full table scan in MongoDB 2.6.0: {code} db.games.find({},{tourney:1}).sort({tourney:-1}).limit(1).explain() {  """"clauses"""" : [   {    """"cursor"""" : """"BasicCursor"""",    """"isMultiKey"""" : false,    """"n"""" : 1,    """"nscannedObjects"""" : 12619952,    """"nscanned"""" : 12619952,    """"scanAndOrder"""" : true,    """"indexOnly"""" : false,    """"nChunkSkips"""" : 0   },   {    """"cursor"""" : """"BasicCursor"""",    """"isMultiKey"""" : false,    """"n"""" : 0,    """"nscannedObjects"""" : 0,    """"nscanned"""" : 0,    """"scanAndOrder"""" : true,    """"indexOnly"""" : false,    """"nChunkSkips"""" : 0   }  ],  """"cursor"""" : """"QueryOptimizerCursor"""",  """"n"""" : 1,  """"nscannedObjects"""" : 12619952,  """"nscanned"""" : 12619952,  """"nscannedObjectsAllPlans"""" : 12619952,  """"nscannedAllPlans"""" : 12619952,  """"scanAndOrder"""" : false,  """"nYields"""" : 98593,  """"nChunkSkips"""" : 0,  """"millis"""" : 77564,  """"server"""" : """"test:27017"""",  """"filterSet"""" : false } {code}  Adding a hint does the correct query:  {code} db.games.find({},{tourney:1}).sort({tourney:-1}).limit(1).hint({tourney:1,tourneyRd:1}).explain() {  """"cursor"""" : """"BtreeCursor tourney_1_tourneyRd_1 reverse"""",  """"isMultiKey"""" : false,  """"n"""" : 1,  """"nscannedObjects"""" : 1,  """"nscanned"""" : 2,  """"nscannedObjectsAllPlans"""" : 1,  """"nscannedAllPlans"""" : 2,  """"scanAndOrder"""" : false,  """"indexOnly"""" : false,  """"nYields"""" : 0,  """"nChunkSkips"""" : 0,  """"millis"""" : 0,  """"indexBounds"""" : {   """"tourney"""" : [    [     {      """"$maxElement"""" : 1     },     {      """"$minElement"""" : 1     }    ]   ],   """"tourneyRd"""" : [    [     {      """"$maxElement"""" : 1     },     {      """"$minElement"""" : 1     }    ]   ]  },  """"server"""" : """"test:27017"""",  """"filterSet"""" : false } {code}  A work-around for me right now is to avoid the empty query with:  {code} db.games.find({tourney:{$gt:0}},{tourney:1}).sort({tourney:-1}).limit(1).explain() {  """"cursor"""" : """"BtreeCursor tourney_1_tourneyRd_1 reverse"""",  """"isMultiKey"""" : false,  """"n"""" : 1,  """"nscannedObjects"""" : 1,  """"nscanned"""" : 2,  """"nscannedObjectsAllPlans"""" : 1,  """"nscannedAllPlans"""" : 2,  """"scanAndOrder"""" : false,  """"indexOnly"""" : false,  """"nYields"""" : 0,  """"nChunkSkips"""" : 0,  """"millis"""" : 0,  """"indexBounds"""" : {   """"tourney"""" : [    [     Infinity,     0    ]   ],   """"tourneyRd"""" : [    [     {      """"$maxElement"""" : 1     },     {      """"$minElement"""" : 1     }    ]   ]  },  """"server"""" : """"test:27017"""",  """"filterSet"""" : false } {code}""",Bug,"Querying,Indexing"
496791,"""Hi,  I'm using db.collection.find().forEach() to modify collection, rearranging <USER>inside, also using 'delete obj.property'.   and depending on collection size, when it grows larger than  some small threshold, (in my tests 1000), some <USER>of modified objects are set to undefiend as result, istead of actual numbers.  i was able to reproduce it in both MongoDB version: 2.4.4, 2.4.8, 2.6.0-rc2   """,Bug,JavaScript
496863,"""Three member replica set, running version 2.4.7. Secondary died with an assert failure on invalid document size (Size must be between 0 and 16793600(16MB)). Two attempts to restart the secondary failed with the same error, on the same document. No issues at all with other secondary or primary. Ended up creating new storage volume and performing a resync to bring it back online.  I've included the info from the mongodb.log below. The document that had the invalid document size had just been inserted into the primary and replicated to the secondaries. Document looks fine on the primary and the surviving secondary. Actual document size is about 3kb.  Log info:  Wed Mar 19 01:05:55.184 [repl prefetch worker] Assertion: 10334:BSONObj size: 1811939328 (0x0000006C) is invalid. Size must be between 0 and 16793600(16MB) First element: Bo²: ?type=111 0xde0151 0xda188b 0xda1dcc 0x6ec92f 0xa2aad8 0xa2bfcc 0xa2c370 0x7fbcf0 0x7fc19b 0x7fc285 0x7fc285 0x7fc285 0x7fc285 0x7fc4c1 0x9d6aca 0x9dd34f 0xb329c4 0xb33865 0xc254b2 0xdada91  /usr/bin/mongod(_ZN5mongo15printStackTraceERSo+0x21) [0xde0151]  /usr/bin/mongod(_ZN5mongo11msgassertedEiPKc+0x9b) [0xda188b]  /usr/bin/mongod() [0xda1dcc]  /usr/bin/mongod(_ZNK5mongo7BSONObj14_assertInvalidEv+0x5bf) [0x6ec92f]  /usr/bin/mongod(_ZNK5mongo5KeyV16toBsonEv+0x98) [0xa2aad8]  /usr/bin/mongod(_ZNK5mongo5KeyV113compareHybridERKS0_RKNS_8OrderingE+0x3c) [0xa2bfcc]  /usr/bin/mongod(_ZNK5mongo5KeyV19woCompareERKS0_RKNS_8OrderingE+0x230) [0xa2c370]  /usr/bin/mongod(_ZNK5mongo11BtreeBucketINS_12BtreeData_V1EE4findERKNS_12IndexDetailsERKNS_5KeyV1ERKNS_7DiskLocERKNS_8OrderingERib+0x120) [0x7fbcf0]  /usr/bin/mongod(_ZNK5mongo11BtreeBucketINS_12BtreeData_V1EE13insertStepOneENS_7DiskLocERNS_30IndexInsertionContinuationImplIS1_EEb+0x6b) [0x7fc19b]  /usr/bin/mongod(_ZNK5mongo11BtreeBucketINS_12BtreeData_V1EE13insertStepOneENS_7DiskLocERNS_30IndexInsertionContinuationImplIS1_EEb+0x155) [0x7fc285]  /usr/bin/mongod(_ZNK5mongo11BtreeBucketINS_12BtreeData_V1EE13insertStepOneENS_7DiskLocERNS_30IndexInsertionContinuationImplIS1_EEb+0x155) [0x7fc285]  /usr/bin/mongod(_ZNK5mongo11BtreeBucketINS_12BtreeData_V1EE13insertStepOneENS_7DiskLocERNS_30IndexInsertionContinuationImplIS1_EEb+0x155) [0x7fc285]  /usr/bin/mongod(_ZNK5mongo11BtreeBucketINS_12BtreeData_V1EE13insertStepOneENS_7DiskLocERNS_30IndexInsertionContinuationImplIS1_EEb+0x155) [0x7fc285]  /usr/bin/mongod(_ZNK5mongo11BtreeBucketINS_12BtreeData_V1EE13twoStepInsertENS_7DiskLocERNS_30IndexInsertionContinuationImplIS1_EEb+0x1a1) [0x7fc4c1]  /usr/bin/mongod(_ZN5mongo18IndexInterfaceImplINS_12BtreeData_V1EE20beginInsertIntoIndexEiRNS_12IndexDetailsENS_7DiskLocERKNS_7BSONObjERKNS_8OrderingEb+0xda) [0x9d6aca]  /usr/bin/mongod(_ZN5mongo19fetchIndexInsertersERSt3setINS_7BSONObjENS_10BSONObjCmpESaIS1_EERNS_14IndexInterface13IndexInserterEPNS_16NamespaceDetailsEiRKS1_NS_7DiskLocEb+0x2cf) [0x9dd34f]  /usr/bin/mongod(_ZN5mongo18prefetchIndexPagesEPNS_16NamespaceDetailsERKNS_7BSONObjE+0x724) [0xb329c4]  /usr/bin/mongod(_ZN5mongo28prefetchPagesForReplicatedOpERKNS_7BSONObjE+0x605) [0xb33865]  /usr/bin/mongod(_ZN5mongo7replset8SyncTail10prefetchOpERKNS_7BSONObjE+0x202) [0xc254b2]  /usr/bin/mongod(_ZN5mongo10threadpool6Worker4loopEv+0x281) [0xdada91] Wed Mar 19 01:05:55.333 [repl writer worker 1] Assertion: 10334:BSONObj size: 1811939328 (0x0000006C) is invalid. Size must be between 0 and 16793600(16MB) First element: Bo²: ?type=111 0xde0151 0xda188b 0xda1dcc 0x6ec92f 0xa2aad8 0xa2bfcc 0xa2c370 0x7fbcf0 0x7fc19b 0x7fc285 0x7fc285 0x7fc285 0x7fc285 0x7fc4c1 0x9d6aca 0x9dd34f 0x9df975 0xac5b7d 0xac6e7f 0xa9198a  /usr/bin/mongod(_ZN5mongo15printStackTraceERSo+0x21) [0xde0151]  /usr/bin/mongod(_ZN5mongo11msgassertedEiPKc+0x9b) [0xda188b]  /usr/bin/mongod() [0xda1dcc]  /usr/bin/mongod(_ZNK5mongo7BSONObj14_assertInvalidEv+0x5bf) [0x6ec92f]  /usr/bin/mongod(_ZNK5mongo5KeyV16toBsonEv+0x98) [0xa2aad8]  /usr/bin/mongod(_ZNK5mongo5KeyV113compareHybridERKS0_RKNS_8OrderingE+0x3c) [0xa2bfcc]  /usr/bin/mongod(_ZNK5mongo5KeyV19woCompareERKS0_RKNS_8OrderingE+0x230) [0xa2c370]  /usr/bin/mongod(_ZNK5mongo11BtreeBucketINS_12BtreeData_V1EE4findERKNS_12IndexDetailsERKNS_5KeyV1ERKNS_7DiskLocERKNS_8OrderingERib+0x120) [0x7fbcf0]  /usr/bin/mongod(_ZNK5mongo11BtreeBucketINS_12BtreeData_V1EE13insertStepOneENS_7DiskLocERNS_30IndexInsertionContinuationImplIS1_EEb+0x6b) [0x7fc19b]  /usr/bin/mongod(_ZNK5mongo11BtreeBucketINS_12BtreeData_V1EE13insertStepOneENS_7DiskLocERNS_30IndexInsertionContinuationImplIS1_EEb+0x155) [0x7fc285]  /usr/bin/mongod(_ZNK5mongo11BtreeBucketINS_12BtreeData_V1EE13insertStepOneENS_7DiskLocERNS_30IndexInsertionContinuationImplIS1_EEb+0x155) [0x7fc285]  /usr/bin/mongod(_ZNK5mongo11BtreeBucketINS_12BtreeData_V1EE13insertStepOneENS_7DiskLocERNS_30IndexInsertionContinuationImplIS1_EEb+0x155) [0x7fc285]  /usr/bin/mongod(_ZNK5mongo11BtreeBucketINS_12BtreeData_V1EE13insertStepOneENS_7DiskLocERNS_30IndexInsertionContinuationImplIS1_EEb+0x155) [0x7fc285]  /usr/bin/mongod(_ZNK5mongo11BtreeBucketINS_12BtreeData_V1EE13twoStepInsertENS_7DiskLocERNS_30IndexInsertionContinuationImplIS1_EEb+0x1a1) [0x7fc4c1]  /usr/bin/mongod(_ZN5mongo18IndexInterfaceImplINS_12BtreeData_V1EE20beginInsertIntoIndexEiRNS_12IndexDetailsENS_7DiskLocERKNS_7BSONObjERKNS_8OrderingEb+0xda) [0x9d6aca]  /usr/bin/mongod(_ZN5mongo19fetchIndexInsertersERSt3setINS_7BSONObjENS_10BSONObjCmpESaIS1_EERNS_14IndexInterface13IndexInserterEPNS_16NamespaceDetailsEiRKS1_NS_7DiskLocEb+0x2cf) [0x9dd34f] /usr/bin/mongod(_ZN5mongo24indexRecordUsingTwoStepsEPKcPNS_16NamespaceDetailsENS_7BSONObjENS_7DiskLocEb+0x175) [0x9df975]  /usr/bin/mongod(_ZN5mongo11DataFileMgr6insertEPKcPKvibbbPb+0x123d) [0xac5b7d]  /usr/bin/mongod(_ZN5mongo11DataFileMgr16insertWithObjModEPKcRNS_7BSONObjEbb+0x4f) [0xac6e7f]  /usr/bin/mongod(_ZN5mongo14_updateObjectsEbPKcRKNS_7BSONObjES4_bbbRNS_7OpDebugEPNS_11RemoveSaverEbRKNS_24QueryPlanSelectionPolicyEb+0x2eda) [0xa9198a] Wed Mar 19 01:05:55.345 [repl writer worker 1] ERROR: writer worker caught exception: BSONObj size: 1811939328 (0x0000006C) is invalid. Size must be between 0 and 16793600(16MB) First element: Bo²: ?type=111 on: { ts: Timestamp 1395191155000|19, h: 8957154836310535201, v: 2, op: """"i"""", ns:   <<<<Document Data Removed - size was about 3kb >>>>  Wed Mar 19 01:05:55.345 [repl writer worker 1]   Fatal Assertion 16360 0xde0151 0xd9ff43 0xc28aac 0xdada91 0xe289d9 0x7f9c4d405e9a 0x7f9c4c7183fd  /usr/bin/mongod(_ZN5mongo15printStackTraceERSo+0x21) [0xde0151]  /usr/bin/mongod(_ZN5mongo13fassertFailedEi+0xa3) [0xd9ff43]  /usr/bin/mongod(_ZN5mongo7replset14multiSyncApplyERKSt6vectorINS_7BSONObjESaIS2_EEPNS0_8SyncTailE+0x12c) [0xc28aac]  /usr/bin/mongod(_ZN5mongo10threadpool6Worker4loopEv+0x281) [0xdada91]  /usr/bin/mongod() [0xe289d9]  /lib/x86_64-linux-gnu/libpthread.so.0(+0x7e9a) [0x7f9c4d405e9a]  /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f9c4c7183fd] Wed Mar 19 01:05:55.350 [repl writer worker 1]  ***aborting after fassert() failure """,Bug,Replication
497025,"""The text search tokenizer considers only the characters {{\\\f\v\t\r\n\'~`!@#$%^&*(-=+[]{}|;:""""<>,. /?}} as token delimiters.  As such, words adjacent to a curly quote (or an emdash, etc) won't be recognized as a stopword or indexed under the correct term.  The set of recognized delimiters needs to be expanded and made unicode-aware.  Original ticket description: {quote} I find that directional quotes can affect the ranking of search terms in the $text search.  This is a problem because our ranking is supposed to look at words, and should ignore symbols.  Steps to reproduce:  I have the following document*:  {code} {   """"_id"""" : {     """"chapter"""" : """"23"""",     """"bookname"""" : """"Ezekiel"""",     """"verse"""" : """"2""""     },   """"text"""" : """"“Son of man, there were two women who were daughters of the same mother."""" } {code} (note that the text character that appears at the beginning of the string in the """"text"""" field is not a standard quote I get from shift-"""")  When I run the following query:  {code} > db.bible.runCommand(""""text"""", { search : """"Israel \""""Son of man\""""""""} ) {code}  the text gets a score of """"score"""" : 0.5833333333333334  But, when I remove that weird quite and re-insert the document under a different _id, it gets a score of """"score"""" : 1.1666666666666667  I would expect them both to get the same score, and for that score to be greater than 1, since it matches one of my search strings.    * Sorry for the heavily <USER>text in the example, I found a random bible verse generating JSON API and just ran with it today.  {quote}""",Improvement,Text Search
497112,"""I have a mongo server with ssl enabled. On this server I want to import data that is present on another server. So I want to mongodump and them mongorestore the data. Here is the mongodump command I ran  mongodump -h <host> -d <db> -u <user> -p <pass>.  I always see the following error assertion: 18 { code: 18, ok: 0.0, errmsg: """"auth fails"""" }  I was able to run this command fine on other servers.""",Bug,Tools
497400,"""Very many MongoDB, Inc. """"field"""" engagements (support, consulting, onboarding, and pre-sales interactions) require collecting information about a MongoDB deployment in order to understand, debug, tune, etc.  A quantity of the information that is desirable in these engagements are host-level properties (hardware details, OS version, library versions, OS settings, etc.) At the moment, collecting this information is easy for an individual host, but wrangling the information for a whole cluster becomes tedious in proportion to the number of hosts running MongoDB server processes. In consequence, customer interactions can become delayed and sidetracked on these data collection steps.  Proposal: (1) add code to mongod and mongos processes to collect the kinds of host-level information the MongoDB, Inc. field organization often wants to gather, and (2) present that information to (suitably authorized) clients via some command (maybe serverStatus, maybe some other existing command, maybe a new command; I don't think it matters).  Benefits: streamlining the collection of diagnostic information will improve efficiency of field activities (support, services, pre-sales, etc.)  Disadvantages: more code, all of it OS-dependent. (However, it ought be possible to implement in a very modular, incremental fashion.)  Documentation changes needed: if the feature is added, and if the feature is not considered """"internal only"""", document the feature.  Driver changes needed: if it's considered desirable to have driver bindings to the proposed command, design, implement, and document them.  Example: a sample Bourne shell script the MongoDB support team has used for Linux follows. This is intended to enumerate the kinds of things we tend to gather, not as a concrete proposal for how to collect the info or present it back to users/client applications.  (Note: the info this script collects includes static information that only needs to be gathered once per process lifetime (OS kernel version, libraries, CPU info) and dynamic state that will change while the process runs (e.g., free disk space, I/O statistics). If this request needs to be broken up into subtasks, I'd propose that a first pass would be to collect the only static info at server startup time; a second or subsequent pass could gather the dynamic info, perhaps on a periodic basis.)  {code} echo """"MongoDB Diag"""" > /tmp/system.data mlog() {     echo -e """"\n\n\n $1 \n ======================="""" >> /tmp/system.data     shift 1     $@ 2>/dev/<USER>>> /tmp/system.data }  mlog uname uname -a                           mlog date date                               mlog blockdev /sbin/blockdev --report            mlog libraries ls -l /lib/libc.so*                mlog libraries /lib/libc.so*                     mlog lsb lsb_release -a                   mlog sysctl /sbin/sysctl -a                 mlog ifconfig /sbin/ifconfig -a              mlog dmesg dmesg                         mlog lspci lspci -vvv                   mlog mount mount                       mlog whoami whoami                     mlog ulimit ulimit -a                 mlog df df -h                    mlog cpuinfo cat /proc/cpuinfo       mlog meminfo cat /proc/meminfo      mlog swaps cat /proc/swaps       mlog modules cat /proc/modules    mlog vmstat cat /proc/vmstat    mlog top -b -n 10                     mlog iostat iostat -xtm 5 10                mlog rpcinfo /usr/sbin/rpcinfo -p           mlog scsidevices cat /sys/bus/scsi/devices/*/model  mlog proc/limits for i in `pgrep mongo`; do echo """"PID: $i""""; cat /proc/$i/cmdline; echo -e """"\nLimits:\n""""; cat /proc/$i/limits; echo; done >> /tmp/system.data mlog smartctl /usr/sbin/smartctl --scan | sed -e 's/#.*$//' | while read i; do /usr/sbin/smartctl --all $i; done >> /tmp/system.data mlog nr_requests for d in `find /sys -name nr_requests`; do (echo $d; cat $d); done  >> /tmp/system.data mlog read_ahead_kb for d in `find /sys -name read_ahead_kb`; do (echo $d; cat $d); done   >> /tmp/system.data mlog scheduler for d in `find /sys -name scheduler`; do (echo $d; cat $d); done   >> /tmp/system.data {code} """,New Feature,Diagnostics
497416,"""Running an """"unset"""" on a key inside of array objects says it updated documents but it didn't.  Example doc: {code} {         """"_id"""" : ObjectId(""""52c5e5d2bd546795dee87ca4""""),         """"c"""" : [                 {""""pi"""" : NumberLong(2368999)},                 {""""pi"""" : NumberLong(2368999)},                 {""""pi"""" : NumberLong(2368999)}         ] } {code} Example query: {code}db.example.update({_id: ObjectId(""""52c5e5d2bd546795dee87ca4"""")}, {$unset: {""""c.pi"""": 1}}){code}  When I run that query using the PHP driver it returns: """"updatedExisting""""= > true """"n"""" => 1  However nothing was actually unset. I know there's another bug filed (SERVER-1243) for a feature request to use a position operator on multiple items, but this should have affected ALL items in the collection, I'm not using $elemMatch or anything else. Is that not supported?  I said affects versions 2.2.3 because that's all we have running at Grooveshark but it might affect newer versions.""",Bug,Write Ops
497444,"""I'm running 2.5.5 as a hidden replica in a replSet that is composed of 2.4.6 instances. About once a day the server segfaults. I've included the backtrace and some of the log prior to the segfault. Let me know if I there is any additional information I can provide that would be useful.  {noformat} Invalid access at address: 0 from thread: rsSyncNotifier Got signal: 11 (Segmentation fault). Backtrace: 0x114d711 0x751cba 0x757402 0x7f090424dcb0 0x1100c2d 0x1102086 0x1102671 0x7b20a5 0x7dc027 0x7dc  /usr/bin/mongod(_ZN5mongo15printStackTraceERSo+0x21) [0x114d711]  /usr/bin/mongod(_ZN5mongo10abruptQuitEi+0x3ba) [0x751cba]  /usr/bin/mongod(_ZN5mongo24abruptQuitWithAddrSignalEiP7siginfoPv+0x282) [0x757402]  /lib/x86_64-linux-gnu/libpthread.so.0(+0xfcb0) [0x7f090424dcb0]  /usr/bin/mongod(_ZN5mongo13MessagingPort4recvERNS_7MessageE+0x87d) [0x1100c2d]  /usr/bin/mongod(_ZN5mongo13MessagingPort4recvERKNS_7MessageERS1_+0x36) [0x1102086]  /usr/bin/mongod(_ZN5mongo13MessagingPort4callERNS_7MessageES2_+0x31) [0x1102671]  /usr/bin/mongod(_ZN5mongo18DBClientConnection4callERNS_7MessageES2_bPSs+0x55) [0x7b20a5]  /usr/bin/mongod(_ZN5mongo14DBClientCursor11requestMoreEv+0x397) [0x7dc027]  /usr/bin/mongod(_ZN5mongo14DBClientCursor4moreEv+0x4d) [0x7dc4ed]  /usr/bin/mongod(_ZN5mongo7replset14BackgroundSync9markOplogEv+0xb3e) [0xd8d00e]  /usr/bin/mongod(_ZN5mongo7replset14BackgroundSync14notifierThreadEv+0x15e) [0xd8f8ce]  /usr/bin/mongod() [0x1192459]  /lib/x86_64-linux-gnu/libpthread.so.0(+0x7e9a) [0x7f0904245e9a]  /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f09035583fd] {noformat}  There were also some interesting logs prior to the segfault:  {noformat} 2014-02-06T13:34:45.883-0500 [rsBackgroundSync] Socket recv() timeout  10.145.170.96:27017 2014-02-06T13:34:45.883-0500 [rsBackgroundSync] SocketException: remote: 10.145.170.96:27017 error: 9001 socket exception [RECV_TIMEOUT] server [10.145.170.96:27017]  2014-02-06T13:34:45.883-0500 [rsBackgroundSync] DBClientCursor::init call() failed 2014-02-06T13:35:08.642-0500 [rsBackgroundSync] upstream updater is unsupported on this version 2014-02-06T13:35:08.911-0500 [rsSyncNotifier] replset setting oplog notifier to [server]:27017 2014-02-06T13:35:08.914-0500 [rsSyncNotifier] upstream updater is unsupported on this version 2014-02-06T13:35:08.922-0500 [rsBackgroundSync] upstream updater is unsupported on this version 2014-02-06T13:35:08.995-0500 [rsBackgroundSync] upstream updater is unsupported on this version 2014-02-06T13:35:09.174-0500 [rsBackgroundSync] upstream updater is unsupported on this version 2014-02-06T13:35:09.219-0500 [rsBackgroundSync] upstream updater is unsupported on this version 2014-02-06T13:35:09.240-0500 [rsBackgroundSync] upstream updater is unsupported on this version 2014-02-06T13:35:09.250-0500 [rsBackgroundSync] upstream updater is unsupported on this version 2014-02-06T13:35:09.259-0500 [rsBackgroundSync] upstream updater is unsupported on this version 2014-02-06T13:35:09.269-0500 [rsBackgroundSync] upstream updater is unsupported on this version 2014-02-06T13:35:09.294-0500 [rsBackgroundSync] upstream updater is unsupported on this version 2014-02-06T13:35:09.307-0500 [rsBackgroundSync] upstream updater is unsupported on this version 2014-02-06T13:35:09.322-0500 [rsBackgroundSync] upstream updater is unsupported on this version 2014-02-06T13:35:09.334-0500 [rsBackgroundSync] upstream updater is unsupported on this version 2014-02-06T13:35:09.344-0500 [rsBackgroundSync] upstream updater is unsupported on this version 2014-02-06T13:35:09.391-0500 [rsBackgroundSync] upstream updater is unsupported on this version {noformat}""",Bug,Replication
497476,"""I'm running an incremental map reduce over a large data set in a sharded environment. The output is set to """"reduce"""".    This strategy works fine until the batch I'm running the MR on exceeds ~14Million documents. At which point the MR will fail with error code 10334 saying:    """"MR Parallel Processing failed. errmsg: 'Exception: BSONObj size 16951756 ... is invalid ...""""    I was under the impression that there are no size concerns when it comes to map/reduce. (Assuming of course your document size doesn't exceed 16MB. All of the documents I'm dealing with are ~400bytes).    It doesn't fail immediately and I suspect it's the cumulative data from one shard or another that is exceeding this size. I wasn't aware that this was an issue with MR in sharded environments.     Any ideas what's going on?""",Bug,MapReduce
497580,"""I have to manage 1 million geo-points localized all over the earth.  For performance reason, I need to group the points which are in the same """"cells"""". I have implemented an algorithm wich computes for each point, its geohash value, so that it is quite easy, using the aggregate framework and functions like $substr, to group and count the points which are in the same cell.    But the documentation says Mongo already computes geoHash values from geo points given as [lon, lat].     It would be nice if this internally computed geoHash value could be returned (as a readonly attribute ?) with the documents.    """,New Feature,Querying
497807,"""Our build system declares that tcmalloc is the default allocator for 64-bit builds, and that the system allocator is the default for 32-bit builds. However, it accomplishes this by examining the host architecture, rather than the target architecture.  As a result, when building on a 64-bit machine with the --32 flag, tcmalloc is enabled. However, our tcmalloc SConscript does not include all of the files necessary to build tcmalloc in 32-bit mode, in particular the files base/vdso_support and base/elf_mem_image.cc.  Being able to do 32 bit builds on a 64 bit machine would be nice. We should either:  1) Change the selection rules so that disabling tcmalloc is done based on the target, rather than the host architecture. 2) Fix our tcmalloc build rules to include the missing files.  If we do the latter, I'm not aware of anything that would prevent us from using tcmalloc in 32-bit environments. """,Bug,Build
497949,"""We're running a couple of MongoDB clusters in AWS EC2 and the other day one of our clusters started to misbehave.  It turned out that the PRIMARY node was unable to contact the other nodes in the cluster. In it's logs we could see the following lines repeated (for brewity I've only included one host in the lines, but it showed the same for the third node as well):  Sat Dec 14 01:55:54 [rsHealthPoll] getaddrinfo(""""host2"""") failed: Temporary failure in name resolution Sat Dec 14 01:55:54 [rsHealthPoll] couldn't connect to host2:27017: couldn't connect to server host2:27017  On the other nodes we observed the following in their logs:  Sat Dec 14 01:56:24 [rsSyncNotifier] replset tracking exception: exception: 10278 dbclient error communicating with server: host1:27017 Sat Dec 14 01:56:24 [rsBackgroundSync] replSet db exception in producer: 10278 dbclient error communicating with server: host1:27017 Sat Dec 14 01:56:24 [rsHealthPoll] replSet member host1:27017 is now in state SECONDARY Sat Dec 14 01:56:24 [rsMgr] not electing self, host1:27017 would veto  The first thing we tested was to manually add the other nodes (host2 and host3) IP addresses to host1's /etc/hosts file, and within a minute the issue seemed resolved (i.e. it could talk to the other nodes again), but it was hardly a permanent solution.  We continued by running rs.stepDown() through the mongo console, removed the manually added lines in /etc/hosts and then restarted mongod on host1. When it booted up it was once again able to connect to the other nodes.  It should be noted that while host1 was unable to contact either host2 and host3, we were able to use dig to verify that our internal DNS was able to resolve the DNS name, and we could SSH from on host to the other. Our other services, that are using the same DNS server, showed no similar behaviour (i.e. problems with DNS resolving), and we also have an alarm check in place for DNS resolving, which didn't trigger.  Another thing of interest is that host1 has been running mongod since 27th of November 2012 while the other hosts (host2 and host3) were recently restarted (23th of November 2013) when we had a similar issue (though then we ended up with two PRIMARY nodes).  Let me know if there is anything else that you regarding this (i.e. log files, etc, etc) and I'll see what I can do.""",Bug,"Replication,Stability"
498316,"""I was running aggregations and writing output to a file, but my database I was analyzing was """"local"""" DB.  Apparently if _aCommitIsNeeded detects that we have a write lock on """"local"""" DB it fasserts() but since I'm running a regular build, it just continues fasserting till I kill -9 it. """,Bug,Concurrency
498554,"""*Syntax*  {noformat}  {$toInt: <arbitrary expression>}  {$toLong: <arbitrary expression>}  {$toDouble: <arbitrary expression>}  {$toDecimal: <arbitrary expression>}  {$toString: <arbitrary expression>}  {noformat}  *Examples*  {noformat}  > db.coll.insert([    {_id: 0, x: """"3.4""""},    {_id: 1, x: 0.99}  ]);  > db.coll.aggregate([{    $project: {      intField: {$toInt: """"$x""""}    }  }])  {_id: 0, intField: 3}  {_id: 1, intField: 0}    // Example 2  > db.coll.insert([    {_id: 0, x: {a: """"b""""}},    {_id: 1, x: 1.22},    {_id: 2, x: """"abc""""}  ]);  > db.coll.aggregate([{    $project: {      stringField: {$toString: """"$x""""}    }  }])  {_id: 0, stringField: """"{a: 'b'}""""}  {_id: 1, stringField: """"1.22""""}  {_id: 2, stringField: """"abc""""}  {noformat}  *Notes*  - Each numeric conversion can convert from any numeric type or from a string (truncating if necessary).  - Calling $toInt on an int is a no-op, similarly for other conversions.    *Errors*  - If the input is of a type or value that cannot be converted:      $toInt, $toLong, $toDouble, $toDecimal accept only int, long, double, decimal, and strings that can be interpreted as the above. $toString accepts anything that can be turned into output that is parseable by the mongo shell.      *Old Description*  There are situations where I want to use a number as (part of) a string but $concat will only take strings and not numbers.  And there are other times where I have a string of a number """"2012"""" and I want to use it as a number and there is no operator to do this.    Seems painful to fall back to map/reduce solely because someone saves a number or date as a string :(  """,Improvement,Aggregation Framework
498575,"""In commit e82786a9 a dassert was added to bson_field.h, however when bson_field.h is included by way of bson.h in a non-mongo-compile environment, the macro dassert is not defined.  This renders the bson.h header unusable for end users.  We can either add a new definition for dassert to bson.h, or we can make bson.h work like dbclient.h and use the redef/undef trick. I believe that the current setup of the bson.h header is intended to afford header only use of the bson library, but I'm not sure that this is still actually possible or desired. If we still want that ability, then the former solution is the only viable one. """,Bug,Internal Client
498604,"""AF's $project allows us to compose arbitrary expressions using referenced <USER>from the document and primitive arithmetic operators.   In contrast, update() only allows us to $inc a field or $set it, but not e.g. to multiply it by a constant (I see this was added in 2.6) or by another field.   As an example, suppose I want to update a document containing the average rating of a product and the number of ratings it got.   I want to be able to add a new rating in an atomic update by writing something like:  {code:javascript} db.products.update({ _id: 7 }, {   $mul: { averageRating: '$ratingCount' },   $add: { averageRating: 5 },   $inc: { ratingCount: 1 },   $div: { averageRating: '$ratingCount' } }); {code}  Alternatively I'd like to be able to run a user-provided JS function as part of the update, like the $where operator for queries, except this function can update the document. """,Improvement,Write Ops
498640,"""I was building a joining utility, in order to retrieve subdocuments, I built a query of or clauses matching each sub document to retrieve. The results were not terribly surprising at first, the query was very slow to respond.  What was surprising was when I began to page my actual queries, that is to say divide the entries in the $or clause into many separate db queries matching 50 of the joining documents at a time, the database began responding much much faster.  A join query for 1000 document took a minute without paging. With a small page size of 5 documents per join query, that time went down to 177ms.  Querying for one document at a time was still at 214ms, way faster than where I started.  So what I mean to say is that I think there is something wrong with the $or clause if it basically takes longer to execute a single query rather that 1000 small queries.  Perhaps there is a limitation I'm not aware and that's why I marked this as an improvement and not a bug  Here are the results of my testing Documents    Page Size     Total Time (ms) 1000       200             1236 1000       100             733 1000       50             407 1000       25             271 1000       10             192 1000       5             177 1000       1             214    10000       1000             59929 10000       500             30566 10000       200             12918 10000       100             6708 10000       50             4082 10000       25             2804 10000       10             1892 10000       5             1696 10000       1             2550 """,Improvement,Querying
498641,"""The listCommands command lists all available commands, and for """"special things"""" it returns a dedicate field for it. These special things include adminOnly and slaveOk.  A command returning a cursor should be considered as a special behaviour and be marked so.  {noformat}   """"aggregate"""" : {    """"help"""" : """"{ pipeline : [ { <data-pipe-op>: {...}}, ... ] }"""",    """"lockType"""" : 0,    """"slaveOk"""" : false,    """"adminOnly"""" : false,    """"slaveOverrideOk"""" : true   }, {noformat}  I'd like to have a """"top level field"""" there called cursor: true/false to clearly differentiate this command from others as it has very specific capabilities.""",Improvement,Usability
498763,"""Luckily I was testing in development environment when this happened but I completely lost 2 databases in the middle of a repair when it crashed with the following error message.  exception in initAndListen: 14043 clear tmp files caught exception exception: boost::filesystem::status: Access is denied: """"F:\Temp\mongodb-win32-x86_64-2.4.7-rc0\data\_tmp"""", terminating  Anyway, after some digging, since I just installed the new instance so I can simulate cloning database before doing so in production, I realized that the instance didn't have write permission to that folder so it just quit without creating the new files on repair. I also noticed that database files were actually in _tmp_* folders but didn't touch them as I wasn't too sure if they were corrupted or not.  What baffled me though is, the instance was able to create _tmp folders but not folders to create databases within the same directory, I would think it will fail completely on everything that needed create permission.  Anyway, by simply giving it  all access """"everything"""" permissions to that folder and retrying everything worked as expected.  I'll suggest that this is documented or at least the internal code should run a quick test before starting the repair process and failing later. I'm sure someone is going to try it in production before testing and blow out databases as I almost did.  As always, love MongoDB, thanks team MongoDB.""",Bug,Internal Code
499202,"""In the following example, I'm attempting to compute the product of all {{x}} <USER>in a collection. After the {{$group}} step, {{vals}} would be an array of these values. I would expect to be able to pass {{vals}} directly to {{$multiply}} and have it compute the product; however, {{$multiply}} instead considers the de-referenced value as an argument _to be multiplied_ instead of the argument array.  I imagine this is related to {{$multiply}} supporting a scalar argument, in which case it returns that numeric value * 1 (and complains if the value is not numeric).  {code} > db.foo.find() { """"_id"""" : ObjectId(""""522784a423582cd7f71acca7""""), """"x"""" : 2 } { """"_id"""" : ObjectId(""""522784a523582cd7f71acca8""""), """"x"""" : 3 } { """"_id"""" : ObjectId(""""522784a623582cd7f71acca9""""), """"x"""" : 4 } > pipeline [  {   """"$group"""" : {    """"_id"""" : 1,    """"vals"""" : {     """"$push"""" : """"$x""""    }   }  },  {   """"$project"""" : {    """"product"""" : {     """"$multiply"""" : """"$vals""""    }   }  } ] > db.foo.aggregate(pipeline) Error: Printing Stack Trace     at printStackTrace (src/mongo/shell/utils.js:37:15)     at DBCollection.aggregate (src/mongo/shell/collection.js:897:9)     at (shell):1:8 Wed Sep  4 15:07:05.712 JavaScript execution failed: aggregate failed: {  """"errmsg"""" : """"exception: $multiply only supports numeric types, not Array"""",  """"code"""" : 16555,  """"ok"""" : 0 } at src/mongo/shell/collection.js:L898 {code}  Here's an example of an array value being passed as an argument to be multiplied (using all literals):  {code} > db.foo.aggregate({$project:{p:{$multiply:[[3]]}}}) Error: Printing Stack Trace     at printStackTrace (src/mongo/shell/utils.js:37:15)     at DBCollection.aggregate (src/mongo/shell/collection.js:897:9)     at (shell):1:8 Wed Sep  4 15:12:32.572 JavaScript execution failed: aggregate failed: {  """"errmsg"""" : """"exception: $multiply only supports numeric types, not Array"""",  """"code"""" : 16555,  """"ok"""" : 0 } at src/mongo/shell/collection.js:L898 {code}""",Bug,Aggregation Framework
499374,"""I have a collection of documents with keys _id, a, and b.  I have an index \{a:1,b:1\}.  When I do find(a).sort(b).limit(5) the index is used, and only 5 documents are scanned.  When I do find(a).sort(b,_id).limit(5) (with or without a hint) it appears the index is only used to match a - as all documents with the matching value for a are scanned (nscanned and nscannedObjects are high).  The behavior I would expect is that it would scan through a and b values enough to get to the limit (5), and then it would stop scanning when it found the next unique b (or a).  It would then sort on _id within the scanned records.""",Improvement,"Querying,Indexing"
499462,"""Last week, there was a failure of AWS DNS resolution which caused a specific Amazon Availability Zone to not be able to resolve DNS. Other AZ's WERE able to resolve DNS, including records of hosts in the """"DNS-failed"""" zone.  In a nutshell, we have the following situation which led to both nodes in """"SECONDARY"""" state:  PRIMARY (db01srv02) - suddenly can't see the SECONDARY or the ARBITER. It steps down. SECONDARY (db01srv01) - CAN see the Primary and the Arbiter. It refuses to elect itself because """"db01srv02.local.:20001 would veto""""  (n.b. - after upgrading to 2.4.5, I now get the more descriptive error """"Sun Jul 28 12:43:36 [rsMgr] not electing self, db01srv02.local.:20001 would veto with 'I don't think db01srv01.local.:10001 is electable'""""  Disclaimer - I'm not a DB Expert, so this may be expected behavior for some reason.... """,Bug,Replication
499545,"""I just attempt to insert a document which has a 'Polygon' column  into a new collection named 'testSharp', after insert I want to add a 2dshpere index to the 'geom' column, error occured when I added the index, I'm not sure this is a bug? but when i try to modify the 'Polygon' to 'LineString', the error didn't occur again.""",Bug,JavaScript
499592,"""I know there are some fundamental limitations of running eval() on a sharded cluster for general purpose JavaScript, but I believe that my specific use case is feasible given the limitations of sharding. The difference for my use case is that I want eval() against only a single document for read or write. You could think of it as a modification of findAndModify() where we want to find and eval, instead of modify. This means that the eval() command can be run on the shard itself (monogd server), and not in the router (mongos server).   I believe that this is possible with the following semantics:   1. Client app issues request to a mongos server. The request contains a document _id (or some other reference containing the entire shard key) and the javascript to eval() against the document.  2. command is routed to the correct shard (rather than executing the command on the mongos) 3. the shard containing the document write locks for the collection 4. the shard pulls the document and executes the JavaScript against the document 5. access to any other document for read/write/whatever is prohibited.  6. once JavScript finished modifying the document, the document is stored.  7. write lock released for collection on that shard.   """,Improvement,JavaScript
499724,"""What are your futuure intentions for the 32-bit servers.  I am currently running 2 instances of mongod -- one a 64-bit instance for transactional data, and the other a 32-bit instance for our """"archival"""" data.    The archival data is quite large:  several times the total memory capacity of the server, but is accessed pretty rarely (a typical record might get hit only 4 or 5 times per year).     Just as a matter of resource management, I want all available memory to go to the transactional server.   The 32-bit server seems like a good solution for us, because it's memory usage caps out (under Windows) at somewhere around 2GB.  But I recently attended a 10gen presentation where I believe your representative said something about discontinuing support for 32-bit server releases  (I'm not exactly sure, because the issue wasn't really on my radar at the time).  But now it has become kind of important to me.  If you ARE discontinuing 32-bit releases, would it be possible to add s startup option allowing a memory allocation cap on a running server instance?    What is the best approach for the kind of scenario (archival, logging, or other large, infrequently-accessed data stores)--are virtual machines the only other viable approach?""",Question,Write Ops
499734,"""The English stop word list includes """"it"""" which prevents someone for searching documents using the Italian country code, """"IT"""". It looks like the original source for the current stop word list includes """"us"""" as a stop word with a warning about care being taken due to it also being a country code and that """"us"""" is omitted from the MongoDB stop word list. I would think that """"it"""" would be omitted from the MongoDB list by that same logic.""",Bug,Text Search
499736,"""A deeply nested document will cause the shell to hang. I'm not totally sure it is just the shell, as all CPUs spin and my system hangs. (Keep that in mind while testing).  I've provided a mongo wire message encoded as a file that can be netcat'ed into the server. Then a supplemental query to the collection will cause the shell (and in my case, entire system, including mouse/video/etc) to hang.  The test document that is inserted is just under 16Mb in size, with recursive documents {'': {'': ...}}. Basically large enough to cause any recursive bson parser to stack overflow.""",Bug,Shell
499848,"""2 shards, 2replica+arbiter each, connecting via mongos.  Hi there. I've dropped a collection, then continued to write data there(it was a lot of stale data, that I had to remove). However, newly written data cannot be read from mongos, via mongod everything looks ok.  here some logs:  mongos>use vertis-creativework mongos>db.pused.drop()  mongod logs: Thu Jun 13 13:20:54 [conn1571507] command vertis-creativework.$cmd command: { drop: """"pushed"""", $auth: { admin: { root: 2 } } } ntoreturn:1 keyUpdates:0 locks(micros) w:12584 reslen:133 12ms Thu Jun 13 13:20:54 [conn1408958] CoveredIndexMatcher::matches() {} 9:70c84318 0 Thu Jun 13 13:20:54 [conn1408966] CoveredIndexMatcher::matches() {} 9:70c84318 0 Thu Jun 13 13:20:54 [conn1408958] Matcher::matches() { ts: Timestamp 1371115254000|1, h: 630211948944288504, v: 2, op: """"c"""", ns: """"vertis-creativework.$cmd"""", o: { drop: """"pushed"""", $auth: { admin: { root: 2 } } } } Thu Jun 13 13:20:54 [conn1408966] Matcher::matches() { ts: Timestamp 1371115254000|1, h: 630211948944288504, v: 2, op: """"c"""", ns: """"vertis-creativework.$cmd"""", o: { drop: """"pushed"""", $auth: { admin: { root: 2 } } } } Thu Jun 13 13:20:54 [conn1408958] CoveredIndexMatcher _docMatcher->matches() returns 1 Thu Jun 13 13:20:54 [conn1408966] CoveredIndexMatcher _docMatcher->matches() returns 1  mongos>db.pushed.insert({_id:1})  mongod logs:  Thu Jun 13 13:23:08 [conn1571507] _reuse extent was:vertis-creativework.pushed now:vertis-creativework.pushed Thu Jun 13 13:23:08 [conn1571507] allocExtent vertis-creativework.pushed size 1024 1 Thu Jun 13 13:23:08 [conn1571507] adding _id index for collection vertis-creativework.pushed Thu Jun 13 13:23:08 [conn1571507] build index vertis-creativework.pushed { _id: 1 } mem info: before index start vsize: 241624 resident: 6123 mapped: 119344 Thu Jun 13 13:23:08 [conn1571507] external sort root: /var/lib/mongodb/_tmp/esort.1371115388.220/ mem info: before final sort vsize: 241624 resident: 6123 mapped: 119344 mem info: after final sort vsize: 241624 resident: 6123 mapped: 119344 Thu Jun 13 13:23:08 [conn1571507]        external sort used : 0 files  in 0 secs Thu Jun 13 13:23:08 [conn1571507] _reuse extent was:vertis-creativework.pushed.$_id_ now:vertis-creativework.pushed.$_id_ Thu Jun 13 13:23:08 [conn1571507] allocExtent vertis-creativework.pushed.$_id_ size 36864 1 Thu Jun 13 13:23:08 [conn1571507] New namespace: vertis-creativework.pushed.$_id_ Thu Jun 13 13:23:08 [conn1571507]        done building bottom layer, going to commit Thu Jun 13 13:23:08 [conn1571507] build index done.  scanned 0 total records. 0.002 secs Thu Jun 13 13:23:08 [conn1571507] New namespace: vertis-creativework.pushed Thu Jun 13 13:23:08 [conn1571507] insert vertis-creativework.pushed keyUpdates:0 locks(micros) w:3851 3ms Thu Jun 13 13:23:08 [conn1408958] CoveredIndexMatcher::matches() {} 9:70c85470 0 Thu Jun 13 13:23:08 [conn1571507] runQuery called vertis-creativework.$cmd { getlasterror: 1.0, w: 1.0, $auth: { admin: { root: 2 } } } Thu Jun 13 13:23:08 [conn1408958] Matcher::matches() { ts: Timestamp 1371115388000|1, h: -1867873785266695569, v: 2, op: """"i"""", ns: """"vertis-creativework.pushed"""", o: { _id: 1.0 } } Thu Jun 13 13:23:08 [conn1571507] run command vertis-creativework.$cmd { getlasterror: 1.0, w: 1.0, $auth: { admin: { root: 2 } } } Thu Jun 13 13:23:08 [conn1408958] CoveredIndexMatcher _docMatcher->matches() returns 1 Thu Jun 13 13:23:08 [conn1571507] Setting temporary authorization to: { admin: { root: 2 } } Thu Jun 13 13:23:08 [conn1571507] command vertis-creativework.$cmd command: { getlasterror: 1.0, w: 1.0, $auth: { admin: { root: 2 } } } ntoreturn:1 keyUpdates:0  reslen:94 0ms Thu Jun 13 13:23:08 [conn1408958] getmore local.oplog.rs query: { ts: { $gte: new Date(5880820189783654401) } } cursorid:2864578847244946404 ntoreturn:0 keyUpdates:0 locks(micros) r:478 nreturned:1 reslen:121 169ms Thu Jun 13 13:23:08 [conn1408966] CoveredIndexMatcher::matches() {} 9:70c85470 0 Thu Jun 13 13:23:08 [conn1408966] Matcher::matches() { ts: Timestamp 1371115388000|1, h: -1867873785266695569, v: 2, op: """"i"""", ns: """"vertis-creativework.pushed"""", o: { _id: 1.0 } } Thu Jun 13 13:23:08 [conn1408966] CoveredIndexMatcher _docMatcher->matches() returns 1  mongos>db.pushed.find()  mongod:  Thu Jun 13 13:24:35 [conn1571507] query vertis-creativework.pushed ntoreturn:0 keyUpdates:0 locks(micros) r:89 nreturned:0 reslen:20 0ms  mongos>db.pushed.stats() {  """"sharded"""" : false,  """"primary"""" : """"cs_mongodb-sh2-testing"""",  """"ns"""" : """"vertis-creativework.pushed"""",  """"count"""" : 1,  """"size"""" : 20,  """"avgObjSize"""" : 20,  """"storageSize"""" : 4096,  """"numExtents"""" : 1,  """"nindexes"""" : 1,  """"lastExtentSize"""" : 4096,  """"paddingFactor"""" : 1,  """"systemFlags"""" : 1,  """"userFlags"""" : 0,  """"totalIndexSize"""" : 8176,  """"indexSizes"""" : {   """"_id_"""" : 8176  },  """"ok"""" : 1 }  mongos>db.pushed.find().explain() {  """"cursor"""" : """"BasicCursor"""",  """"isMultiKey"""" : false,  """"n"""" : 0,  """"nscannedObjects"""" : 1,  """"nscanned"""" : 1,  """"nscannedObjectsAllPlans"""" : 1,  """"nscannedAllPlans"""" : 1,  """"scanAndOrder"""" : false,  """"indexOnly"""" : false,  """"nYields"""" : 0,  """"nChunkSkips"""" : 1,  """"millis"""" : 0,  """"indexBounds"""" : {   },  """"server"""" : """"cs-mongodb02gt.yandex.ru:27017"""",  """"millis"""" : 0 }  in config.collections there is no document for mentioned collection.""",Bug,Querying
499891,"""We recently had an event where our primary server had to step down and after the new election occurred was put into a rollback state. During the rollback mongo encountered a unique index violation while attempting to write to the view_stats collection and promptly crashed.    {code}  Mon Jun  3 18:50:32 [repl writer worker 2] ERROR: writer worker caught exception: E11000 duplicate key error index: songza.view_stats.$view_1_host_1_pid_1  dup key: { : <USER> : <USER> : <USER>} on: { ts: Timestamp 1370285310000|4, h: 1593562783886765644, v: 2, op: """"u"""", ns: """"songza.view_stats"""", o2: { _id: ObjectId('51ace4772a5a3f13accdbe17') }, o: { $set: { count: 2 }, $set: { sum_ms: 29 }, $set: { sum_square_ms: 565 } } }  Mon Jun  3 18:50:32 [repl writer worker 2]   Fatal Assertion 16360  0xb07561 0xacc8b3 0x9abaf6 0xadab5d 0xb4d3d9 0x7f845177be9a 0x7f8450a8ecbd   /usr/bin/mongod(_ZN5mongo15printStackTraceERSo+0x21) [0xb07561]  /usr/bin/mongod(_ZN5mongo13fassertFailedEi+0xa3) [0xacc8b3]  /usr/bin/mongod(_ZN5mongo7replset14multiSyncApplyERKSt6vectorINS_7BSONObjESaIS2_EEPNS0_8SyncTailE+0x156) [0x9abaf6]  /usr/bin/mongod(_ZN5mongo10threadpool6Worker4loopEv+0x26d) [0xadab5d]  /usr/bin/mongod() [0xb4d3d9]  /lib/x86_64-linux-gnu/libpthread.so.0(+0x7e9a) [0x7f845177be9a]  /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f8450a8ecbd]  Mon Jun  3 18:50:32 [repl writer worker 2]   ***aborting after fassert() failure  Mon Jun  3 18:50:32 Got signal: 6 (Aborted).  Mon Jun  3 18:50:32 Backtrace:  0xb07561 0x5598c9 0x7f84509d14a0 0x7f84509d1425 0x7f84509d4b8b 0xacc8ee 0x9abaf6 0xadab5d 0xb4d3d9 0x7f845177be9a 0x7f8450a8ecbd   /usr/bin/mongod(_ZN5mongo15printStackTraceERSo+0x21) [0xb07561]  /usr/bin/mongod(_ZN5mongo10abruptQuitEi+0x399) [0x5598c9]  /lib/x86_64-linux-gnu/libc.so.6(+0x364a0) [0x7f84509d14a0]  /lib/x86_64-linux-gnu/libc.so.6(gsignal+0x35) [0x7f84509d1425]  /lib/x86_64-linux-gnu/libc.so.6(abort+0x17b) [0x7f84509d4b8b]  /usr/bin/mongod(_ZN5mongo13fassertFailedEi+0xde) [0xacc8ee]  /usr/bin/mongod(_ZN5mongo7replset14multiSyncApplyERKSt6vectorINS_7BSONObjESaIS2_EEPNS0_8SyncTailE+0x156) [0x9abaf6]  /usr/bin/mongod(_ZN5mongo10threadpool6Worker4loopEv+0x26d) [0xadab5d]  /usr/bin/mongod() [0xb4d3d9]  /lib/x86_64-linux-gnu/libpthread.so.0(+0x7e9a) [0x7f845177be9a]  /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f8450a8ecbd]  {code}    I brought the machine back up out of the replicaset and was able to verify that there were 4 bad documents that lacked the host, view and pid <USER> No other active hosts had these bad documents and it's technically highly unlikely that any of our code inserted them in the first place. I removed the bad documents and brought the member back into the replicaset, but it crashed again shortly thereafter with the same error. We were only able to resolve the issue by restoring the database from an EBS snapshot and letting it <USER>up again.    I've attached a log from the primary to this ticket.""",Bug,Replication
499963,"""{panel:title=Issue Status as of Jul 22, 2014|borderColor=#ccc|titleBGColor=#6CB33F|bgColor=#EEEEEE}  *ISSUE SUMMARY* When reading from a sharded cluster via mongos with a specific read preference, mongos never re-evaluates the preference as long as it connects to a valid member. This can in certain circumstances lead to situations where mongos reads from nodes for prolonged times that do not match the user's intention and expectation.  Example:   When the """"secondaryPreferred"""" read preference is set, mongos connects to an available secondary on a new connection for reads. If there are no longer any available secondaries, mongos correctly switches to a primary node. However, even when a secondary node is available again, mongos does not switch back to read from the secondary node. The connection is pinned to the primary because under """"secondaryPreferred"""", the primary is a valid target to read from and no re-evaluation is carried out until the the target becomes invalid or unreachable.  *USER IMPACT* Reads can go to primary nodes for prolonged times even though the user specified that they prefer secondary reads. Users may not even be aware of this fact, if they don't closely monitor the state of their replica sets at all times. Depending on the application architecture, this can lead to degraded read and write throughput.  *WORKAROUNDS* The only workaround is to forcibly unpin the connection by specifying a different readPreference on said connection.   *AFFECTED VERSIONS* All previous production releases are affected by this issue.  *FIX VERSION* The fix is included in the 2.6.4 production release.  *RESOLUTION DETAILS* # Secondary connections are now drawn from the global pool. # For mongos, the active ReplicaSet connection will release its secondary connection back to the pool after the end of the query/command. This also has a side effect of 'unpinning' the read preference settings. In other words, when this connection is reused again, the node selection will be evaluated again according to the read preference.  As these changes could not be backported to 2.6, a different fix was implemented specifically for 2.6: a new mongos server parameter, {{internalDBClientRSReselectNodePercentage}} was introduced. This can be set to any value from 0 to 100 (defaults to 0) and represents the probability (expressed in percentage) of a replica set connection in mongos to reevaluate replica set node selection from scratch, regardless of the compatibility of the current read preference to the last-used secondary node. Extra care should be taken since reselecting a replica set node will destroy the old connection and create a new connection.  This means in extreme cases (for example, 100%), mongos can be creating and destroying connections for every read request.  {panel}  h6. Original description  During lab tests with 1 primary, 1 secondary and 1 arbiter I'm running into the following issue when using the Java drivers' """"secondaryPreferred"""" read preference :  We start with a healthy 3 member repset and start our load tests. This load test connects to mongos. All reads go to the secondary member. We kill the secondary and reads are correctly routed to the primary. We restart the secondary but reads continue to go to the primary indefinitely.  Might be a Java driver issue since I was not able to reproduce in shell due to the lack of support for this read mode there (I think?)""",Bug,Sharding
500049,"""Have been trying to upgrade our QA from 2.2.2 to 2.4.3. Historically, our upgrades have gone very smoothly. This one is really giving us some trouble. We first disabled replication on QA and just upgraded master to 2.4.3 to test it out. When satisfied, we upgraded the two replicas and let them <USER>up but they kept failing very similarly to SERVER-6975. So we simplified and attempted a full resync instead. What we observe now is the replicas losing connectivity with master several times during the resync process and eventually crashing in the same manner as reported in SERVER-9057 and SERVER-9199:  {code}  Mon May 13 19:07:45.614 [rsHealthPoll] replSet member ip-10-71-26-90.ec2.internal:27017 is now in state SECONDARY Mon May 13 19:07:45.700 [initandlisten] connection accepted from 10.71.26.90:50732 #2175 (28 connections now open) Mon May 13 19:07:45.703 [conn2175]  authenticate db: local { authenticate: 1, nonce: """"779ae9aba7a706e4"""", user: """"__system"""", key: """"e7f4453059498c244523f2794a04c216"""" } Mon May 13 19:07:45.705 [conn2175] replSet info voting yea for ip-10-71-26-90.ec2.internal:27017 (0) Mon May 13 19:07:47.617 [rsHealthPoll] replSet member ip-10-71-26-90.ec2.internal:27017 is now in state PRIMARY Mon May 13 19:07:59.673 [initandlisten] connection accepted from 10.71.26.90:50738 #2176 (29 connections now open) Mon May 13 19:07:59.677 [conn2176]  authenticate db: admin { authenticate: 1, user: """"mongo"""", nonce: """"8f94b9003dc3c980"""", key: """"ad59ed669b28d961e6db34033c785980"""" } Mon May 13 19:07:59.691 [conn2176] end connection 10.71.26.90:50738 (28 connections now open) Mon May 13 19:08:13.751 [conn2175] end connection 10.71.26.90:50732 (27 connections now open) Mon May 13 19:08:13.753 [initandlisten] connection accepted from 10.71.26.90:50749 #2177 (28 connections now open) Mon May 13 19:08:13.755 [conn2177]  authenticate db: local { authenticate: 1, nonce: """"f4fb648338aebbc9"""", user: """"__system"""", key: """"77d24c9ffe7f0e3fe2749d1c042562d0"""" } Mon May 13 19:08:15.606 [rsSync]   Fatal Assertion 16233 0xdcf361 0xd8f0d3 0xc03b0f 0xc21811 0xc218ad 0xc21b7c 0xe17cb9 0x7f313696ce9a 0x7f3135c7fcbd  /home/mongo/mongodb/bin/mongod(_ZN5mongo15printStackTraceERSo+0x21) [0xdcf361]  /home/mongo/mongodb/bin/mongod(_ZN5mongo13fassertFailedEi+0xa3) [0xd8f0d3]  /home/mongo/mongodb/bin/mongod(_ZN5mongo11ReplSetImpl17syncDoInitialSyncEv+0x6f) [0xc03b0f]  /home/mongo/mongodb/bin/mongod(_ZN5mongo11ReplSetImpl11_syncThreadEv+0x71) [0xc21811]  /home/mongo/mongodb/bin/mongod(_ZN5mongo11ReplSetImpl10syncThreadEv+0x2d) [0xc218ad]  /home/mongo/mongodb/bin/mongod(_ZN5mongo15startSyncThreadEv+0x6c) [0xc21b7c]  /home/mongo/mongodb/bin/mongod() [0xe17cb9]  /lib/x86_64-linux-gnu/libpthread.so.0(+0x7e9a) [0x7f313696ce9a]  /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f3135c7fcbd] Mon May 13 19:08:15.667 [rsSync]  ***aborting after fassert() failure   Mon May 13 19:08:15.667 Got signal: 6 (Aborted).  Mon May 13 19:08:15.671 Backtrace: 0xdcf361 0x6cf729 0x7f3135bc24a0 0x7f3135bc2425 0x7f3135bc5b8b 0xd8f10e 0xc03b0f 0xc21811 0xc218ad 0xc21b7c 0xe17cb9 0x7f313696ce9a 0x7f3135c7fcbd  /home/mongo/mongodb/bin/mongod(_ZN5mongo15printStackTraceERSo+0x21) [0xdcf361]  /home/mongo/mongodb/bin/mongod(_ZN5mongo10abruptQuitEi+0x399) [0x6cf729]  /lib/x86_64-linux-gnu/libc.so.6(+0x364a0) [0x7f3135bc24a0]  /lib/x86_64-linux-gnu/libc.so.6(gsignal+0x35) [0x7f3135bc2425]  /lib/x86_64-linux-gnu/libc.so.6(abort+0x17b) [0x7f3135bc5b8b]  /home/mongo/mongodb/bin/mongod(_ZN5mongo13fassertFailedEi+0xde) [0xd8f10e]  /home/mongo/mongodb/bin/mongod(_ZN5mongo11ReplSetImpl17syncDoInitialSyncEv+0x6f) [0xc03b0f]  /home/mongo/mongodb/bin/mongod(_ZN5mongo11ReplSetImpl11_syncThreadEv+0x71) [0xc21811]  /home/mongo/mongodb/bin/mongod(_ZN5mongo11ReplSetImpl10syncThreadEv+0x2d) [0xc218ad]  /home/mongo/mongodb/bin/mongod(_ZN5mongo15startSyncThreadEv+0x6c) [0xc21b7c]  /home/mongo/mongodb/bin/mongod() [0xe17cb9]  /lib/x86_64-linux-gnu/libpthread.so.0(+0x7e9a) [0x7f313696ce9a]  /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f3135c7fcbd] {code}   We have even simplified the setup to one replica trying to resync from master so that we reduce the load on the network. Same issue. I'll be attaching the full logs from both master and the crashing secondary in a bit. Following the suggestions in the other tickets, I also ran tcpdump during the tests to capture any possible network issues. Those logs are huge so I'm trying to truncate them to the time of the crash so that I can upload them.""",Bug,Replication
500057,"""I'm seeing odd behavior:  I have a shard that has a tagged member for ETL work, it's _id:5 in the following rs.conf():  {code}s9:SECONDARY> rs.conf() {  """"_id"""" : """"s9"""",  """"version"""" : 16,  """"members"""" : [   {    """"_id"""" : 3,    """"host"""" : """"ec2-184-169-144-16.us-west-1.compute.amazonaws.com:27017"""",    """"priority"""" : 0,    """"hidden"""" : true,    """"buildIndexes"""" : false   },   {    """"_id"""" : 4,    """"host"""" : """"50.23.75.133:28009"""",    """"priority"""" : 2   },   {    """"_id"""" : 5,    """"host"""" : """"198.23.68.151:28009"""",    """"votes"""" : 0,    """"priority"""" : 0,    """"tags"""" : {     """"etlstafe"""" : """"true""""    }   },   {    """"_id"""" : 6,    """"host"""" : """"50.23.100.8:28009"""",    """"priority"""" : 3   }  ] } {code}  When I connect to this secondary directly with (while logged on to the hosts console):  {code}mongo --port 28009{code}  I get the following:  {code} MongoDB shell version: 2.2.3 connecting to: 127.0.0.1:28009/test s9:SECONDARY> db.getMongo().setSlaveOk() s9:SECONDARY> db.blocks.getIndexes() [ ] s9:SECONDARY> {code}  When in fact, I should see  {code} [  {   """"v"""" : 1,   """"key"""" : {    """"_id"""" : 1   },   """"ns"""" : """"blocks.blocks"""",   """"name"""" : """"_id_""""  },  {   """"v"""" : 1,   """"key"""" : {    """"user_id"""" : 1,    """"visible_ts"""" : 1,    """"state"""" : 1,    """"private"""" : 1,    """"created"""" : 1   },   """"ns"""" : """"blocks.blocks"""",   """"name"""" : """"user_id_1_visible_ts_1_state_1_private_1_created_1""""  },  {   """"v"""" : 1,   """"key"""" : {    """"user_id"""" : 1   },   """"ns"""" : """"blocks.blocks"""",   """"name"""" : """"user_id_1""""  },  {   """"v"""" : 1,   """"key"""" : {    """"custom_id"""" : 1   },   """"ns"""" : """"blocks.blocks"""",   """"name"""" : """"custom_id_1""""  },  {   """"v"""" : 1,   """"key"""" : {    """"short_hash"""" : 1   },   """"ns"""" : """"blocks.blocks"""",   """"name"""" : """"short_hash_1""""  },  {   """"v"""" : 1,   """"key"""" : {    """"user_id"""" : 1,    """"private"""" : 1,    """"created"""" : -1   },   """"ns"""" : """"blocks.blocks"""",   """"name"""" : """"user_id_1_private_1_created_-1""""  } ] {code}  I'm only able to get the full list of indexes when I restart the member in stand alone mode (not part of replset and on a unique port, as though I'm doing maintenance).  Is this because of the tag in place on that member?""",Question,"Shell,Replication"
500202,"""I have a simple replica set with a primary a secondary and an arbiter. I experienced a hardware failure a few days ago which required provisioning a brand new machine to replace one of the primary/secondary servers.  I setup the server with the same IPs as the previous server and started up Mongo with an empty data directory to allow it to perform a full re-sync.  Each time I tried the full re-sync it would successfully sync all the data but I believe it is at the point at which it attempts to apply the oplog before actually coming up as a secondary that it failed each time until I downgraded the secondary to 2.2.4. At that point (without having to fully re-sync again) everything came up as expected.  I have attached PRIMARY and SECONDARY logs for the appropriate timeframes to show from start of sync through the failure. I also included the log of the SECONDARY after I downgraded to 2.2.4 and what it did following.  Please let me know if I need to provide other information.""",Bug,Replication
500283,"""I'm working on setting up a sharded cluster.  Do all replica sets in a cluster need to have unique names?  Normally I would name each replica set something like """"rs1"""", """"rs2"""", etc. but the automation I'm using right now is currently setting each replica set name to just """"mongo"""".  Can multiple replica sets with the same name -- each acting as a shard --  be part of the same cluster?  Or do the replica set names need to be unique across the cluster?""",Question,Sharding
500288,"""I've a single Mongo instance running on a 4-CPU 16GB RAM CenOS 6.2 VM. It's running very slowly mostly of the time. Here is the mongostat output:  {code} insert  query update delete getmore command flushes mapped  vsize    res        faults       locked db         idx miss %     qr|qw   ar|aw  netIn netOut  conn repl       time     *0     62     12        *0           0             9|0               0          82g   174g       2.61g     10       gowmain:13.2%          0            52|0     1|5      12k    64k     315  PRI   14:41:27     *0     90      26       *0           0            21|0               0         82g   174g       2.61g     12       gowmain:56.0%          0            63|0     0|1      23k   145k    315  PRI   14:41:29     *0    157     38       *0           0            19|0              1         82g   174g        2.6g        44       gowmain:56.4%         0            61|0     1|0      36k   330k    315  PRI   14:41:31     *0    372     95       *0           0            62|0              0         82g   174g       2.61g      141      gowmain:38.1%         0            53|0     1|0     208k    1m    315  PRI   14:41:32     *0    303     58       *0           0            31|0              0         82g   174g       2.62g       43       gowmain:33.5%         0            52|0     1|0      68k    429k    315  PRI   14:41:33     *0    277     43       *0           0            17|0              0         82g   174g       2.61g       53       gowmain:21.8%          0            55|0     0|1    103k   1m      315  PRI   14:41:34     *0    176     24       *0           0             9|0               0         82g   174g       2.61g       17       gowmain:63.8%          0            58|0     1|0     48k     581k   315  PRI   14:41:35     *0    506     79       *0           0            38|0              0         82g   174g        2.6g        91       gowmain:47.6%          0            62|0     1|0     143k   852k   316  PRI   14:41:37     *0    236     39       *0           0            15|0              0         82g   174g       2.61g       95       gowmain:21.2%          0            55|1     0|1     86k    1m      316  PRI   14:41:38     *0    195     41       *0           0            15|0              0         82g   174g        2.6g       86       gowmain:28.3%          0            50|1     2|0       89k   1m      316  PRI   14:41:40  {code}  The command line options are:  --rest --fork --master --oplogSize=22770 --port 27017 --dbpath /data/mongodb/dbdata/ --logpath /data/logs/mongodb.log --auth  Here is the top output:  {code} top - 15:05:58 up 18 days, 21:03,  2 users,  load average: 2.37, 2.79, 2.83 Tasks: 121 total,   1 running, 120 sleeping,   0 stopped,   0 zombie Cpu(s):  0.5%us,  0.5%sy,  0.0%ni, 74.9%id, 24.1%wa,  0.0%hi,  0.0%si,  0.0%st Mem:  16139308k total, 15988616k used,   150692k free,    11336k buffers Swap:  4194296k total,      536k used,  4193760k free, 14952988k cached    PID   USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+   COMMAND 21253  root        20    0   173g  2.6g   2.4g  S    7.0        17.0   468:05.07  mongod {code} The weird thing is MongoDB consistently only consumes only 2.6G of RAM! However, the output from 'top' shows all 16GB RAM is used up.  A couple of other observations:  - The CPU load from 'top' is usually between 2.00 to 3.00 - The lock% range between 40 to 80. - When the read queue goes up to 150 to 200, the system is unusable. - The number of faults is not too bad, but iostat shows 100% IO itilization on the mongo partition. Also, sometimes the await time in iostat is more than 1000ms!!!  The DB has one large collection having about 50 million documents; all other collections are small. The DB serves about 2000 users frontended by Tomcat (which has almost no load).   I cannot figure out why it uses less than 3GB of RAM while the system has 16GB.  Are we doing something obviously wrong here?""",Bug,Performance
500362,"""During our initial sync, our mongodb instance acting as a replica crashed. I've attached the result of running """"rs.conf()"""" and """"rs.status()"""" on the master as well as the full mongod.log from the instance that crashed. """,Bug,Replication
500487,"""When building a replica set of 3 nodes (db1, db2, db3), when db1 is primary, internal replication logic usually builds a chain db1 -> db2 -> db3 instead of db1 -> db2 + db1 -> db3. This leads to greatly increased latencies if w:all (or w:majority with sets of 4+ members) is used. I know about replSetSyncFrom command, but it's only a temporary measure. I understand that this chaining is done to reduce load on master and (probably) reduce amount of reconnections whem primary goes down. I would strongly suggest a method (RS configuration item, command-line/configuration option, etc.) to either specify a node that can handle all replication needs or just globally override the algorithm so it always preferse primary as a sync source. Since building robust applications involves writes being done with w:majority (to minimize general data loss and rollbacks), a method to reduce """"visible"""" replication latency is certainly a must. """,Improvement,Replication
500519,"""Let's start from an example: suppose that each document in a collection contains a UserAgent string as a field. Androids have something like """"xxxxxx Android xxxx"""", iOS have something like """"xxxxxxx iPad/iPhone xxxxxx"""".  I would like to count those documents, grouped by platform (Android, iPad, iPhone). To get the platform, all I need is to count them by a substring match. For example, /Android/ vs /iPad/ vs /iPhone/ (each counted separately).  Any solution with aggregation framework will make it for me. I can clarify/whirpool on any request with pleasure.""",New Feature,Aggregation Framework
500561,"""Hello!  I have a proposition for impovements in MongoDB. I want to tell about db.collection.find() improvements.  Currently syntax of query with projection is like: db.things.find( \{ param : """"value"""" \} , \{ field1 : 1, field2: 1 \} )  Why don't change the syntax to db.things.find( \{ param : """"value"""" \} , \{ (include/exclude): \[<USER>]\} )  I'll try to give you examples that you could understand what I mean:  db.things.find( \{ x : 4 \} , \{include:\[_id, j\]\} ) db.things.find( \{ x : 4 \} , \{exclude:\[_id, x\]\} )  In such syntax we can use _id field as a usual field, an it is not included to projection implicitly. Another advantage is that we can just pass array of <USER>instead of giving 0 or 1 values to field names.""",New Feature,Querying
500600,"""{code} db.coll.find({$text: {$search: """"\""""a phrase\"""" term1 term2 -negterm"""",                       $language: """"spanish""""},               name: /a.*/},              {description: 1, _id: 0})        .sort({date: 1}).skip(10).limit(10); {code}  original:  {quote} I have been using text search against a large database of needs feeds with some success.  The problem is that only being able to get the results based on score is not sufficient.  What I mainly need to do, and I think a lot of others will want to do, is qualify a document based on it meeting the text search criteria (as one possible filter, to be combined with others), but to return a result set ordered by some other criteria (like date), with the same kind of paging support that we have with queries in general.  So I guess specifically, this just means having a query filter to indicate whether a document does or doesn't match a given text search string.  In my case, I have millions of news feed items from hundreds of feeds.  If I want to search Slashdot for the term """"iPhone"""", what I want is to see the news items from that feed that contain """"iPhone"""" in order of most recent first.  With text search as it is now, I cannot do that.  Users don't want text search *only* because they want weighted results that find the most relevant documents, the also want to be able to use text search criteria as additional functionality in their current applications and queries. {quote}""",New Feature,Text Search
500603,"""Due to SERVER-4692, it is no longer possible to do a full mongodump using a read-only user.  SERVER-9012 has been proposed as a solution to this.  However, but that means it is still impossible to dump all data using a read-only account.  I believe it *should* be possible to do such a mongodump, including dumping all users, so that they can be restored correctly.  Here are a couple of ways it might be possible:   - Allow read-only admin users access to system.users collections.   - Add a another user permissions beyond read-only, that would allow access to system.users collections. - Do some sort of encryption of system.users when read by read-only users that can only be decrypted by a user with full access.  That allows the encrypted version to be backed up, and then when restored, a user with write access has to do the restore, so they could do the decrypting before restore.  This is obviously much more involved than the previous 2, but I'm just trying to throw out possible alternatives.""",Improvement,Security
500630,"""{panel:title=Issue Status as of Sep 02, 2014|borderColor=#ccc|titleBGColor=#6CB33F|bgColor=#EEEEEE}  *ISSUE SUMMARY* On linux systems, if the user running {{mongod}} does not have a locale set or the locale is misconfigured, {{mongod}} fails to start printing a stack trace.  *USER IMPACT* {{mongod}} can't be started with a missing or misconfigured locale.  *WORKAROUNDS* Configure a locale for the user running {{mongod}}.  *AFFECTED VERSIONS* MongoDB production releases up to 2.6.4 are affected by this issue.  *FIX VERSION* The fix is included in the 2.6.5 production release.  *RESOLUTION DETAILS* Validate that the user running {{mongod}} has a locale set. If a locale is not set, print a useful error message and exit.  {panel}  h6. Original description  Hi guys!  I'm trying to start mongod process from python script, using subprocess module, and this is what mongod crashes with:  {code} about to fork child process, waiting until server is ready for connections. forked process: 2462 Wed Mar 20 13:43:27.436 terminate() called, printing stack (if implemented for platform): 0xdcae01 0x6cd72e 0x7faab9b23846 0x7faab9b23873 0x7faab9b2396e 0x7faab9ad0a07 0x7faab9ae0284 0x7faab9ad4a74 0x7faab9ad565f 0xde86df 0xde902b 0xde5cd0 0x9e63ff 0x6dba43 0x6dd7e9 0x7faab91be76d 0x6cd519   mongod(_ZN5mongo15printStackTraceERSo+0x21) [0xdcae01]  mongod(_ZN5mongo11myterminateEv+0x3e) [0x6cd72e]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb5846) [0x7faab9b23846]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb5873) [0x7faab9b23873]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb596e) [0x7faab9b2396e]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZSt21__throw_runtime_errorPKc+0x57) [0x7faab9ad0a07]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x72284) [0x7faab9ae0284]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt6locale5_ImplC1EPKcm+0x44) [0x7faab9ad4a74]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt6localeC1EPKc+0x22f) [0x7faab9ad565f]  mongod(_ZN5boost11filesystem34path21wchar_t_codecvt_facetEv+0x4f) [0xde86df]  mongod(_ZNK5boost11filesystem34path14root_directoryEv+0xbb) [0xde902b]  mongod(_ZN5boost11filesystem38absoluteERKNS0_4pathES3_+0x40) [0xde5cd0]  mongod(_ZN5mongo27initializeServerGlobalStateEb+0x15f) [0x9e63ff]  mongod() [0x6dba43]  mongod(main+0x9) [0x6dd7e9]  /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xed) [0x7faab91be76d]  mongod(__gxx_personality_v0+0x499) [0x6cd519] Wed Mar 20 13:43:27.531 Got signal: 6 (Aborted).  Wed Mar 20 13:43:27.534 Backtrace: 0xdcae01 0x6ce879 0x7faab91d34a0 0x7faab91d3425 0x7faab91d6b8b 0x6cd733 0x7faab9b23846 0x7faab9b23873 0x7faab9b2396e 0x7faab9ad0a07 0x7faab9ae0284 0x7faab9ad4a74 0x7faab9ad565f 0xde86df 0xde902b 0xde5cd0 0x9e63ff 0x6dba43 0x6dd7e9 0x7faab91be76d   mongod(_ZN5mongo15printStackTraceERSo+0x21) [0xdcae01]  mongod(_ZN5mongo10abruptQuitEi+0x399) [0x6ce879]  /lib/x86_64-linux-gnu/libc.so.6(+0x364a0) [0x7faab91d34a0]  /lib/x86_64-linux-gnu/libc.so.6(gsignal+0x35) [0x7faab91d3425]  /lib/x86_64-linux-gnu/libc.so.6(abort+0x17b) [0x7faab91d6b8b]  mongod(_ZN5mongo11myterminateEv+0x43) [0x6cd733]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb5846) [0x7faab9b23846]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb5873) [0x7faab9b23873]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb596e) [0x7faab9b2396e]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZSt21__throw_runtime_errorPKc+0x57) [0x7faab9ad0a07]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x72284) [0x7faab9ae0284]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt6locale5_ImplC1EPKcm+0x44) [0x7faab9ad4a74]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt6localeC1EPKc+0x22f) [0x7faab9ad565f]  mongod(_ZN5boost11filesystem34path21wchar_t_codecvt_facetEv+0x4f) [0xde86df]  mongod(_ZNK5boost11filesystem34path14root_directoryEv+0xbb) [0xde902b]  mongod(_ZN5boost11filesystem38absoluteERKNS0_4pathES3_+0x40) [0xde5cd0]  mongod(_ZN5mongo27initializeServerGlobalStateEb+0x15f) [0x9e63ff]  mongod() [0x6dba43]  mongod(main+0x9) [0x6dd7e9]  /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xed) [0x7faab91be76d]  ERROR: child process failed, exited with error number 14 <err>:  <args>: ('/usr/bin/sudo', '-u', 'mongodb', 'mongod', '--fork', '--config=/etc/mongodb.shardsrv.conf', '-vv') {code}  I've tried all combinations of arguments and subprocess flags, and i still getting the same error. If i run mongod with the same args from shell - it starts, and i don't get no errors.  It worked with previous versions of mongo (2.0, 2.2), but it doesn't in 2.4.""",Bug,Stability
500775,"""While testing features3.js, I noticed write operations for the config database being sent to one of the shard servers.  The following was collected from the server on port 30000, while the config server was running on port 30999: {noformat}   {    """"opid"""" : 5688,    """"active"""" : false,    """"op"""" : """"update"""",    """"ns"""" : """""""",    """"query"""" : {     """"_id"""" : """"leaf.local:30999""""    },    """"client"""" : """"127.0.0.1:50395"""",    """"desc"""" : """"conn15"""",    """"threadId"""" : """"0x12c72f000"""",    """"connectionId"""" : 15,    """"locks"""" : {     """"^"""" : """"w"""",     """"^config"""" : """"W""""    },    """"waitingForLock"""" : true,    """"numYields"""" : 0,    """"lockStats"""" : {     """"timeLockedMicros"""" : {           },     """"timeAcquiringMicros"""" : {           }    }   } {noformat}  Since features3.js runs a 10-second sleep operation in a $where query, this operation blocks other readers until completion.  It looks like this shard server has all of the config collections you would expect to see on a config server:  {noformat} > use config switched to db config > show collections changelog chunks collections databases lockpings locks mongos settings shards system.indexes version {noformat}  This may be unrelated, but I've also seen the following error a few times while running this test:  {noformat}  m30000| Tue Mar  5 11:59:18.411 [initandlisten] connection accepted from 127.0.0.1:50907 #14 (14 connections now open)  m30999| Tue Mar  5 11:59:34.565 [LockPinger] creating new connection to:localhost:30000  m30999| Tue Mar  5 11:59:34.565 BackgroundJob starting: ConnectBG  m30000| Tue Mar  5 11:59:34.565 [initandlisten] connection accepted from 127.0.0.1:50909 #15 (15 connections now open)  m30999| Tue Mar  5 11:59:34.565 [LockPinger] connected connection!  m30999| Tue Mar  5 11:59:40.581 [Balancer] Socket recv() timeout  127.0.0.1:30000  m30999| Tue Mar  5 11:59:40.581 [Balancer] SocketException: remote: 127.0.0.1:30000 error: 9001 socket exception [3] server [127.0.0.1:30000]   m30999| Tue Mar  5 11:59:40.581 [Balancer] DBClientCursor::init call() failed  m30999| Tue Mar  5 11:59:40.581 [Balancer] Assertion: 13632:couldn't get updated shard list from config server  m30999| 0x1088d0c9b 0x1088ad71e 0x108842859 0x10879523b 0x1088aedb4 0x1088af4ea 0x1088af5b6 0x1088af676 0x108903425 0x7fff8745f742 0x7fff8744c181   m30999|  0   mongos                              0x00000001088d0c9b _ZN5mongo15printStackTraceERSo + 43  m30999|  1   mongos                              0x00000001088ad71e _ZN5mongo11msgassertedEiPKc + 174  m30999|  2   mongos                              0x0000000108842859 _ZN5mongo15StaticShardInfo6reloadEv + 403  m30999|  3   mongos                              0x000000010879523b _ZN5mongo8Balancer3runEv + 401  m30999|  4   mongos                              0x00000001088aedb4 _ZN5mongo13BackgroundJob7jobBodyEN5boost10shared_ptrINS0_9JobStatusEEE + 652  m30999|  5   mongos                              0x00000001088af4ea _ZNK5boost4_mfi3mf1IvN5mongo13BackgroundJobENS_10shared_ptrINS3_9JobStatusEEEEclEPS3_S6_ + 68  m30999|  6   mongos                              0x00000001088af5b6 _ZN5boost3_bi5list2INS0_5valueIPN5mongo13BackgroundJobEEENS2_INS_10shared_ptrINS4_9JobStatusEEEEEEclINS_4_mfi3mf1IvS4_S9_EENS0_5list0EEEvNS0_4typeIvEERT_RT0_i + 54  m30999|  7   mongos                              0x00000001088af676 _ZN5boost6detail11thread_dataINS_3_bi6bind_tIvNS_4_mfi3mf1IvN5mongo13BackgroundJobENS_10shared_ptrINS7_9JobStatusEEEEENS2_5list2INS2_5valueIPS7_EENSD_ISA_EEEEEEE3runEv + 42  m30999|  8   mongos                              0x0000000108903425 thread_proxy + 229  m30999|  9   libsystem_c.dylib                   0x00007fff8745f742 _pthread_start + 327  m30999|  10  libsystem_c.dylib                   0x00007fff8744c181 thread_start + 13  m30999| Tue Mar  5 11:59:40.596 [Balancer] Detected bad connection created at 1362513544578032 microSec, clearing pool for localhost:30000  m30999| Tue Mar  5 11:59:40.596 [Balancer] scoped connection to localhost:30000 not being returned to the pool  m30999| Tue Mar  5 11:59:40.596 [Balancer] caught exception while doing balance: couldn't get updated shard list from config server  m30999| Tue Mar  5 11:59:40.596 [Balancer] *** End of balancing round ^CTue Mar  5 11:59:45.437 got signal 2 (Interrupt: 2), will terminate after current cmd ends Tue Mar  5 11:59:45.437 [interruptThread] now exiting Tue Mar  5 11:59:45.437 dbexit:  Tue Mar  5 11:59:45.437 [interruptThread] shutdown: going to close listening sockets... Tue Mar  5 11:59:45.437 [interruptThread] closing listening socket: 9 Tue Mar  5 11:59:45.437 [interruptThread] closing listening socket: 10 Tue Mar  5 11:59:45.437 [interruptThread] closing listening socket: 11 Tue Mar  5 11:59:45.438 [interruptThread] removing socket file: /tmp/mongodb-27999.sock Tue Mar  5 11:59:45.438 [interruptThread] shutdown: going to flush diaglog... {noformat}    """,Bug,"Test,Sharding"
501003,"""Let's say I have a collection of 100.000 objects like this:  {""""user"""":""""<USER>"",""""created"""":""""2013-10-10"""",""""updates"""":[ {""""uDate"""":""""2013-10-10"""",...} , {""""uDate"""":""""2013-10-11"""",...}  ]}  And the following index that should be used on my queries:  {""""updates.uDate"""":1}  The query:  db.users.find({""""updates"""":{$elemMatch:{""""uDate"""":{$gte:""""2013-10-10"""",$lt:""""2013-10-12""""}}}}).count();  if I try to query this way, the results are correct but the usage of index is not, that's why it takes so much time to get results. This has a total of 559 results, what is correct, but to try to understand the delay on this query, I tried:  db.users.find({""""updates"""":{$elemMatch:{""""uDate"""":{$gte:""""2013-10-10"""",$lt:""""2013-10-12""""}}}}).explain();  So I got:  {         """"cursor"""" : """"BtreeCursor userDate"""",         """"isMultiKey"""" : true,         """"n"""" : 559,         """"nscannedObjects"""" : 434513,         """"nscanned"""" : 434513,         """"nscannedObjectsAllPlans"""" : 434513,         """"nscannedAllPlans"""" : 434513,         """"scanAndOrder"""" : false,         """"indexOnly"""" : false,         """"nYields"""" : 3,         """"nChunkSkips"""" : 0,         """"millis"""" : 4547,         """"indexBounds"""" : {                 """"updates.uDate"""" : [                         [                                 """"2013-10-10"""",                                 {                                  }                         ]                 ]         },         """"server"""" : """"fzdv1:27017"""" }  After thinking a lot, I could not realise why the index does not have an End (2013-10-12), just a Start (2013-10-10).. a RANGE. It could be because it's a MultiKey index, so what about forcing the index to work as expected:  db.users.find().min( {""""updates.uDate"""":""""2013-10-10""""} ).max( {""""updates.uDate"""":""""2013-10-12""""} ).hint( {""""updates.uDate"""":1}).explain();  {         """"cursor"""" : """"BtreeCursor userDate"""",         """"isMultiKey"""" : true,         """"n"""" : 559,         """"nscannedObjects"""" : 559,         """"nscanned"""" : 560,         """"nscannedObjectsAllPlans"""" : 559,         """"nscannedAllPlans"""" : 560,         """"scanAndOrder"""" : false,         """"indexOnly"""" : false,         """"nYields"""" : 0,         """"nChunkSkips"""" : 0,         """"millis"""" : 9,         """"indexBounds"""" : {                 """"start"""" : {                         """"updates.uDate"""" : """"2013-10-10""""                 },                 """"end"""" : {                         """"updates.uDate"""" : """"2013-10-12""""                 }         },         """"server"""" : """"fzdv1:27017"""" }  Perfect..it worked really fast and with a correct result of 559 objects. My """"workaround"""" would solve my problem but not yet. What I need is to use aggregate function so that I can $unwind the """"updates.uDate"""".  Using $match produces the same index error/delay and, as far as I know, there is no way to force index range (""""hint""""/""""min""""/""""max"""") on an aggregate $match.  Because of that, my only solution is to break my users collection into two, """"users"""" and """"users_updates"""". Then I could use something like:  db.users.find({""""created"""":{$gte:""""2013-10-10"""",$lt:""""2013-10-12""""}}).explain();  {         """"cursor"""" : """"BtreeCursor created"""",         """"isMultiKey"""" : false,         """"n"""" : 126,         """"nscannedObjects"""" : 126,         """"nscanned"""" : 126,         """"nscannedObjectsAllPlans"""" : 126,         """"nscannedAllPlans"""" : 126,         """"scanAndOrder"""" : false,         """"indexOnly"""" : false,         """"nYields"""" : 0,         """"nChunkSkips"""" : 0,         """"millis"""" : 8,         """"indexBounds"""" : {                 """"created"""" : [                         [                                 """"2013-10-10"""",                                 """"2013-10-12""""                         ]                 ]         },         """"server"""" : """"fzdv1:27017"""" }  Why $elemMatch queries only works with indexes (range - start/end) as expected when forced to? Why querying objects in an array of objects does not uses indexes as normal queries since it is technicaly possible, as shown above?  Here is the query that would solve my problem if it worked properly with its index range (start/END).  db.users.aggregate(   {$match   : {""""updates"""":{$elemMatch:{""""uDate"""":{$gte:""""2013-10-10"""",$lt:""""2013-10-12""""}}}}},   {$unwind  : """"$updates""""},   {$project : {""""updates.uDate"""":1}},   {$match   : {""""updates.uDate"""":{$gte:""""2013-10-10"""",$lte:""""2013-10-12""""}}},   {$sort    : {""""updates.uDate"""":1}} );  Am I missing something or it's a real issue?  Thanks.""",Bug,Querying
501081,"""The $external useSource was somehow created as a database, which doesn't seem to be quite logical. It should be an external artifact I'd have thought.  I discovered this during testing of adding a sharded cluster infrastructure consisting of two already-Kerborised replica sets.  1. Step : added realm4 replica set as a shard  {code} mongos> sh.addShard( """"realm4/kserver1a.realm4.10gen.me:27017"""" ) { """"shardAdded"""" : """"realm4"""", """"ok"""" : 1 } {code}  2. Step : added realm5 set as a shard, however, this is rejected due to the existence of the $external database on both realms. This is logical but it means some reconfiguration if making two Kerberos-enabled replica sets part of a sharded cluster.  {code} mongos> sh.addShard( """"realm5/kserver1a.realm5.10gen.me:27017"""" ) {  """"ok"""" : 0,  """"errmsg"""" : """"can't add shard realm5/kserver1a.realm5.10gen.me:27017 because a local database '$external' exists in another realm4:realm4/kserver1a.realm4.10gen.me:27017,kserver1b.realm4.10gen.me:27017,kserver1c.realm4.10gen.me:27017"""" } {code}  So as expected, I can drop the $external database below -  {code} realm5:PRIMARY> db.dropDatabase() { """"dropped"""" : """"$external"""", """"ok"""" : 1 } realm5:PRIMARY>  realm5:PRIMARY>  realm5:PRIMARY>  realm5:PRIMARY> show dbs $SERVER (empty) admin 0.0625GB db1 0.0625GB fred (empty) local 1.07763671875GB {code}  After removing the $external database, I can now add realm5 as the second shard -   {code} mongos> sh.addShard( """"realm5/kserver1a.realm5.10gen.me:27017"""" ) { """"shardAdded"""" : """"realm5"""", """"ok"""" : 1 } {code}  {code} mongos> sh.status() --- Sharding Status ---    sharding version: {  """"_id"""" : 1,  """"version"""" : 3,  """"minCompatibleVersion"""" : 3,  """"currentVersion"""" : 4,  """"clusterId"""" : ObjectId(""""511a58f8bc0ae36e0fab6574"""") }   shards:  {  """"_id"""" : """"realm4"""",  """"host"""" : """"realm4/kserver1a.realm4.10gen.me:27017,kserver1b.realm4.10gen.me:27017,kserver1c.realm4.10gen.me:27017"""" }  {  """"_id"""" : """"realm5"""",  """"host"""" : """"realm5/kserver1a.realm5.10gen.me:27017,kserver1b.realm5.10gen.me:27017,kserver1c.realm5.10gen.me:27017"""" }   databases:  {  """"_id"""" : """"admin"""",  """"partitioned"""" : false,  """"primary"""" : """"config"""" }  {  """"_id"""" : """"fred"""",  """"partitioned"""" : false,  """"primary"""" : """"realm4"""" }  {  """"_id"""" : """"$external"""",  """"partitioned"""" : false,  """"primary"""" : """"realm4"""" }  {  """"_id"""" : """"db1"""",  """"partitioned"""" : false,  """"primary"""" : """"realm5"""" } {code}  Out of curiousity what is the $SERVER database? I never created it. """,Bug,"Sharding,Security"
501147,"""I set up a replica set with 3 nodes, one acting as arbiter.   after initializing and adding the hosts, I added the third as a arbiter using rs.addArb(host)  rs.conf()  {code} """"_id"""" : """"api"""",  """"version"""" : 7,  """"members"""" : [   {    """"_id"""" : 0,    """"host"""" : """"minnow-data-1.cloud.appcelerator.com:27017""""   },   {    """"_id"""" : 1,    """"host"""" : """"minnow-data-2.cloud.appcelerator.com:27017""""   },   {    """"_id"""" : 2,    """"host"""" : """"minnow-data-arbiter-1.cloud.appcelerator.com:27017"""",    """"arbiterOnly"""" : true   }  rs.status()   """"set"""" : """"api"""",  """"date"""" : ISODate(""""2013-02-06T23:51:40Z""""),  """"myState"""" : 2,  """"syncingTo"""" : """"minnow-data-2.cloud.appcelerator.com:27017"""",  """"members"""" : [   {    """"_id"""" : 0,    """"name"""" : """"minnow-data-1.cloud.appcelerator.com:27017"""",    """"health"""" : 1,    """"state"""" : 2,    """"stateStr"""" : """"SECONDARY"""",    """"uptime"""" : 4884,    """"optime"""" : Timestamp(1360189735000, 1),    """"optimeDate"""" : ISODate(""""2013-02-06T22:28:55Z""""),    """"self"""" : true   },   {    """"_id"""" : 1,    """"name"""" : """"minnow-data-2.cloud.appcelerator.com:27017"""",    """"health"""" : 1,    """"state"""" : 1,    """"stateStr"""" : """"PRIMARY"""",    """"uptime"""" : 4882,    """"optime"""" : Timestamp(1360189735000, 1),    """"optimeDate"""" : ISODate(""""2013-02-06T22:28:55Z""""),    """"lastHeartbeat"""" : ISODate(""""2013-02-06T23:51:39Z""""),    """"pingMs"""" : 1   },   {    """"_id"""" : 2,    """"name"""" : """"minnow-data-arbiter-1.cloud.appcelerator.com:27017"""",    """"health"""" : 1,    """"state"""" : 2,    """"stateStr"""" : """"SECONDARY"""",    """"uptime"""" : 4882,    """"lastHeartbeat"""" : ISODate(""""2013-02-06T23:51:39Z""""),    """"pingMs"""" : 12   }  ],  """"ok"""" : 1  I would expect the third node to be in state 7 for arbiter.  """,Improvement,Replication
501231,"""I'm not sure if this is by design or not, but it leads to kind of weird behavior.  If I have 2 mongos processes and move a chunk on one, then try to move the chunk again from the other, the chunk info doesn't update before the second move attempt so it fails with unintuitive messages.  For example, on mongos #1, I do:  {code} test> sh.moveChunk(""""test.not.hashed"""", {user_id:""""user438""""}, """"test-rs2"""") { """"millis"""" : 3441, """"ok"""" : 1 } {code}  On mongos #2, I then do:  {code} config> // first, checking it was actually moved: config> db.chunks.find({min:{user_id:""""user438""""}}) { """"_id"""" : """"test.not.hashed-user_id_\""""user438\"""""""", """"lastmod"""" : { """"t"""" : 52000, """"i"""" : 0 }, """"lastmodEpoch"""" : ObjectId(""""510681b502471f5418db3e35""""), """"ns"""" : """"test.not.hashed"""", """"min"""" : { """"user_id"""" : """"user438"""" }, """"max"""" : { """"user_id"""" : """"user441"""" }, """"shard"""" : """"test-rs2"""" } config> config> // yup, it's in the right place, but this mongos doesn't think so: config> sh.moveChunk('test.not.hashed', {user_id:'user438'}, 'test-rs1') { """"ok"""" : 0, """"errmsg"""" : """"that chunk is already on that shard"""" } config> config> // let's try another shard config> sh.moveChunk('test.not.hashed', {user_id:'user438'}, 'test-rs4') {         """"cause"""" : {                 """"from"""" : """"test-rs1"""",                 """"official"""" : """"test-rs2"""",                 """"ok"""" : 0,                 """"errmsg"""" : """"location is outdated (likely balance or migrate occurred)""""         },         """"ok"""" : 0,         """"errmsg"""" : """"move failed"""" } config> config> // and now it finally works config> sh.moveChunk('test.not.hashed', {user_id:'user438'}, 'test-rs1') { """"millis"""" : 3859, """"ok"""" : 1 } {code} """,Improvement,Sharding
501247,"""Support Query Hinting by Index Name in the Mongo shell, as addressed in RUBY-493:   {code} db.coll.ensureIndex({ k: 1 }, { name: """"mySortKey"""" });  db.coll.save({ k: 1, v: """"someValue"""" });  // ... insert more documents  db.coll.find().hint(""""mySortKey"""");  {code}  I noticed an old ticket (SERVER-8) about removing index names as redundant.  I don't know if that's still intended, and I do appreciate the necessity of simplicity.  At the same time, I want to defend index names as an important means of hiding implementation details.    Sure, developers """"should"""" normally know the structure of the indices they're using.  However, in some cases we might just be using an index to ensure quick sorting on an abstract property, regardless of which document <USER>that property happens to cover.  Query hints are a key area to realize the value of index names, because symbolic names can allow developers to vary index definition in response to """"schema"""" adjustments (albeit Mongo is NoSQL, we still have pseudo-schemas) without impacting the optimization of downstream queries.  And, given that field names are often optimized to a fine point just to reduce storage size (see SERVER-863), the field structure of an index tends to be far less meaningful than its name.    Indices are the pith of queries, and index names are the closest Mongo has to named views.  Let's take them to the next level and allow hinting on index names!  :) """,Improvement,"Querying,Indexing,Usability"
501425,"""The server currently only allows you to authenticate one user at a time from a given database per connection. Now that we support storing users in one database and granting them access to other databases, here's a reasonable use case:  1. I create users """"X"""" and """"Y"""" in the """"users"""" database 2. I grant """"X"""" access to some databases 3. I grant """"Y"""" access to some other databases 4. I want to authenticate as both """"X"""" and """"Y"""" on the same connection to get access to both sets of databases  This is in contradiction to SERVER-8144 which requests that logging on as """"Y"""" (after """"X"""") will automatically log off """"X"""". But I think the behavior being requested in this JIRA is more logical and useful.  A related change would be that the logout command should have an additional parameter called """"username"""".  This would be slightly backward breaking if a program was counting on the previous behavior, but only in the sense that they would have slightly more privileges than if """"X"""" was automatically logged out.""",Improvement,Security
501641,"""After upgrading to 2.2.2 (and php driver 1.3.1) we've started noticing some odd timeouts. We use a multikey index on a set of """"digested"""" words as a pseudo-fulltext search, and then query with $all : [ ... ], which has worked very well.   After the upgrade, the query optimizer doesn't seem to want to use those indexes in many cases when combined with sort, which results in awful nscanned:  {code} data:PRIMARY> db.a_ftx.find( { ftx: { $all: [ """"the"""", """"xx"""" ] } } ).sort( { last_seen: -1 } ).limit(500).explain(); {         """"cursor"""" : """"BtreeCursor last_seen_-1"""",         """"isMultiKey"""" : false,         """"n"""" : 382,         """"nscannedObjects"""" : 596759,         """"nscanned"""" : 596759,         """"nscannedObjectsAllPlans"""" : 716532,         """"nscannedAllPlans"""" : 716532,         """"scanAndOrder"""" : false,         """"indexOnly"""" : false,          """"nYields"""" : 131,         """"nChunkSkips"""" : 0,         """"millis"""" : 3710,         """"indexBounds"""" : {                 """"last_seen"""" : [                         [                                 {                                         """"$maxElement"""" : 1                                 },                                 {                                         """"$minElement"""" : 1                                 }                         ]                  ]          },          """"server"""" : """"xx:27017""""  } {code}  If we provide a hint, performance is better, but still not great because only the first term inside $all appears to be used for the index. I'm not sure if the previous behavior would have been to use both (i.e. find both sets then compute intersection), but performance used to be *much* better.  {code} data:PRIMARY> db.a_ftx.find( { ftx: { $all: [ """"the"""", """"xx"""" ] } } ).hint({ ftx : 1 }).sort( { last_seen: -1 } ).limit(500).explain(); {         """"cursor"""" : """"BtreeCursor ftx_1"""",         """"isMultiKey"""" : true,         """"n"""" : 382,         """"nscannedObjects"""" : 131956,         """"nscanned"""" : 131956,         """"nscannedObjectsAllPlans"""" : 131956,         """"nscannedAllPlans"""" : 131956,         """"scanAndOrder"""" : true,         """"indexOnly"""" : false,         """"nYields"""" : 174,         """"nChunkSkips"""" : 0,         """"millis"""" : 5901,         """"indexBounds"""" : {                 """"ftx"""" : [                         [                                 """"the"""",                                 """"the""""                         ]                 ]         },         """"server"""" : """"xx:27017"""" } {code}  If we manually reorder the terms inside $all, the result is pretty good, similar to past performance:  {code} data:PRIMARY> db.a_ftx.find( { ftx: { $all: [ """"xx"""", """"the"""" ] } } ).hint({ ftx : 1 }).sort( { last_seen: -1 } ).limit(500).explain(); {         """"cursor"""" : """"BtreeCursor ftx_1"""",         """"isMultiKey"""" : true,         """"n"""" : 382,         """"nscannedObjects"""" : 511,         """"nscanned"""" : 511,         """"nscannedObjectsAllPlans"""" : 511,         """"nscannedAllPlans"""" : 511,         """"scanAndOrder"""" : true,         """"indexOnly"""" : false,         """"nYields"""" : 3,         """"nChunkSkips"""" : 0,         """"millis"""" : 20,         """"indexBounds"""" : {                 """"ftx"""" : [                         [                                  """"xx"""",                                 """"xx""""                         ]                 ]         },         """"server"""" : """"xx:27017"""" } {code} {code} data:PRIMARY> db.a_ftx.find( { ftx: { $all: [ """"the"""", """"xx"""" ] } } ).hint({ ftx : 1 }).limit(500).explain(); {         """"cursor"""" : """"BtreeCursor ftx_1"""",         """"isMultiKey"""" : true,         """"n"""" : 382, {code}  Finally, the behavior of the optimizer seems unpredictable, for example some (absolutely catastrophic) slow queries logged like this:  {code} Wed Dec 12 14:30:21 [conn50070] query ftx.a_ftx query: { $query: { ftx: { $all: [ """"hamilton"""", """"jo"""" ] } }, $orderby: { last_seen: -1 } } ntoreturn:500 ntoskip:0 nscanned:596752 keyUpdates:0 numYields: 724 locks(micros) r:39541636 nreturned:3 reslen:143 20917ms {code} behave reasonably if reproduced exactly via the mongo shell:  {code} data:PRIMARY> db.a_ftx.find( { ftx: { $all: [ """"hamilton"""", """"jo"""" ] } } ).sort( { last_seen: -1 } ).limit(500).explain();  {          """"cursor"""" : """"BtreeCursor ftx_1"""",          """"isMultiKey"""" : true,          """"n"""" : 3,          """"nscannedObjects"""" : 148,          """"nscanned"""" : 148,          """"nscannedObjectsAllPlans"""" : 444,          """"nscannedAllPlans"""" : 444,          """"scanAndOrder"""" : true,          """"indexOnly"""" : false,          """"nYields"""" : 0,          """"nChunkSkips"""" : 0,          """"millis"""" : 1,          """"indexBounds"""" : {                  """"ftx"""" : [                          [                                  """"hamilton"""",                                  """"hamilton""""                          ]                  ]          },          """"server"""" : """"xx:27017""""  }  {code} """,Bug,Indexing
501882,"""Posted to mongo users group also.  Looking through the existing bugs I found a bug which sounded similar which was that hint() didn't work with count().  There was a vague comment that the results returned when using hint() would not be valid, but nothing more.  I'll look up the bug number after this.   I'm attempting to isolate a performance problem with some PHP code and MongoDB. I'm beginning to think that either hint() or explain() doesn't do what I think it should do. In the PHP below, I use to not have the hint.  It was painfully slow.  Putting the hint in made things quicker, but the results don't seem correct.  I'm sure I'm missing something, but have a read.  Thanks.  MongoDB version 2.2.1.  I'm doing the following in PHP:     $results = $db->Results->find(array('Test Results' => array('$exists' => true), 'atSchool' => 'yes', 'onNetwork' => 'yes', 'schoolID' => (string)$s['_id']), array('_id' => 1));     $results = $results->hint(array(""""Test Results"""" => 1, """"atSchool"""" => 1,                                         """"onNetwork"""" => 1, """"schoolID"""" => 1));     $numR = 0;     foreach ($results as $r) {         $numR = $numR + 1;     }  or in mongo shell:  db.Results.find( { """"Test Results"""" : { """"$exists"""" : true }, """"atSchool"""" : """"yes"""", """"onNetwork"""" : """"yes"""", """"schoolID"""" : """"550627000691"""" }).hint({""""Test Results"""" : 1, """"atSchool"""" : 1, """"onNetwork"""" : 1, """"schoolID"""" : 1}).explain()  Now here's the issue. explain() returns n=113, which if I read the document correctly should be the number of results returned. Yet if I run the above mongo shell command with count() instead of explain it returns 5641, which I believe is the correct number. The PHP code seems to return the same as explain().  explain() output: {         """"cursor"""" : """"BtreeCursor Test Results_1_atSchool_1_onNetwork_1_schoolID_1"""",         """"isMultiKey"""" : false,         """"n"""" : 113,         """"nscannedObjects"""" : 177,         """"nscanned"""" : 6531,         """"nscannedObjectsAllPlans"""" : 177,         """"nscannedAllPlans"""" : 6531,         """"scanAndOrder"""" : false,         """"indexOnly"""" : false,         """"nYields"""" : 0,         """"nChunkSkips"""" : 0,         """"millis"""" : 50,         """"indexBounds"""" : {                 """"Test Results"""" : [                         [                                 {                                         """"$minElement"""" : 1                                 },                                 {                                         """"$maxElement"""" : 1                                 }                         ]                 ],                 """"atSchool"""" : [                         [                                 """"yes"""",                                 """"yes""""                         ]                 ],                 """"onNetwork"""" : [                         [                                 """"yes"""",                                 """"yes""""                         ]                 ],                 """"schoolID"""" : [                         [                                 """"550627000691"""",                                 """"550627000691""""                         ]                 ]         },         """"server"""" : """"ip-1-1-1-1:27017"""" }  count() output: > db.Results.find( { """"Test Results"""" : { """"$exists"""" : true }, """"atSchool"""" : """"yes"""", """"onNetwork"""" : """"yes"""", """"schoolID"""" : """"550627000691"""" }).hint({""""Test Results"""" : 1, """"atSchool"""" : 1, """"onNetwork"""" : 1, """"schoolID"""" : 1}).count() 5641  If I check the index I have: [         {                 """"v"""" : 1,                 """"key"""" : {                         """"Test Results"""" : 1,                         """"atSchool"""" : 1,                         """"onNetwork"""" : 1,                         """"schoolID"""" : 1                 },                 """"ns"""" : """"Edu.Results"""",                 """"name"""" : """"Test Results_1_atSchool_1_onNetwork_1_schoolID_1""""         } ]   I've rebuilt the indexes, run ensureIndex, etc.  No changes.  Anyone have ideas? """,Question,"Querying,Indexing"
501961,"""Implement stats similar to those found on the db and col levels.    Currently there is no efficient way of obtaining stats such as the size of a document without sending the document down the wire to the client and bson encoding the document.    Suggest storing document stats as meta data beside each document in a collection but only return such stats data when requested as shown in the following examples.    Return a summary (aggregation of stats):    db.col.findOne({}).stats();  db.col.find({}).stats();  * would return a document similar to db.stats() and col.stats() and contain an aggregation of all documents in the server cursor  * in the case of findOne it would represent the stats of a single document because only one document in the cursor (thus by implementing it at the cursor level it covers both single document and aggregation scenarios)    Return documents and stats embedded using a flag on the find() operation:    db.col.find({}, {stats:true});  * stats could be attached as an embedded document in the _stats key on each document  * as the stats would be located beside the document on disk it should be a quick and efficient operation to perform    As you can see from the examples above this would be best implemented on the server cursor. I would suggest storing stats meta data beside documents on disk as opposed to storing them in a separate hash table or other data structure. This is to ensure efficient retrievals of both documents and stats in a flexible manner and to ensure writes remain fast.""",New Feature,"Storage,Admin"
502017,"""I have a relatively simple use case, IMHO.  The system receives events that eventually end up as objects in a Mongo collection.  The system has an """"event cache"""", which is used to accumulate single event data over some time. The event cache utilizes some primary key system to identify updates to the same events. This primary key is what is then used as document ID for Mongo.  Once the event cache deems a number of events to be """"completed"""", the events are flushed out into Mongo, and are removed from the cache. This """"flushing"""" is done using bulk insert.  Once in a blue moon, however, there is a problem, and the cache can not purge the events that have been flushed out. As a result, when the next flush occurs, the bulk insert fails because there is a document ID collision.  Now, I've set the """"ContinueOnLastError"""" to true, and I hope that this will prevent exceptions (I'm using Java driver) in the bulk inserts. However, I would prefer that in case of a collision, the documents are overwritten, instead of preferring the one that's already in the collection.  Would it not be reasonable to add a feature to overwrite documents during bulk insert? This is not an upsert, as the document is completely overwritten, and not updated.""",Improvement,Write Ops
502226,"""in a system with 3 config servers, i shut down one of them.  i then did the following which returned an error.  (1) the message isn't very clear i would really like to know that a config server was unreachable not a mongod for example.  (2) arguably i shouldn't get an error at all the query could be retried to another server.  however be careful retries could add latency if multiple down and/or have a multiplicative effect on load. also is there a background detector of heartbeat?  that might help some as another approach.""",Improvement,Sharding
502338,"""After investigating this more, and discussing this with others, here is what I've found:    * The entries (lock time specifically) in the profile collection include the time taken to write to the system.profile collection, in addition to the orig. op  * We do not separate the user operation from the system logging/profiling.  * This is confusing for read-only operations like queries and getmores, since they never need a write lock  * It would be good if we can get trace or separate levels of reporting for the various stages of the whole operation so it is clear what the cost of the user operation would be without profiling    Originally I had expected, as did the users who noticed this, that """"lockStats.timeAcquiringMicros.w"""" should be zero since it was a read-only operation.  {code}  {   """"ts"""" : ISODate(""""2012-09-21T18:50:05.301Z""""),   """"op"""" : """"query"""",   """"ns"""" : """"l.c"""",   """"query"""" : {       },   """"cursorid"""" : NumberLong(""""168104220735197301""""),   """"ntoreturn"""" : 0,   """"ntoskip"""" : 0,   """"nscanned"""" : 102,   """"keyUpdates"""" : 0,   """"numYield"""" : 0,   """"lockStats"""" : {    """"timeLockedMicros"""" : {     """"r"""" : NumberLong(73),     """"w"""" : NumberLong(0)    },    """"timeAcquiringMicros"""" : {     """"r"""" : NumberLong(3),     """"w"""" : NumberLong(2)    }   },   """"nreturned"""" : 101,   """"responseLength"""" : 17815,   """"millis"""" : 0,   """"client"""" : """"127.0.0.1"""",   """"user"""" : """"""""  }  {   """"ts"""" : ISODate(""""2012-09-21T18:50:05.425Z""""),   """"op"""" : """"getmore"""",   """"ns"""" : """"l.c"""",   """"cursorid"""" : NumberLong(""""168104220735197301""""),   """"ntoreturn"""" : 0,   """"keyUpdates"""" : 0,   """"numYield"""" : 0,   """"lockStats"""" : {    """"timeLockedMicros"""" : {     """"r"""" : NumberLong(8781),     """"w"""" : NumberLong(0)    },    """"timeAcquiringMicros"""" : {     """"r"""" : NumberLong(11),     """"w"""" : NumberLong(5)    }   },   """"nreturned"""" : 865,   """"responseLength"""" : 150008,   """"millis"""" : 8,   """"client"""" : """"127.0.0.1"""",   """"user"""" : """"""""  }    {code}""",Bug,"Internal Code,Concurrency"
502580,"""mongod should store its startup options in a file in the dbpath, so that we can check on startup whether our current options are different from our previous options in a possibly dangerous way. Here's why: for some server options, toggling them across mongod reboots can lead to badness that it'd probably be worthwhile to try to detect, at least so we can warn about the settings change at startup, if not more. For example:  {code} --directoryperdb toggling this across mongod reboots without also moving files around causes the mongod to start up with an empty set of databases, which can be construed as a data loss situation.  --replSet toggling this per se isn't a problem, but frobbing other flags when restarting a process that was previously a replica set member can create problems. Occasionally people rebuild their data files after a crash by starting with --repair, which, due of the nature of --repair, can lead to data skew among replica set members. And changing --bind_ip or --port on a mongod that was a replica set member will cause that process to operate independent of the replica set.  --auth/--noauth toggling auth will either break people's programs or invalidate people's security. {code}  There are probably other combinations I'm not remembering. The ways these server flags tend to get toggled across reboots have included situations where config files got garbled into a deployment (e.g., the operator used a customized config file, but something replaced the config file by the time the mongod got rebooted), and cases where, during an emergency or operational transition scenario, an operator starts up a mongod with manually specified flags, possibly the wrong ones).  Relatedly, in the mongos, changing the configdb parameter (especially from 3 configsvrs to 1) could be very bad. Unfortunately the mongos doesn't have a dbpath, so preserving old options isn't obviously easy. """,New Feature,Usability
502624,"""I upgraded one server in a 6-server 2.0.1 replicaSet to test MongoDB 2.2.0-rc1. I wanted to test read and locking performance so I switched to a db with a very large collection that easily exceeds RAM. I then ran a query that I knew would match no records: db.collection.find({x: 1}). This caused the mongostat I was running in another window to stop responding, I could connect with another shell but """"show dbs"""" would not return until the above query completed. Also, hitting Ctrl-C on the find() query caused """"Fri Aug 17 19:07:19 Assertion: 13111:field not found, expected type 2"""" in the shell. I was unable to manually kill the find() operation.  I also tried this a second time so I could be sure that as much of the database that could be hot in memory was available.""",Bug,Performance
502644,"""The DB profiler creates illegal documents. These documents contain: 1) keys with $ in the name 2) keys with . in the name  This problem makes working with these documents incredibly difficult.  Here's an example: {code} PRIMARY> test.record = db.system.profile.findOne({""""ts"""" : ISODate(""""2012-08-12T14:26:18.806Z"""")}) {  """"ts"""" : ISODate(""""2012-08-12T14:26:18.806Z""""),  """"op"""" : """"query"""",  """"ns"""" : """"gimmebar.collectionassets"""",  """"query"""" : {   """"$query"""" : {    """"collection_id"""" : """"redacted""""   },   """"$orderby"""" : {    """"date"""" : -1   }  },  """"ntoreturn"""" : 5,  """"nscanned"""" : 169,  """"scanAndOrder"""" : true,  """"nreturned"""" : 5,  """"responseLength"""" : 325,  """"millis"""" : 227,  """"client"""" : """"redacted"""",  """"user"""" : """""""" }  PRIMARY> db.tmp_profiler_test.insert(testrecord) Tue Aug 14 18:39:40 uncaught exception: field names cannot start with $ [$query] {code} The same thing happens for $orderby, $or, $set, etc., as well as for keys with """"."""" in the name.  I think the profiler should not create documents that can't be inserted. (I had to insert them into another collection because I wanted to filter the profiler data set, but I can't delete from a capped collection, which db.system.profile is.)  I realize this changes the data format of the profiler, but it seems like the right move to me.  ---  As a workaround, for those developers who might stumble upon this, I managed to get the following transposition to work, but it's far from ideal: {code} // recursion is fun function rename$(r) {     for (k in r) {         var changed = false;         var newk = k;         if (newk.indexOf(""""."""") != -1) {             changed = true;             newk = newk.replace(""""."""", """"_DOT_"""", """"g"""");         }         if (newk.indexOf(""""$"""") != -1) {             changed = true;             newk = newk.replace(""""$"""", """"_DOLLAR_"""", """"g"""");         }         if (changed) {             r[newk] = r[k];             delete r[k];         }         if (typeof r[newk] == """"object"""") {             r[newk] = rename$(r[newk]);         }     }     return r; } {code} {code} db.tmp_profiler_test.drop(); db.system.profile.find(query_parameters).forEach(function(r) { r = rename$(r); db.tmp_profiler_test.insert(r);}) {code} (I also couldn't get this box to respect wiki markup, so editor: feel free to fix.) [Done.]""",Improvement,Diagnostics
502857,"""I have 2 sets of """"ored"""" conditions because I need all rows that meet conditions A or B or C, and that also meet conditions D or E or F. Since I can only have a single $or statement, I use this: $or:[{A},{B},{C}],$and[{$or:[{D},{E},{F}]}] This returns the correct results, and by the way, uses indexes on A, B and C and runs in parallel, which is fantastic (good job!!!). However, if my indexes on D,E and F are even better choices than A,B and C, let's say they have better selectivity, the query optimizer completely ignores them as viable plans. Moreover, there is no way for me to give a hint to use them, because hint can only take a single index. Even if I had a combined index on D_E_F and passed it as a hint, it would not help because the query on E and on F would not be able to use it.  I think we need either a mechanism to specify multiple first level $ors (I know JS does not make it easy) or a smarter query optimizer that considers more alternatives, even if they are hidden inside an $and.""",Bug,Querying
502943,"""I am trying to set up Mongo DB as a service under Win XP sp3 (mongod says sp2), version 2.0.6 pdfile version 4.5  To do this I have set up a config file -    # mongodb.cfg    # data directory   dbpath = """"C:\Documents and Settings\xxx\Ubiquiti UniFi\data\db""""    # log file directory   logpath = """"C:\logs\server.log""""    # append to log file   logappend = true    # port mongodb listens on   port = 27117  My command line is -    """"C:\Documents and Settings\Robin StClair\Ubiquiti UniFi\bin\mongod.exe"""" --config """"C:\Documents and Settings\Robin StClair\Ubiquiti UniFi\conf\mongodb.cfg"""" --install  as I wish to set up Mongo DB as a service.  Mongod.exe executes, finds and opens the config file and then an errno:22 is thrown and mongod.exe shuts down gracefully.  I believe this is a Visual C++ error (doesn't like long command strings, hence my short path to log file (to see if that would get rid of the problem)).  I would be grateful for some guidance  R+C""",Bug,Packaging
503056,"""When running some tests on EC2, I was using mongoperf to evaluate disk performance. mongoperf was run with settings {nThreads:16, w:true, fileSizeMB:10000}. Despite ramping up the number of threads (tried up to 64 threads) mongoperf could never do more than ~1100 ops / second. However, if I run multiple mongoperf processes, I was able to get the aggregate up to 5000 ops / second. Seems like 1 mongoperf instance should be able to saturate the disk. I've attached a chart showing iops reported from iostat during the test. the """"steps"""" on the left hand side show the aggregate throughput as i went from 3 -> 4 -> 5 mongo perf instances. the """"valleys"""" are the time during which the new mongoperf is allocating the data file. the bulk of the graph is the performance with 5 mongo perf's running simultaneously. """,Bug,Tools
503119,"""I'm trying to output the results of a query as a csv file using mongoexport. The query runs fine, here's a sample of the output:  > db.discussions.find({},{subject:1,number_of_comments:1}) { """"_id"""" : ObjectId(""""4d094991516bcb9029000002""""), """"number_of_comments"""" : 1, """"subject"""" : """"APH10132614"""" } { """"_id"""" : ObjectId(""""4d094992516bcb9029000004""""), """"number_of_comments"""" : 5, """"subject"""" : """"APH10112880"""" } { """"_id"""" : ObjectId(""""4d094992516bcb9029000006""""), """"number_of_comments"""" : 1, """"subject"""" : """"APH10067253"""" } { """"_id"""" : ObjectId(""""4d094992516bcb9029000008""""), """"number_of_comments"""" : 0, """"subject"""" : """"APH10042670"""" } { """"_id"""" : ObjectId(""""4d094992516bcb902900000a""""), """"number_of_comments"""" : 25, """"subject"""" : """"APH10042299"""" }  I figured this would be an easy one to output as csv since it's already in that sort of format, but this is what happens when I attempt to in mongoexport:  [911]galloway@host1 ~/mongodb-linux-x86_64-2.0.6/bin>  ./mongoexport -d sellers-ph-production_190512 -c discussions -q '{subject:1,number_of_comments:1}' -f _id,subject,number_of_comments --csv >startable3.csv connected to: 127.0.0.1 exported 0 records [912]galloway@host1 ~/mongodb-linux-x86_64-2.0.6/bin>  My best guess is that I'm missing some sort of syntax when writing the query portion here, but I haven't seen many examples of this so I don't know what is wrong. Any advice is appreciated!   """,Question,Querying
503135,"""Never had issues with mongo before. I'd recently installed it on a cluster at my University because there were several .bson files I needed to analyze there, and I've been working with it from my laptop. Was able to restore all the files, make queries successfully, etc. Yesterday when I started mongo and typed """"show dbs"""" I got some error and found a lock file in data/db (sorry I don't have the specific error anymore), but searching online indicated that the databases may have been corrupted due to not exiting mongo properly (very likely, I didn't realize being careless with this would cause problems. Lesson learned.) I couldn't find a way to fix this, so I simply removed everything from data/db, the goal being to just restore them all. However, then when I started mongod, the following error occurred. I tried removing and reinstalling mongo, but I still get this error. Please help!   ./mongod --dbpath='/home/user1/data/db'  Thu Jun 28 21:38:08 terminate() called, printing stack: 0xa957de 0x34cecbce16 0x34cecbce43 0x34cecbcf2a 0xaabbf1 0x57338c 0xa9cfaf 0x34c9c1d994 0x501b89  ./mongod(_ZN5mongo11myterminateEv+0x5e) [0xa957de]  /usr/lib64/libstdc++.so.6 [0x34cecbce16]  /usr/lib64/libstdc++.so.6 [0x34cecbce43]  /usr/lib64/libstdc++.so.6 [0x34cecbcf2a]  ./mongod(_ZN5boost6thread12start_threadEv+0xf1) [0xaabbf1]  ./mongod(_ZN5mongo13BackgroundJob2goEv+0x18c) [0x57338c]  ./mongod(main+0x694f) [0xa9cfaf]  /lib64/libc.so.6(__libc_start_main+0xf4) [0x34c9c1d994]  ./mongod(__gxx_personality_v0+0x3e9) [0x501b89] Thu Jun 28 21:38:08 Got signal: 6 (Aborted).  Thu Jun 28 21:38:08 Backtrace: 0xa95ce9 0x34c9c302d0 0x34c9c30265 0x34c9c31d10 0xa958ab 0x34cecbce16 0x34cecbce43 0x34cecbcf2a 0xaabbf1 0x57338c 0xa9cfaf 0x34c9c1d994 0x501b89  ./mongod(_ZN5mongo10abruptQuitEi+0x3a9) [0xa95ce9]  /lib64/libc.so.6 [0x34c9c302d0]  /lib64/libc.so.6(gsignal+0x35) [0x34c9c30265]  /lib64/libc.so.6(abort+0x110) [0x34c9c31d10]  ./mongod(_ZN5mongo11myterminateEv+0x12b) [0xa958ab]  /usr/lib64/libstdc++.so.6 [0x34cecbce16]  /usr/lib64/libstdc++.so.6 [0x34cecbce43]  /usr/lib64/libstdc++.so.6 [0x34cecbcf2a]  ./mongod(_ZN5boost6thread12start_threadEv+0xf1) [0xaabbf1]  ./mongod(_ZN5mongo13BackgroundJob2goEv+0x18c) [0x57338c]  ./mongod(main+0x694f) [0xa9cfaf]  /lib64/libc.so.6(__libc_start_main+0xf4) [0x34c9c1d994]  ./mongod(__gxx_personality_v0+0x3e9) [0x501b89]   """,Bug,Performance
503286,"""The map function allows emit() to emit an undefined value for the key.    When many such emits occur, the reduce step will fail complaining that the value is too large to reduce.  I can't see a situation where I would ever want to allow a key to be undefined, so I would like emit to throw an exception on the emit().  If changing the default behavior is undesirable, some sort of 'strict' flag that enforced this would be nice.  Personally, I ran into this because I ran a new M/R against an old collection.  I don't see it as a big deal for production systems, but at least the option to fail fast might speed up the development cycle.""",Improvement,MapReduce
503287,"""When server already runs a MongoDB instance on default port and default (hostname-determined) IP address, initializing another instance with the same port but different IP address fails. We have server with 2 interfaces, ip1 and ip2. server hostname resolves to ip1. ip1 hosts a node of replica set called """"A"""". I want to have a node of another replica set called """"B"""" to be hosted on ip2. The """"A"""" RS node has bind_ip=ip1,127.0.0.1 and """"B"""" RS node has bind_ip=ip2. After initiating RS """"B"""" on the node (using rs.initiate()) I get the """"[rsStart] replSet exception loading our local replset configuration object : 13132 nonmatching repl set name in _id field; check --replSet command line"""" which is obviously wrong since I make sure that the node data directory was clean before. I spent two hours trying to diagnose the problem. Solution was subtle - I had to change port from default one to another one, and it worked. Now it appears that RS code tries to do some """"magic"""" using host names and <USER>and connects to wrong ip address, determines the """"wrong replica set name"""" and then bails out.   This was _very_ frustrating and I hope you will fix this soon. Also the message might disclose a little more information (for example what replica set name was encountered instead of the expected one). I think I've seen the case that was asking for that, but can't find this now. Also, I believe that connecting to a wrong machine might lead to disastrous events in case of same-named but different replica sets.  And while you're at it, please fix the """"mongo host"""" vs """"mongo host.zone"""" problem (if there are no dots in name, shell always tries local host instead of host, relative to search order, specified in resolv.conf), I think it's the same problem. """,Bug,Replication
503329,"""We have been testing replica set reliability under a few different failure scenarios. One scenario that failed is when we misconfigured network routing to a mongod primary. We blocked all inbound traffic to port 27017, but allowed it to continue making outbound connections. The replica set was a 3-node set where the primary (node A) had a higher priority than the other two (node B and node C).  What happened when we blocked port 27017 to node A is that node B assumed the primary role, as expected. However, node A then made an outbound connection to node B, and since it had a higher priority A told B to step down as primary, which it did. However, since neither B nor C could make a connection to node A, they both eventually voted that node B should become master again. A again connects to B and the whole process repeats indefinitely.  Not that this is at all a typical failure scenario, but I'm thinking that node A should not have been able to tell B to step down as primary in this situation.  Here are the relevant log entries from node A:   Mon Jun  4 15:23:29 [ReplicaSetMonitorWatcher] trying reconnect to graphdb-4-2.strcst.net:27017   Mon Jun  4 15:23:29 [ReplicaSetMonitorWatcher] reconnect graphdb-4-2.strcst.net:27017 ok   Mon Jun  4 15:23:31 [rsHealthPoll] replSet member graphdb-4-2.strcst.net:27017 is now in state PRIMARY   Mon Jun  4 15:23:32 [rsSync] replSet syncing to: graphdb-4-2.strcst.net:27017   Mon Jun  4 15:23:37 [rsMgr] stepping down graphdb-4-2.strcst.net:27017   Mon Jun  4 15:23:37 [rsSync] replSet syncThread: 10278 dbclient error communicating with server: graphdb-4-2.strcst.net:27017   Mon Jun  4 15:23:37 [rsHealthPoll] replSet member graphdb-4-2.strcst.net:27017 is now in state SECONDARY  And here are the corresponding log entries from node B:   Mon Jun  4 15:23:25 [initandlisten] connection accepted from 10.209.29.204:56081 #419426   Mon Jun  4 15:23:30 [rsMgr] replSet info electSelf 1   Mon Jun  4 15:23:30 [rsMgr] replSet PRIMARY   Mon Jun  4 15:23:37 [conn419426] replSet info stepping down as primary secs=1   Mon Jun  4 15:23:37 [conn419426] replSet relinquishing primary state   Mon Jun  4 15:23:37 [conn419426] replSet SECONDARY   Mon Jun  4 15:23:37 [conn419426] replSet closing client sockets after reqlinquishing primary """,Bug,Replication
503351,"""Hello.  I'm hoping you can help me figure out why my mongod instance crashes when I execute a certain query.  When I make the below query, I consistently kill the running mongod instance and receive same error message.   I make the below query to a collection containing 170,941,610 documents.  The collection has an index on the field, """"transformed"""".  Tue Jun  5 22:40:25 [conn933] query <DB>.stats_cumulative_stats query: { transformed: false } ntoreturn:500000 nscanned:58 255 nreturned:58254 reslen:4194308 607ms    The log shows this error:   Tue Jun  5 22:40:33 Invalid access at address: 0x3d27000  Tue Jun  5 22:40:33 Got signal: 11 (Segmentation fault).  Tue Jun  5 22:40:33 Backtrace: 0xa95ce9 0xa9640c 0x7fbcbcb5dc60 0x7fbcbc0a3885 0x571617 0x911b3f 0x941b2c 0x885a24 0x88e6da 0xaa0bc8 0x6389f7 0x7fbcbcb54d8c 0x7fbcbc0fec2d.  /usr/bin/mongod(_ZN5mongo10abruptQuitEi+0x3a9) [0xa95ce9]  /usr/bin/mongod(_ZN5mongo24abruptQuitWithAddrSignalEiP7siginfoPv+0x22c) [0xa9640c]  /lib/x86_64-linux-gnu/libpthread.so.0(+0xfc60) [0x7fbcbcb5dc60]  /lib/x86_64-linux-gnu/libc.so.6(memcpy+0x425) [0x7fbcbc0a3885]  /usr/bin/mongod(_ZNK5mongo10Projection9transformERKNS_7BSONObjERNS_14BSONObjBuilderE+0x147) [0x571617]  /usr/bin/mongod(_ZN5mongo22fillQueryResultFromObjERNS_11_BufBuilderINS_16TrivialAllocatorEEEPNS_10ProjectionERKNS_7BSONObjEPNS_7DiskLocE+0xaf) [0x911b3f]  /usr/bin/mongod(_ZN5mongo14processGetMoreEPKcixRNS_5CurOpEiRb+0x87c) [0x941b2c]  /usr/bin/mongod(_ZN5mongo15receivedGetMoreERNS_10DbResponseERNS_7MessageERNS_5CurOpE+0x1b4) [0x885a24]  /usr/bin/mongod(_ZN5mongo16assembleResponseERNS_7MessageERNS_10DbResponseERKNS_11HostAndPortE+0xf5a) [0x88e6da]  /usr/bin/mongod(_ZN5mongo16MyMessageHandler7processERNS_7MessageEPNS_21AbstractMessagingPortEPNS_9LastErrorE+0x78) [0xaa0bc8]  /usr/bin/mongod(_ZN5mongo3pms9threadRunEPNS_13MessagingPortE+0x287) [0x6389f7]  /lib/x86_64-linux-gnu/libpthread.so.0(+0x6d8c) [0x7fbcbcb54d8c]  /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7fbcbc0fec2d]  Logstream::get called in uninitialized state Tue Jun  5 22:40:33 ERROR: Client::~Client _context should be <USER>but is not; client:conn Logstream::get called in uninitialized state Tue Jun  5 22:40:33 ERROR: Client::shutdown not called: conn   ---     To recover the server, I follow these steps (my db is journaled):  First, I start it up with /etc/init.d/mongodb start.  At this point, I cannot log in via mongo client (ie mongo --host <HOST>) returns """"connect failed"""".  This is the error message:   MongoDB shell version: 2.0.4 connecting to: <HOST>:27017/<DB> Tue Jun  5 19:03:12 Error: couldn't connect to server <HOST>:<PORT> shell/mongo.js:86 exception: connect failed  Then, I restart server via /etc/init.d/mongodb restart and I can log in as usual via the mongo client.""",Bug,"Internal Code,Logging"
503427,"""I was under impression, that update w/upsert is atomic (since it operates on a single, though maybe not-yet-existing document). Unfortunately, when under moderate load we see duplicate key errors on _id for upsert operations. I've read the docs and got the impression that $atomic modifier is not a solution (docs describe it as a way to do _multiple_ document updates concurrently). Is there something broken in the server, documentation or my own head? =) If it's a documentation issue (I suspect that, but I feel sorry for myself updating all our code that uses upserts), please update the docs with a clear statement that """"upsert still can fail on non-unique IDs, use XXX instead"""".""",Question,Write Ops
503789,"""Very simple to replicate. Create a collection with a document containing a current date. Then do a simple: ``db.collection.find({dateField: {$gte: new Date(""""1/1/1969"""")})`` and it won't find the document. Do it with """"1/1/1970"""" and it will, so it appears that the internal ranging doesn't properly handle negative epoch time (perhaps treating the value as an unsigned rather than signed integer?).  I originally chose """"1/1/1900"""" as the date to query from when I'm initially populating an Apache Solr server with data from my MongoDB database for easier fuzzy matching, and was surprised that nothing was found. I don't know if this is Linux specific or not, since all of my development has been on various Linux distros.  I personally have my solution (1900 was chosen at a whimsy, so I can replace it with something else, like 1/1/1984), but I don't like the idea that something as intrinsic to DBs as date querying could be wrong, so I wanted to let you know.""",Bug,Querying
504089,"""We setup a new instance of 3 replica sets.  Each setup has a primary, a secondary and an arbiter.  Once they were all setup and running and the replica created.  Each instance now has 52 GB or files in the local folder.  I've attached the .conf files from the /etc directory, the init.d files from /etc/init.d and the replica set creation commands.""",Bug,Replication
504094,"""When testing the aggregation framework, tried to group by a field that has been previously stored as both 64-bit and 32-bit numbers, depending on the number.  Running the aggregation query results in a code 16016, or """"exception: can't compare values of BSON types 16 and 18""""  Running the old db.collection.group query still works as intended.  Heavily simplified versions of the aggregation framework query (not working) and the original collection.group query (working) are attached.  In both of these queries, it is understood that the field 'ip1' may contain either a 32-bit number (BSON type 16) or a 64-bit number (type 18)  I would expect both queries to return groupings based on the field 'ip1'.  A probable work-around (implementing this currently) is to somehow ensure that all data is written as a 64-bit number, so that all comparisons are between like types, but this seems like something that a green mongo user might find cumbersome to have to understand and deal with.  New to the bug-fix cycle, and to mongodb in general-- let me know if there's more information I can provide.""",Bug,"Internal Code,Aggregation Framework"
504151,"""When serializing operations (eg. for two-phase commit systems), it's useful to be able to wait until data has been sent to *all* replicas.  Currently, this requires knowing the correct, maximum value of """"w"""" to pass to getLastError (or setting up replica tags), and even that won't be correct if some replicas are offline.  I'd recommend allowing {w: 'online'}, akin to 'majority', to allow waiting for all online replicas.  ('$online' may be better than """"online"""", so it can't clash with replica tags in existing deployments.) """,New Feature,Replication
504256,"""While load() is fine for loading and executing an external javascript script, if we're going to move the shell towards being a more general purpose testing and scripting environment, it's going to need a system for loading modules that can track what modules are already loaded, and avoid reloading them, and a convention for avoiding polluting the global namespace.  I'm familiar with the module system from Dojo (dojotoolkit.org).  The commonjs initiative (commonjs.org) also has a specification for module loading, for which implementations may be available.""",New Feature,"Shell,JavaScript"
504292,"""When {{mongod}} accepts a new connection, it prints out a counter, spawns a thread, and then the new thread prints a similar counter.  Because spawning a thread is an asynchronous operation, there's no guarantee the new thread will start in the time it takes to accept new connections and spawn another thread.  So essentially {noformat} connection accepted from 1.2.3.4:5678 #123 {noformat} is saying 1.2.3.4:5678 established the 123rd connection. {noformat} [conn120] end connection 1.2.3.4:5678 {noformat} is saying 1.2.3.4:5678 was being handled by the 120th thread to execute.  It's useful for users to know when a connection was accepted, when it did something worth logging, and when the connection ended.  I would propose moving the initial 'connection accepted' log message to the connection thread (after the call to setThreadName()) to ensure both counters are in sync.""",Bug,Networking
504415,"""It looks like SERVER-1900 was closed as part of the v8 switch, but there's an additional test marked as disabled due to SERVER-1900 - the mr_killop.js test.  I would recommend reenabling this test as an additional check for SERVER-1900.""",Bug,"Test,MapReduce"
504485,"""replSetInitiate with no arguments doesn't work when the mongod's host's name isn't resolvable.  (This is the default behavior on MacBooks, and comes up constantly in trainings.)  It's possible to configure the set with an explicit, valid configuration object, of course, though in many training scenarios, the only resolvable hostname you have is localhost, so you have to choose between localhost and IP addresses.  The trouble is I'm not sure what the bug is:  (1) File this as a documentation bug and say that a no-arg initiate doesn't work if your hosts don't know their own names.  (We probably don't really want people deploying onto hosts with dodgy hostname resolution behaviors anyway, i.e., say that a good DNS configuration is a prerequisite like NTP is.)  This is a principled position, but it means we will continue not to have an easy way to initiate a replica set in these cases.  (2) Otherwise, to support a simple way to initiate even in these cases, let the user give just a """"hostname:port"""" or """"hostip:port"""" string.  Patch attached.  Strictly speaking, (2) is an incompatible change, since replSetInitiate currently ignores any non-document argument.  (Probably it should have ignored only <USER>arguments, but oh well.)  So there's some quibbling possible about whether to do (2).""",Improvement,Replication
504645,"""Server should be able to determine what os/distro/platform it is running on.  To get a better picture of what is going on """"under the hood"""", as well as be able to provide platform-specific guidance in regards to particular operating systems/distributions, we should have a method of reporting back these bits of information.  Possibly collect them via mms_agent and report them back so we can then inspect and act on the details.    Here's a few methods for collecting relevant information from the linux command line, and you can see there are differences. Possibly have a decision-making process to determine which method should be used to collect the rest?    {code}  # -----------------  # Centos System:  $ cat /proc/version  Linux version 2.6.32-220.2.1.el6.x86_64 (<EMAIL>) (gcc version 4.4.6 20110731 (Red Hat 4.4.6-3) (GCC) ) #1 SMP Fri Dec 23 02:21:33 CST 2011    $ uname -a  Linux myserver.biz 2.6.32-220.2.1.el6.x86_64 #1 SMP Fri Dec 23 02:21:33 CST 2011 x86_64 x86_64 x86_64 GNU/Linux    $ uname -rs  Linux 2.6.32-220.2.1.el6.x86_64    $ lsb_release -a  LSB Version:     :core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch  Distributor ID:     CentOS  Description:     CentOS release 6.2 (Final)  Release:     6.2  Codename:     Final    $ cat /etc/*-release  CentOS release 6.2 (Final)  CentOS release 6.2 (Final)  CentOS release 6.2 (Final)    $ ll /etc/*-release  -rw-r--r--. 1 root root 27 Dec  8 22:05 /etc/centos-release  lrwxrwxrwx. 1 root root 14 Dec 23 16:21 /etc/redhat-release -> centos-release  lrwxrwxrwx. 1 root root 14 Dec 23 16:21 /etc/system-release -> centos-release    $ cat /etc/issue  CentOS release 6.2 (Final)  Kernel \r on an \m    # -----------------  # On an Amazon AMI instance (modeled after Fedora):  $ cat /proc/version  Linux version 2.6.35.14-97.44.amzn1.x86_64 (mockbuild@build-31006.build) (gcc version 4.4.5 20110214 (Red Hat 4.4.5-6) (GCC) ) #1 SMP Mon Oct 24 16:03:08 UTC 2011    $ uname -a  Linux myserver.com 2.6.35.14-97.44.amzn1.x86_64 #1 SMP Mon Oct 24 16:03:08 UTC 2011 x86_64 x86_64 x86_64 GNU/Linux    $ uname -rs  Linux 2.6.35.14-97.44.amzn1.x86_64    $ lsb_release -a  -bash: lsb_release: command not found    $ cat /etc/*-release  Amazon Linux AMI release 2011.09    $ ll /etc/*-release  -rw-r--r-- 1 root root 33 Oct 27 21:24 /etc/system-release    $ cat /etc/issue  Amazon Linux AMI release 2011.09  Kernel \r on an \m    # -----------------  # Ubunutu Maverick 10.10:  $ cat /proc/version  Linux version 2.6.35-22-server (buildd@allspice) (gcc version 4.4.5 (Ubuntu/Linaro 4.4.4-14ubuntu4) ) #33-Ubuntu SMP Sun Sep 19 20:48:58 UTC 2010    $ uname -a  Linux myserver.net 2.6.35-22-server #33-Ubuntu SMP Sun Sep 19 20:48:58 UTC 2010 x86_64 GNU/Linux    $ uname -rs  Linux 2.6.35-22-server    $ lsb_release -a  No LSB modules are available.  Distributor ID:     Ubuntu  Description:     Ubuntu <USER>(development branch)  Release:     10.10  Codename:     <USER>   $ cat /etc/*-release  DISTRIB_ID=Ubuntu  DISTRIB_RELEASE=10.10  DISTRIB_CODENAME=<USER> DISTRIB_DESCRIPTION=""""Ubuntu <USER>(development branch)""""    $ ll /etc/*-release  -rw-r--r-- 1 root root 125 2010-05-04 12:41 /etc/lsb-release    $ cat /etc/issue  Ubuntu <USER>(development branch) \n \l    # -----------------  # On OSX 10.7.2:  $ cat /proc/version  cat: /proc/version: No such file or directory    $ uname -a  Darwin fledgling.local 11.2.0 Darwin Kernel Version 11.2.0: Tue Aug  9 20:54:00 PDT 2011; root:xnu-1699.24.8~1/RELEASE_X86_64 x86_64    $ uname -rs  Darwin 11.2.0    $ lsb_release -a  -bash: lsb_release: command not found    $ cat /etc/*-release  cat: /etc/*-release: No such file or directory    $ ll /etc/*-release  ls: /etc/*-release: No such file or directory    $ cat /etc/issue  cat: /etc/issue: No such file or directory    {code}    So it seems like on a *nix system, running a `uname` will provide whether it is Darwin vs Linux, and then other tests could be run to grab relevant information.    Things I'd care about knowing:  Distro (CentOS, RHEL, Debian, Ubuntu)  Distro Version (6.2, 5.7, 10.04, etc)  Architecture (i386 vs x64_86)  Kernel version (2.6.18, 2.6.32, etc)      Windows has a different method, using the `systeminfo` command line tool, like so:  {code}  C:\>systeminfo | findstr /C:""""OS""""  OS Name:                   Microsoft Windows Server 2008 R2 Enterprise  OS Version:                6.1.7600 N/A Build 7600  OS Manufacturer:           Microsoft Corporation  OS Configuration:          Standalone Server  OS Build Type:             Multiprocessor Free  BIOS Version:              IBM -[BWE117AUS-1.05]-, 7/28/2005  {code}  This returns too much, including the BIOS, so using a better find like this: `findstr /B /C:""""OS Name"""" /C:""""OS Version""""` should get the relevant information.""",Improvement,Admin
504651,"""        StringBuilder& operator<<( int x ) {             return SBNUM( x , 11 , """"%d"""" );         }  One of the longest signed integers is -2147483648, which is 11 bytes to print.  And sprintf will add a <USER>byte as a terminator, so 12 total bytes need to be allocated.  I'd recommend checking the other max sizes passed to SBNUM and also switching to snprintf and checking its return value to ensure the 'n' limit isn't exceeded at runtime.""",Bug,Stability
504653,"""Analysis below is relatively speculative, and I haven't proven any of it via testing.  Hopefully this ticket is at least useful as a survey of cases where we've seen memory corruption during shutdown, often as a cascade failure after an earlier failure triggered the shutdown itself.  It looks like memory corruption can be triggered after an unclean shutdown and potentially can occur after a clean shutdown too.  From the examples I've seen it kind of looks like double frees are occurring because global objects with members that manage their own heap memory are destroyed as the process is exiting, and then the heap memory of these members are freed again as a result of actions taken by a thread that's still running.  Here are some historical jira cases where I've seen this occur:  1)  When Top::cloneMap called by SnapshotData::takeSnapshot.  Potentially caused by the global statsSnapshots variable getting destroyed, and then one of its SnapshotData also getting destroyed but having takeSnapshot called on it and trying to reassign its _usage map.  Potentially the _usage map was already destroyed, and its heap memory was freed, but the reassignment attempts to free the heap memory again.  Stack traces that might be related to this found in:  FREE-3600 SERVER-2695 SERVER-4190  It looks like SnapshotThread::run checks inShutdown(), but a shutdown occurs after after the inShutdown() check but before or during the call to takeSnapshot() the same double free might occur as in the unclean shutdown cases.  2)  Maybe when an an immediate exit occurs during a shutdown, global objects required for shutdown may be destroyed while still in use?  SERVER-3869  3)  Here it kind of looks like there is a global freed or left in a bad state and then another exit call attempts to free it again.  CS-501  4)  Mystery failure after a clean shutdown  SERVER-414 (very old mongo version)  5)  On mongos these failures may potentially have occurred in similar situations:  SERVER-3082 SERVER-4367 CS-1903 SERVER-2930 FREE-3696 SERVER-4576   I would recommend that we do a closer examination of 2-5 above, do an audit for additional cases, and then fix all known cases.""",Bug,Internal Code
504776,"""We are currently testing mongodb for a document store, and in the process of inserting more than 3 million documents, mongodb used up so much memory on the machine that the box froze and had to be rebooted.  There is currently only the _id index on the collection, and no other collections or databases currently defined.  Because the size of the index is so small (way less than the memory assigned to the machine, I would expect it to continue to operate up to the point where the indexes no longer fit in memory (then slow way down).  It would be nice if there was an option to tell mongodb to flush/release memory it is using for the memory mapped files and only keep the indexes in memory (even at the expense of some performance).  After reboot, we are able to continue adding files to the collection since there is now more than an adequate amount of free memory for mongod to use.  Thanks.  mongod log file: {code} Tue Dec 13 06:24:19 [conn723] SocketException handling request, closing client connection: 9001 socket exception [2] server [192.168.4.2:22847]  Tue Dec 13 06:24:31 [clientcursormon] mem (MB) res:9412 virt:32802 mapped:16329 Tue Dec 13 06:25:03 [conn722] SocketException handling request, closing client connection: 9001 socket exception [2] server [192.168.4.2:22843]    ***** SERVER RESTARTED *****   Tue Dec 13 09:31:39 [initandlisten] MongoDB starting : pid=1428 port=27017 dbpath=c:\mongodb\data 64-bit host=sqldev1 Tue Dec 13 09:31:39 [initandlisten] db version v2.0.1, pdfile version 4.5 Tue Dec 13 09:31:39 [initandlisten] git version: 3a5cf0e2134a830d38d2d1aae7e88cac31bdd684 Tue Dec 13 09:31:39 [initandlisten] build info: windows (6, 1, 7601, 2, 'Service Pack 1') BOOST_LIB_VERSION=1_42 Tue Dec 13 09:31:39 [initandlisten] options: { dbpath: """"c:\mongodb\data"""", directoryperdb: true, journal: true, logappend: true, logpath: """"c:\mongodb\mongod.log"""", noauth: true, rest: true, service: true } Tue Dec 13 09:31:39 [initandlisten] journal dir=c:/mongodb/data/journal Tue Dec 13 09:31:39 [initandlisten] recover begin Tue Dec 13 09:31:39 [initandlisten] recover lsn: 1490690287 Tue Dec 13 09:31:39 [initandlisten] recover c:/mongodb/data/journal/j._6 Tue Dec 13 09:31:39 [initandlisten] recover skipping application of section seq:1488245664 < lsn:1490690287 Tue Dec 13 09:31:39 [initandlisten] recover skipping application of section seq:1488283604 < lsn:1490690287 Tue Dec 13 09:31:39 [initandlisten] recover skipping application of section seq:1488321524 < lsn:1490690287 Tue Dec 13 09:31:39 [initandlisten] recover skipping application of section seq:1488359454 < lsn:1490690287 Tue Dec 13 09:31:40 [initandlisten] recover skipping application of section seq:1488397298 < lsn:1490690287 Tue Dec 13 09:31:40 [initandlisten] recover skipping application of section seq:1488435234 < lsn:1490690287 Tue Dec 13 09:31:40 [initandlisten] recover skipping application of section seq:1488473198 < lsn:1490690287 Tue Dec 13 09:31:40 [initandlisten] recover skipping application of section seq:1488511059 < lsn:1490690287 Tue Dec 13 09:31:40 [initandlisten] recover skipping application of section seq:1488548991 < lsn:1490690287 Tue Dec 13 09:31:40 [initandlisten] recover skipping application of section more... Tue Dec 13 09:31:45 [initandlisten] recover c:/mongodb/data/journal/j._7 Tue Dec 13 09:31:49 [initandlisten] recover cleaning up Tue Dec 13 09:31:49 [initandlisten] removeJournalFiles Tue Dec 13 09:31:49 [initandlisten] recover done Tue Dec 13 09:31:50 [initandlisten] query local.system.namespaces reslen:20 171ms Tue Dec 13 09:31:50 [initandlisten] waiting for connections on port 27017 Tue Dec 13 09:31:50 [websvr] admin web console waiting for connections on port 28017 Tue Dec 13 09:32:50 [clientcursormon] mem (MB) res:20 virt:61 mapped:0 Tue Dec 13 09:33:36 [initandlisten] connection accepted from 127.0.0.1:49226 #1 Tue Dec 13 09:33:39 [conn1] command admin.$cmd command: { listDatabases: 1.0 } ntoreturn:1 reslen:175 922ms Tue Dec 13 09:33:50 [clientcursormon] mem (MB) res:36 virt:32722 mapped:16329 Tue Dec 13 09:33:52 [conn1] command Atom.$cmd command: { collstats: """"Moreover"""", scale: undefined } ntoreturn:1 reslen:256 484ms Tue Dec 13 09:36:20 [conn1] end connection 127.0.0.1:49226 Tue Dec 13 09:38:50 [clientcursormon] mem (MB) res:37 virt:32719 mapped:16329 Tue Dec 13 09:40:12 [initandlisten] connection accepted from 127.0.0.1:49230 #2 Tue Dec 13 09:41:04 [conn2] assertion 10068 invalid operator: $eq ns:Atom.Moreover query:{ DuplicateId: { $eq: """"3888384356"""" } } Tue Dec 13 09:41:50 [clientcursormon] mem (MB) res:3390 virt:32720 mapped:16329 Tue Dec 13 09:42:50 [clientcursormon] mem (MB) res:5095 virt:32720 mapped:16329 Tue Dec 13 09:42:50 [PeriodicTask::Runner] task: DBConnectionPool-cleaner took: 15ms Tue Dec 13 09:42:50 [PeriodicTask::Runner] task: WriteBackManager::cleaner took: 15ms Tue Dec 13 09:43:48 [conn2] query Atom.Moreover nscanned:3961932 nreturned:6 reslen:23542 155387ms Tue Dec 13 09:47:36 [conn2] command Atom.$cmd command: { collstats: """"Moreover"""", scale: undefined } ntoreturn:1 reslen:256 171ms Tue Dec 13 09:47:50 [clientcursormon] mem (MB) res:5086 virt:32720 mapped:16329 Tue Dec 13 09:47:57 [initandlisten] connection accepted from 192.168.4.2:31957 #3 Tue Dec 13 09:47:57 [initandlisten] connection accepted from 192.168.4.2:31958 #4 Tue Dec 13 09:47:57 [initandlisten] connection accepted from 192.168.4.2:31959 #5 Tue Dec 13 09:47:57 [initandlisten] connection accepted from 192.168.4.2:31960 #6 Tue Dec 13 09:47:57 [initandlisten] connection accepted from 192.168.4.2:31961 #7 Tue Dec 13 09:47:57 [initandlisten] connection accepted from 192.168.4.2:31962 #8 Tue Dec 13 09:47:57 [initandlisten] connection accepted from 192.168.4.2:31963 #9 Tue Dec 13 09:47:57 [initandlisten] connection accepted from 192.168.4.2:31965 #10 Tue Dec 13 09:47:57 [initandlisten] connection accepted from 192.168.4.2:31964 #11 Tue Dec 13 09:47:57 [initandlisten] connection accepted from 192.168.4.2:31966 #12 {code}   Collection details: {code} C:\mongodb\mongodb-win32-x86_64-2.0.1\bin>mongo MongoDB shell version: 2.0.1 connecting to: test > show dbs Atom    15.9462890625GB local   (empty) > use Atom switched to db Atom > show collections Moreover system.indexes > db.Moreover.stats(); {         """"ns"""" : """"Atom.Moreover"""",         """"count"""" : 3961932,         """"size"""" : 12043437732,         """"avgObjSize"""" : 3039.7891059210506,         """"storageSize"""" : 12880310256,         """"numExtents"""" : 32,         """"nindexes"""" : 1,         """"lastExtentSize"""" : 2146426864,         """"paddingFactor"""" : 1,         """"flags"""" : 1,         """"totalIndexSize"""" : 170976512,         """"indexSizes"""" : {                 """"_id_"""" : 170976512         },         """"ok"""" : 1 } {code}""",Bug,Stability
504849,"""If a query or a command (such as count) is doing a table scan, we should note that in the logs explicitly.  In addition, we should add a table scans counter for both queries and commands to the server status results. That way, we can graphs table scans in MMS. This would help us diagnose these kinds of problems without having to look at the logs.  If it's difficult to determine exactly what qualifies as a table scan, I'd recommend a simple heuristic. For instance, we could log whenever nscanned > 10n and when nscanned > 1000.""",Improvement,"Querying,Logging"
504880,"""As a user of a MongoDB database served from an N-node replica set (running in --auth mode) that is responding to a steady trickle of read and write requests, I want to be able to complete a large map/reduce task without material disruption to the operation of the replica set.  By """"large,"""" I mean that I cannot guarantee the result set fits in a single 16MB document -- i.e., my task is a poor candidate for """"out: { inline: 1 }"""".  By """"material disruption,"""" I mean a variety of pathologies I'd like to avoid if possible.  For example:    # failover to the SECONDARY because the PRIMARY becomes unresponsive ** (with successive failure of the mapReduce -- db assertion 13312 -- when it cannot commit its result collection because former PRIMARY is no longer PRIMARY) # error RS102 (""""too stale to <USER>up"""") on a secondary because of the deluge of a suddenly-appearing result set being replicated;  # significant degradation of overall performance re: the latency of servicing other requests; # etc.  For the sake of argument, let's further assume my map/reduce task is not amenable to an incremental application of successive mapReduce calls; and that I would potentially like to preserve the map/reduce output collection as a target for future queries, though it would be acceptable to have to manually move that result data around among hosts.  I've spoken with a couple members of the 10gen engineering team, and they've been very helpful in brainstorming approaches and workarounds.  But for the sake of the product, I want to make sure the use case is captured here, as I don't think any of the approaches I'm currently aware of really addresses it satisfactorily.   Working with current constraints, here's where we get ...   The """"large"""" / no inline requirement today will force us to execute the mapReduce() call on the PRIMARY.  The reason being:  an output collection will have to be created, and only the PRIMARY is permitted to do these writes.  But today, for a certain class of job, executing on the PRIMARY is a non-starter.  It will consume too many system resources, monopolize the one and only javascript thread, hold the write lock for extended periods of time, etc. -- risking running afoul of pathology #1 above.  If that failure mode is dodged, it runs the risk of pathology #2, as the large result set (created much faster than any more """"natural"""" insertion of data) floods the SECONDARY nodes and overflows their oplogs.  And of course failure mode #3 is always a concern -- the PRIMARY is too critical to the overall responsiveness of any app that uses the database.  That's the problem, in a nutshell.  Some potential, suggested solutions -- all of which involve changes to core server :   A.  If we could relax or finesse the only-PRIMARY-can-write constraint for this kind of operation, we could run the mapReduce() call on a SECONDARY -- perhaps even a """"hidden"""" node -- and leave the overall cluster and any apps almost completely unaffected.  Then we'd take responsibility for getting the result collection out of that node as a separate task.  Currently, though, this does not seem possible -- cf. issue SERVER-4264 -- as we can't target even the 'local' collection there.  (And even if we could target 'local', today it requires admin/separate privileges to do so.)  B.  Alternately, if we could alter the mapReduce() command in such a way as to allow """"out:"""" to target a remote database -- perhaps in an entirely disjoint replica set -- we could allow a secondary to run the calculation, save the result set, but still not violate the proscription against writing on a secondary.  C.  Another possibility is to flag the result set for exclusion from replication -- though this leaves us running on the PRIMARY, where failure modes #1 and #3 are still an issue.  My intent here, though, is not to describe or attach to any particular solution; rather, I'm just seeking to articulate the problem standing in the way of (what I consider) a rather important use case.  Thanks for reading this far!  TD """,Improvement,MapReduce
504908,"""If I build a 32-bit debug version of mongod.exe for Windows and run it on 32-bit Windows XP and then press ctrl-C, the ctrl-C is reported and immediately followed by an """"unhandled windows exception"""" with reported value """"ec=0xc00000fd"""".  This turns out to be """"stack overflow"""".  I put breakpoints in the code and stepped through it in assembly language (required in this case) and I think I've got a decent handle on the problem now.  We start out on-track to do our normal cleanup-and-exit on ctrl-C, but we blow up instantly when db/db.cpp routine ctrlCTerminate() calls Client::initThread( """"ctrlCTerminate"""" ) in db/client.cpp.  It really is a stack overflow, but it is debug-only diagnostic code that doesn't even get used in this build that is killing us.  On Windows, the ctrl-C handler function we register by calling SetConsoleCtrlHandler() gets called on a new thread.  Before I hit ctrl-C, I can see that we have 6 threads running.  When we hit the breakpoint I set in the code after hitting ctrl-C, we are running on a new 7th thread.  In order for exitCleanly( EXIT_KILL ) to work correctly, the new thread has to be set up as a """"client"""" thread, hence the call to Client::initThread().  But we never get to execute the first source code line in initThread(), because the 256K buffer we allocate for StackChecker is too big and the _chkstk() (aka _alloca_probe()) routine that tests for stack overflow triggers.  Apparently, the designers of Windows XP figured that the default Windows thread size of 1MB was excessive for a simple ctrl-C handler thread, so they picked 256K as the reserved stack size.  _alloca_probe() touches memory every 4K (page size) until it either gets to the end of the requested allocation (you win) or gets an """"unable to expand the stack"""" exception (you lose).  The somewhat ironic part is that the 256K allocated by the StackChecker object is only used if ( sizeof(void*) == 8 ) and so no stack checking takes place in this build but we overflow the stack anyway.  I don't know how much value this stack checking (when used) is providing, but I can stop the crashing by changing 'enum { SZ = 256 * 1024 };' to 'enum { SZ = 192 * 1024 };'.  I am not sure why we only test and report on stack usage in 64-bit builds but consume stack space (forcing memory to be allocated) in both 32-bit and 64-bit builds.  If limiting ourselves to checking for up to 192K of usage instead of 256K of usage is acceptable, this is a one line fix.  And no actual customer is running a 32-bit debug version on Windows XP, it's just developers (like me) who will hit this bug.""",Bug,Stability
504912,"""All of the projects in Visual Studio create 32-bit and 64-bit versions except for the shell.  The shell only has 32-bit defined as a target.  There is no real reason for this except that it would take a few minutes to set it up.  The shell works fine in 64-bit, SCons builds it in 64-bit, and you get this weird setting of """"Mixed platforms"""" in the Visual Studio UI because most projects can do 64-bit but the shell can't.  I've done this (created a 64-bit VS build of the shell) at least three times on my home machine and it would save additional work to just check it in.""",Improvement,Shell
504940,"""my server is running Windows 7 64bits,24GB RAM, MongoDB2.0.1 with journaling enable.  i use c# driver to insert data into MongoDB, each insert data is about 2MB. After a while, MongoDB consumed all of 24GB memory, then out of memory dialog of Windows pops up, I turned the dialog off(twice), then MongoDb was alive for a while then crashed.  The virtual memory(pagefile size) of Windows is 73595MB, it is managed automatically by Windows  console message:  Thu Nov 17 09:23:20 [conn1817] update surveyData.photos query: { _id: 1099111116065824545 } update: { _id: 1099111116065824545, sqliteId: """"129495"""", gps_time: new Date(1321397904545), loc: { lon: 121.640237, lat: 25.061985 }, heading: 0.0, gps_accuracy: 0.0, velocity: 0.0, cam_0: """"285757496"""", cam_0_blob: BinData, cam_1: """"285876579"""", cam_1_blob: BinData, cam_2: """"285876590"""", cam_2_blob: BinData, cam_3: """"285885861"""", cam_3_blob: BinData, cam_4: """"285876226"""", cam_4_blob: BinData, cam_5: """"285885859"""", cam_5_blob: BinData, cam_6: """"285876230"""", cam_6_blob: BinData,cam_7: """"285757478"""", cam_7_blob: BinData, cam_8: """"285757493"""", cam_8_blob: BinData, created_at: new Date(1321444899758), mod_at: new Date(1321493204950) } idhack:1 234ms Thu Nov 17 09:23:21 [conn1817] update surveyData.photos query: { _id: 1099111116065824894 } update: { _id: 1099111116065824894, sqliteId: """"129496"""", gps_time: new Date(1321397904894), loc: { lon: 121.640237, lat: 25.061985 }, heading: 0.0, gps_accuracy: 0.0, velocity: 0.0, cam_0: """"285757496"""", cam_0_blob: BinData, cam_1: """"285876579"""", cam_1_blob: BinData, cam_2: """"285876590"""", cam_2_blob: BinData, cam_3: """"285885861"""", cam_3_blob: BinData, cam_4: """"285876226"""", cam_4_blob: BinData, cam_5: """"285885859"""", cam_5_blob: BinData, cam_6: """"285876230"""", cam_6_blob: BinData,cam_7: """"285757478"""", cam_7_blob: BinData, cam_8: """"285757493"""", cam_8_blob: BinData, created_at: new Date(1321444900171), mod_at: new Date(1321493205579) } idhack:1 171ms Thu Nov 17 09:23:21 [conn1817] end connection 10.77.1.34:63669 Thu Nov 17 09:23:22 [conn1815] Assertion: 10000:out of memory BufBuilder Thu Nov 17 09:23:22 [conn1815] assertion 10000 out of memory BufBuilder ns:surveyData.photos query:{ _id: 1099111116065819550 } Thu Nov 17 09:23:22 [conn1815] end connection 10.77.1.34:63665 Thu Nov 17 09:23:26 [conn1819] Assertion: 10000:out of memory BufBuilder Thu Nov 17 09:23:26 [conn1819] assertion 10000 out of memory BufBuilder ns:surveyData.photos query:{ _id: 1099111116065819887 } Thu Nov 17 09:23:26 [conn1819] end connection 10.77.1.34:63676 Thu Nov 17 09:23:29 [initandlisten] connection accepted from 10.77.1.34:64147 #1864 Thu Nov 17 09:23:29 [conn1864] Assertion: 10000:out of memory BufBuilder Thu Nov 17 09:23:29 [conn1864] assertion 10000 out of memory BufBuilder ns:surveyData.photos query:{ _id: 1099111116065820249 } Thu Nov 17 09:23:29 [conn1864] end connection 10.77.1.34:64147 Thu Nov 17 09:23:31 [initandlisten] connection accepted from 10.77.1.34:64148 #1865 Thu Nov 17 09:24:16 [clientcursormon] mem (MB) res:2036 virt:217008 mapped:108397 . . . (turn off Windows """"out of memory"""" warning dialog,keep iusert/update data) . . . Thu Nov 17 09:59:25 [conn1873] update surveyData.photos query: { _id: 1099111116064457111 } update: { _id: 1099111116064457111, sqliteId: """"127201"""", gps_time: new Date(1321397097111), loc: { lon: 121.640238, lat: 25.061983 }, heading: 0.0, gps_accuracy: 0.0, velocity: 0.0, cam_0: """"285757496"""", cam_0_blob: BinData, cam_1: """"285876579"""", cam_1_blob: BinData, cam_2: """"285876590"""", cam_2_blob: BinData, cam_3: """"285885861"""", cam_3_blob: BinData, cam_4: """"285876226"""", cam_4_blob: BinData, cam_5: """"285885859"""", cam_5_blob: BinData, cam_6: """"285876230"""", cam_6_blob: BinData,cam_7: """"285757478"""", cam_7_blob: BinData, cam_8: """"285757493"""", cam_8_blob: BinData, created_at: new Date(1321445789421), mod_at: new Date(1321495365770) } idhack:1 483ms Thu Nov 17 09:59:25 [conn1873] Assertion: 10000:out of memory BufBuilder Thu Nov 17 09:59:25 [conn1873] assertion 10000 out of memory BufBuilder ns:surveyData.photos query:{ _id: 1099111116064457454 } Thu Nov 17 09:59:25 [conn1873] end connection 10.77.1.34:65286 Thu Nov 17 09:59:52 [conn1886] VirtualProtect failed (mcw) e:/db/panorama.27 20381fd8000000 4000000 errno:1455 ??瑼云撠??⊥?摰????? Thu Nov 17 09:59:52 [conn1886]  panorama.Pano Assertion failure false db\mongommf.cpp 72 Thu Nov 17 09:59:52 [conn1886] insert panorama.Pano exception: assertion db\mongommf.cpp:72 31ms . . . (turn off Windows """"out of memory"""" warning dialog,keep iusert/update data) . . . Thu Nov 17 10:03:21 [conn1899] Assertion: 10000:out of memory BufBuilder Thu Nov 17 10:03:21 [conn1900] Assertion: 10000:out of memory BufBuilder Thu Nov 17 10:03:21 [conn1899] assertion 10000 out of memory BufBuilder ns:surveyData.photos query:{ _id: 1099111116064325374 } Thu Nov 17 10:03:21 [conn1900] assertion 10000 out of memory BufBuilder ns:surveyData.photos query:{ _id: 1099111116064330919 } Thu Nov 17 10:03:21 [conn1899] end connection 10.77.1.34:49382 Thu Nov 17 10:03:21 [conn1900] end connection 10.77.1.34:49383 Thu Nov 17 10:03:23 dbexit: malloc fails Thu Nov 17 10:03:23 [conn1897] shutdown: going to close listening sockets... Thu Nov 17 10:03:23 [conn1897] closing listening socket: 448 Thu Nov 17 10:03:23 [initandlisten] now exiting Thu Nov 17 10:03:23 dbexit: ; exiting immediately Thu Nov 17 10:03:23 [conn1897] closing listening socket: 452 Thu Nov 17 10:03:23 [conn1897] shutdown: going to flush diaglog... Thu Nov 17 10:03:23 [conn1897] shutdown: going to close sockets... Thu Nov 17 10:03:23 [conn1897] shutdown: waiting for fs preallocator... Thu Nov 17 10:03:23 [conn1897] shutdown: lock for final commit...   There is no such issue if I turn off journaling function, but I need it to reduce time consuming for repairDatabase  """,Bug,Storage
504979,"""mongod segfaulted apparently for no reason (unable to correlate this with anything remarkable in the application logs).  For some reason it was building indexes just before the crash. Not sure why it was doing that (nothing in the application's log suggested an action that would have triggered this).  As you can see from the logs, there are some truly nasty queries, a lot of suboptimal stuff going on. I've got some big improvements in the pipeline but they're not deployed yet.  As a result of this crash I've upgraded to mongod 2.0.1 and am now setting up MMS.""",Bug,Stability
505175,"""If you type some text and then use ctrl-b or left arrow to move into the middle of it and then press ctrl-u, bash and the mongo shell version 1.8.4 remove the text to the left of the cursor, leaving only the text that was to the cursor's right.  In versions 1.9 and higher in the mongo shell, the entire line is erased.  MongoDB shell version: 1.8.4-rc1-pre- > abc def // sample text, to be recalled with up-arrow Sun Oct  9 13:33:33 SyntaxError: missing ; before statement (shell):1 > def // position the cursor on the """"d"""" and press ctrl-u ... only """"abc """" is removed Sun Oct  9 13:33:39 ReferenceError: def is not defined (shell):1 >   MongoDB shell version: 2.1.0-pre- > abc def // same typing as above Sun Oct  9 13:33:07 SyntaxError: missing ; before statement (shell):1 > // though the cursor is on the """"d"""", the entire line is removed  Also, I'm using the word """"removed"""" to not step on a separate issue of cut versus delete and kill rings.  That's not this bug.  We could fix this bug without addressing killed text and ctrl-y by deleting the text that bash or 1.8.4 would kill.  """,Bug,Shell
505257,"""I'd like to be able to track the changes made to a collection. This should be something that you could choose to activate on a collection or not.  I'll attempt to describe how this could be implemented so it's clear what I'm proposing, but I don't know if this would be the best way.  Once this feature has been activate in a collection, any change made to that collection would cause a new document to be created in a another collection. That collection could be called """"ChangeHistory"""" (or something like that), and that doc would contain info regarding what exactly changed.  The change document could look like:     _id : ObjectID()        // unique id for this change    author : """"<USER>""       // the user who made this change    timeStamp : date/time   // when the change was made    parentChanges : []      // array of 0, 1 or 2 parent change ids    previousContent: {}     // an object that contains the data that was modified as it was before being modified    newContent: {}          // an object that contains the data that was modified after it was modified  The parentChanges array above is used so we can track the order in which the changes were made. If the array is empty, then this is the first change. If the array has one change, that is the parent change. If the array has 2 changes, then this change is a merge change (much like the mercurial or git track their changes).  The previousContent and newContent properties only store the information that was actually changed and not the whole document that was changed.  As I said before, I don't know if this would be the best way to implement this feature, but the important points in this feature are:  - changes to a collection are tracked (only the changed data, not the whole document where the change was made) - changes contain enough information to allow you to reproduce a collection at any point in its history. - changes can be merged from one collection (or database) to another in the correct order that they happened. - each change has a uniqueID so that the same change doesn't get merged twice  """,Improvement,Storage
505271,"""It would be really great if we were able to store MongoDB queries, query snippets, and atomic update directives in MongoDB itself. The problem right now is that, since field name with '$' are disallowed, one has to either string-encode or otherwise mangle the query JSON in order to store it in a document.   It seems like we could fix this by allowing '$' characters in field names and introducing a $quote operator (yes, Lisp inspired). Such an operator could be used as a way to disambiguate intent in a query (or update, etc...), and therefore eliminate the ambiguities that led to the need to disallow field names starting with '$'.   Example:  Let's say there exists a document, D, like this is used to store query contraint:     { """"_id"""" : 1234,      """"constraint"""" : { """"$lt"""" : 0 }}  The query:     { """"constraint"""" : { """"$lt"""" : 0 }} would NOT match D (this query matches when documents where """"constraint"""" < 0)   but this query:     { """"constraint"""" : { """"$quote"""" : { """"$lt"""" : 0 }}}} would match D    I'm sure there are other corner cases to think through, but this is the basic idea.  -will """,New Feature,"Querying,Storage"
505291,"""When an update is issued which mutates objects, I would like to be able to get an array of object IDs that were changed as a result of the update.""",Improvement,Write Ops
505379,"""I am trying to update the mongodb port in MacPorts to version 2.0.0 and it fails with a zillion errors like this:  third_party/pcre-7.4/pcre_exec.c: In function 'int match(const unsigned char*, const uschar*, const uschar*, int, match_data*, long unsigned int, eptrblock*, int, unsigned int)': third_party/pcre-7.4/pcre_exec.c:690: error: jump to label 'L_RM1' third_party/pcre-7.4/pcre_exec.c:4248: error:   from here third_party/pcre-7.4/pcre_exec.c:690: error:   skips initialization of 'heapframe* newframe' third_party/pcre-7.4/pcre_exec.c:749: error: jump to label 'L_RM2' third_party/pcre-7.4/pcre_exec.c:4248: error:   from here third_party/pcre-7.4/pcre_exec.c:749: error:   skips initialization of 'heapframe* newframe'  I don't even want to use your bundled pcre 7.4; I want to use the pcre 8.12 MacPorts has already installed.  mongodb 1.8.3 built fine with MacPorts.""",Bug,Build
505414,"""It looks like the compact command will create a new extent, at least if there isn't an extent on the free list.  For large collections, the new extents will be large and usually require a new datafile to be allocated every time compact is run.  Experimentally I've run several compact commands back to back and seen a new file get allocated every or almost every time.  This was after hitting the max file size, and it seemed like compact was creating a new extent rather than reusing a free list extent in this case.""",Improvement,Storage
505496,"""It would be nice to have some basic db access tracking. Two things I would like to be able to see in db.stats() (or some other reporting mechanism):    - last time the db was accessed  - last time each db user authenticated to the db (when running in -auth mode)  """,New Feature,Admin
505565,"""See QA-19  - I haven't identified any code, tests, or docs for findRO(), find().readOnly(), etc - maybe I am not looking in the right places.  - I'm not seeing where V8STR_RO is getting set for cursor objects. I was able to set it up manually using:          cur = db.c.find();          cur._exec();          cur._cursor['_ro'] = true;    but maybe the helper still needs to be implemented or I didn't see it.  - I added some logging and I think I confirmed that once _ro was set, I could read a doc and write it using the original bson representation, as in the example for SERVER-3107. """,Task,JavaScript
505634,"""Hello,  I'm using MongoDb in a GWT(2.3) project (with Morphia 0.99). My GWT project is calling a mongoside function (stored in db.system.js) thanks to DB.eval(). One of my function's argument is a Regex. When i'm using a Regex like  """"\bFOO\b"""", it's working fine, but when my regexp become like """"(?<!<[^>]*)\bFOO\b(?>![^<]*>)"""" then, i get this error:  Erreur serveur: eval failed: { """"assertion"""" : """"assertion scripting/engine_spidermonkey.cpp:634"""" , """"errmsg"""" : """"db assertion failure"""" , """"ok"""" : 0.0} com.mongodb.MongoException: eval failed: { """"assertion"""" : """"assertion scripting/engine_spidermonkey.cpp:634"""" , """"errmsg"""" : """"db assertion failure"""" , """"ok"""" : 0.0}  at com.mongodb.DB.eval(DB.java:223)  This error occurs in my GWT project (so, in jetty, as it is on the server side of my GWT project). The regex i'm passing as an argument to my db.eval() is compiled by Java.  So my java code is something like :    String exec = """"return serverSideFunction(args[0]);"""";    Pattern regex = Pattern.compile(""""(?<!<[^>]{0,10})foo(?>![^<]{0,10}>)"""");    Object result = db.eval(exec, regex);  Nb : In my Pattern.compile(), I use {0,10}, as Java doesn't support * in regex's look-around.   Nb2 : I made some little test without all javaDrivers and db.eval(),  this looks like  > db.Test.save({'key':'<<USER>key=""""value"""">value</<USER>'}); > db.Test.find();                                         { """"_id"""" : ObjectId(""""4e40d9c750d109bc7de98858""""), """"key"""" : """"<<USER>key=\""""value\"""">value</<USER>"""" } > db.Test.find().forEach(function(x){x.key.replace(new RegExp('(?<!<[^>]*)value(?>![^<]*>)',''),'VALUE'); db.Test.save(x);}); Tue Aug  9 09:04:47 SyntaxError: invalid quantifier ?<!<[^>]*)value(?>![^<]*>) (shell):1  The error I get is not the same, but unless i'm doing it wrong, lookaround's features aren't really working in Mongo, aren't them?   This is my first bug reporting ever, so please tell me if I'm doing it wrong.  I stay at your disposal if you need any further information.""",Bug,JavaScript
505857,"""Our set up is as follows:  2 shards consisting of 3 machines each (1 primary, 1 secondary, 1 arbiter). Each shard has about 35GB of data, running on 1.8.1.  We lost a secondary today, so are trying to resync a new secondary from scratch. Two things have happened at least twice in this process:  1. The primary segfaults; we had this happen while secondary was mid-sync, but also happen when the secondary was shut down and not communicating with the primary at all (three times). 2. The secondary, once it's finished syncing and building its indexes, complains over and over about """"DR102 too much data written uncommitted"""" (same error as SERVER-2737 but different situation as far as I can tell).  We've tried stopping all the mongods, removing the local files on the primary, starting it up and re-initializing its replica set, and then syncing again, but this led to the same results (we cleared all data off the secondary first, too).  I've attached the logs for both seg faults (one was running with verbose=false, the other =true) and a sample of the DR102 errors on the secondary.""",Bug,"Storage,Replication"
506041,"""If there's a compound index, you query and sort on the last key, then MongoDB doesn't take limit into account and does a lot of unnecessary scanning.  For example, if I have an index on \{i:1, j:1\}, I query with a range on i and sort on j with limit 10, then explain and nscanned shows that a whole range from \[i1, jmin\] to \[i2, jmax\] was scanned, even for for each i only the first 10 could be scanned and the rest could be skipped. Attached is an example script that generates the following output:  {code} MongoDB shell version: 1.9.0 connecting to: test {  """"cursor"""" : """"BtreeCursor i_1_j_1"""",  """"nscanned"""" : 3000,  """"nscannedObjects"""" : 3000,  """"n"""" : 10,  """"scanAndOrder"""" : true,  """"millis"""" : 6,  """"nYields"""" : 0,  """"nChunkSkips"""" : 0,  """"isMultiKey"""" : false,  """"indexOnly"""" : false,  """"indexBounds"""" : {   """"i"""" : [    [     2,     4    ]   ],   """"j"""" : [    [     {      """"$minElement"""" : 1     },     {      """"$maxElement"""" : 1     }    ]   ]  } } {code}  If BtreeCursor could know that it is a sorting query on a field with a particular limit, then it is maybe possible to optimize this case by advancing to the next group of keys if limit number of objects have been scanned with the same """"prefix"""" of <USER>that are before that field (I hope I say it correctly).  I real life scenario I have a collection of """"events"""" (which stores that an event of particular """"type"""" happened, where """"type"""" is a string) and use an index on \{type:1, _id:1\}. On the web-interface I need to show e.g. last 10 events of some types. When querying with a specific type, it works perfectly. But when trying to show last e.g. 10 events of several types it basically starts using BasicCursor, because it turns out to be faster.  In some places I currently workaround it by doing several queries on each type in parallel and merging them at client side. It works when exact types are known in advance, but doesn't when they are not, e.g. when trying to query using a regex like /^prefix/ (and it's hard to know which types there are, because distinct has a similar problem with excessive scanning).  Could you please improve this, or at least point me how exactly MongoDB constructs cursors for sorting and where sorting is happening, so I could tinker with it myself?""",Improvement,"Querying,Performance"
506195,"""Hi,   i'm facing a problem while trying to store geolocalised points and key / values attached to it. The problem i'm encountering is that i need to check if a given point (identified by a couple lat / long) is already existing in the database or not. Depending on the way i'm inserting my geolocalised points (either from pymongo, javascript shell or mongoimport), i get different results.  For instance, i try to store value 0.7 as a float using each of the 3 ways.  The results i get are the following :  { """"_id"""" : ObjectId(""""4da6e294aea05d721a4a87b9""""), """"value_from_mongoimport"""" : 0.7000000000000001 } { """"_id"""" : ObjectId(""""4da6e2a3e36333098e18d40c""""), """"value_from shell"""" : 0.7 } { """"_id"""" : ObjectId(""""4da6e2aa11f4ac1bc0000000""""), """"value_from_pymongo"""" : 0.7 }  I'm precisely using the """"mongoimport way"""" to store data as it offers better performances than other ways.  The problem is, at a later time, when i do, from python, somethink like :   db.testfind_one({""""mykey"""" : 0.7}), i get none ......  I understand the problem of floating point representation, but why is it only affecting mongoimport ? is this something that implies the json to bson transformation ?  Help appreciated.  Regards.   """,Question,Tools
506201,"""MongoDb does not support the dotall 's' flag from Perl Compatible Regular Expressions. This flag makes the '.' dot character match really all characters including the newline character '\n'.  Example in pseudo code :  t = """"foo\nbar\nnil"""" r = """"foo.*nil"""" r.match(t, 's') => false;  If the dotall 's' flag was supported then the regular expression would match. There is a workaround for this : Instead of using the dot character in the regex we can use the [\s\S] pattern :  r = """"foo[\s\S]*nil"""" r.match(t, 's') => true;  Since the documentation mention that MongoDB supports PCRE I consider this as a bug, none critical since a workaround exists. One could consider this as a feature request, I have no issue with that :-)  The bug is trivial to fix. See attached patch below. However the patch does not solve potential issues with other drivers such the javascript shell. Indeed Javascript does not support the 's' flag ...  Since this is my first contact with the MongoDb community I would like to take the opportunity to thank you all for the amazing work around MongoDb :-)  Cheers, CH""",Bug,Querying
506293,"""I've observed that whenever the shell inserts an integer, whether it be by using a $set or an $inc in an update, or by inserting a new document, the integer gets stored in Mongo as a double. This becomes a problem when you're using the new $bit modifier as it doesn't work with doubles. I've narrowed the problem down to the Shell rather than a deeper problem, as 1.2.4 of the Ruby driver doesn't exhibit this problem.  As a side issue, I'd suggest having the Shell displays doubles as such, e.g. with a trailing """".0"""" such as """"5.0"""" instead of just 5. This would make it more obvious to identify such problems as this one in the future. It's only through Ruby that I can actually see that the integers being inserted by the Shell are actually doubles.""",Bug,Shell
506436,"""Firstly...a s a new user... brilliant package.... thanks. (And stupidly I posted this on the Ubuntu/mongo log as well... sorry... monday morning syndrome)  Now.. I have 6 instances in a replication set, spread over 2 physical machines. All works fine. If I then take down one of the machines, I end up with 3 instances, all being secondaries. This is a basic setup with default voting rights, and no arbiter. The result of a rs.status() is below:  mycache:SECONDARY> rs.status() {  """"set"""" : """"mycache"""",  """"date"""" : ISODate(""""2011-03-04T15:49:01Z""""),  """"myState"""" : 2,  """"members"""" : [   {    """"_id"""" : 0,    """"name"""" : """"n.n.n.1:27017"""",    """"health"""" : 1,    """"state"""" : 2,    """"stateStr"""" : """"SECONDARY"""",    """"uptime"""" : 202,    """"optime"""" : {     """"t"""" : 1299250255000,     """"i"""" : 1    },    """"optimeDate"""" : ISODate(""""2011-03-04T14:50:55Z""""),    """"lastHeartbeat"""" : ISODate(""""2011-03-04T15:49:01Z"""")   },   {    """"_id"""" : 1,    """"name"""" : """"n.n.n.2:27018"""",    """"health"""" : 1,    """"state"""" : 2,    """"stateStr"""" : """"SECONDARY"""",    """"optime"""" : {     """"t"""" : 1299250255000,     """"i"""" : 1    },    """"optimeDate"""" : ISODate(""""2011-03-04T14:50:55Z""""),    """"self"""" : true   },   {    """"_id"""" : 2,    """"name"""" : """"n.n.n.3:27019"""",    """"health"""" : 1,    """"state"""" : 2,    """"stateStr"""" : """"SECONDARY"""",    """"uptime"""" : 202,    """"optime"""" : {     """"t"""" : 1299250255000,     """"i"""" : 1    },    """"optimeDate"""" : ISODate(""""2011-03-04T14:50:55Z""""),    """"lastHeartbeat"""" : ISODate(""""2011-03-04T15:49:01Z"""")   },   {    """"_id"""" : 3,    """"name"""" : """"n.n.1.1:27017"""",    """"health"""" : 0,    """"state"""" : 2,    """"stateStr"""" : """"(not reachable/healthy)"""",    """"uptime"""" : 0,    """"optime"""" : {     """"t"""" : 1299250255000,     """"i"""" : 1    },    """"optimeDate"""" : ISODate(""""2011-03-04T14:50:55Z""""),    """"lastHeartbeat"""" : ISODate(""""2011-03-04T15:46:45Z""""),    """"errmsg"""" : """"socket exception""""   },   {    """"_id"""" : 4,    """"name"""" : """"n.n.1.2:27018"""",    """"health"""" : 0,    """"state"""" : 1,    """"stateStr"""" : """"(not reachable/healthy)"""",    """"uptime"""" : 0,    """"optime"""" : {     """"t"""" : 1299250255000,     """"i"""" : 1    },    """"optimeDate"""" : ISODate(""""2011-03-04T14:50:55Z""""),    """"lastHeartbeat"""" : ISODate(""""2011-03-04T15:46:45Z""""),    """"errmsg"""" : """"socket exception""""   },   {    """"_id"""" : 5,    """"name"""" : """"n.n.1.3:27019"""",    """"health"""" : 0,    """"state"""" : 2,    """"stateStr"""" : """"(not reachable/healthy)"""",    """"uptime"""" : 0,    """"optime"""" : {     """"t"""" : 1299250255000,     """"i"""" : 1    },    """"optimeDate"""" : ISODate(""""2011-03-04T14:50:55Z""""),    """"lastHeartbeat"""" : ISODate(""""2011-03-04T15:46:45Z""""),    """"errmsg"""" : """"socket exception""""   }  ],  """"ok"""" : 1 }  1. I tried reconfig, but that needs a primary, which I don't have. 2. Tried taking an instance down, freezing the other two, and bringing the third back up.... came back as a secondary. 3. Am going to try creating a new instance, and setting up as an arbiter, to see if that can help find a primary. However, this is not a long term solution. (see 4 below) 4. If I have more than one machine taking part in a replication set, in theory, for a resilient system, each machine would need to have an arbiter, in case another machine got taken out. With an even number of machines, that gives uis an even number of arbiters, which doesn't help if they are all in play (unless I am missing something obvious.... not for teh first time ). If, however, we assign bitwise voting rights to each instance in a replicatuion set (1,2,4,8,16 .....), then any instance can be downed, or a whole machine can be downed, and a definite primary will also be voted in. This removes the need for an arbiter, and also gives the admins a chance to prioritise the servers taking part.... but I need a primary to change the config.  Thanks in advance for any help""",Bug,"Replication,Usability,Admin"
506860,"""Several of our js tests rely on spawned shells to run operations in parallel with the main test script.  If the interface for managing these spawned shells was a bit more flexible we might be able to make test failures more clear and eliminate some bad failure conditions (for example waiting forever).  Some things which would be immediately helpful: - being able to check if a shell has finished without waiting for it to finish - being able to wait for a shell to finish, but with a timeout - explicit interface to audit errors from the returned shell (via shell return code or another mechanism)  I'm sure other features would be useful too - these are just ones that come to mind immediately.  As an aside - another option would be to allow threading in shells.  This might make our tests clearer as well as easier to control (since we don't have to serialize input scripts for separate processes) and errors might be easier to detect as well.  This functionality is currently implemented for our v8 mode but not for spidermonkey.""",Improvement,Shell
506883,"""I've made a mistake by sharding a GridFS chunks collection with it's '_id'.   Then i got told that, the chunks collection should be sharded with {files_id: 1, n: 1}.  I tried to runCommand({shardcollection: """"xx.xx.chunks', key:  {files_id: 1, n: 1}}), then mongo told me something like 'the collection has already been sharded'. Cannot find a command to 'remove an existing sharding rule' so i drop the chunks collection, then rerun the command above, no errors.  Then, with in database 'admin', i runned 'db.printShardingStatus()'. The result showed that the sharding rules on """"xx.xx.chunks"""" had been modified as i wish.  But when i run my script to load data into this GridFS, the same error message from pymongo raised.   I've already found a solution, quite ugly by renaming the GridFS....""",Bug,Sharding
507034,"""Hello, and thank-you in advance.  1) I'm able to import a CSV file by using the following:  mongoimport -d test01 -c items --type csv --headerline item-file- v1.csv   - this file has 20 <USER>listed on the csv header, two of which are  Item Number and Price.  - the database 'test01' has a collection 'items' with an Index for  Item Number as the key field (no duplicates).  - requirement is to update prices wholesale by importing a concise  file of only two <USER>Item Number and Price and leaving the  remaining 18 <USER>as is.   I then try the following:  mongoimport -d test01 -c items --type csv --upsertFields """"Item Number"""" --headerline item-file-update-v1.csv   - with the index on Item Number I get the following:  -------------------  connected to: 127.0.0.1                  430220/443150   97%                          23600   7866/second  imported 24323 objects  -------------------  -> no prices have been updated in the collection  - with no index on the Item Number, I get the following:  -------------------  connected to: 127.0.0.1  imported 24323 objects  -------------------  -> the items from the csv were inserted as new items duplicating the  collection which is expected.  - I also tried this:  mongoimport -d test01 -c items -f """"Item Number"""",Price --type csv --upsertFields """"Item Number"""",Price --headerline item-file-update-v1.csv   -> same results of prices not being updated.  I'm not able to update the prices in the collection by using the  second  update csv file. I searched for other upsertFields examples  but to no avail. Help on this would be very much appreciated.   also,  2) would there be an RoR gem that leverages mongoimport for csv file uploads? """,Question,Write Ops
507040,"""I would like to ask for the following feature:  on inserts/updates I would like to have a """"placeholder"""" like """"$sysdate"""" as argument, telling the database engine to replace the """"$sysdate-placeholder"""" by the current database UTC before putting it in the internal mongodb queue.  e.g.: db.coll.insert({""""tx"""":""""test string"""",""""timestamp"""":$sysdate})  The advantage would be that one can be sure that sorting by the timestamp of the document field results in the 100% sortorder of insertion; if I insert data from different machines, I have no guarantee about the insertion sortorder in the internal queue of MongoDB for persisting ( depending on the application delay for """"getting the data and pushing it to mongodb"""" ).  Thank you for taking this into consideration,´    """,New Feature,Usability
507099,"""I'm having an issue that I can only assume is a 32-bit integer overflow issue.  I'm running on 64-bit Linux boxes with the 64-bit builds of MongoDB v1.6.3.  Here's the deal:  I have a collection used for logging and aggregate counters.  I'm using an upsert with $inc to increment these counters.  When the counters get over 2^31, it rolls over to negative.  I've been able to reproduce it with the <USER>driver:  require 'rubygems' require 'mongo'  mongo = Mongo::Connection.new """"localhost"""" db = mongo.db """"test""""  db.collection('junk').drop (0..20).each do |i|   db.collection('junk').update({:name=>'luke'}, {""""$inc""""=>{:total=>123456789}}, {:upsert=>true})   puts db.collection('junk').find({}, {:<USER>>{:_id=>0}}).to_a.inspect end  and that outputs: (I've bolded the line where it flips negative)  [{""""name""""=>""""luke"""", """"total""""=>123456789}] [{""""name""""=>""""luke"""", """"total""""=>246913578}] [{""""name""""=>""""luke"""", """"total""""=>370370367}] [{""""name""""=>""""luke"""", """"total""""=>493827156}] [{""""name""""=>""""luke"""", """"total""""=>617283945}] [{""""name""""=>""""luke"""", """"total""""=>740740734}] [{""""name""""=>""""luke"""", """"total""""=>864197523}] [{""""name""""=>""""luke"""", """"total""""=>987654312}] [{""""name""""=>""""luke"""", """"total""""=>1111111101}] [{""""name""""=>""""luke"""", """"total""""=>1234567890}] [{""""name""""=>""""luke"""", """"total""""=>1358024679}] [{""""name""""=>""""luke"""", """"total""""=>1481481468}] [{""""name""""=>""""luke"""", """"total""""=>1604938257}] [{""""name""""=>""""luke"""", """"total""""=>1728395046}] [{""""name""""=>""""luke"""", """"total""""=>1851851835}] [{""""name""""=>""""luke"""", """"total""""=>1975308624}] [{""""name""""=>""""luke"""", """"total""""=>2098765413}] [{""""name""""=>""""luke"""", """"total""""=>-2072745094}] [{""""name""""=>""""luke"""", """"total""""=>-1949288305}] [{""""name""""=>""""luke"""", """"total""""=>-1825831516}] [{""""name""""=>""""luke"""", """"total""""=>-1702374727}]   I suspect that initially MongoDB is storing the integer using the 4-byte integer data type, but when my upsert increments it, it isn't converted to the 8-byte integer.  Just for fun, I tried upserting a large number and it indeed is stored just fine (I suspect it is created as an 8-byte integer):  db.collection('junk').drop db.collection('junk').update({:name=>'luke'}, {""""$inc""""=>{:total=>9999999999999}}, {:upsert=>true}) puts db.collection('junk').find({}, {:<USER>>{:_id=>0}}).to_a.inspect  and that outputs (which is clearly an 8-byte integer):  [{""""name""""=>""""luke"""", """"total""""=>9999999999999}]""",Bug,Write Ops
507112,"""while trying to split a chunk manually, the server responded with an assertion error and data retrieval was not possible.  > db.runCommand({split:""""storage.object_interaction"""", middle:{_id:877070867}})   {          """"assertion"""" : """"saving chunks failed.  cmd: { applyOps: [ { op:  \""""u\"""", b: true, ns: \""""config.chunks\  """", o: { _id: \""""storage.object_interaction-_id_832352546\"""", lastmod:  Timestamp 179000|4, ns: \""""storage.objec  t_interaction\"""", min: { _id: 832352546 }, max: { _id: 832352546.0 },  shard: \""""shard0001\"""" }, o2: { _id: \""""s  torage.object_interaction-_id_832352546\"""" } }, { op: \""""u\"""", b: true,  ns: \""""config.chunks\"""", o: { _id: \""""sto  rage.object_interaction-_id_832352546.0\"""", lastmod: Timestamp 179000|  5, ns: \""""storage.object_interaction\"""",   min: { _id: 832352546.0 }, max: { _id: 832352547.0 }, shard:  \""""shard0001\"""" }, o2: { _id: \""""storage.object_  interaction-_id_832352546.0\"""" } }, { op: \""""u\"""", b: true, ns:  \""""config.chunks\"""", o: { _id: \""""storage.object_  interaction-_id_877070867\"""", lastmod: Timestamp 179000|6, ns:  \""""storage.object_interaction\"""", min: { _id: 8  77070867 }, max: { _id: 877070867.0 }, shard: \""""shard0001\"""" }, o2:  { _id: \""""storage.object_interaction-_id_  877070867\"""" } }, { op: \""""u\"""", b: true, ns: \""""config.chunks\"""", o:  { _id: \""""storage.object_interaction-_id_877070867.0\"""", lastmod:  Timestamp 179000|7, ns: \""""storage.object_interaction\"""", min: { _id:  877070867.0 }, max: { _id: 877217527 }, shard: \""""shard0001\"""" }, o2:  { _id: \""""storage.object_interaction-_id_877070867.0\"""" } } ],  preCondition: [ { ns: \""""config.chunks\"""", q: { query: { ns:  \""""storage.object_interaction\"""" }, orderby: { lastmod: -1 } }, res:  { lastmod: Timestamp 179000|3 } } ] } result: { got: { _id:  \""""storage.object_interaction-_id_832352546\"""", lastmod: Timestamp  179000|4, ns: \""""storage.object_interaction\"""", min: { _id: 832352546 },  max: { _id: 832352546.0 }, shard: \""""shard0001\"""" }, whatFailed: { ns:  \""""config.chunks\"""", q: { query: { ns: \""""storage.object_interaction\"""" },  orderby: { lastmod: -1 } }, res: { lastmod: Timestamp 179000|3 } },  errmsg: \""""pre-condition failed\"""", ok: 0.0 }"""",          """"assertionCode"""" : 13327,          """"errmsg"""" : """"db assertion failure"""",          """"ok"""" : 0  }   > db.runCommand({split:""""storage.object_interaction"""", find:{_id:854711706}})  {          """"assertion"""" : """"Couldn't load a valid config for  storage.object_interaction after 3 tries. Giving up"""",          """"assertionCode"""" : 13282,          """"errmsg"""" : """"db assertion failure"""",          """"ok"""" : 0  }   > db.object_interaction.findOne({_id:854711706})  Fri Oct 22 00:04:24 uncaught exception: error { """"$err"""" : """"assertion s/chunk.cpp:768"""", """"code"""" : 0 }   I'm attaching the cunklist (db.printShardingStatus as well as a mongos log file that contain the events)  What can i do in order to split the chunk? right now the index is not even on 2 of the shard servers.""",Bug,Sharding
507242,"""I've worked at places that tend to keep their hardware around pretty long. Also, many windows shops still run 32 bit OSes on their 64 bit hardware, I don't see the 32 bit problem disappearing as fast as10gen assumed.  Also netbook atom processors are still 32 bit. Mongo has potential for a datastore for desktop apps as an alternative to MS Access and SQLite. Finally, most smart phones have 32 bit CPUs, and there is potential for using mongo on some smart phones.""",Improvement,"Performance,Usability"
507438,"""The file debian/changelog, both in head and 1.5.1 has a malformed line.   The packages will probably not build on any Debian system, but I've only tested Ubuntu Lucid and Hardy.  Fix is simple, change the line: -- <USER><<EMAIL>>  Wed, 22 3 May 2010 16:56:28 -0500 To: -- <USER><<EMAIL>>  Wed, 3 May 2010 16:56:28 -0500  Here's the log: $ debuild parsechangelog/debian: warning:     debian/changelog(l5): badly formatted trailer line LINE:  -- <USER><<EMAIL>>  Wed, 22 3 May 2010 16:56:28 -0500 parsechangelog/debian: warning:     debian/changelog(l7): found start of entry where expected more change data or trailer LINE: mongodb (1.5.0) unstable; urgency=low parsechangelog/debian: warning:     debian/changelog(l7): found eof where expected more change data or trailer  dpkg-buildpackage -rfakeroot -D -us -uc  """,Bug,Build
507575,"""While performing simultaneous activities I was able to put a mongod 1.5.6 instance into a supposed deadlock. The server still accepts connections but will not perform further queries or mapreduces. I've included a currentOp output and a portion of stdout and stderr from mongod. Samples of the types of mapreduces and queries used can be gleaned from this output. The server was under heavy use with many processes/hosts concurrently querying and inserting and performing mapreduces.   The mongod and the system is idle. I've tried interrupting the server with a response of """"got kill or ctrl c or hup signal 2 (Interrupt), will terminate after current cmd ends"""". I've issued db.killOp requests which do not take effect.""",Bug,Querying
507844,"""I want to add two more options to mongod.exe that would be used with --install and --reinstall. They would be for configuring mongod to run as a windows service with reduced permissions. Each would take an argument. Both would be required.  There would be two exceptions in which you could only specify the username and no password. Thous would be setting the service to run as Local Service (default, higher than administrator level services run at) or """"Network Service""""   Also, in keeping with the Services MMC snapin behavior, if the user lacks the """"login as a service"""" permission it will be granted it.""",Improvement,"Tools,Usability,Admin"
507948,"""If you pass in an object with .constructor set to <USER>as argument to tojson(), the tojson call fails: v1.3.3 with spidermonkey: > var a = new Object() > a.constructor = <USER><USER>> tojson(a) Fri May 14 14:46:28 JS Error: TypeError: x.constructor has no properties (anon):393  error emssage on 1.4.2 with v8: > tojson(a) Fri May 14 14:51:14 exec error: shell setup:397 TypeError: Cannot read property 'tojson' of <USER>if ( typeof( x.constructor.tojson ) == """"function"""" && x.constructor.tojson != t                           ^ I'm not sure whether have a <USER>value for constructor has sane uses, but there are at least some situations where mongoDB produces it (as 'this' of the map funciton in the mapreduce when using v8, but I""""ll open another bug for that). If it doesn't make sense to have a <USER>value there, then please close this bug.  """,Bug,Shell
508125,"""I occasionally get an assertion error when running mongos with sharding. I've got a mini-cluster with 3 nodes running, where each node has a mongod instance and a mongos instance running. One of the machines also has an additional mongod instance running, acting as the config server. The setup is basically the same as described on the """"Sample Configuration Session"""" wiki page (i.e. i have a """"test.people"""" collection, which uses the """"name"""" as the shard key). I've inserted about a million rows in the collection, resulting in 123 shard chunks.   To load test, I've set up each of the nodes to run a bunch of simple queries against their """"localhost"""" mongos instances. The queries are of the form {name:{$gte:""""foo"""",$lt:""""bar""""}}. Things work great about 90% of the time, but sometimes I get this error in the mongos logs:  Wed Sep 23 18:46:41 connection accepted from 10.244.254.3:34215 #6 Wed Sep 23 18:46:41 Request::process ns: test.people msg id:1 attempt: 0 Extra: {} Wed Sep 23 18:46:41 Request::process ns: test.people msg id:2 attempt: 0 Wed Sep 23 18:46:41 Request::process ns: test.people msg id:3 attempt: 0 ******************** ERROR: MessagingPort::call() wrong id got:2225548905 expect:2242326122   old:10922   response msgid:677081682   response len:  1061978 Wed Sep 23 18:46:41  Assertion failure false util/message.cpp 366 0x44f856 0x455f78 0x454267 0x45b7ec 0x45f116 0x45f2c5 0x4a78d3 0x4a50d3 0x48ce34 0x4a34e9 0x4ae75e 0x484bf6 0x4d2297 0x4d1e9f 0x2aaaaacd33ba 0x2aaaab778fcd   /opt/mongodb/bin/mongos [0x44f856]  /opt/mongodb/bin/mongos [0x455f78]  /opt/mongodb/bin/mongos [0x454267]  /opt/mongodb/bin/mongos [0x45b7ec]  /opt/mongodb/bin/mongos [0x45f116]  /opt/mongodb/bin/mongos [0x45f2c5]  /opt/mongodb/bin/mongos [0x4a78d3]  /opt/mongodb/bin/mongos [0x4a50d3]  /opt/mongodb/bin/mongos [0x48ce34]  /opt/mongodb/bin/mongos [0x4a34e9]  /opt/mongodb/bin/mongos [0x4ae75e]  /opt/mongodb/bin/mongos(_ZN5mongo3pms9threadRunEv+0x86) [0x484bf6]  /opt/mongodb/bin/mongos [0x4d2297]  /opt/mongodb/bin/mongos [0x4d1e9f]  /lib/libpthread.so.0 [0x2aaaaacd33ba]  /lib/libc.so.6(clone+0x6d) [0x2aaaab778fcd]  On the client side, the error shows up as:  java.lang.RuntimeException: db error []  at com.mongodb.DBApiLayer$MyCollection.find(DBApiLayer.java:407)  at com.mongodb.DBCursor._check(DBCursor.java:237)  at com.mongodb.DBCursor._hasNext(DBCursor.java:367)  at com.mongodb.DBCursor.hasNext(DBCursor.java:392)  Any ideas? """,Bug,Sharding
508128,"""I've got an 8-machine mongodb cluster set up, with 2 mongodb servers running on each machine (so 16 mongod processes total, plus a config server running on a 9th machine). I've enabled sharding on a collection, using the """"time"""" field as a key (I initially tried sharding on the _id field, but when I did that the chunks would never split i.e. no matter how much data I inserted, there was only ever one entry in config.chunks).  When I start inserting data into this collection, I can see in config.chunks that additional chunks are being created on different machines throughout the cluster, as expected. But after a while, it stops creating chunks on new shards and just keeps creating chunks on the same shard over and over. From looking at the logs it seems like its trying to move chunks and failing, but I'm inserting things in order (i.e. the """"time"""" field is always the current time), so I'm not sure why it's even trying to move chunks (shouldn't it just create a new chunk and start inserting things there?).  Here's what I'm seeing in the mongos log:   Fri Oct  9 18:13:28 autosplitting test.foo size: 62662123 shard: shard  ns:test.foo shard: mongod1:10001 min: { time: MinKey } max: { time: MaxKey } ... Fri Oct  9 18:13:28 moving chunk (auto): shard  ns:test.foo shard: mongod1:10001 min: { time: MinKey } max: { time: new Date(1255112014526) } to: mongod2.com:10000 #objcets: 0 Fri Oct  9 18:13:28 moving chunk ns: test.foo moving chunk: shard  ns:test.foo shard: mongod1:10001 min: { time: MinKey } max: { time: new Date(1255112014526) } mongod1:10001 -> mongod2:10000   And after a while:   Fri Oct  9 18:28:32 Assertion: moveAndCommit failed: movechunk.start failed: { errmsg: """""""", ok: 0.0 } Fri Oct  9 18:28:32 UserException: moveAndCommit failed: movechunk.start failed: { errmsg: """""""", ok: 0.0 }""",Bug,Sharding
508203,"""See emails below for an example:  I don't think the ObjectId representation in the server's embedded interpreter has a method to extract the timestamp currently. Could open a separate JIRA for that as well. - Hide quoted text -  On Wed, Mar 31, 2010 at 3:19 PM, <USER><<EMAIL>> wrote: > Yes thank you. That makes perfect sense now. What about grouping by time > stamp is that possible? > > On 31 March 2010 20:04, <USER><<EMAIL>> wrote: >> >> You would do it the way you did in your example: >> >> {""""_id"""": {""""$gt"""": dummy_id}} >> >> Where dummy_id has been manually created from a timestamp or created >> using a helper if we add one. Make more sense? >> >> On Wed, Mar 31, 2010 at 2:58 PM, <USER><<EMAIL>> >> wrote: >> > sorry, I don't understand you. If I could create a dummy_id (or you add >> > the >> > function you mentioned) how would I do the query? >> > >> > On 31 March 2010 19:13, <USER><<EMAIL>> wrote: >> >> >> >> Right now you'd need to manually create a dummy _id value w/ the right >> >> timestamp and use that to sort on. We could probably add an >> >> ObjectId.from_datetime() method to make that a bit nicer. Mind filing >> >> a JIRA for this? >> >> >> >> On Wed, Mar 31, 2010 at 2:04 PM, <USER><<EMAIL>> >> >> wrote: >> >> > Hi >> >> > >> >> > I'm currently developing an application on top of mongo. At the >> >> > moment >> >> > I have a time stamp field in my documents which allows me to query >> >> > documents created in the last 24 hours and do a group by where I >> >> > group >> >> > into 5 minute buckets. I've just been reading that the _id field >> >> > contains a time stamp and thought I could just use this and drop my >> >> > extra field. Is this possible? How to I refer to the time stamp part >> >> > of _id in my query? >> >> > >> >> > Here is some example python of the kind of thing I'm doing: >> >> > >> >> > # just a function to get me the time as seconds since epoc >> >> > def ts(dt): >> >> >  return int(mktime(dt.timetuple())) >> >> > >> >> > # here is an insert >> >> > db.box.insert(dict(ts=ts(datetime.now()),  x=randint(0, 500), >> >> > y=randint(0, 100))) >> >> > >> >> > # here is a query >> >> > db.box.group(key       = 'function(obj){ return { ts : 300 * >> >> > parseInt(obj.ts/300) }; }', >> >> >                      condition = dict(ts={'$gt': ts(datetime.now() - >> >> > timedelta(hours=24)) }), >> >> >                      initial   =  dict( xmax=0, ysum=0, count=0 ), >> >> >                      reduce    = 'function(obj,prev) { prev.xmax = >> >> > Math.max(prev.xmax, obj.x); prev.ysum += obj.y; prev.count++; }', >> >> >                      finalize  = 'function(out){ out.yavg = >> >> > out.ysum / out.count }') >> >> > >> >> > Ideally I'd replace it with something like: >> >> > >> >> > # here is an insert - look no ts >> >> > db.box.insert(dict(x=randint(0, 500), y=randint(0, 100))) >> >> > >> >> > # here is a query >> >> > db.box.group(key       = 'function(obj){ return { ts : 300 * >> >> > parseInt(obj._id.timestamp/300) }; }', >> >> >                      condition = {'_id.timestamp' : {'$gt': >> >> > ts(datetime.now() - timedelta(hours=24)) }}, >> >> >                      initial   =  dict( xmax=0, ysum=0, count=0 ), >> >> >                      reduce    = 'function(obj,prev) { prev.xmax = >> >> > Math.max(prev.xmax, obj.x); prev.ysum += obj.y; prev.count++; }', >> >> >                      finalize  = 'function(out){ out.yavg = >> >> > out.ysum / out.count }') >> >> > >> >> > Is this kind of thing possible? what property/method do I need to use >> >> > on _id? >> >> > >> >> > Thanks >> >> > <USER>>> >> >""",Improvement,Querying
508373,"""When given a csv file (att: recordshelf-genres.csv) and imported via 'mongoimport -d recordshelf -c genres --file recordshelf-genres.csv --drop -f name --type csv --headerline --ignoreBlanks' the imported collection looks (is) incorrect possible due to whitespace issues in the column value 'Drum and Bass'. See next snippet from mongo shell showing the corrupted data.  db.genres.find() { """"_id"""" : ObjectId(""""4b886702982b57076c82e282""""), """"name"""" : """"Drum and Bass"""" } { """"_id"""" : ObjectId(""""4b886702982b57076c82e283""""), """"name"""" : """"Rap"""", """"field1"""" : """"and Bass\"""""""" } { """"_id"""" : ObjectId(""""4b886702982b57076c82e284""""), """"name"""" : """"House"""", """"field1"""" : """"d Bass\"""""""" } { """"_id"""" : ObjectId(""""4b886702982b57076c82e285""""), """"name"""" : """"Reggae"""", """"field1"""" : """"Bass\"""""""" } { """"_id"""" : ObjectId(""""4b886702982b57076c82e286""""), """"name"""" : """"Dubstep"""", """"field1"""" : """"Bass\"""""""" }  When column value 'Drum and Bass' get replaced (att: recordshelf-genres-jazzed.csv) with e.g. Jazz the csv mongoimport (same call as above with modified import csv) works as expected as shown in the next mongo shell excerpt.  db.genres.find() { """"_id"""" : ObjectId(""""4b886860982b57076c82e287""""), """"name"""" : """"Jazz"""" } { """"_id"""" : ObjectId(""""4b886860982b57076c82e288""""), """"name"""" : """"Rap"""" } { """"_id"""" : ObjectId(""""4b886860982b57076c82e289""""), """"name"""" : """"House"""" } { """"_id"""" : ObjectId(""""4b886860982b57076c82e28a""""), """"name"""" : """"Reggae"""" } { """"_id"""" : ObjectId(""""4b886860982b57076c82e28b""""), """"name"""" : """"Dubstep"""" }  Looks like a bug to me, but maybe I'm just to tired for seeing a trivial solution for this issue or I'm issuing mongoimport in a false way. If that's the case sorry for opening this issue ;D""",Bug,Tools
508390,"""simple cases should get special case for speed i'm thinking  single index key  or single index key + sort as a baseline i special cased _id in runQuery, so a good comparison is testing _id lookup vs non _id lookup and seeing query optimizer overhead """,Improvement,"Querying,Performance"
508638,"""It seems that the $exists operator does not work on keys which have been indexed. If a key has an index, the $exists operator will never return any documents. I've attached some tests in <USER>which demonstrate this, along with their output, and an example mongo console session showing that it's not just an issue with the <USER>driver. """,Bug,Querying
